# hbase-2.2.7分布式搭建文档

## 1、上传解压配置环境变量

```shell
# 1、解压
tar -xvf hbase-2.2.7-bin.tar.gz.gz

# 2、配置环境变量
vim /etc/profile

# 3、在最后增加配置
export HBASE_HOME=/usr/local/soft/hbase-2.2.7
export PATH=$PATH:$HBASE_HOME/bin

# 4、使环境变量剩下
source /etc/profile
```

## 2、修改配置文件

```shell
#1、修改hbase-site.xml文件
vim hbase-site.xml

# 增加以下配置
<!--指定 zookeeper 服务器 -->
<property>
<name>hbase.zookeeper.quorum</name>
<value>master,node1,node2</value>
</property>

<!--指定 hbase 根路径 -->
<property>
<name>hbase.rootdir</name>
<value>hdfs://master:9000/hbase</value>
</property>

<!--将 hbase 设置为分布式部署。 -->
<property>
<name>hbase.cluster.distributed</name>
<value>true</value>
</property>

<!-- 避免出现启动错误。 -->
<property>
<name>hbase.unsafe.stream.capability.enforce</name>
<value>false</value>
</property>

#2、修改hbase-env.sh文件
vim hbase-env.sh

# 增加配置
export HBASE_MANAGES_ZK=false
export JAVA_HOME=/usr/local/soft/jdk1.8.0_171


#3、修改regionservers文件
vim regionservers
# 增加配
node1
node2
```

### 同步到所有节点（如果是伪分布式不需要同步）

```
scp -r hbase-1.4.6 node1:`pwd`
scp -r hbase-1.4.6 node2:`pwd`
```



## 3、启动Hbase集群

```shell
hbase启动顺序：
zk-->hadoop-->hbase


hbase关闭顺序：
hbase-->hadoop-->zk


# 启动
start-hbase.sh
# hbase web ui
http://master:16010

# 关闭hbase启动
#stop-hbase.sh
```

## 解决 log4j 兼容性问题

> 警告信息，不影响使用
> LF4J: Class path contains multiple SLF4J bindings.

````shell
cd /usr/local/soft/hbase-2.2.7/lib/client-facing-thirdparty

# HBase 与 Hadoop 在运行时会出现 log4j 兼容性问题，这是因为 HBase 的 log4j 版本与 Hadoop 的产生了冲突，我们这里将 HBase 的 log4j 设置为备份。
mv slf4j-log4j12-1.7.25.jar slf4j-log4j12-1.7.25.jar.bak
````





# HBase架构与基础命令

## 一、了解HBase

官方文档：https://hbase.apache.org/

![image-20220608204643002](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608204643002.png)

### 1.1	HBase概述

> **HBase 是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，用于存储海量的结构化或者半结构化，非结构化的数据（底层是字节数组做存储的）**
>
> HBase是Hadoop的生态系统之一，是建立在Hadoop文件系统（HDFS）之上的分布式、面向列的数据库，通过利用Hadoop的文件系统提供容错能力。如果需要进行实时读写或者随机访问大规模的数据集的时候，会考虑使用HBase。
>
> HBase作为Google Bigtable的开源实现，Google Bigtable利用GFS作为其文件存储系统类似，则HBase利用Hadoop HDFS作为其文件存储系统；Google通过运行MapReduce来处理Bigtable中的海量数据，同样，HBase利用Hadoop MapReduce来处理HBase中的海量数据；Google Bigtable利用Chubby作为协同服务，HBase利用Zookeeper作为元数据的元数据存储和容灾。在2010年5月，成为apache顶级项目

![image-20220608204054001](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608204054001.png)

### 1.2	HBase处理数据

> 虽然Hadoop是一个高容错、高延时的分布式文件系统和高并发的批处理系统，但是它不适用于提供实时计算；
>
> HBase是可以提供实时计算的分布式数据库，数据被保存在HDFS分布式文件系统上，由HDFS保证期高容错性;
>
> 但是再生产环境中，HBase是如何基于hadoop提供实时性呢？ 
>
> ​		HBase上的数据是以StoreFile(HFile)二进制流的形式存储在HDFS上block块儿中；
>
> ​		但是HDFS并不知道的HBase用于存储什么，它只把存储文件认为是二进制文件，也就是说，HBase的存储数据对于HDFS文件系统是透明的。

### 1.3	HBase与HDFS

> 在下面的表格中，我们对HDFS与HBase进行比较：	

| HDFS                                      | HBase                                                        |
| ----------------------------------------- | ------------------------------------------------------------ |
| HDFS适于存储大容量文件的分布式文件系统。  | HBase是建立在HDFS之上的数据库。                              |
| HDFS不支持快速单独记录查找。              | HBase提供在较大的表快速查找                                  |
| HDFS提供了高延迟批量处理;没有批处理概念。 | HBase提供了数十亿条记录低延迟访问单个行记录（随机存取）。    |
| HDFS提供的数据只能顺序访问。              | HBase内部使用哈希表和提供随机接入，并且其存储索引，可将在HDFS文件中的数据进行快速查找。 |

Hbase--->HashMap

## 二、HBase相关概念

### 2.1	分布式数据库

> 1、画图理解分布式是什么样子（region）



### 2.2	列式存储

> 2、画图理解列式存储  拿与mysql（必须项：表+列）中的表做对比(必须项：表+列簇)



### 2.3	稀疏性

> 3、画图理解稀疏（rowkey）
>
> HBase中需要根据行键、列族、列限定符和时间戳来确定一个单元格，因此，可以视为一个“**四维坐标**”，即**[行键, 列族, 列限定符, 时间戳]**



### 2.4	数据模型

> HBase通过表格的模式存储数据，每个表格由列和行组成，其中，每个列又被划分为若干个列族（colnum family），请参考下面的图：

![image-20220608214455582](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608214455582.png)

>           **表：**HBase的数据同样是用表来组织的，表由行和列组成，列分为若干个列族，行和列的坐标交叉决定了一个单元格。
>
>        **行：**每个表由若干行组成，**每个行有一个行键作为这一行的唯一标识**。访问表中的行只有三种方式：通过单个行键进行查询、通过一个行键的区间来访问、全表扫描。
>
>        **列簇：**一个HBase表被分组成许多“列族”的集合，它是基本的访问控制单元。
>
>        **列修饰符（列限定符）：**列族里的数据通过列限定符（或列）来定位
>
>        **单元格：**在HBase表中，通过行、列族和列限定符确定一个“单元格”（cell），单元格中存储的数据没有数据类型，**总被视为字节数组byte[]**
>
>        **时间戳：**每个单元格都保存着同一份数据的多个版本，这些版本采用时间戳进行索引

#### 2.4.1	Hbase数据模型

> HBase将数据存放在带有标签的**表**中，表由**行和列**组成，行和列交叉确定一个**单元格**，单元格有**版本号**，版本号自动分配，为数据插入该单元格时的**时间戳**。单元格的内容没有数据类型，**所有数据都被视为未解释的字节数组**。
>
>   表格中每一行有一个**行键**（也是字节数组，任何形式的数据都可以表示成字符串，比如数据结构进行序列化之后），**整个表根据行键的字节序来排序**，所有对表的访问必须通过行键。
>
>   表中的列又划分为多个**列族**（column family），同一个列族的所有成员具有相同的前缀，具体的列由列修饰符标识，因此，**列族和列修饰符**合起来才可以表示某一列，比如：info:format、cotents:image

![image-20220608215452989](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608215452989.png)

> 在创建一个表的时候，列族必须作为模式定义的一部分预先给出，而**列族是支持动态扩展的**，也就是列族成员可以随后按需加入。物理上，所有的列族成员一起存放在文件系统上，所以实际上说HBase是面向列的数据库，更准确的应该是**面向列族**，调优和存储都是在列族这个层次上进行的。一般情况下，同一个列族的成员最后具有相同的访问模式和大小特征。
>
>   总结起来，HBase表和我们熟知的RDBMS的表很像，不同之处在于：**行按行键排序，列划分为列族，单元格有版本号，没有数据类型。**

#### 2.4.2	Hbase数据坐标

> HBase中需要根据行键、列族、列限定符和时间戳来确定一个**单元格(cell)**，cell中的数据是没有类型的，全部是**字节码**形式存贮。，因此，可以视为一个“**四维坐标**”，即**[行键, 列族, 列限定符, 时间戳]**。

![image-20220608215545217](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608215545217.png)

> 对于上图这样一个HBase表，其数据坐标举例如下：

| 键                                            | 值            |
| --------------------------------------------- | ------------- |
| [“201505003”, “Info”, “email”, 1174184619081] | “xie@qq.com”  |
| [“201505003”, “Info”, “email”, 1174184620720] | “you@163.com” |

#### 2.4.3	HBase区域

> HBase自动把表水平划分为**区域**（Region），每个区域都是有若干连续行构成的，一个区域由**所属的表、起始行、终止行（不包括这行）**三个要素来表示。
>
>   一开始，一个表只有一个区域，但是随着数据的增加，区域逐渐变大，等到它超出设定的阈值（128M）大小，就会在某行的边界上进行拆分，分成两个大小**基本相同**的区域。然后随着数据的再增加，区域就不断的增加，如果超出了单台服务器的容量，就可以把一些区域放到其他节点上去，构成一个集群。也就是说：**集群中的每个节点（Region Server）管理整个表的若干个区域**。所以，我们说：**区域是HBase集群上分布数据的最小单位**。

![image-20220608215649244](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608215649244.png)

## 三、HBase系统架构

### 3.1	架构图

![image-20220608221621833](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608221621833.png)

### 3.2	组件介绍

HBase由三种类型的服务器以主从模式构成：

- Region Server：负责数据的读写服务，用户通过与Region server交互来实现对数据的访问。
- HBase HMaster：负责Region的分配及数据库的创建和删除等操作。
- ZooKeeper：负责维护集群的状态（某台服务器是否在线，服务器之间数据的同步操作及master的选举等）。

HDFS的DataNode负责存储所有Region Server所管理的数据，即HBase中的所有数据都是以HDFS文件的形式存储的。出于使Region server所管理的数据更加本地化的考虑，Region server是根据DataNode分布的。HBase的数据在写入的时候都存储在本地。但当某一个region被移除或被重新分配的时候，就可能产生数据不在本地的情况。这种情况只有在所谓的compaction之后才能解决。

#### Client

> 包含访问HBase的接口并维护cache来加快对HBase的访问

#### Zookeeper

> 保证任何时候，集群中只有一个master
>
> 存贮所有Region的寻址入口。
>
> 实时监控Region server的上线和下线信息。并实时通知Master
>
> 存储HBase的schema和table元数据的meta信息

#### Master

> **为Region server分配region**
>
> 负责Region server的负载均衡
>
> 发现失效的Region server并重新分配其上的region
>
> 管理用户对table的增删改操作

#### RegionServer

> Region server维护region，处理对这些region的IO请求
>
> Region server负责切分在运行过程中变得过大的region　

####  HLog(WAL log)：

> HLog文件就是一个普通的Hadoop Sequence File，Sequence File 的Key是 HLogKey对象，HLogKey中记录了写入数据的归属信息，除了table和 region名字外，同时还包括sequence number和timestamp，timestamp是” 写入时间”，sequence number的起始值为0，或者是最近一次存入文件系 统sequence number。
>
> HLog SequeceFile的Value是HBase的KeyValue对象，即对应HFile中的 KeyValue

#### Region

> HBase自动把表水平划分成多个区域(region)，每个region会保存一个表里面某段连续的数据；每个表一开始只有一个region，随着数据不断插 入表，region不断增大，当增大到一个阀值的时候，region就会等分会两个新的region（裂变）；
>
> 当table中的行不断增多，就会有越来越多的region。这样一张完整的表被保存在多个Regionserver上。

#### Memstore 与 storefile

> 1. 一个region由多个store组成，一个store对应一个CF（列簇）
>
> 2. store包括位于内存中的memstore和位于磁盘的storefile写操作先写入 memstore，当memstore中的数据达到某个阈值，hregionserver会启动 flashcache进程写入storefile，每次写入形成单独的一个storefile
>
> 3. 当storefile文件的数量增长到一定阈值后，系统会进行合并（minor、 major compaction），在合并过程中会进行版本合并和删除工作 （majar），形成更大的storefile。
>
> 4. 当一个region所有storefile的大小和超过一定阈值后，会把当前的region 分割为两个，并由hmaster分配到相应的regionserver服务器，实现负载均衡。
>
> 5. 客户端检索数据，先在memstore找，找不到再找storefile
>
> 6. HRegion是HBase中分布式存储和负载均衡的最小单元。最小单元就表 示不同的HRegion可以分布在不同的HRegion server上。
>
> 7. HRegion由一个或者多个Store组成，每个store保存一个columns family。
>
> 8. 每个Strore又由一个memStore和0至多个StoreFile组成。
>
> 如图：StoreFile 以HFile格式保存在HDFS上。

![image-20220608221644247](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608221644247.png)

![image-20220608221654847](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608221654847.png)

### 3.3	理解难点

```
　　1、flush刷新在HDFS上呈现究竟是怎么刷新的呢？？
　　　　我们目前刚刚学习的时候，添加数据，都是一条一条的put进去，而我们在put的数据比较少（小于128M）的时候，我们put完去HDFS上并未查看到我们put的文件，这是因为数据还在内存中，也就是还在memStore中，所以要想在HDFS中查看到，我们必须手动刷新到磁盘中，这是将memStore的数据刷新到StoreFile中去，这样我们在HDFS中就可以查看到了。　　

　　2、为什么Hbase不可以使用像Mysql那样进行查询？？
　　　　首先，我们应该可以感受到，我们在插入的时候，每行数据，有多少列，列名叫什么完全是我们自己定义的，之所以不支持像MySql那样对列进行查询和操作，因为不确定列的个数和名称。

　　3、数据最后存在HDFS上的，HDFS不支持删改，为什么Hbase就可以呢？？
　　　　这里有个思想误区，的确，数据是以HFile形式存在HDFS上的，而且HDFS的确是不支持删改的，但是为什么Hbase就支持呢？首先，这里的删除并不是真正意义上的对数据进行删除，而是对数据进行打上标记，我们再去查的时，就不会查到这个打过标记的数据，这个数据Hmaster会每隔1小时清理。修改是put两次，Hbase会取最新的数据，过期数据也是这个方式被清理。
```

## 四、HBase 2.2.7安装搭建

### 4.1 hbase下载

官网下载地址：https://www.apache.org/dyn/closer.lua/hbase/1.4.6/hbase-1.4.6-bin.tar.gz

![image-20220907193935229](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220907193935229.png)

### 4.2 前期准备（Hadoop,zookeeper,jdk）

> 启动hadoop

```shell
start-all.sh
```

> 验证

```shell
http://master:50070
```

> 启动zookeeper（三台分别启动）

```shell
zkServer.sh start
```

> 检查状态

```
zkServer.sh status
```

### 4.3	搭建Hbase

#### 1、上传解压

```shell
tar -zxvf hbase-1.4.6-bin.tar.gz
```

#### 2、配置环境变量

```shell
export HBASE_HOME=/usr/local/soft/hbase-1.4.6

$HBASE_HOME/bin
```

> source /etc/profile

#### 3、修改hbase-env.sh文件

> 增加java配置

```shell
export JAVA_HOME=/usr/local/soft/jdk1.8.0_171
```

> 关闭默认zk配置（原本是注释的，放开修改false）

```shell
export HBASE_MANAGES_ZK=false
```

#### 4、修改hbase-site.xml文件

```xml
# 增加以下配置
<!--指定 zookeeper 服务器 -->
<property>
<name>hbase.zookeeper.quorum</name>
<value>master,node1,node2</value>
</property>

<!--指定 hbase 根路径 -->
<property>
<name>hbase.rootdir</name>
<value>hdfs://master:9000/hbase</value>
</property>

<!--将 hbase 设置为分布式部署。 -->
<property>
<name>hbase.cluster.distributed</name>
<value>true</value>
</property>

<!-- 避免出现启动错误。 -->
<property>
<name>hbase.unsafe.stream.capability.enforce</name>
<value>false</value>
</property> 
```

#### 5、修改regionservers文件

> 如果是伪分布式版本，增加master即可

```xml
node1
node2
```

#### 6、同步到所有节点（如果是伪分布式不需要同步）

```shell
scp -r hbase-1.4.6 node1:`pwd`
scp -r hbase-1.4.6 node2:`pwd`
```

#### 7、启动hbase集群 ， 在master上执行

```shell
start-hbase.sh
```

![image-20220608224430921](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608224430921.png)

#### 8、验证hbase

```
http://master:16010
```

![image-20220608224544266](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608224544266.png)

> hbase日志文件所在的目录: /usr/local/soft/hbase-1.7.1/logs

#### 9、关闭集群的命令

```shell
stop-hbase.sh
```

### 4.4	启动顺序

```shell
启动顺序
Hadoop及hbase集群启动顺序 zookeepeer -> hadoop -> hbase

停止顺序
Hadoop及hbase集群关闭顺序 hbase -> hadoop -> zookeepeer
```

### 4.5	重置hbase

##### 1、关闭hbase集群

```shell
   1)杀死进程
   
   2)stop-hbase.sh
```

##### 2、删除数据   hdfs

 ```shell
hdfs dfs -rmr /hbase
 ```

##### 3、删除元数据 zk

```shell
   zkCli.sh
   rmr /hbase
```

##### 4、重新启动hbase

```shell
   start-hbase.sh
```

##### 时间同步

```shell
yum install ntp -y

ntpdate -u time.windows.com
```

## 五、hbase shell

| 命名            | 描述                                                         | 语法                                                         |
| --------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| help ‘命名名’   | 查看命令的使用描述                                           | help ‘命令名’                                                |
| whoami          | 我是谁                                                       | whoami                                                       |
| version         | 返回hbase版本信息                                            | version                                                      |
| status          | 返回hbase集群的状态信息                                      | status                                                       |
| table_help      | 查看如何操作表                                               | table_help                                                   |
| **create**      | 创建表                                                       | create ‘表名’, ‘列族名1’, ‘列族名2’, ‘列族名N’               |
| **alter**       | 修改列族                                                     | 添加一个列族：alter ‘表名’, ‘列族名’ <br />删除列族：alter ‘表名’, {NAME=> ‘列族名’, METHOD=> ‘delete’} |
| describe        | 显示表相关的详细信息                                         | describe ‘表名’                                              |
| **list**        | 列出hbase中存在的所有表                                      | list                                                         |
| exists          | 测试表是否存在                                               | exists ‘表名’                                                |
| **put**         | 添加或修改的表的值                                           | put ‘表名’, ‘行键’, ‘列族名’, ‘列值’ <br />put ‘表名’, ‘行键’, ‘列族名:列名’, ‘列值’ |
| **scan**        | 通过对表的扫描来获取对用的值                                 | scan ‘表名’<br/>扫描某个列族： scan ‘表名’, {COLUMN=>‘列族名’}<br/>扫描某个列族的某个列： scan ‘表名’, {COLUMN=>‘列族名:列名’}<br/>查询同一个列族的多个列： scan ‘表名’, {COLUMNS => [ ‘列族名1:列名1’, ‘列族名1:列名2’, …]} |
| **get**         | 获取行或单元（cell）的值                                     | get ‘表名’, ‘行键’ <br />get ‘表名’, ‘行键’, ‘列族名’        |
| count           | 统计表中行的数量                                             | count ‘表名’                                                 |
| incr            | 增加指定表行或列的值                                         | incr ‘表名’, ‘行键’, ‘列族:列名’, 步长值                     |
| get_counter     | 获取计数器                                                   | get_counter ‘表名’, ‘行键’, ‘列族:列名’                      |
| **delete**      | 删除指定对象的值（可以为表，行，列对应的值，另外也可以指定时间戳的值） | 删除列族的某个列： delete ‘表名’, ‘行键’, ‘列族名:列名’      |
| deleteall       | 删除指定行的所有元素值                                       | deleteall ‘表名’, ‘行键’                                     |
| **truncate**    | 重新创建指定表                                               | truncate ‘表名’                                              |
| **enable**      | 使表有效                                                     | enable ‘表名’                                                |
| is_enabled      | 是否启用                                                     | is_enabled ‘表名’                                            |
| **disable**     | 使表无效                                                     | disable ‘表名’                                               |
| **is_disabled** | 是否无效                                                     | is_disabled ‘表名’                                           |
| **drop**        | 删除表                                                       | drop的表必须是disable的 <br />disable ‘表名’ <br />drop ‘表名’ |
| shutdown        | 关闭hbase集群（与exit不同）                                  |                                                              |
| tools           | 列出hbase所支持的工具                                        |                                                              |
| **exit**        | 退出hbase shell                                              |                                                              |

HBase Shell 是官方提供的一组命令，用于操作HBase。如果配置了HBase的**环境变量**了，就可以知己在命令行中输入hbase shell 命令进入命令行。

```shell
hbase shell
```

![image-20220608225844374](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608225844374.png)

> 在hbase中如果输入错误，按住ctrl+退格 才能删除（新版本不用）

### 5.1	help命令

> 可以通过 `help '命名名称'`来查看**命令行**的具体使用，包括命令的作用和用法。
> 通过help ‘hbase’ 命名来查看hbase shell 支持的所有命令，hbase将命令进行分组，其中ddl、dml使用较多。

![image-20220608230009607](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608230009607.png)

```shell
help 'list'
```

![image-20220608230050274](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608230050274.png)

### 5.2	general 类

#### 5.2.1	显示集群状态status

![image-20220608230310182](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608230310182.png)

#### 5.2.2	 查询数据库版本version

![image-20220608230440754](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608230440754.png)

#### 5.2.3	显示当前用户与组 whoami

![image-20220608230519490](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608230519490.png)

#### 5.2.4	查看操作表的命令table_help

![image-20220608230553346](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608230553346.png)

#### 5.2.5	退出HBase Shell exit

![image-20220608230623889](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608230623889.png)

### 5.3	DDL相关

#### 5.3.1. 创建表create

> 注意：创建表时只需要指定列族名称，不需要指定列名。

```sql
# 语法
create '表名', {NAME => '列族名1'}, {NAME => '列族名2'}, {NAME => '列族名3'}
# 此种方式是上上面的简写方式，使用上面方式可以为列族指定更多的属性，如VERSIONS、TTL、BLOCKCACHE、CONFIGURATION等属性
create '表名', '列族名1', '列族名2', '列族名3'

create '表名', {NAME => '列族名1', VERSIONS => 版本号, TTL => 过期时间, BLOCKCACHE => true}


# 示例
create 'tbl_user', 'info', 'detail'
create 't1', {NAME => 'f1', VERSIONS => 1, TTL => 2592000, BLOCKCACHE => true}，{NAME => 'f2',..}
```

![image-20220608230826425](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608230826425.png)

#### 5.3.2	修改(添加、删除)表结构Schema alter

##### 5.3.2.1	添加一个列簇

```sql
# 语法 
alter '表名', '列族名'

# 示例
alter 'tbl_user', 'address'
```

![image-20220608230938763](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608230938763.png)

##### 5.3.2.2	删除一个列簇

```sql
# 语法 
alter '表名', {NAME=> '列族名', METHOD=> 'delete'}

alter 't1',{NAME => 'cf2', METHOD => 'delete'}
# 示例
alter 'tbl_user', {NAME=> 'address', METHOD=> 'delete'}
```

![image-20220608231027899](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608231027899.png)

##### 5.3.2.3	修改列族的属性

> 可以修改列族的VERSIONS、IN_MEMORY

```sql
# 修改f1列族的版本为5
alter 't1', NAME => 'f1', VERSIONS => 5

# 修改多个列族，修改f2为内存，版本号为5
alter 't1', 'f1', {NAME => 'f2', IN_MEMORY => true}, {NAME => 'f3', VERSIONS => 5}

# 也可以修改table-scope属性，例如MAX_FILESIZE, READONLY,MEMSTORE_FLUSHSIZE, DEFERRED_LOG_FLUSH等。
# 例如，修改region的最大大小为128MB：
alter 't1', MAX_FILESIZE => '134217728'
```

#### 5.3.3	获取表的描述describe

```sql
# 语法 
describe '表名'

# 示例
describe 'tbl_user'
```

![image-20220608231308673](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608231308673.png)

#### 5.3.4	列举所有表list

![image-20220608231334575](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608231334575.png)

#### 5.3.5	表是否存在exists

```sql
# 语法 
exists '表名'

# 示例
exists 'tbl_user'
```

![image-20220608231431898](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608231431898.png)

#### 5.3.6	启用表enable和禁用表disable

> 通过enable和disable来启用/禁用这个表,相应的可以通过is_enabled和is_disabled来检查表是否被禁用。

```sql
# 语法
enable '表名'
is_enabled '表名'

disable '表名'
is_disabled '表名'

# 示例
disable 'tbl_user'
is_disabled 'tbl_user'

enable 'tbl_user'
is_enabled 'tbl_user'
```

#### 5.3.7	禁用满足正则表达式的所有表disable_all

- `.`匹配除“\n”和"\r"之外的任何单个字符
- `*`匹配前面的子表达式任意次

```sql
# 匹配以t开头的表名
disable_all 't.*'
# 匹配指定命名空间ns下的以t开头的所有表
disable_all 'ns:t.*'
# 匹配ns命名空间下的所有表
disable_all 'ns:.*'
```

#### 5.3.8	启用满足正则表达式的所有表enable_all

```shell
enable_all 't.*'
enable_all 'ns:t.*'
enable_all 'ns:.*'
```

#### 5.3.9	删除表drop

> 需要先禁用表，然后再删除表，启用的表是不允许删除的

```sql
# 语法
disable '表名'
drop '表名'

# 示例
disable 'tbl_user'
drop 'tbl_user'
```

> 直接删除报错：

![image-20220608231641417](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608231641417.png)

> 先禁用后删除

![image-20220608231726087](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608231726087.png)

#### 5.3.10	删除满足正则表达式的所有表drop_all

```shell
drop_all 't.*'
drop_all 'ns:t.*'
drop_all 'ns:.*'
```

#### 5.3.11	获取某个表赋值给一个变量 get_table

> 通过 var = get_table ‘表名’ 赋值给一个变量对象，然后对象.来调用，就像面向对象编程一样，通过对象.方法来调用，这种方式在操作某个表时就不必每次列举表名了。

![image-20220608232059349](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608232059349.png)

#### 5.3.12	获取rowKey所在的区 locate_region

```shell
locate_region '表名', '行键'
```

#### 5.3.13	显示hbase所支持的所有过滤器show_filters

> 过滤器用于get和scan命令中作为筛选数据的条件，类型关系型数据库中的where的作用

![image-20220608232227422](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608232227422.png)

### 5.4	namespace

> **hbase中没有数据库的概念 , 可以使用namespace来达到数据库分类别管理表的作用**

#### 5.4.1	列举命名空间 list_namespace

![image-20220608232407636](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608232407636.png)

#### 5.4.2	获取命名空间描述 describe_namespace

```sql
describe_namespace 'default'
```

![image-20220608232448689](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608232448689.png)

#### 5.4.3	查看命名空间下的所有表 list_namespace_tables

```sql
list_namespace_tables 'default'

list_namespace_tables 'hbase'
```

![image-20220608232555107](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608232555107.png)

#### 5.4.4	创建命名空间create_namespace

```sql
create_namespace 'bigdata17'
```

#### 5.4.5	删除命名空间drop_namespace

```sql
drop_namespace '命名空间名称'
```

### 5.5	DML

#### 5.5.1	插入或者修改数据put

![image-20220608232846139](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608232846139.png)

```sql
# 语法
# 当列族中只有一个列时'列族名:列名'使用'列族名'
put '表名', '行键', '列族名', '列值'
put '表名', '行键', '列族名:列名', '列值'

# 示例

# 创建表
create 'users', 'info', 'detail', 'address'

# 第一行数据
put 'users', 'rk1001', 'info:id', '1'
put 'users', 'rk1001', 'info:name', '张三'
put 'users', 'rk1001', 'info:age', '28'
put 'users', 'rk1001', 'detail:birthday', '1990-06-26'
put 'users', 'rk1001', 'detail:email', 'abc@163.com'
put 'users', 'rk1001', 'detail:create_time', '2019-03-04 14:26:10'
put 'users', 'rk1001', 'address', '上海市'

# 第二行数据
put 'users', 'rk1002', 'info:id', '2'
put 'users', 'rk1002', 'info:name', '李四'
put 'users', 'rk1002', 'info:age', '27'
put 'users', 'rk1002', 'detail:birthday', '1990-06-27'
put 'users', 'rk1002', 'detail:email', 'xxx@gmail.com'
put 'users', 'rk1002', 'detail:create_time', '2019-03-05 14:26:10'
put 'users', 'rk1002', 'address', '北京市'


# 第三行数据
put 'users', 'rk1003', 'info:id', '3'
put 'users', 'rk1003', 'info:name', '王五'
put 'users', 'rk1003', 'info:age', '26'
put 'users', 'rk1003', 'detail:birthday', '1990-06-28'
put 'users', 'rk1003', 'detail:email', 'xyz@qq.com'
put 'users', 'rk1003', 'detail:create_time', '2019-03-06 14:26:10'
put 'users', 'rk1003', 'address', '杭州市'
```

#### 5.5.2	全表扫描scan

```sql
# 语法
scan '表名'

# 示例
scan 'users'   // 效果类似于sql语句中select * from users
```

![image-20220608233212478](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608233212478.png)

> 扫描整个列簇

```sql
# 语法
scan '表名', {COLUMN=>'列族名'}

# 示例
scan 'users', {COLUMN=>'info'}
```

> 扫描整个列簇的某个列

```sql
# 语法
scan '表名', {COLUMN=>'列族名:列名'}

# 示例
scan 'users', {COLUMN=>'info:age'}
```

#### 5.5.3	 获取数据get

```sql
# 语法
get '表名', '行键'

# 示例
get 'users', 'xiaoming'
```

> 根据某一行某列族的数据

```sql
# 语法
get '表名', '行键', '列族名'

# 示例
get 'users', 'xiaoming', 'info'
```

```sql
# 创建表，c1版本为4， 元数据mykey=myvalue
hbase(main):009:0> create 'test1', {NAME => 'cf1', VERSIONS => 4}
0 row(s) in 2.2810 seconds

=> Hbase::Table - t1
# 添加列族c2, c3
hbase(main):010:0> alter 't1', 'c2', 'c3'
Updating all regions with the new schema...
1/1 regions updated.
Done.
Updating all regions with the new schema...
1/1 regions updated.
Done.
0 row(s) in 3.8320 seconds

# 出入数据，c1 插入4个版本的值
hbase(main):011:0> put 't1', 'r1', 'c1', 'v1'
0 row(s) in 0.1000 seconds

hbase(main):012:0> put 't1', 'r1', 'c1', 'v11'
0 row(s) in 0.0180 seconds

hbase(main):013:0> put 't1', 'r1', 'c1', 'v111'
0 row(s) in 0.0140 seconds

hbase(main):014:0> put 't1', 'r1', 'c1', 'v1111'
0 row(s) in 0.0140 seconds

# 插入c2、c3的值
hbase(main):015:0> put 't1', 'r1', 'c2', 'v2'
0 row(s) in 0.0140 seconds

hbase(main):016:0> put 't1', 'r1', 'c3', 'v3'
0 row(s) in 0.0210 seconds

# 获取rowKey=r1的一行记录
hbase(main):017:0> get 't1', 'r1'
COLUMN                                              CELL
 c1:                                                timestamp=1552819382575, value=v1111
 c2:                                                timestamp=1552819392398, value=v2
 c3:                                                timestamp=1552819398244, value=v3
3 row(s) in 0.0550 seconds

# 获取rowKey=r1并且 1552819392398 <= 时间戳范围 < 1552819398244
hbase(main):018:0> get 't1', 'r1', {TIMERANGE => [1552819392398, 1552819398244]}
COLUMN                                              CELL
 c2:                                                timestamp=1552819392398, value=v2
1 row(s) in 0.0090 seconds

# 获取指定列的值
hbase(main):019:0> get 't1', 'r1', {COLUMN => 'c1'}
COLUMN                                              CELL
 c1:                                                timestamp=1552819382575, value=v1111
1 row(s) in 0.0160 seconds

# 获取指定列的值，多个值使用数组表示
hbase(main):020:0> get 't1', 'r1', {COLUMN => ['c1', 'c2', 'c3']}
COLUMN                                              CELL
 c1:                                                timestamp=1552819382575, value=v1111
 c2:                                                timestamp=1552819392398, value=v2
 c3:                                                timestamp=1552819398244, value=v3
3 row(s) in 0.0170 seconds

# 获取c1的值，获取4个版本的值，默认是按照时间戳降续排序的
hbase(main):021:0> get 't1', 'r1', {COLUMN => 'c1', VERSIONS => 4}
COLUMN                                              CELL
 c1:                                                timestamp=1552819382575, value=v1111
 c1:                                                timestamp=1552819376343, value=v111
 c1:                                                timestamp=1552819368993, value=v11
 c1:                                                timestamp=1552819362975, value=v1
4 row(s) in 0.0180 seconds

# 获取c1的3个版本值
hbase(main):027:0* get 't1', 'r1', {COLUMN => 'c1', VERSIONS => 3}
COLUMN                                               CELL
 c1:                                                 timestamp=1552819382575, value=v1111
 c1:                                                 timestamp=1552819376343, value=v111
 c1:                                                 timestamp=1552819368993, value=v11
3 row(s) in 0.0090 seconds

# 获取指定时间戳版本的列
hbase(main):022:0> get 't1', 'r1', {COLUMN => 'c1', TIMESTAMP => 1552819376343}
COLUMN                                              CELL
 c1:                                                timestamp=1552819376343, value=v111
1 row(s) in 0.0170 seconds

hbase(main):023:0> get 't1', 'r1', {COLUMN => 'c1', TIMESTAMP => 1552819376343, VERSIONS => 4}
COLUMN                                              CELL
 c1:                                                timestamp=1552819376343, value=v111
1 row(s) in 0.0130 seconds

# 获取rowKey=r1中的值等于v2的所有列
hbase(main):024:0> get 't1', 'r1', {FILTER => "ValueFilter(=, 'binary:v2')"}
COLUMN                                              CELL
 c2:                                                timestamp=1552819392398, value=v2
1 row(s) in 0.0510 seconds


hbase(main):025:0> get 't1', 'r1', {COLUMN => 'c1', ATTRIBUTES => {'mykey'=>'myvalue'}}
COLUMN                                              CELL
 c1:                                                timestamp=1552819382575, value=v1111
1 row(s) in 0.0100 seconds
```

#### 5.5.4	删除某个列族中的某个列delete

```shell
# 语法
delete '表名', '行键', '列族名:列名'

delete 'users','xiaoming','info:age'

create 'tbl_test', 'columnFamily1'

put 'tbl_test', 'rowKey1', 'columnFamily1:column1', 'value1'
put 'tbl_test', 'rowKey1', 'columnFamily1:column2', 'value2'

delete 'tbl_test', 'rowKey1', 'columnFamily1:column1'
```

#### 5.5.5	 删除某行数据deleteall

```sql
# 语法
deleteall '表名', '行键'

# 示例
deleteall 'users', 'xiaoming'
```

#### 5.5.6	清空整个表的数据truncate

> 先disable表，然后再drop表，最后重新create表

```
truncate '表名'
```

![image-20220608233602414](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608233602414.png)

#### 5.5.7	自增incr

```sql
# 语法
incr '表名', '行键', '列族:列名', 步长值

# 示例 
# 注意：incr 可以对不存的行键操作，如果行键已经存在会报错，如果使用put修改了incr的值再使用incr也会报错
# ERROR: org.apache.hadoop.hbase.DoNotRetryIOException: Field is not a long, it's 2 bytes wide
incr 'tbl_user', 'xiaohong', 'info:age', 1
```

![image-20220608233701046](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608233701046.png)

#### 5.5.8	计数器get_counter

```sql
# 点击量：日、周、月
create 'counters', 'daily', 'weekly', 'monthly'
incr 'counters', '20240415', 'daily:hits', 1
incr 'counters', '20110101', 'daily:hits', 1
get_counter 'counters', '20110101', 'daily:hits'
```

#### 5.5.9	修饰词

##### 1、修饰词

```sql
# 语法
scan '表名', {COLUMNS => [ '列族名1:列名1', '列族名1:列名2', ...]}

# 示例
scan 'tbl_user', {COLUMNS => [ 'info:id', 'info:age']}
```

##### 2、TIMESTAMP 指定时间戳	

```sql
# 语法
scan '表名',{TIMERANGE=>[timestamp1, timestamp2]}

# 示例
scan 'tbl_user',{TIMERANGE=>[1551938004321, 1551938036450]}
```

##### 3、VERSIONS

> 默认情况下一个列只能存储一个数据，后面如果修改数据就会将原来的覆盖掉，可以通过指定VERSIONS时HBase一列能存储多个值。

```sql
create 'tbl_test', 'columnFamily1'
describe 'tbl_test'

# 修改列族版本号
alter 'tbl_test', { NAME=>'columnFamily1', VERSIONS=>3 }

put 'tbl_test', 'rowKey1', 'columnFamily1:column1', 'value1'
put 'tbl_test', 'rowKey1', 'columnFamily1:column1', 'value2'
put 'tbl_test', 'rowKey1', 'columnFamily1:column1', 'value3'

# 默认返回最新的一条数据
get 'tbl_test','rowKey1','columnFamily1:column1'

# 返回3个
get 'tbl_test','rowKey1',{COLUMN=>'columnFamily1:column1', VERSIONS=>3}
# 返回2个
get 'tbl_test','rowKey1',{COLUMN=>'columnFamily1:column1', VERSIONS=>2}
```

##### 4、STARTROW

> ROWKEY起始行。会先根据这个key定位到region，再向后扫描

```sql
# 语法
scan '表名', { STARTROW => '行键名'}

# 示例
scan 'tbl_user', { STARTROW => 'vbirdbest'}
```

##### 5、STOPROW ：截止到STOPROW行，STOPROW行之前的数据，不包括STOPROW这行数据

```sql
# 语法
scan '表名', { STOPROW => '行键名'}

# 示例
scan 'tbl_user', { STOPROW => 'xiaoming'}
```

##### 6、LIMIT 返回的行数

```sql
# 语法
scan '表名', { LIMIT => 行数}

# 示例
scan 'tbl_user', { LIMIT => 2 }
```

#### 5.5.10	FILTER条件过滤器

> 过滤器之间可以使用AND、OR连接多个过滤器。

##### 1、ValueFilter 值过滤器

```sql
# 语法：binary 等于某个值
scan '表名', FILTER=>"ValueFilter(=,'binary:列值')"
# 语法 substring:包含某个值
scan '表名', FILTER=>"ValueFilter(=,'substring:列值')"

# 示例
scan 'tbl_user', FILTER=>"ValueFilter(=, 'binary:26')"
scan 'tbl_user', FILTER=>"ValueFilter(=, 'substring:6')"
```

##### 2、ColumnPrefixFilter 列名前缀过滤器

```
# 语法 substring:包含某个值
scan '表名', FILTER=>"ColumnPrefixFilter('列名前缀')"

# 示例
scan 'tbl_user', FILTER=>"ColumnPrefixFilter('birth')"
# 通过括号、AND和OR的条件组合多个过滤器
scan 'tbl_user', FILTER=>"ColumnPrefixFilter('birth') AND ValueFilter(=,'substring:26')"
```

##### 3、rowKey字典排序

> Table中的所有行都是按照row key的字典排序的

![image-20220608234458949](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220608234458949.png)





# HBase进阶与API

## 一、Hbase shell

### 1、Region信息观察

#### 创建表指定命名空间

> 在创建表的时候可以选择创建到bigdata17这个namespace中，如何实现呢？
> 使用这种格式即可：‘命名空间名称:表名’
> 针对default这个命名空间，在使用的时候可以省略不写

```shell
create 'bigdata29:t1','info'
```

![image-20220609214828043](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220609214828043.png)

> 此时使用**list**查看所有的表

![image-20220609214843582](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220609214843582.png)

> 如果只想查看bigdata17这个命名空间中的表，如何实现呢？
> 可以使用命令list_namespace_tables

```shell
list_namespace_tables 'n1'
```

![image-20220609214907399](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220609214907399.png)

> 查看region中的某列簇数据

```shell
hbase hfile -p -f /hbase/data/default/tbl_user/92994712513a45baaa12b72117dda5e5/info/d84e2013791845968917d876e2b438a5


# 行键的设计在hbase中有三大设计原则：唯一性 长度不宜过长 散列性
put 'students','1500100001','info:name','施笑槐'
put 'students','1500100001','info:age','22'
put 'students','1500100001','info:gender','女'
put 'students','1500100001','info:clazz','文科六班'

put 'students','1500100002','info:name','吕金鹏'
put 'students','1500100002','info:age','24'
put 'students','1500100002','info:gender','男'
put 'students','1500100002','info:clazz','文科六班'


put 'students','1500100003','info:name','单乐蕊'
put 'students','1500100003','info:age','22'
put 'students','1500100003','info:gender','女'
put 'students','1500100003','info:clazz','理科六班'


put 'students','1500100004','info:name','葛德曜'
put 'students','1500100004','info:age','24'
put 'students','1500100004','info:gender','男'
put 'students','1500100004','info:clazz','理科三班'

```

```
刷新数据：flush 'tb'
合并数据：major_compact 'tb'
```



#### 1.1	查看表的所有region

```shell
list_regions '表名'
```

![image-20220609215203693](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220609215203693.png)

#### 1.2	强制将表切分出来一个region

```shell
split '表名','行键'
```

![image-20220609215654881](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220609215654881.png)

> 但是在页面上可以看到三个：过一会会自动的把原来的删除

![image-20220609215721140](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220609215721140.png)

#### 1.2	查看某一行在哪个region中

```shell
locate_region '表名','行键'
```

![image-20220609215929647](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220609215929647.png)

> 可以hbase hfile -p -f xxxx 查看一下
>
> 画图带同学理解

### 2、预分region解决热点问题(面试题)

> 面试题：如何解决hbase中遇到的热点问题？

> row设计的一个关键点是查询维度
>
> (在建表的时候根据具体的查询业务  设计rowkey   预拆分)
>
> 在默认的拆分策略中 ,region的大小达到一定的阈值以后才会进行拆分,并且拆分的region在同一个regionserver中 ,只有达到负载均衡的时机时才会进行region重分配!并且开始如果有大量的数据进行插入操作,那么并发就会集中在单个RS中, 形成热点问题,所以如果有并发插入的时候尽量避免热点问题 ,应当预划分 Region的rowkeyRange范围 ,在建表的时候就指定预region范围 

> 查看命令使用(指定4个切割点，就会有5个region)

```shell
help 'create'
```

![image-20220609221719260](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220609221719260.png)

```shell
create 'tb_split','cf',SPLITS => ['e','h','l','r']
```

```shell
list_regions 'tb_split'
```

![image-20220609222140125](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220609222140125.png)

> 添加数据试试

```shell
put 'tb_split','c001','cf:name','first'
put 'tb_split','f001','cf:name','second'
put 'tb_split','z001','cf:name','last'
```

> hbase hfile -p --f xxxx 查看数据

> 如果没有数据，因为数据还在内存中，需要手动刷新内存到HDFS中，以HFile的形式存储

### 3、总结（写一个文档总结回顾）

### 4、日志查看

> 演示不启动hdfs 就启动hbase

```
日志目录：
/usr/local/soft/hbase-1.7.1/logs
```

![image-20220609225923182](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220609225923182.png)

> start-all.sh发现HMaster没启动，hbase shell客户端也可以正常访问
>
> 再启动hbase就好了

### 5、scan进阶使用

> 查看所有的命名空间

```shell
list_namespace
```

> 查看某个命名空间下的所有表

```shell
list_namespace_tables 'default'
```

> 修改命名空间,设置一个属性

```shell
alter_namespace 'bigdata25',{METHOD=>'set','author'=>'wyh'}
```

> 查看命名空间属性

```shell
describe_namespace 'bigdata17'
```

> 删除一个属性

```shell
alter_namespace 'bigdata17',{METHOD=>'unset', NAME=>'author'}
```

> 删除一个命名空间

```shell
drop_namespace 'bigdata17'
```

> 创建一张表

```sh
create 'teacher','cf'
```

> 添加数据

```shell
put 'teacher','tid0001','cf:tid',1
put 'teacher','tid0002','cf:tid',2
put 'teacher','tid0003','cf:tid',3
put 'teacher','tid0004','cf:tid',4
put 'teacher','tid0005','cf:tid',5
put 'teacher','tid0006','cf:tid',6
```

> 显示三行数据

```shell
scan 'teacher',{LIMIT=>3}
```

```shell
put 'teacher','tid00001','cf:name','wyh'
scan 'teacher',{LIMIT=>3}
```

![image-20220609232359610](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220609232359610.png)

> 从后查三行

```shell
scan 'teacher',{LIMIT=>3,REVERSED=>true}
```

![image-20220609232457186](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220609232457186.png)

> 查看包含指定列的行

```shell
scan 'teacher',{LIMIT=>3,COLUMNS=>['cf:name']}
```

![image-20220609232755396](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220609232755396.png)

> 简化写法：

```shell
scan 'teacher',LIMIT=>3,COLUMNS=>['cf:name']
```

> 在已有的值后面追加值

```shell
append 'teacher','tid00001','cf:name','123'
```

### 6、get进阶使用

> 简单使用，获取某一行数据

```shell
get 'teacher','tid0001'
```

> 获取某一行的某个列簇

```shell
get 'teacher','tid0001','cf'
```

> 获取某一行的某一列（属性 ）

```shell
get 'teacher','tid0001','cf:name'
```

> 可以新增一个列簇数据测试

> **查看历史版本**
>
> 1、修改表可以存储多个版本

```shell
alter 'teacher',NAME=>'cf',VERSIONS=>3
```

> 2、put四次相同rowkey和列的数据

```shell
put 'teacher','tid0001','cf:name','xiaohu1'
put 'teacher','tid0001','cf:name','xiaohu2'
put 'teacher','tid0001','cf:name','xiaohu3'
put 'teacher','tid0001','cf:name','xiaohu4'
```

> 3、查看历史数据，默认是最新的

```shell
get 'teacher','tid0001',COLUMN=>'cf:name',VERSIONS=>2
```

> 修改列簇的过期时间 TTL单位是秒，这个时间是与插入的时间比较，而不是现在开始60s

```shell
alter 'teacher',{NAME=>'cf2',TTL=>'60'}
```

### 7、插入时间指定时间戳

```shell
put 'students','sid0007','info:clazz','bigdata29',1693644893843
```

> 数据时间：数据产生那一刻的时间
>
> 事务时间（操作时间）：接收到数据并处理的那一刻时间

### 8、delete(只能删除一个单元格，不能删除列簇)

> 删除某一列

```shell
delete 'teacher','tid0004','cf:tid'
```

### 9、deleteall(删除不了某个列簇，但是可以删除多个单元格)

> 删除一行，如果不指定列簇，删除的是一行中的所有列簇

```shell
deleteall 'teacher','tid0006'
```

> 删除单元格

```shell
deleteall 'teacher','tid0006','cf:name','cf2:job'
```

### 10、incr和counter

> 统计表有多少行(**统计的是行键的个数**)

```shell
count 'teacher'
```

> 新建一个自增的一列

````shell
incr 'teacher','tid0001','cf:cnt',1
````

> 每操作一次，自增1

```shell
incr 'teacher','tid0001','cf:cnt',1
incr 'teacher','tid0001','cf:cnt',10
incr 'teacher','tid0001','cf:cnt',100
```

![image-20220610000847703](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220610000847703.png)

> 配合counter取出数据,只能去incr字段

```shell
get_counter 'teacher','tid0001','cf:cnt'
```

### 11、获取region的分割点，清除数据，快照

> 获取region的分割点

```shell
get_splits 'tb_split'
```

> 清除表数据

```shell
truncate 'teacher'
```

> 拍摄快照

```shell
snapshot 'tb_split','tb_split_20240416'
```

> 列出所有快照

```shell
list_table_snapshots 'tb_split'
```

> 再添加一些数据

```shell
put 'tb_split','k001','cf:name','wyh'
```

> 恢复快照(先禁用)

```shell
disable 'tb_split'
```

```shell
restore_snapshot 'tb_split_20240416'
```

```shell
enable 'tb_split'
```

## 二、JAVA API

> pom文件

```xml
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.12</version>
        </dependency>
        <!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-client -->
<dependency>
    <groupId>org.apache.hbase</groupId>
    <artifactId>hbase-client</artifactId>
    <version>2.2.7</version>
</dependency>

        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
            <version>2.7.6</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>2.7.6</version>
        </dependency>
```





# Hbase之过滤器

> HBase 的基本 API，包括增、删、改、查等。
> 增、删都是相对简单的操作，与传统的 RDBMS 相比，这里的查询操作略显苍白，只能根据特性的行键进行查询（Get）或者根据行键的范围来查询（Scan）。
> HBase 不仅提供了这些简单的查询，而且提供了更加高级的过滤器（Filter）来查询。
>
> 
>
> 过滤器可以根据列族、列、版本等更多的条件来对数据进行过滤，
>
> 基于 HBase 本身提供的三维有序（行键，列，版本有序），这些过滤器可以高效地完成查询过滤的任务，带有过滤器条件的 RPC 查询请求会把过滤器分发到各个 RegionServer（这是一个服务端过滤器），这样也可以降低网络传输的压力。
>
> 使用过滤器至少需要两类参数：
>
> **一类是抽象的操作符，另一类是比较器**

#### 作用

- 过滤器的作用是在**服务端**判断数据是否满足条件，然后只将满足条件的数据返回给**客户端**
- 过滤器的类型很多，但是可以分为三大类：
  - 比较过滤器：可应用于rowkey、列簇、列、列值过滤器
  - 专用过滤器：只能适用于特定的过滤器
  - 包装过滤器：包装过滤器就是通过包装其他过滤器以实现某些拓展的功能。

#### 比较过滤器

> 所有比较过滤器均继承自 `CompareFilter`。创建一个比较过滤器需要两个参数，分别是**比较运算符**和**比较器实例**。

```java
 public CompareFilter(final CompareOp compareOp,final ByteArrayComparable comparator) {
    this.compareOp = compareOp;
    this.comparator = comparator;
  }
```

##### 比较运算符

* LESS  <

* LESS_OR_EQUAL <=

* EQUAL =

* NOT_EQUAL <>

* GREATER_OR_EQUAL >=

* GREATER >

* NO_OP 排除所有

##### 常见的六大比较器

###### BinaryComparator

> 按字节索引顺序比较指定字节数组，采用Bytes.compareTo(byte[])

###### BinaryPrefixComparator

> 通BinaryComparator，只是比较左端前缀的数据是否相同

###### NullComparator

> 判断给定的是否为空

###### BitComparator

> 按位比较

###### RegexStringComparator

> 提供一个正则的比较器，仅支持 EQUAL 和非EQUAL

###### SubstringComparator

> 判断提供的子串是否出现在中

##### 代码演示（行键，列簇，列名，列值）

###### rowKey过滤器：RowFilter  行键过滤器

> 通过RowFilter与BinaryComparator过滤比rowKey 1500100010小的所有值出来

```java
    /**
     *  需求1：通过RowFilter与BinaryComparator过滤比rowKey 1500100010小的所有值出来
     */
    @Test
    public void rowFilterAndBinaryFun(){
        try {
            //将表名封装成TableName的对象
            TableName students = TableName.valueOf("students");

            //获取表的实例
            Table table = conn.getTable(students);

            //创建Scan对象
            Scan scan = new Scan();
            //创建BinaryComparator比较器
            BinaryComparator binaryComparator = new BinaryComparator(Bytes.toBytes("1500100010"));

            //创建行键过滤
            //老版本：RowFilter(final CompareOp rowCompareOp,final ByteArrayComparable rowComparator)
//            RowFilter rowFilter = new RowFilter(CompareFilter.CompareOp.LESS, binaryComparator);

            //新版本：public RowFilter(final CompareOperator op,final ByteArrayComparable rowComparator)
            RowFilter rowFilter = new RowFilter(CompareOperator.LESS, binaryComparator);


            //设置过滤器
            scan.setFilter(rowFilter);

            ResultScanner resultScanner = table.getScanner(scan);
            //输出结果
            printResultScanner(resultScanner);


        }catch (Exception e){
            e.printStackTrace();
        }
    }
```

###### 列簇过滤器：FamilyFilter

> 通过FamilyFilter与SubstringComparator查询列簇名包含in的所有列簇下面的数据

```java
       /**
     * 通过FamilyFilter与SubstringComparator查询列簇名包含in的所有列簇下面的数据
     */
    @Test
    public void FamilyFilterAndSubstringFun(){
        try {
            TableName students = TableName.valueOf("students2");

            Table table = conn.getTable(students);

            Scan scan = new Scan();

            //创建包含比较器
            SubstringComparator substringComparator = new SubstringComparator("jia");

            //创建列簇过滤器
            //public FamilyFilter(final CompareOperator op, final ByteArrayComparable familyComparator)
            FamilyFilter familyFilter = new FamilyFilter(CompareOperator.EQUAL, substringComparator);

            scan.setFilter(familyFilter);

            ResultScanner resultScanner = table.getScanner(scan);

            printResultScanner(resultScanner);

        }catch (Exception e){
            e.printStackTrace();
        }
    }
```



> 通过FamilyFilter与 BinaryPrefixComparator 过滤出列簇以i开头的列簇下的所有数据

```java
//二进制前缀比较器 BinaryPrefixComparator
BinaryPrefixComparator binaryPrefixComparator = new BinaryPrefixComparator(Bytes.toBytes("i"));

//创建列簇过滤器
FamilyFilter familyFilter = new FamilyFilter(CompareOperator.EQUAL, binaryPrefixComparator);
```



###### 列名过滤器：QualifierFilter

> 通过QualifierFilter与SubstringComparator查询列名包含ge的列的值

```java
    /**
     * 通过QualifierFilter与SubstringComparator查询列名包含 ‘级’ 的列的值
     */
    @Test
    public void qualifierFilterAndSubstringFun(){
        try {
            TableName students = TableName.valueOf("students2");

            Table table = conn.getTable(students);

            Scan scan = new Scan();

            //创建包含比较器
            SubstringComparator substringComparator = new SubstringComparator("级");

            //创建列名过滤器
            //public QualifierFilter(final CompareOperator op, final ByteArrayComparable qualifierComparator)
            QualifierFilter qualifierFilter = new QualifierFilter(CompareOperator.EQUAL, substringComparator);

            scan.setFilter(qualifierFilter);

            ResultScanner scanner = table.getScanner(scan);

            printResultScanner(scanner);
        }catch (Exception e){
            e.printStackTrace();
        }
    }   
```

> 过滤出列的名字中 包含 "别" 所有的列及列的值



###### 列值过滤器：ValueFilter

> 通过ValueFilter与BinaryPrefixComparator过滤出所有的cell中值以 "张" 开头的学生  只会查询出列值是张前缀的这一列，其他列不查

```java
    /**
     * 通过ValueFilter与BinaryPrefixComparator过滤出所有的cell中值以 "张" 开头的学生
     */
    @Test
    public void valueFilterAndBinaryPrefixFun(){
        try {
            TableName students = TableName.valueOf("students2");

            Table table = conn.getTable(students);

            Scan scan = new Scan();
            //创建二进制前缀比较器
            BinaryPrefixComparator binaryPrefixComparator = new BinaryPrefixComparator(Bytes.toBytes("张"));

            //创建列值过滤器
            //public ValueFilter(final CompareOperator valueCompareOp, final ByteArrayComparable valueComparator)
            ValueFilter valueFilter = new ValueFilter(CompareOperator.EQUAL, binaryPrefixComparator);

            scan.setFilter(valueFilter);

            ResultScanner scanner = table.getScanner(scan);

            printResultScanner(scanner);
        }catch (Exception e){
            e.printStackTrace();
        }
    }  
```



> 过滤出文科的学生，只会返回以文科开头的数据列，其他列的数据不符合条件，不会返回

```java
    
```



#### 专用过滤器

###### 单列值过滤器：SingleColumnValueFilter

> SingleColumnValueFilter会返回满足条件的cell所在行的所有cell的值（即会返回一行数据）
>
> 通过SingleColumnValueFilter与查询文科班所有学生信息

```java
   
```



###### 列值排除过滤器：SingleColumnValueExcludeFilter

> 与SingleColumnValueFilter相反，会排除掉指定的列，其他的列全部返回
>
> 通过SingleColumnValueExcludeFilter与BinaryComparator查询文科一班所有学生信息，最终不返回clazz列

```java
   
```



###### rowkey前缀过滤器：PrefixFilter

> 通过PrefixFilter查询以15001001开头的所有前缀的rowkey

```java
    
```



###### 分页过滤器PageFilter

> 通过PageFilter查询三页的数据，每页10条
>
> 使用PageFilter分页效率比较低，每次都需要扫描前面的数据，直到扫描到所需要查的数据
>
> 可设计一个合理的rowkey来实现分页需求

```markdown
# 注意事项：
客户端进行分页查询，需要传递 startRow(起始 RowKey)，知道起始 startRow 后，就可以返回对应的 pageSize 行数据。这里唯一的问题就是，对于第一次查询，显然 startRow 就是表格的第一行数据，但是之后第二次、第三次查询我们并不知道 startRow，只能知道上一次查询的最后一条数据的 RowKey（简单称之为 lastRow）。

我们不能将 lastRow 作为新一次查询的 startRow 传入，因为 scan 的查询区间是[startRow，endRow) ，即前开后闭区间，这样 startRow 在新的查询也会被返回，这条数据就重复了。

同时在不使用第三方数据库存储 RowKey 的情况下，我们是无法通过知道 lastRow 的下一个 RowKey 的，因为 RowKey 的设计可能是连续的也有可能是不连续的。

由于 Hbase 的 RowKey 是按照字典序进行排序的。这种情况下，就可以在 lastRow 后面加上 0 ，作为 startRow 传入，因为按照字典序的规则，某个值加上 0 后的新值，在字典序上一定是这个值的下一个值，对于 HBase 来说下一个 RowKey 在字典序上一定也是等于或者大于这个新值的。

所以最后传入 lastRow+0，如果等于这个值的 RowKey 存在就从这个值开始 scan,否则从字典序的下一个 RowKey 开始 scan。

25 个字母以及数字字符，字典排序如下:

'0' < '1' < '2' < ... < '9' < 'a' < 'b' < ... < 'z'
```

> 需要注意的是在多台 Regin Services 上执行分页过滤的时候，由于并行执行的过滤器不能共享它们的状态和边界，所以有可能每个过滤器都会在完成扫描前获取了 PageCount 行的结果，这种情况下会返回比分页条数更多的数据，分页过滤器就有失效的可能。

```java

```



#### 包装过滤器

###### SkipFilter过滤器

> SkipFilter包装一个过滤器，当被包装的过滤器遇到一个需要过滤的 KeyValue 实例时，则拓展过滤整行数据。下面是一个使用示例：

```java
// 定义 ValueFilter 过滤器
Filter filter1 = new ValueFilter(CompareOperator.NOT_EQUAL,
      new BinaryComparator(Bytes.toBytes("xxx")));
// 使用 SkipFilter 进行包装
Filter filter2 = new SkipFilter(filter1);
```

###### WhileMatchFilter过滤器

> WhileMatchFilter 包装一个过滤器，**当被包装的过滤器遇到一个需要过滤的 KeyValue 实例时**，**WhileMatchFilter 则结束本次扫描**，返回已经扫描到的结果。下面是其使用示例：

```java
    
```



#### 多过滤器综合查询

以上都是讲解单个过滤器的作用，当需要多个过滤器共同作用于一次查询的时候，就需要使用 `FilterList`。`FilterList` 支持通过构造器或者 `addFilter` 方法传入多个过滤器。

> 通过运用4种比较器过滤出姓于，年纪大于23岁，性别为女，且是理科的学生。

```java
   
```



> 过滤出学号是以15001001开头的文科学生

```java
    
```



> 作业：查询文科一班学生总分排名前10的学生（输出：学号，姓名，班级，总分）结果写到hbase

#### 布隆过滤器

> 本质上布隆过滤器是一种数据结构，比较巧妙的概率型数据结构，特点是高效地插入和查询，可以用来告诉你 “**某样东西一定不存在或者可能存在**”。
>
> 相比于传统的 List、Set、Map 等数据结构，它更高效、占用空间更少，但是缺点是其返回的结果是概率性的，而不是确切的。
>
> 实际上，布隆过滤器广泛应用于**网页黑名单系统**、**垃圾邮件过滤系统**、**爬虫网址判重系统**等，Google 著名的分布式数据库 Bigtable 使用了布隆过滤器来查找不存在的行或列，以减少磁盘查找的 IO 次数，Google Chrome 浏览器使用了布隆过滤器加速安全浏览服务。
>
> 在很多 Key-Value 系统中也使用了布隆过滤器来加快查询过程，如 Hbase，Accumulo，Leveldb，一般而言，Value 保存在磁盘中，访问磁盘需要花费大量时间，然而使用布隆过滤器可以快速判断某个 Key 对应的 Value 是否存在，因此可以避免很多不必要的磁盘 IO 操作。
>
> 通过一个 Hash 函数将一个元素映射成一个位阵列（Bit Array）中的一个点。这样一来，我们只要看看这个点是不是 1 就知道可以集合中有没有它了。这就是布隆过滤器的基本思想。

##### 运用场景

> 1、目前有 10 亿数量的自然数，乱序排列，需要对其排序。限制条件在 32 位机器上面完成，内存限制为 2G。如何完成？
>
> 2、如何快速在亿级黑名单中快速定位 URL 地址是否在黑名单中？(每条 URL 平均 64 字节)
>
> 3、需要进行用户登陆行为分析，来确定用户的活跃情况？
>
> 4、网络爬虫-如何判断 URL 是否被爬过？
>
> 5、快速定位用户属性（黑名单、白名单等）？
>
> 6、数据存储在磁盘中，如何避免大量的无效 IO？
>
> 7、判断一个元素在亿级数据中是否存在？
>
> 8、缓存穿透。

##### 实现原理

> 假设我们有个集合 A，A 中有 n 个元素。利用**k个哈希散列**函数，将A中的每个元素**映射**到一个长度为 a 位的数组 B中的不同位置上，这些位置上的二进制数均设置为 1。如果待检查的元素，经过这 k个哈希散列函数的映射后，发现其 k 个位置上的二进制数**全部为 1**，这个元素很可能属于集合A，反之，**一定不属于集合A**。
>
> 比如我们有 3 个 URL `{URL1,URL2,URL3}`，通过一个hash 函数把它们映射到一个长度为 16 的数组上，如下：

![image-20220610231213183](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220610231213183.png)

> 若当前哈希函数为 `Hash1()`，通过哈希运算映射到数组中，假设`Hash1(URL1) = 4`，`Hash1(URL2) = 6`，`Hash1(URL3) = 6`，如下：

![image-20220610231227684](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220610231227684.png)

> 因此，如果我们需要判断`URL1`是否在这个集合中，则通过`Hash(urL1)`计算出其下标，并得到其值若为 1 则说明存在。
>
> 由于 Hash 存在哈希冲突，如上面`URL2,URL3`都定位到一个位置上，假设 Hash 函数是良好的，如果我们的数组长度为 m 个点，那么如果我们想将冲突率降低到例如 **1%**， 这个散列表就只能容纳 `m/100` 个元素，显然空间利用率就变低了，也就是没法做到**空间有效**（space-efficient）。
>
> 解决方法也简单，就是使用多个 Hash 算法，如果它们有一个说元素不在集合中，那肯定就不在，如下：

```shell
Hash1(URL1) = 3,Hash2(URL1) = 5,Hash3(URL1) = 6
Hash1(URL2) = 5,Hash2(URL2) = 7,Hash3(URL2) = 13
Hash1(URL3) = 4,Hash2(URL3) = 7,Hash3(URL3) = 10
```

![image-20220610231252819](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220610231252819.png)

> 以上就是布隆过滤器做法，使用了**k个哈希函数**，每个字符串跟 k 个 bit 对应，从而降低了冲突的概率。

##### 误判现象

> 上面的做法同样存在问题，因为随着增加的值越来越多，被置为 1 的 bit 位也会越来越多，这样某个值即使没有被存储过，但是万一哈希函数返回的三个 bit 位都被其他值置位了 1 ，那么程序还是会判断这个值存在。比如此时来一个不存在的 `URL1000`，经过哈希计算后，发现 bit 位为下：

```shell
Hash1(URL1000) = 7,Hash2(URL1000) = 8,Hash3(URL1000) = 14
```

![image-20220610231342905](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220610231342905.png)

> 但是上面这些 bit 位已经被`URL1,URL2,URL3`置为 1 了，此时程序就会判断 `URL1000` 值存在。
>
> 这就是布隆过滤器的误判现象，所以，**布隆过滤器判断存在的不一定存在，但是，判断不存在的一定不存在。**
>
> 布隆过滤器可精确的代表一个集合，可精确判断某一元素是否在此集合中，精确程度由用户的具体设计决定，达到 100% 的正确是不可能的。但是布隆过滤器的优势在于，**利用很少的空间可以达到较高的精确率**。

##### 控制粒度

> **a）ROW**
> 	根据KeyValue中的行来过滤storefile 
> 	举例：假设有2个storefile文件sf1和sf2， 
> 		sf1包含kv1（r1 cf：q1 v），kv2（r2 cf：q1 v） 
> 		sf2包含kv3（r3 cf：q1 v），kv4（r4 cf：q1 v） 
> 		若是设置了CF属性中的bloomfilter为ROW，那么得（r1）时就会过滤sf2，get（r3）就会过滤sf1 
> **b）ROWCOL**
> 	根据KeyValue中的行+限定符来过滤storefile
> 	举例：假设有2个storefile文件sf1和sf2， 
> 		sf1包含kv1（r1 cf：q1 v），kv2（r2 cf：q1 v） 
> 		sf2包含kv3（r1 cf：q2 v），kv4（r2 cf：q2 v） 
> 		若是设置了CF属性中的布隆过滤器为ROW，不管获得（R1，Q1）仍是获得（R1，Q2），都会读取SF1 + SF2;而若是设置了CF属性中的布隆过滤器为		ROWCOL，那么GET（R1， q1）就会过滤sf2，get（r1，q2）就会过滤sf1
> **c）NO**
> 	默认的值，默认不开启布隆过滤器

##### 实现：

> 在建立表时加入一个参数就能够了

```java
	 try {
            //使用HTableDescriptor类创建一个表对象
            HTableDescriptor students = new HTableDescriptor("students");

            //在创建表的时候，至少指定一个列簇
            HColumnDescriptor info = new HColumnDescriptor("info");
            info.setBloomFilterType(BloomType.ROW); //<===========================================

            //将列簇添加到表中
            students.addFamily(info);
            //真正的执行，是由HMaster
            //hAdmin
            hAdmin.createTable(students);
            System.out.println(Bytes.toString(students.getName()) + "表 创建成功。。。");
        } catch (IOException e) {
            e.printStackTrace();
        }
```





# HBase进阶与Phenix

## 一、HBase的读写流程

>  画出架构



### 1.1	HBase读流程

```
Hbase读取数据的流程：
1）是由客户端发起读取数据的请求，首先会与zookeeper建立连接
2）从zookeeper中获取一个hbase:meta表位置信息，被哪一个regionserver所管理着
     hbase:meta表：hbase的元数据表，在这个表中存储了自定义表相关的元数据，包括表名，表有哪些列簇，表有哪些region,每个region存储的位置，每个region被哪个regionserver所管理，这个表也是存储在某一个region上的，并且这个meta表只会被一个regionserver所管理。这个表的位置信息只有zookeeper知道。
3）连接这个meta表对应的regionserver,从meta表中获取当前你要读取的这个表对应的regionsever是谁。
     当一个表多个region怎么办呢？
     如果我们获取数据是以get的方式，只会返回一个regionserver
     如果我们获取数据是以scan的方式，会将所有的region对应的regionserver的地址全部返回。
4）连接要读取表的对应的regionserver,从regionserver上的开始读取数据：
       读取顺序：memstore-->blockcache-->storefile-->Hfile中
       注意：如果是scan操作，就不仅仅去blockcache了，而是所有都会去找。
```

### 1.2	HBase写流程

```
--------------------------1-4步是客户端写入数据的流程-----------------

Hbase的写入数据流程：
1）由客户端发起写数据请求，首先会与zookeeper建立连接
2）从zookeeper中获取hbase:meta表被哪一个regionserver所管理
3）连接hbase:meta表中获取对应的regionserver地址 (从meta表中获取当前要写入数据的表对应的region所管理的regionserver) 只会返回一个regionserver地址
4）与要写入数据的regionserver建立连接，然后开始写入数据，将数据首先会写入到HLog，然后将数据写入到对应store模块中的memstore中
（可能会写多个），当这两个地方都写入完成之后，表示数据写入完成。


-------------------------后面的步骤是服务器内部的操作-----------------
异步操作
5）随着客户端不断地写入数据，memstore中的数据会越来多，当内存中的数据达到阈值（128M/1h）的时候，放入到blockchache中，生成新的memstore接收用户过来的数据，然后当blockcache的大小达到一定阈值（0.85）的时候，开始触发flush机制，将数据最终刷新到HDFS中形成小的Hfile文件。

6）随着不断地刷新，storefile不断地在HDFS上生成小HFIle文件，当小的HFile文件达到阈值的时候（3个及3个以上）,就会触发Compaction机制，将小的HFile合并成一个大的HFile.

7）随着不断地合并，大的HFile文件会越来越大，当达到一定阈值（2.0版本之后最终10G）的时候，会触发分裂机制（split）,将大的HFile文件进行一分为二，同时管理这个大的HFile的region也会被一分为二，形成两个新的region和两个新的HFile文件，一对一的进行管理，将原来旧的region和分裂之前大的HFile文件慢慢地就会下线处理。
```



## 二、Region的分裂策略

> region中存储的是一张表的数据，当region中的数据条数过多的时候，会直接影响查询效率。当region过大的时候，region会被拆分为两个region，HMaster会将分裂的region分配到不同的regionserver上，这样可以让请求分散到不同的RegionServer上，已达到负载均衡 , 这也是HBase的一个优点 。

* ConstantSizeRegionSplitPolicy

  > 0.94版本前，HBase region的默认切分策略 

  当region中最大的store大小超过某个阈值(hbase.hregion.max.filesize=10G)之后就会触发切分，一个region等分为2个region。

  但是在生产线上这种切分策略却有相当大的弊端（切分策略对于大表和小表没有明显的区分）：

  * 阈值(hbase.hregion.max.filesize)设置较大对大表比较友好，但是小表就有可能不会触发分裂，极端情况下可能就1个，形成热点，这对业务来说并不是什么好事。
  * 如果设置较小则对小表友好，但一个大表就会在整个集群产生大量的region，这对于集群的管理、资源使用、failover来说都不是一件好事。

* IncreasingToUpperBoundRegionSplitPolicy

  > 0.94版本~2.0版本默认切分策略 

  ​		总体看和ConstantSizeRegionSplitPolicy思路相同，一个region中最大的store大小大于设置阈值就会触发切分。
  但是这个阈值并不像ConstantSizeRegionSplitPolicy是一个固定的值，而是会在一定条件下不断调整，调整规则和region所属表在当前regionserver上的region个数有关系.

  region split阈值的计算公式是：

  * 设regioncount：是region所属表在当前regionserver上的region的个数

  * 阈值 = regioncount^3 * 128M * 2，当然阈值并不会无限增长，最大不超过MaxRegionFileSize（10G),当region中最大的store的大小达到该阈值的时候进行region split

  例如：

  * 第一次split阈值 = 1^3 * 256 = 256MB
  * 第二次split阈值 = 2^3 * 256 = 2048MB
  * 第三次split阈值 = 3^3 * 256 = 6912MB
  * 第四次split阈值 = 4^3 * 256 = 16384MB > 10GB，因此取较小的值10GB
  * 后面每次split的size都是10GB了

  **特点**

  * 相比ConstantSizeRegionSplitPolicy，可以自适应大表、小表；
  * 在集群规模比较大的情况下，对大表的表现比较优秀
  * 对小表不友好，小表可能产生大量的小region，分散在各regionserver上
  * 小表达不到多次切分条件，导致每个split都很小，所以分散在各个regionServer上

* SteppingSplitPolicy

  > 2.0版本默认切分策略

  ​	相比 IncreasingToUpperBoundRegionSplitPolicy 简单了一些
  ​	region切分的阈值依然和待分裂region所属表在当前regionserver上的region个数有关系

  * 如果region个数等于1，切分阈值为flush size 128M
  * 否则为MaxRegionFileSize。

  > 这种切分策略对于大集群中的大表、小表会比 IncreasingToUpperBoundRegionSplitPolicy 更加友好，小表不会再产生大量的小region，而是适可而止。

* KeyPrefixRegionSplitPolicy

  > 根据rowKey的前缀对数据进行分区，这里是指定rowKey的前多少位作为前缀，比如rowKey都是16位的，指定前5位是前缀，那么前5位相同的rowKey在相同的region中。

* DelimitedKeyPrefixRegionSplitPolicy

  > 保证相同前缀的数据在同一个region中，例如rowKey的格式为：userid_eventtype_eventid，指定的delimiter为 _ ，则split的的时候会确保userid相同的数据在同一个region中。
  > 按照分隔符进行切分，而KeyPrefixRegionSplitPolicy是按照指定位数切分。

* BusyRegionSplitPolicy

  > 按照一定的策略判断Region是不是Busy状态，如果是即进行切分
  >
  > 如果你的系统常常会出现热点Region，而你对性能有很高的追求，那么这种策略可能会比较适合你。它会通过拆分热点Region来缓解热点Region的压力，但是根据热点来拆分Region也会带来很多不确定性因素，因为你也不知道下一个被拆分的Region是哪个。

* DisabledRegionSplitPolicy

  > 不启用自动拆分, 需要指定手动拆分

## 三、Compaction操作

#### Minor Compaction：

* 指选取一些小的、相邻的StoreFile将他们合并成一个更大的StoreFile，在这个过程中不会处理已经Deleted或Expired的Cell。一次 Minor Compaction 的结果是更少并且更大的StoreFile。

#### Major Compaction：

* 指将**所有的StoreFile**合并成一个StoreFile，这个过程会清理三类没有意义的数据：**被删除的数据**、**TTL过期数据**、**版本号超过设定版本号的数据**。另外，一般情况下，major compaction时间会持续比较长，整个过程会消耗大量系统资源，对上层业务有比较大的影响。因此线上业务都会将关闭自动触发major compaction功能，改为手动在业务低峰期触发。

> 参考文档：https://cloud.tencent.com/developer/article/1488439

## 四、面对百亿数据，HBase为什么查询速度依然非常快？（面试题）

HBase适合存储PB级别的海量数据（百亿千亿量级条记录），如果根据记录主键Rowkey来查询，能在几十到百毫秒内返回数据。

**那么HBase是如何做到的呢？**

接下来，简单阐述一下数据的查询思路和过程。

## 查询过程

##### 第1步：

项目有100亿业务数据，存储在一个HBase集群上（由多个服务器数据节点构成），每个数据节点上有若干个Region（区域），每个Region实际上就是HBase中一批数据的集合（一段连续范围rowkey的数据）。

我们现在开始根据主键RowKey来查询对应的记录，通过meta表可以帮我们迅速定位到该记录所在的数据节点，以及数据节点中的Region，目前我们有100亿条记录，占空间10TB。所有记录被切分成5000个Region，那么现在，每个Region就是2G。

**由于记录在1个Region中，所以现在我们只要查询这2G的记录文件，就能找到对应记录。**

##### 第2步：

由于HBase存储数据是按照列族存储的。比如一条记录有400个字段，前100个字段是人员信息相关，这是一个列簇（列的集合）；中间100个字段是公司信息相关，是一个列簇。另外100个字段是人员交易信息相关，也是一个列簇；最后还有100个字段是其他信息，也是一个列簇

这四个列簇是分开存储的，这时，假设2G的Region文件中，分为4个列族，那么每个列族就是500M。

**到这里，我们只需要遍历这500M的列簇就可以找到对应的记录。**

##### 第3步：

如果要查询的记录在其中1个列族上，1个列族在HDFS中会包含1个或者多个HFile。

如果一个HFile一般的大小为100M，那么该列族包含5个HFile在磁盘上或内存中。

由于HBase的内存进入磁盘中的数据是排好序（字典顺序）的，要查询的记录有可能在最前面，也有可能在最后面，按平均来算，**我们只需遍历2.5个HFile共250M，即可找到对应的记录。**

##### 第4步：

每个HFile中，是以键值对(key/value)方式存储，只要遍历文件中的key位置并判断符合条件即可

一般key是有限的长度，假设key/value比是1:24，**最终只需要10M的数据量，就可获取的对应的记录。**

如果数据在机械磁盘上，按其访问速度100M/S，只需0.1秒即可查到。

如果是SSD的话，0.01秒即可查到。

当然，扫描HFile时还可以通过布隆过滤器快速定位到对应的HFile，以及HBase是有内存缓存机制的，如果数据在内存中，效率会更高。

#### 总结

正因为以上大致的查询思路，保证了HBase即使随着数据量的剧增，也不会导致查询性能的下降。

同时，HBase是一个面向列存储的数据库（列簇机制），当表字段非常多时，可以把其中一些字段独立出来放在一部分机器上，而另外一些字段放到另一部分机器上，分散存储，分散列查询。

正由于这样复杂的存储结构和分布式的存储方式，保证了HBase海量数据下的查询效率。

## 五、HBase与Hive的集成

> HBase与Hive的对比

**hive:**

数据仓库：Hive的本质其实就相当于将HDFS中已经存储的文件在Mysql中做了一个双射关系，以方便使用HQL去管理查询。

用于数据分析、清洗：Hive适用于离线的数据分析和清洗，延迟较高。

基于HDFS、MapReduce：Hive存储的数据依旧在DataNode上，编写的HQL语句终将是转换为MapReduce代码执行。

**HBase**

数据库：是一种面向列族存储的非关系型数据库。

用于存储结构化和非结构化的数据：适用于单表非关系型数据的存储，不适合做关联查询，类似JOIN等操作。

基于HDFS：数据持久化存储的体现形式是HFile，存放于DataNode中，被ResionServer以region的形式进行管理。

延迟较低，接入在线业务使用：面对大量的企业数据，HBase可以直线单表大量数据的存储，同时提供了高效的数据访问速度。

> 在`hive-site.xml`中添加zookeeper的属性

```xml
	<property>
        <name>hive.zookeeper.quorum</name>
        <value>master,node1,node2</value>
    </property>

    <property>
        <name>hive.zookeeper.client.port</name>
        <value>2181</value>
    </property>

```

> ## HBase中已经存储了某一张表，在Hive中创建一个外部表来关联HBase中的这张表

> 建立外部表的字段名要和hbase中的列名一致
>
> 前提是hbase中已经有表了

```sql
create external table students_hbase
(
id string,
name string,
age string,
gender string, 
clazz string
)
stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
with serdeproperties ("hbase.columns.mapping" = "
:key,
info:name,
info:age,
info:gender,
info:clazz
")
tblproperties("hbase.table.name" = "default:students");

create external table score_hbase2
(
id string,
score_dan string
)
stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
with serdeproperties ("hbase.columns.mapping" = "
:key,
info:score_dan
")
tblproperties("hbase.table.name" = "default:score");
```

> 关联后就可以使用Hive函数进行一些分析操作了

## 六、Phoenix

> Hbase适合存储大量的对关系运算要求低的NOSQL数据，受Hbase 设计上的限制不能直接使用原生的API执行在关系数据库中普遍使用的条件判断和聚合等操作。Hbase很优秀，一些团队寻求在Hbase之上提供一种更面向普通开发人员的操作方式，Apache Phoenix即是。

> Phoenix 基于Hbase给面向业务的开发人员提供了以标准SQL的方式对Hbase进行查询操作，并支持标准SQL中大部分特性:条件运算,分组，分页，等高级查询语法。

### 1、Phoenix搭建

Phoenix 5.1.0  HBase 2.2.7 hadoop 3.1.1

#### 1、关闭hbase集群，在master中执行

```shell
stop-hbase.sh
```

#### 2、上传解压配置环境变量

解压

```tar -xvf apache-phoenix-4.15.0-HBase-1.4-bin.tar.gz -C /usr/local/soft/```

改名

```mv apache-phoenix-4.15.0-HBase-1.4-bin phoenix-4.15.0```

#### 3、将phoenix-4.15.0-HBase-1.4-server.jar复制到所有节点的hbase lib目录下

```shell
scp /usr/local/soft/phoenix-4.15.0/phoenix-4.15.0-HBase-1.4-server.jar master:/usr/local/soft/hbase-1.4.6/lib/

scp /usr/local/soft/phoenix-4.15.0/phoenix-4.15.0-HBase-1.4-server.jar node1:/usr/local/soft/hbase-1.4.6/lib/

scp /usr/local/soft/phoenix-4.15.0/phoenix-4.15.0-HBase-1.4-server.jar node2:/usr/local/soft/hbase-1.4.6/lib/

```

#### 4、启动hbase ， 在master中执行

```shell
start-hbase.sh
```

#### 5、配置环境变量

```
vim /etc/profile
```



### 2、Phoenix使用

#### 1、连接sqlline

```shell
sqlline.py master,node1,node2

# 出现
163/163 (100%) Done
Done
sqlline version 1.5.0
0: jdbc:phoenix:master,node1,node2> 


```

#### 2、常用命令

> phoneix使用语法注意事项

```markdown
# 使用注意事项
	1、在phoneix内部创建表的时候，表名最后可以使用!table或者show tables命令查看，并且以大写的形式展示给我们，但是我们在使用sql语句查询的时候既可以用大写也可以用小写。（列名和表名大小写无所谓）
	
	2、直接在phoneix内部创建的表，在hbase中可以以大写的方式查看到，但是在hbase中建的表，在phoneix中看不到。
	
	3、如何在phoneix中使用hbase原本的数据表呢？
		视图映射：视图并不是真正意义上的表，而是在phoneix创建一个映射关系，以表的形式将hbase中原本数据映射过来，可以在基础之上编写sql语句进行分析，需要注意的是，我们在视图上sql分析的时候，表名和列名需要加双引号。删除视图不会影响原本hbase中的数据，视图无法做修改，只能查询，视图在phoneix中被看作成一个只读表。
		表映射：建表的语法来说与视图映射相差一个单词，其他的没啥区别。使用上，表映射可以直接在phoneix中对表数据进行增删改查。将phoneix中表映射删了，原来hbase中的表也对应删除。
		
	4、映射查询的时候，主键可以不用加双引号，非主键的列必须加双引号
```





```sql
# 1、创建表

CREATE TABLE IF NOT EXISTS students_p1 (
 id VARCHAR NOT NULL PRIMARY KEY, 
 name VARCHAR,
 age BIGINT, 
 gender VARCHAR ,
 clazz VARCHAR
);

# 2、显示所有表
 !table

# 3、插入数据
upsert into "students" values('1500101001','小虎',22,'男','理科一班');
upsert into STUDENT values('1500100005','宣谷芹',24,'男','理科六班');
upsert into STUDENT values('1500100006','羿彦昌',24,'女','理科三班');


# 4、查询数据,支持大部分sql语法，
select * from STUDENT ;
select * from STUDENT where age=24;
select gender ,count(*) from STUDENT group by gender;
select * from student order by gender;

# 5、删除数据
delete from STUDENT where id='1500100004';


# 6、删除表
drop table STUDENT;
 
 
# 7、退出命令行
!quit

更多语法参照官网
https://phoenix.apache.org/language/index.html#upsert_select

```

#### 3、phoenix表映射

>  默认情况下，直接在hbase中创建的表，通过phoenix是查看不到的

> 如果需要在phoenix中操作直接在hbase中创建的表，则需要在phoenix中进行表的映射。映射方式有两种：视图映射和表映射

##### 3.1、视图映射

>  Phoenix创建的视图是只读的，所以只能用来做查询，无法通过视图对源数据进行修改等操作

```sql
# hbase shell 进入hbase命令行
hbase shell 

# 创建hbase表
create 'test','name','company' 

# 插入数据
put 'test','001','name:firstname','zhangsan1'
put 'test','001','name:lastname','zhangsan2'
put 'test','001','company:name','数加'
put 'test','001','company:address','合肥'


upsert into TEST values('002','xiaohu','xiaoxiao','数加','合肥');


# 在phoenix创建视图， primary key 对应到hbase中的rowkey

create view "test"(
empid varchar primary key,
"name"."firstname" varchar,
"name"."lastname"  varchar,
"company"."name"  varchar,
"company"."address" varchar
);

CREATE view "students" (
 id VARCHAR NOT NULL PRIMARY KEY, 
 "info"."name" VARCHAR,
 "info"."age" VARCHAR, 
 "info"."gender" VARCHAR,
 "info"."clazz" VARCHAR
) column_encoded_bytes=0;

# 在phoenix查询数据，表名通过双引号引起来
select * from "test";

# 删除视图
drop view "test";

```

##### 3.2、表映射

使用Apache Phoenix创建对HBase的表映射，有两类：

1） 当HBase中已经存在表时，可以以类似创建视图的方式创建关联表，只需要将create view改为create table即可。

2）当HBase中不存在表时，可以直接使用create table指令创建需要的表，并且在创建指令中可以根据需要对HBase表结构进行显示的说明。

第1）种情况下，如在之前的基础上已经存在了test表，则表映射的语句如下：

```sql
create table "test" (
empid varchar primary key,
"name"."firstname" varchar,
"name"."lastname"varchar,
"company"."name"  varchar,
"company"."address" varchar
)column_encoded_bytes=0;

upsert into "students" values('150011000100','xiaohu','24','男','理科三班');

upsert into  "test"  values('1001','xiaohu','xiaoxiao','数加','合肥');

CREATE table "students" (
 id VARCHAR NOT NULL PRIMARY KEY, 
 "info"."name" VARCHAR,
 "info"."age" VARCHAR, 
 "info"."gender" VARCHAR,
 "info"."clazz" VARCHAR
) column_encoded_bytes=0;

upsert into "students" values('150011000100','xiaohu','24','男','理科三班');

CREATE table  "scores" (
 id VARCHAR NOT NULL PRIMARY KEY, 
 "info"."score" VARCHAR
) column_encoded_bytes=0;

1、过滤出文科一班的学生
select ID as id,"name" as name,"clazz" as clazz from "students" where "clazz"='文科一班';


2、将成绩表做切分转换
select regexp_split(ID,'-')[1] as student_id,regexp_split(ID,'-')[2] as subject_id,"score" as score from "scores";
|         ID         | score |
+--------------------+-------+
| 1500100001-1000001 | 98    |
| 1500100001-1000002 | 5     |
| 1500100001-1000003 | 137   |
| 1500100001-1000004 | 29    |
| 1500100001-1000005 | 85    |
| 1500100001-1000006 | 52    |
| 1500100002-1000001 | 139   |
| 1500100002-1000002 | 102   |
| 1500100002-1000003 | 44    |
| 1500100002-1000004 | 18    |

3、在第二步的基础之上求每个学生的总分
select t1.student_id as student_id,sum(to_number(t1.score)) as sum_score from (select regexp_split(ID,'-')[1] as student_id,regexp_split(ID,'-')[2] as subject_id,"score" as score from "scores") t1 group by t1.student_id;

4、与步骤1的文科学生进行关联
select b1.id as student_id,b1.name as name,b1.clazz as clazz,to_char(b2.sum_score) as sum_score from (select ID as id,"name" as name,"clazz" as clazz from "students" where "clazz"='文科一班') b1 join (select t1.student_id as student_id,sum(to_number(t1.score)) as sum_score from (select regexp_split(ID,'-')[1] as student_id,regexp_split(ID,'-')[2] as subject_id,"score" as score from "scores") t1 group by t1.student_id) b2 on (b1.id=b2.student_id) order by b2.sum_score desc limit 10;

通过这个例子遇到的注意点：
1、切分字符串的函数不是split，而是regexp_split
2、Phoenix中，数组的索引是从1开始的
3、给字段起别名之后的嵌套查询，就不需要再加双引号了，主键本身就可以不用加
4、sum函数中的数据类型必须是数值类型，如果是10的整数倍，会以科学计数法进行标识 580->5.8E+2(5.8*10^2)
5、to_number()转数值  to_char()转字符串


C:\Users\shujia\AppData\Roaming\DBeaverData\drivers

"jdbc:phoenix:master,node2,node3:2181
```

使用create table创建的关联表，如果对表进行了修改，源数据也会改变，同时如果关联表被删除，源表也会被删除。但是视图就不会，如果删除视图，源数据不会发生改变。

## 七、bulkLoad实现批量导入

### 优点：

1. 如果我们一次性入库hbase巨量数据，处理速度慢不说，还特别占用Region资源， 一个比较高效便捷的方法就是使用 “Bulk Loading”方法，即HBase提供的HFileOutputFormat类。

2. 它是利用hbase的数据信息按照特定格式存储在hdfs内这一原理，直接生成这种hdfs内存储的数据格式文件，然后上传至合适位置，即完成巨量数据快速入库的办法。配合mapreduce完成，高效便捷，而且不占用region资源，增添负载。

### 限制：

1. 仅适合初次数据导入，即表内数据为空，或者每次入库表内都无数据的情况。
2. HBase集群与Hadoop集群为同一集群，即HBase所基于的HDFS为生成HFile的MR的集群

### 代码编写：

> 提前在Hbase中创建好表
>
> 生成Hfile基本流程：
>
> 1. 设置Mapper的输出KV类型：	 
>
>    K： ImmutableBytesWritable（代表行键）
>
>    V： KeyValue  （代表cell）
>
> 
>
> ​	2.  开发Mapper
>
> ​		读取你的原始数据，按你的需求做处理
>
> ​		输出rowkey作为K，输出一些KeyValue（Put）作为V
>
> ​	3.  配置job参数
>
> ​		a. Zookeeper的连接地址
>
> ​		b. 配置输出的OutputFormat为HFileOutputFormat2，并为其设置参数
>
> 
>
> ​	4.  提交job
>
> ​			导入HFile到RegionServer的流程
>
> ​    			构建一个表描述对象
>
> ​			构建一个region定位工具
>
> ​			然后用LoadIncrementalHFiles来doBulkload操作

> pom文件：

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <parent>
        <artifactId>hadoop-bigdata17</artifactId>
        <groupId>com.shujia</groupId>
        <version>1.0-SNAPSHOT</version>
    </parent>
    <modelVersion>4.0.0</modelVersion>

    <artifactId>had-hbase-demo</artifactId>

    <properties>
        <maven.compiler.source>8</maven.compiler.source>
        <maven.compiler.target>8</maven.compiler.target>
    </properties>
    <dependencies>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-hdfs</artifactId>
        </dependency>

        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-client</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-server</artifactId>
        </dependency>
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.phoenix</groupId>
            <artifactId>phoenix-core</artifactId>
        </dependency>
        <dependency>
            <groupId>com.lmax</groupId>
            <artifactId>disruptor</artifactId>
        </dependency>


    </dependencies>

    <build>
        <plugins>
            <!-- compiler插件, 设定JDK版本 -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>2.3.2</version>
                <configuration>
                    <encoding>UTF-8</encoding>
                    <source>1.8</source>
                    <target>1.8</target>
                    <showWarnings>true</showWarnings>
                </configuration>
            </plugin>


            <!-- 带依赖jar 插件-->
            <plugin>
                <artifactId>maven-assembly-plugin</artifactId>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>

    </build>

</project>
```

> 电信数据

```
手机号,网格编号,城市编号,区县编号,停留时间,进入时间,离开时间,时间分区
D55433A437AEC8D8D3DB2BCA56E9E64392A9D93C,117210031795040,83401,8340104,301,20180503190539,20180503233517,20180503

手机号和进入时间
```



### 说明

1. 最终输出结果，无论是map还是reduce，输出部分key和value的类型必须是： < ImmutableBytesWritable, KeyValue>或者< ImmutableBytesWritable, Put>。
2. 最终输出部分，Value类型是KeyValue 或Put，对应的Sorter分别是KeyValueSortReducer或PutSortReducer。
3. MR例子中HFileOutputFormat2.configureIncrementalLoad(job, dianxin_bulk, regionLocator);自动对job进行配置。SimpleTotalOrderPartitioner是需要先对key进行整体排序，然后划分到每个reduce中，保证每一个reducer中的的key最小最大值区间范围，是不会有交集的。因为入库到HBase的时候，作为一个整体的Region，key是绝对有序的。

4. MR例子中最后生成HFile存储在HDFS上，输出路径下的子目录是各个列族。如果对HFile进行入库HBase，相当于move HFile到HBase的Region中，HFile子目录的列族内容没有了，但不能直接使用mv命令移动，因为直接移动不能更新HBase的元数据。

5. HFile入库到HBase通过HBase中 LoadIncrementalHFiles的doBulkLoad方法，对生成的HFile文件入库

## 八、HBase中rowkey的设计（重点！！面试题）

**HBase的RowKey设计**

HBase是三维有序存储的，通过rowkey（行键），column key（column family和qualifier）和TimeStamp（时间戳）这个三个维度可以对HBase中的数据进行快速定位。

HBase中rowkey可以唯一标识一行记录，在HBase查询的时候，有两种方式：

 通过get方式，指定rowkey获取唯一一条记录

 通过scan方式，设置startRow和stopRow参数进行范围匹配

 全表扫描，即直接扫描整张表中所有行记录

**rowkey长度原则**

rowkey是一个二进制码流，可以是任意字符串，最大长度 *64kb* ，实际应用中一般为10-100bytes，以 byte[] 形式保存，一般设计成定长。

建议越短越好，不要超过16个字节，原因如下：

 数据的持久化文件HFile中是按照KeyValue存储的，如果rowkey过长，比如超过100字节，1000w行数据，光rowkey就要占用100*1000w=10亿个字节，将近1G数据，这样会极大影响HFile的存储效率；

 MemStore将缓存部分数据到内存，如果rowkey字段过长，内存的有效利用率就会降低，系统不能缓存更多的数据，这样会降低检索效率。

 目前操作系统都是64位系统，内存8字节对齐，控制在16个字节，8字节的整数倍利用了操作系统的最佳特性。

**rowkey散列原则**

如果rowkey按照时间戳的方式递增，不要将时间放在二进制码的前面，建议将rowkey的高位作为散列字段，由程序随机生成，低位放时间字段，这样将提高数据均衡分布在每个RegionServer，以实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息，所有的数据都会集中在一个RegionServer上，这样在数据检索的时候负载会集中在个别的RegionServer上，造成热点问题，会降低查询效率。

**rowkey唯一原则**

必须在设计上保证其唯一性，rowkey是按照字典顺序排序存储的，因此，设计rowkey的时候，要充分利用这个排序的特点，将经常读取的数据存储到一块，将最近可能会被访问的数据放到一块。

**什么是热点**

HBase中的行是按照rowkey的字典顺序排序的，这种设计优化了scan操作，可以将相关的行以及会被一起读取的行存取在临近位置，便于scan。然而糟糕的rowkey设计是热点的源头。 热点发生在大量的client直接访问集群的一个或极少数个节点（访问可能是读，写或者其他操作）。大量访问会使热点region所在的单个机器超出自身承受能力，引起性能下降甚至region不可用，这也会影响同一个RegionServer上的其他region，由于主机无法服务其他region的请求。 设计良好的数据访问模式以使集群被充分，均衡的利用。

为了避免写热点，设计rowkey使得不同行在同一个region，但是在更多数据情况下，数据应该被写入集群的多个region，而不是一个。

下面是一些常见的避免热点的方法以及它们的优缺点：

**加盐**

这里所说的加盐不是密码学中的加盐，而是在rowkey的前面增加随机数，具体就是给rowkey分配一个随机前缀以使得它和之前的rowkey的开头不同。分配的前缀种类数量应该和你想使用数据分散到不同的region的数量一致。加盐之后的rowkey就会根据随机生成的前缀分散到各个region上，以避免热点。

**哈希**

哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完整的rowkey，可以使用get操作准确获取某一个行数据

**反转**

第三种防止热点的方法时反转固定长度或者数字格式的rowkey。这样可以使得rowkey中经常改变的部分（最没有意义的部分）放在前面。这样可以有效的随机rowkey，但是牺牲了rowkey的有序性。

反转rowkey的例子以手机号为rowkey，可以将手机号反转后的字符串作为rowkey，这样的就避免了以手机号那样比较固定开头导致热点问题

**时间戳反转**

一个常见的数据处理问题是快速获取数据的最近版本，使用反转的时间戳作为rowkey的一部分对这个问题十分有用，可以用 Long.Max_Value - timestamp 追加到key的末尾，例如 [key]reverse_timestamp , [key] 的最新值可以通过scan [key]获得[key]的第一条记录，因为HBase中rowkey是有序的，第一条记录是最后录入的数据。

比如需要保存一个用户的操作记录，按照操作时间倒序排序，在设计rowkey的时候，可以这样设计

[userId反转]Long.Max_Value - timestamp，在查询用户的所有操作记录数据的时候，直接指定反转后的userId，startRow是[userId反转]000000000000,stopRow是[userId反转]Long.Max_Value - timestamp

如果需要查询某段时间的操作记录，startRow是[user反转]Long.Max_Value - 起始时间，stopRow是[userId反转]Long.Max_Value - 结束时间

其他一些建议

 尽量减少行和列的大小在HBase中，value永远和它的key一起传输的。当具体的值在系统间传输时，它的rowkey，列名，时间戳也会一起传输。如果你的rowkey和列名很大，甚至可以和具体的值相比较，那么你将会遇到一些有趣的问题。HBase storefiles中的索引（有助于随机访问）最终占据了HBase分配的大量内存，因为具体的值和它的key很大。可以增加block大小使得storefiles索引再更大的时间间隔增加，或者修改表的模式以减小rowkey和列名的大小。压缩也有助于更大的索引。

 列族尽可能越短越好，最好是一个字符

 冗长的属性名虽然可读性好，但是更短的属性名存储在HBase中会更好

```
# 原数据：以时间戳_user_id作为rowkey
# 时间戳高位变化不大，太连续，最终可能会导致热点问题
1638584124_user_id
1638584135_user_id
1638584146_user_id
1638584157_user_id
1638584168_user_id
1638584179_user_id

# 解决方案：加盐、反转、哈希

# 加盐
# 加上随即前缀，随机的打散
# 该过程无法预测 前缀时随机的
00_1638584124_user_id
05_1638584135_user_id
03_1638584146_user_id
04_1638584157_user_id
02_1638584168_user_id
06_1638584179_user_id

# 反转
# 适用于高位变化不大，低位变化大的rowkey
4214858361_user_id
5314858361_user_id
6414858361_user_id
7514858361_user_id
8614858361_user_id
9714858361_user_id

# 散列 md5、sha1、sha256......
25531D7065AE158AAB6FA53379523979_user_id
60F9A0072C0BD06C92D768DACF2DFDC3_user_id
D2EFD883A6C0198DA3AF4FD8F82DEB57_user_id
A9A4C265D61E0801D163927DE1299C79_user_id
3F41251355E092D7D8A50130441B58A5_user_id
5E6043C773DA4CF991B389D200B77379_user_id

# 时间戳"反转"
# rowkey：时间戳_user_id
# rowkey是字典升序的，那么越新的记录会被排在最后面，不容易被获取到
# 需求：让最新的记录排在最前面

# 大数：9999999999
# 大数-小数

1638584124_user_id => 8361415875_user_id
1638584135_user_id => 8361415864_user_id
1638584146_user_id => 8361415853_user_id
1638584157_user_id => 8361415842_user_id
1638584168_user_id => 8361415831_user_id
1638584179_user_id => 8361415820_user_id

1638586193_user_id => 8361413806_user_id


```

### 合理设计rowkey实战（电信）

```
手机号,网格编号,城市编号,区县编号,停留时间,进入时间,离开时间,时间分区
D55433A437AEC8D8D3DB2BCA56E9E64392A9D93C,117210031795040,83401,8340104,301,20180503190539,20180503233517,20180503


将用户位置数据保存到hbase
    查询需求
        1、通过手机号查询用户最近10条位置记录

        2、获取用户某一天在一个城市中的所有位置

    怎么设计hbase表
        1、rowkey
        2、时间戳
```

## 九、二级索引

> **二级索引的本质就是建立各列值与行键之间的映射关系**

**Hbase的局限性：**

　　HBase本身只提供基于行键和全表扫描的查询，而行键索引单一，对于多维度的查询困难。

 

**所以我们引进一个二级索引的概念**

### **常见的二级索引：**

HBase的一级索引就是rowkey，我们只能通过rowkey进行检索。如果我们相对hbase里面列族的列列进行一些组合查询，就需要采用HBase的二级索引方案来进行多条件的查询。 

  　　1. MapReduce方案 
    　　2. ITHBASE（Indexed-Transanctional HBase）方案 
      　　3. IHBASE（Index HBase）方案 
        　　4. Hbase Coprocessor(协处理器)方案 
          　　5. Solr+hbase方案  redis+hbase 方案

            　　6. CCIndex（complementalclustering index）方案

### **二级索引的种类**

```
　　1、创建单列索引

　　2、同时创建多个单列索引

　　3、创建联合索引（最多同时支持3个列）

　　4、只根据rowkey创建索引
```

**单表建立二级索引**

```
1.首先disable ‘表名’
2.然后修改表

alter 'LogTable',METHOD=>'table_att','coprocessor'=>'hdfs:///写好的Hbase协处理器（coprocessor）的jar包名|类的绝对路径名|1001'

3. enable '表名'
```

**二级索引的设计思路**

![image-20220613014320944](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220613014320944.png)

```
二级索引的本质就是建立各列值与行键之间的映射关系

如上图1，当要对F:C1这列建立索引时，只需要建立F:C1各列值到其对应行键的映射关系，如C11->RK1等，这样就完成了对F:C1列值的二级索引的构建，当要查询符合F:C1=C11对应的F:C2的列值时（即根据C1=C11来查询C2的值,图1青色部分）

其查询步骤如下：

1. 根据C1=C11到索引数据中查找其对应的RK，查询得到其对应的RK=RK1

2. 得到RK1后就自然能根据RK1来查询C2的值了 这是构建二级索引大概思路，其他组合查询的联合索引的建立也类似。
```

### **Mapreduce的方式创建二级索引**

使用整合MapReduce的方式创建hbase索引。主要的流程如下：

1.1扫描输入表，使用hbase继承类TableMapper

1.2获取rowkey和指定字段名称和字段值

1.3创建Put实例， value=” “, rowkey=班级，column=学号

1.4使用IdentityTableReducer将数据写入索引表

#### 案例：

> **1、在hbase中创建索引表 student_index**

```sql
create 'student_index','info'
```

> **2、编写mapreduce代码**

```java

```

> **3、打成jar包上传到hadoop中运行**

```shell
hadoop jar had-hbase-demo-1.0-SNAPSHOT-jar-with-dependencies.jar com.shujia.hbaseapi.hbaseindexdemo.HbaseIndex
```

> **4、编写查询代码，测试结果（先查询索引表，在查数据）**

```java

```

## 十、Phoenix二级索引

> 对于Hbase，如果想精确定位到某行记录，唯一的办法就是通过rowkey查询。如果不通过rowkey查找数据，就必须逐行比较每一行的值，对于较大的表，全表扫描的代价是不可接受的。

### 1、开启索引支持

```xml
# 关闭hbase集群
stop-hbase.sh

# 在/usr/local/soft/hbase-1.4.6/conf/hbase-site.xml中增加如下配置

<property>
  <name>hbase.regionserver.wal.codec</name>
  <value>org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec</value>
</property>
<property>
    <name>hbase.rpc.timeout</name>
    <value>60000000</value>
</property>
<property>
    <name>hbase.client.scanner.timeout.period</name>
    <value>60000000</value>
</property>
<property>
    <name>phoenix.query.timeoutMs</name>
    <value>60000000</value>
</property>


# 同步到所有节点
scp hbase-site.xml node1:`pwd`
scp hbase-site.xml node2:`pwd`

# 修改phoenix目录下的bin目录中的hbase-site.xml
<property>
    <name>hbase.rpc.timeout</name>
    <value>60000000</value>
</property>
<property>
    <name>hbase.client.scanner.timeout.period</name>
    <value>60000000</value>
</property>
<property>
    <name>phoenix.query.timeoutMs</name>
    <value>60000000</value>
</property>


# 启动hbase
start-hbase.sh
# 重新进入phoenix客户端
sqlline.py master,node1,node2
```

### 2、创建索引

#### 2.1、全局索引

> 全局索引适合读多写少的场景。如果使用全局索引，读数据基本不损耗性能，所有的性能损耗都来源于写数据。数据表的添加、删除和修改都会更新相关的索引表（数据删除了，索引表中的数据也会删除；数据增加了，索引表的数据也会增加）

> 注意: 对于全局索引在默认情况下，在查询语句中检索的列如果不在索引表中，Phoenix不会使用索引表将，除非使用hint。

```sql
手机号 进入网格的时间 离开网格的时间 区县编码 经度 纬度 基站标识 网格编号 业务类型

# 创建DIANXIN.sql
CREATE TABLE IF NOT EXISTS DIANXIN (
     mdn VARCHAR ,
     start_date VARCHAR ,
     end_date VARCHAR ,
     county VARCHAR,
     x DOUBLE ,
     y  DOUBLE,
     bsid VARCHAR,
     grid_id  VARCHAR,
     biz_type VARCHAR, 
     event_type VARCHAR , 
     data_source VARCHAR ,
     CONSTRAINT PK PRIMARY KEY (mdn,start_date)
) column_encoded_bytes=0;

# 上传数据DIANXIN.csv

# 导入数据
psql.py master,node1,node2 DIANXIN.sql DIANXIN.csv

# 创建全局索引
CREATE INDEX DIANXIN_INDEX ON DIANXIN ( end_date );

# 查询数据 ( 索引未生效)
select * from DIANXIN where end_date = '20180503154014';

# 强制使用索引 （索引生效） hint
select /*+ INDEX(DIANXIN DIANXIN_INDEX) */  * from DIANXIN where end_date = '20180503154014';

select /*+ INDEX(DIANXIN DIANXIN_INDEX) */  * from DIANXIN where end_date = '20180503154014'  and start_date = '20180503154614';

# 取索引列，（索引生效）
select end_date from DIANXIN where end_date = '20180503154014';

# 创建多列索引
CREATE INDEX DIANXIN_INDEX1 ON DIANXIN ( end_date,COUNTY );

# 多条件查询 （索引生效）
select end_date,MDN,COUNTY from DIANXIN where end_date = '20180503154014' and COUNTY = '8340104';

# 查询所有列 (索引未生效)
select  * from DIANXIN where end_date = '20180503154014'  and COUNTY = '8340104';

# 查询所有列 （索引生效）
select /*+ INDEX(DIANXIN DIANXIN_INDEX1) */ * from DIANXIN where end_date = '20180503154014' and COUNTY = '8340104';

# 单条件  (索引未生效)
select end_date from DIANXIN where  COUNTY = '8340103';
# 单条件  (索引生效) end_date 在前
select COUNTY from DIANXIN where end_date = '20180503154014';

# 删除索引
drop index DIANXIN_INDEX on DIANXIN;
```

#### 2.2、本地索引

> 本地索引适合写多读少的场景，或者存储空间有限的场景。和全局索引一样，Phoenix也会在查询的时候自动选择是否使用本地索引。本地索引因为索引数据和原数据存储在同一台机器上，避免网络数据传输的开销，所以更适合写多的场景。由于无法提前确定数据在哪个Region上，所以在读数据的时候，需要检查每个Region上的数据从而带来一些性能损耗。

> 注意:对于本地索引，查询中无论是否指定hint或者是查询的列是否都在索引表中，都会使用索引表。


```sql
# 创建本地索引
CREATE LOCAL INDEX DIANXIN_LOCAL_IDEX ON DIANXIN(grid_id);

# 索引生效
select grid_id from dianxin where grid_id='117285031820040';

# 索引生效
select * from dianxin where grid_id='117285031820040';

```

#### 2.3、覆盖索引

> 覆盖索引是把原数据存储在索引数据表中，这样在查询时不需要再去HBase的原表获取数据就，直接返回查询结果。

> 注意：查询是 select 的列和 where 的列都需要在索引中出现。

```sql
# 创建覆盖索引
CREATE INDEX DIANXIN_INDEX_COVER ON DIANXIN ( x,y ) INCLUDE ( county );

# 查询所有列 (索引未生效)
select * from DIANXIN where x=117.288 and y =31.822;

# 强制使用索引 (索引生效)
select /*+ INDEX(DIANXIN DIANXIN_INDEX_COVER) */ * from DIANXIN where x=117.288 and y =31.822;

# 查询索引中的列 (索引生效) mdn是DIANXIN表的RowKey中的一部分
select x,y,county from DIANXIN where x=117.288 and y =31.822;
select mdn,x,y,county from DIANXIN where x=117.288 and y =31.822;

# 查询条件必须放在索引中  select 中的列可以放在INCLUDE （将数据保存在索引中）
select /*+ INDEX(DIANXIN DIANXIN_INDEX_COVER) */ x,y,count(*) from DIANXIN group by x,y;

```

## 十一、Phoenix JDBC    

```xml
# 导入依赖
<dependency>
    <groupId>org.apache.phoenix</groupId>
    <artifactId>phoenix-core</artifactId>
    <version>4.15.0-HBase-1.4</version>
</dependency>
<dependency>
    <groupId>com.lmax</groupId>
    <artifactId>disruptor</artifactId>
    <version>3.4.2</version>
</dependency>
```





# HBase学习 HBase调优（五）                                                                

## 一、HBase索引案例（使用redis存储索引）

```
在这里是简单模拟将索引存到redis中，再通过先查询索引再将Hbase中的数据查询出来。

需要考虑的问题：

　　1、建立redis的连接，建立Hbase的连接
　　2、如何创建索引，即创建索引的key和value的设计
　　3、如何通过将查到的索引，去查询到对应Hbase的数据
```

> 添加依赖

```xml
        <!-- https://mvnrepository.com/artifact/redis.clients/jedis -->
        <dependency>
            <groupId>redis.clients</groupId>
            <artifactId>jedis</artifactId>
            <version>4.2.3</version>
        </dependency>
```

> 启动redis服务

```shell
nohup redis-server ./redis.conf &
```

> 代码编写



## 二、HBase优化

### 2.1	行键的设计（重点）

> 1. 实际上底层存储是按列族线性地存储单元格
>
> 2. 列包括了HBase特有的列族和列限定符，从而组成列键。
>
> 3. 磁盘上一个列族下所有的单元格都存储在一个存储文件中，不同列族的单元格不会出现在同一个存储文件中。
>
> 4. 每个单元格在实际存储时保存了行键和列键，所以每个单元格都单独存储了它在表中所处位置的相关信息。
>
> 5. 单元格按时间戳降序排列。
>
> 6. 含有结构信息的整个单元格在HBase中被叫做KeyValue。

![image-20220615221522565](C:\Users\ASUS\Desktop\hbase调优.assets\image-20220615221522565.png)

> 从存储结构上看，列限定符开始就需要检查每个送到过滤器的KeyValue。值筛选性能会更差。

![image-20220615221601047](C:\Users\ASUS\Desktop\hbase调优.assets\image-20220615221601047.png)

### 2.2	设计的时候，列不要太多（优先考虑高表,其次宽表）

> HBase只能按行分片，因此高表更有优势。

### 2.3	部分键扫描

> HBase的扫描功能和基于HTable的API更适合在高表上筛选数据，用户可以通过只包含部分键的扫描检索数据
> 行键是按字典序排序的，因此将行键进行设计，把每一个需要的字段都进行补齐，可以利用这种机制。

### 2.4	不同方式解决顺序读的性能变化对比图

![image-20220615221928420](C:\Users\ASUS\Desktop\hbase调优.assets\image-20220615221928420.png)

![image-20220615221941997](C:\Users\ASUS\Desktop\hbase调优.assets\image-20220615221941997.png)

### 2.5	布隆过滤器(重点)

> 在读取数据时，hbase会首先在布隆过滤器中查询，根据布隆过滤器的结果，再在MemStore中查询，最后再在对应的HFile中查询。

![image-20220615222312560](C:\Users\ASUS\Desktop\hbase调优.assets\image-20220615222312560.png)

> 当我们随机读get数据时，如果采用hbase的块索引机制，hbase会加载很多块文件。如果采用布隆过滤器后，它能够准确判断该HFile的所有数据块中，是否含有我们查询的数据，从而大大减少不必要的块加载，从而增加hbase集群的吞吐率。

### 2.6	选择合适的 GC 策略

> Hbase是java开发的，也是运行在java虚拟机jvm中，所以也可以通过GC参数配置调优
>
> 主要调节的是RegionServer节点的JVM垃圾回收参数
>
> 垃圾回收策略：Parraller New Collector垃圾回收策略； PC
>
> 并行标记回收器（Concurrent Mark-Sweep Collector），避免GC停顿,  CMS
>
> 使用：
>
> ​	一般用于用于写在hbase-env.sh
>
> ```bash
> export  HBASE_REGIONSERVER_OPTS="-Xmx8g -Xms8G -Xmn128m -XX:UseParNewGC -XX:UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:$HBASE_HOME/logs/gc-${hostname}-hbase.log"
> ```

### 2.7	HBase内存管理

> HBase上的Regionserver的内存主要分为两部分，一部分作为Memstore，主要用来写；一部分作为BlockCache，主要用于读。
> 写请求会先写入Memstore，Regionserver会给每个region的store提供一个Memstore，当Memstore满128M(hbase.hregion.memstore.flush.size)以后，会启动flush刷新到磁盘，当Memstore的总大小超过限制时（heapsize*hbase.regionserver.global.memstore.upperLimit*0.9），会强行启动flush进程，从最大的Memstore开始flush直到低于限制

![image-20220615223218352](C:\Users\ASUS\Desktop\hbase调优.assets\image-20220615223218352.png)

![image-20220615223231696](C:\Users\ASUS\Desktop\hbase调优.assets\image-20220615223231696.png)

> 读请求先到Memstore中查数据，查不到就到BlockCache中查，再查不到就会到磁盘上读，并把读的结果放入BlockCache。由于BlockCache采用的是LRU（最近最少使用）策略，因此BlockCache达到上限（heapsize*hfile.block.cache.size*0.85）后，会启动淘汰机制，淘汰掉最老的一批数据。
> 在注重读响应时间的应用场景下，可以将BlockCache设置大些，Memstore设置小些，以加大缓存命中率。

```xml
如果不希望自动触发溢写，就将值调大
 <name>hbase.hregion.memstore.flush.size</name>
    <value>134217728</value>
一般在企业中这个参数是禁用的

<name>hbase.hregion.majorcompaction</name>
    <value>604800000</value>
直接将值设置为0就可以了，表示禁用

何时执行split
<name>hbase.hregion.max.filesize</name>
    <value>10737418240</value>
一般建议将值调大，在期间手动去触发split
```

> Memstore刷写数据到磁盘时，造成RegionServer内存碎片增多，当生存时间较长的数据从堆的老年代空间刷写到磁盘，就会产生内存孔洞。由于碎片过多导致没有足够大的连续内存空间，JVM就会暂停工作进程，进行垃圾回收（GC），导致HBase的RegionServer对外服务停顿
> 本地Memstore缓存机制：启用本地memstore分配缓存区（Memstore-Local Allocation Buffers，MSLAB），也就是允许从堆中分配相同大小的对象，一旦这些对象分配并且最终被回收，就会在堆中留下固定大小的孔洞，这些孔洞可被重复利用，GC就无需使应用程序进程停顿来回收内存空间，配置参数hbase.hregion.memstore.mslab.enabled，默认为true。

### 2.8	预创建Region

> 创建HBase时，就预先根据可能的RowKey划分出多个Region而不是默认的一个，从而可以将后续的读写操作负载均衡到不同的Region上，避免热点现象；
> HBase表的预分区需要紧密结合业务场景来选择分区的key值，每个region都有一个startKey和一个endKey来表示该region存储的rowKey范围；

```shell
//有以下四种创建方式：
create 'ns1:t1' , 'f1' , SPLITS => ['10','20','30','40'] ;
create 't1','f1',SPLITS_FILE => 'splits.txt', OWNER=> 'johnode' ;
          ——其中splits.txt文件内容是每行一个rowkey值
create 't1','f1',{NUMREGIONS => 15, SPLITALGO =>'HexStringSplit'}

JavaAPI
            HTableDescriptor desc = new HTableDescriptor(TableName.valueOf(weibo_content));
            HColumnDescriptor family = new HColumnDescriptor(Bytes.toBytes("cf"));
            // 开启列簇 -- store的块缓存
            family.setBlockCacheEnabled(true);
            family.setBlocksize(1024 * 1024 * 2);

            family.setCompressionType(Algorithm.SNAPPY);

            family.setMaxVersions(1);
            family.setMinVersions(1);

            desc.addFamily(family);

            // admin.createTable(desc);
            byte[][] splitKeys = { Bytes.toBytes("100"), Bytes.toBytes("200"), Bytes.toBytes("300") };
            admin.createTable(desc, splitKeys);
```

### 2.9	避免Region热点

> 热点现象：某个小的时段内，对HBase的读写请求集中到极少数的Region上，导致这些Region所在的RegionServer处理请求量骤增，负载量明显偏大，而其他的RegionServer明显空闲；
> 出现的原因：主要是因为Hbase表设计时，rowKey设计不合理造成的；
> 解决办法：Rowkey的随机散列+创表预分区
>
> RowKey设计原则：
> 1、总的原则：避免热点现象，提高读写性能；
> 2、**长度**原则：最大长度64KB，开发通常8个字节倍数，因为Hbase中每个单元格是以key-value进行存储的，因此每个value都会存储rowkey，所以rowkey越来越占空间；
> 3、**散列**原则：将时间上连续产生的rowkey散列化，以避免集中到极少数Region上
> 4、**唯一**原则：必须在设计上保证rowkey的唯一性
> RowKey设计结合业务：
> 在满足rowkey设计原则的基础上，往往需要将经常用于查询的字段整合到rowkey上，以提高检索查询效率

### 2.10	Hbase参数调优

**hbase.regionserver.handler.count**

该设置决定了处理RPC的线程数量，默认值是10，通常可以调大，比如：150，当请求内容很大（上MB，比如大的put、使用缓存的scans）的时候，如果该值设置过大则会占用过多的内存，导致频繁的GC，或者出现OutOfMemory，因此该值不是越大越好。

 

**hbase.hregion.max.filesize**

配置region大小，默认是10G，region大小一般控制在几个G比较合适，可以在建表时规划好region数量，进行预分区，做到一定时间内，每个region的数据大小在一定的数据量之下，当发现有大的region，或者需要对整个表进行region扩充时再进行split操作，一般提供在线服务的hbase集群均会弃用hbase的自动split，转而自己管理split。

 

 **hbase.hregion.majorcompaction**

配置major合并的间隔时间，默认值604800000，单位ms。表示major compaction默认7天调度一次，HBase 0.96.x及之前默认为1天调度一次，设置为 0 时表示禁用自动触发major compaction。一般major compaction持续时间较长、系统资源消耗较大，对上层业务也有比较大的影响，一般生产环境下为了避免影响读写请求，会禁用自动触发major compaction，可手动或者通过脚本定期进行major合并。

 

**hbase.hstore.compaction.min**

默认值 3，一个列族下的HFile数量超过该值就会触发Minor Compaction，这个参数默认值小了，一般情况下建议调大到5~10之间，注意相应调整下一个参数

 

**hbase.hstore.compaction.max**

默认值 10，一次Minor Compaction最多合并的HFile文件数量，这个参数基本控制着一次压缩即Compaction的耗时。这个参数要比上一个参数hbase.hstore.compaction.min值大，通常是其2~3倍。



**hbase.hstore.blockingStoreFiles**

默认值 10，一个列族下HFile数量达到该值，flush操作将会受到阻塞，阻塞时间为hbase.hstore.blockingWaitTime，默认90000，即1.5分钟，在这段时间内，如果compaction操作使得HFile下降到blockingStoreFiles配置值，则停止阻塞。另外阻塞超过时间后，也会恢复执行flush操作。这样做可以有效地控制大量写请求的速度，但同时这也是影响写请求速度的主要原因之一。生产环境中默认值太小了，一般建议设置大点比如100，避免出现阻塞更新的情况

 

**hbase.regionserver.global.memstore.size**

默认值0.4，RS所有memstore占用内存在总内存中的比例，当达到该值，则会从整个RS中找出最需要flush的region进行flush，直到总内存比例降至该数限制以下，并且在降至限制比例前，将阻塞所有的写memstore的操作，在以写为主的集群中，可以调大该配置项，不建议太大，因为block cache和memstore cache的总大小不会超过0.8，而且不建议这两个cache的大小总和达到或者接近0.8，避免OOM，在偏向写的业务时，可配置为0.45



**hbase.regionserver.global.memstore.size.lower.limit**

默认值0.95，相当于上一个参数的0.95

如果有 16G 堆内存，默认情况下:

* ```
  # 达到该值会触发刷写
  16 * 0.4 * 0.95 = 6.08
  # 达到该值会触发阻塞
  16 * 0.4 = 6.4
  ```



| 新参数                                              | 老参数                                        |
| :-------------------------------------------------- | :-------------------------------------------- |
| hbase.regionserver.global.memstore.size             | hbase.regionserver.global.memstore.upperLimit |
| hbase.regionserver.global.memstore.size.lower.limit | hbase.regionserver.global.memstore.lowerLimit |



**hfile.block.cache.size**

RS的block cache的内存大小限制，默认值0.4，在偏向读的业务中，可以适当调大该值，具体配置时需试hbase集群服务的业务特征，结合memstore的内存占比进行综合考虑。

 

**hbase.hregion.memstore.flush.size**

默认值128M，单位字节，超过将被flush到hdfs，该值比较适中，一般不需要调整。

 

 **hbase.hregion.memstore.block.multiplier**

默认值4，如果memstore的内存大小已经超过了hbase.hregion.memstore.flush.size的4倍，则会阻塞memstore的写操作，直到降至该值以下，为避免发生阻塞，最好调大该值，比如：6，不可太大，如果太大，则会增大导致整个RS的memstore内存超过global.memstore.size限制的可能性，进而增大阻塞整个RS的写的几率，如果region发生了阻塞会导致大量的线程被阻塞在到该region上，从而其它region的线程数会下降，影响整体的RS服务能力。



**hbase.regionserver.regionSplitLimit**

控制最大的region数量，超过则不可以进行split操作，默认是Integer.MAX（2147483647），可设置为1，禁止自动的split，通过人工，或者写脚本在集群空闲时执行。如果不禁止自动的split，则当region大小超过hbase.hregion.max.filesize时会触发split操作（具体的split有一定的策略，不仅仅通过该参数控制，前期的split会考虑region数据量和memstore大小），每次flush或者compact之后，regionserver都会检查是否需要Split，split会先下线老region再上线split后的region，该过程会很快，但是会存在两个问题：1、老region下线后，新region上线前client访问会失败，在重试过程中会成功但是如果是提供实时服务的系统则响应时长会增加；2、split后的compact是一个比较耗资源的动作。

 

**hbase.regionserver.maxlogs**：默认值32，HLOG最大的数量

**hbase.regionserver.hlog.blocksize**：默认为 2 倍的*HDFS block size(128MB)*，即256MB

 JVM调整：

内存大小：master默认为1G，可增加到2G，regionserver默认1G，可调大到10G，或者更大，zk并不耗资源，可以不用调整，需要注意的是，调整了rs的内存大小后，需调整hbase.regionserver.maxlogs和hbase.regionserver.hlog.blocksize这两个参数，WAL的最大值由hbase.regionserver.maxlogs * hbase.regionserver.hlog.blocksize决定（默认32\*2\*128M=8G），一旦达到这个值，就会被触发flush memstore，如果memstore的内存增大了，但是没有调整这两个参数，实际上对大量小文件没有任何改进，调整策略：hbase.regionserver.hlog.blocksize * hbase.regionserver.maxlogs 设置为略大于hbase.regionserver.global.memstore.size* HBASE_HEAPSIZE。

---

#### 什么时候触发 MemStore Flush?

有很多情况会触发 MemStore 的 Flush 操作，主要有以下几种情况：

* **Region 中任意一个 MemStore 占用的内存超过相关阈值**

  > ​		当一个 Region 中所有 MemStore 占用的内存大小超过刷写阈值的时候会触发一次刷写，这个阈值由 hbase.hregion.memstore.flush.size 参数控制，默认为128MB。我们每次调用 put、delete 等操作都会检查的这个条件的。
  >
  > ​		但是如果我们的数据增加得很快，达到了 hbase.hregion.memstore.flush.size * hbase.hregion.memstore.block.multiplier 的大小，hbase.hregion.memstore.block.multiplier 默认值为4，也就是128*4=512MB的时候，那么除了触发 MemStore 刷写之外，HBase 还会在刷写的时候同时阻塞所有写入该 Store 的写请求！这时候如果你往对应的 Store 写数据，会出现 RegionTooBusyException 异常。

* **整个 RegionServer 的 MemStore 占用内存总和大于相关阈值**

  > ​		如果达到了 RegionServer 级别的 Flush，那么当前 RegionServer 的所有写操作将会被阻塞，而且这个阻塞可能会持续到分钟级别。

* **WAL数量大于相关阈值或WAL的大小超过一定阈值**

  > ​        如果设置了 `hbase.regionserver.maxlogs`，那就是这个参数的值；否则是 `max(32, hbase_heapsize * hbase.regionserver.global.memstore.size * 2 / logRollSize)`
  >
  > (logRollSize 默认大小为：0.95 * HDFS block size)
  >
  > 如果某个 RegionServer 的 WAL 数量大于 `maxLogs` 就会触发 MemStore 的刷写。
  >
  > ​		WAL的最大值由hbase.regionserver.maxlogs * hbase.regionserver.hlog.blocksize决定（默认32\*2\*128M=8G），一旦达到这个值，就会被触发flush memstore，如果memstore的内存增大了，但是没有调整这两个参数，实际上对大量小文件没有任何改进，调整策略：hbase.regionserver.hlog.blocksize * hbase.regionserver.maxlogs 设置为略大于hbase.regionserver.global.memstore.size* HBASE_HEAPSIZE。

* **定期自动刷写**

  > ​		如果我们很久没有对 HBase 的数据进行更新，这时候就可以依赖定期刷写策略了。RegionServer 在启动的时候会启动一个线程 PeriodicMemStoreFlusher 每隔 hbase.server.thread.wakefrequency 时间（服务线程的sleep时间，默认10000毫秒）去检查属于这个 RegionServer 的 Region 有没有超过一定时间都没有刷写，这个时间是由 hbase.regionserver.optionalcacheflushinterval 参数控制的，默认是 3600000，也就是1小时会进行一次刷写。如果设定为0，则意味着关闭定时自动刷写。
  >
  > ​		为了防止一次性有过多的 MemStore 刷写，定期自动刷写会有 0 ~ 5 分钟的延迟

* **数据更新超过一定阈值**

  > 如果 HBase 的某个 Region 更新的很频繁，而且既没有达到自动刷写阀值，也没有达到内存的使用限制，但是内存中的更新数量已经足够多，比如超过 `hbase.regionserver.flush.per.changes` 参数配置，默认为30000000，那么也是会触发刷写的。

* **手动触发刷写**

  > 分别对某张表、某个 Region 进行刷写操作。
  >
  > 可以在 Shell 中执行 flush 命令

---

#### 什么操作会触发 MemStore 刷写？

常见的 put、delete、append、incr、调用 flush 命令、Region 分裂、Region Merge、bulkLoad HFiles 以及给表做快照操作都会对上面的相关条件做检查，以便判断要不要做刷写操作。

#### MemStore 刷写策略（FlushPolicy）

在 HBase 1.1 之前，MemStore 刷写是 Region 级别的。就是说，如果要刷写某个 MemStore ，MemStore 所在的 Region 中其他 MemStore 也是会被一起刷写的！这会造成一定的问题，比如小文件问题。可以通过 hbase.regionserver.flush.policy 参数选择不同的刷写策略。

> 目前 HBase 2.x 的刷写策略全部都是实现 FlushPolicy 抽象类的。并且自带三种刷写策略：FlushAllLargeStoresPolicy、FlushNonSloppyStoresFirstPolicy 以及 FlushAllStoresPolicy。

* **FlushAllStoresPolicy**

  >这种刷写策略实现最简单，直接返回当前 Region 对应的所有 MemStore。也就是每次刷写都是对 Region 里面所有的 MemStore 进行的，这个行为和 HBase 1.1 之前是一样的。

* **FlushAllLargeStoresPolicy**

  > 在 HBase 2.0 之前版本是 FlushLargeStoresPolicy，后面被拆分成分 FlushAllLargeStoresPolicy 和FlushNonSloppyStoresFirstPolicy
  >
  > 这种策略会先判断 Region 中每个 MemStore 的使用内存是否大于某个阀值，大于这个阀值的 MemStore 将会被刷写
  >
  > hbase.hregion.percolumnfamilyflush.size.lower.bound.min 默认值为 16MB
  >
  > hbase.hregion.percolumnfamilyflush.size.lower.bound 没有默认值，计算规则如下：
  >
  > 比如当前表有3个列族，那么 flushSizeLowerBound = max((long)128 / 3, 16) = 42。
  >
  > 如果 Region 中没有 MemStore 的使用内存大于上面的阀值，FlushAllLargeStoresPolicy 策略就退化成 FlushAllStoresPolicy 策略了，也就是会对 Region 里面所有的 MemStore 进行 Flush。

* **FlushNonSloppyStoresFirstPolicy**

  > HBase 2.0 引入了 in-memory compaction，如果我们对相关列族 hbase.hregion.compacting.memstore.type 参数的值不是 NONE，那么这个 MemStore 的 isSloppyMemStore 值就是 true，否则就是 false。
  >
  > FlushNonSloppyStoresFirstPolicy 策略将 Region 中的 MemStore 按照 isSloppyMemStore 分到两个 HashSet 里面（sloppyStores 和 regularStores）。然后
  >
  > - 判断 regularStores 里面是否有 MemStore 内存占用大于相关阀值的 MemStore ，有的话就会对这些 MemStore 进行刷写，其他的不做处理，这个阀值计算和 FlushAllLargeStoresPolicy 的阀值计算逻辑一致。
  > - 如果 regularStores 里面没有 MemStore 内存占用大于相关阀值的 MemStore，这时候就开始在 sloppyStores 里面寻找是否有 MemStore 内存占用大于相关阀值的 MemStore，有的话就会对这些 MemStore 进行刷写，其他的不做处理。
  > - 如果上面 sloppyStores 和 regularStores 都没有满足条件的 MemStore 需要刷写，这时候就 FlushNonSloppyStoresFirstPolicy 策略久退化成 FlushAllStoresPolicy 策略了。





# 实验1-1：`HBase`入门介绍及数据模型

## 实验概述

在大数据相关的应用之中，假设你要存储用户的地址和喜好。这些当然可以存储到关系型数据库之中，但是假设用户从上海搬到了北京。那么之前上海的地址就要update覆盖掉吗？这种应用场景下，我们需要计算分析用户的整个人生周期的活动记录和喜好，进而推测他的行为、收入、知识层次、信用等等。这些历史行为是不能被丢弃的，`HBase`可以很好的适应这样的场景。所以在大数据的特定应用场景下`Hbase`的价值还是非常重要的！

那么，本实验的任务就是先搞清楚`Hbase`为什么重要，了解它的数据模型！

## 实验环境

- AtStudy 实训平台
- `Hadoop`
- `HBase`

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16871374638552843.png)

## 实验目标

学习完成本实验后，您将能够

- 为什么需要`HBase`
- 了解`HBase`和HDFS、Hive不同之处
- 了解`HBase`的数据模型

## 实验任务

### 任务一、`HBase`简介

#### **【任务目标】**

认识`HBase`，认清`HBase`和HDFS和Hive的不同之处。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab1_1/image-20210824110318276.png?fileid=5576678020614762285)

视频-1、HBase简介

##### 1.1 Hadoop局限

- 从 1970 年开始，大多数的公司数据存储和维护使用的是关系型数据库
- 大数据技术出现后，很多拥有海量数据的公司开始选择像Hadoop的方式来存储海量数据
- Hadoop使用分布式文件系统HDFS来存储海量数据，并使用 MapReduce 来处理。Hadoop擅长于存储各种格式的庞大的数据，任意的格式甚至非结构化的处理

**但是，**![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab1_1/warning.png)

- Hadoop主要是实现批量数据的处理，并且通过顺序方式访问数据
- 要查找数据必须搜索整个数据集， 如果要进行随机读取数据，效率较低，更别说修改了！

`Hbase`就出现了！

##### 1.2 `HBase`简介

官方地址：`https://hbase.apache.org/`

![image-20230613144208439](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab1_1/image-20230613144208439.png)

- `NoSQL`是一个通用术语，泛指一个数据库并不是使用SQL作为主要语言的非关系型数据库

- `HBase`是建立在HDFS之上，提供高可靠性、高性能、列存储、可伸缩、实时读写`NoSQL`的数据库系统

- `HBase`仅能通过主键(row key)和主键的range来检索数据，仅支持单行事务

- 主要用来存储结构化和半结构化的松散数据

- `HBase`查询数据功能很简单，不支持join等复杂操作，不支持复杂的事务（行级的事务），从技术上来说，`HBase`更像是一个「数据存储」而不是「数据库」，因为`HBase`缺少RDBMS中的许多特性，例如带类型的列、二级索引以及高级查询语言等

- 与Hadoop一样，`HBase`目标主要依靠横向扩展，通过不断增加廉价的商用服务器，来增加存储和处理能力，例如，把集群从10个节点扩展到20个节点，存储能力和处理能力都会加倍

- ```
  HBase
  ```

  中的表一般有这样的特点

  - 大：一个表可以有上十亿行，上百万列
  - 面向列:面向列(族)的存储和权限控制，列(族)独立检索
  - 稀疏:对于为空(null)的列，并不占用存储空间，因此，表可以设计的非常稀疏

##### 1.3 `HBase`和HDFS、Hive不同之处

###### 1.3.1 和HDFS的不同

- HDFS是一个非常适合存储大型文件的分布式文件系统
- HDFS它不是一个通用的文件系统，也无法在文件中快速查询某个数据

而`HBase`

- `HBase`构建在HDFS之上，并为大型表提供快速记录查找(和更新)
- `HBase`内部将大量数据放在HDFS中名为「`StoreFiles`」的索引中，以便进行高速查找
- `Hbase`比较适合做快速查询等需求，而不适合做大规模的`OLAP`应用

###### 1.3.2 和Hive的不同

- ```
  Hive
  ```

  是数据仓库工具

  - 本质其实就相当于将`HDFS`中已经存储的文件在`Mysql`中做了一个双射关系，以方便使用`HQL`去管理查询

- ```
  Hive
  ```

  用于数据分析、清洗

  - 适用于离线的数据分析和清洗，延迟较高

- ```
  Hive
  ```

  基于

  ```
  HDFS
  ```

  、

  ```
  MapReduce
  ```

  - 存储的数据依旧在`DataNode`上，编写的`HQL`语句终将是转换为`MapReduce`代码执行

而`HBase`

- 是`NoSQL`数据库，一种面向列存储的非关系型数据库。
- 用于存储结构化和非结构化的数据，适用于单表非关系型数据的存储，不适合做关联查询，类似`JOIN`等操作。
- 基于`HDFS`，数据持久化存储的体现形式是`Hfile`，存放于DataNode中，被`ResionServer`以`region`的形式进行管理
- 延迟较低，接入在线业务使用，面对大量的企业数据，`HBase`可以直线单表大量数据的存储，同时提供了高效的数据访问速度

不过，这两种工具是可以同时使用的。就像用Google来搜索，用`FaceBook`进行社交一样，Hive可以用来进行统计查询，`HBase`可以用来进行实时查询，数据也可以从Hive写到`HBase`，或者从`HBase`写回Hive

### 任务二、`HBase`数据模型

#### **【任务目标】**

启动`HBase`，熟悉`HBase`环境，并掌握它的数据模型。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab1_1/image-20210824110318276.png?fileid=5576678020616136862)

视频-2、HBase数据模型

##### 2.1 `HBase`环境准备

###### 2.1.1 启动`HBase`

打开终端

![image-20230613170550386](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab1_1/image-20230613170550386.png)

环境生效：

```
source /etc/profile复制代码
```

![image-20230613171121438](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab1_1/image-20230613171121438.png)

启动服务

```
myhadoop.sh start # 启动Hadoop服务复制代码
```

![image-20230613171314457](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab1_1/image-20230613171314457.png)

```
zk.sh start 	# 启动zookeeper服务复制代码
```

![image-20230613171352365](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab1_1/image-20230613171352365.png)

```
start-hbase.sh  #启动hbase服务复制代码
```

![image-20230613171946915](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab1_1/image-20230613171946915.png)

目前服务进程情况，使用`JPS`命令查看

```
jps复制代码
```

![image-20230613172055268](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab1_1/image-20230613172055268.png)

验证`HBase`是否启动成功

```
# 启动hbase shell客户端
hbase shell
# 输入status

[root@1d7212a0a479 ~]# hbase shell
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/module/hbase/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/module/hadoop-3.1.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
HBase Shell
Use "help" to get list of supported commands.
Use "exit" to quit this interactive shell.
For Reference, please visit: http://hbase.apache.org/2.0/book.html#shell
Version 2.0.5, r76458dd074df17520ad451ded198cd832138e929, Mon Mar 18 00:41:49 UTC 2019
Took 0.0024 seconds                                                                                                               
hbase(main):001:0> status
1 active master, 0 backup masters, 1 servers, 0 dead, 59.0000 average load
Took 0.4198 seconds                                                                                                               
hbase(main):002:0> 复制代码
```

###### 2.1.2 `WebUI`

```
http://localhost:16010/master-status
```

|                                                              |
| ------------------------------------------------------------ |
| ![image-20230613172743379](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab1_1/image-20230613172743379.png) |

安装目录说明

| 目录名          | 说明                                |
| --------------- | ----------------------------------- |
| `bin`           | 所有`hbase`相关的命令都在该目录存放 |
| `conf`          | 所有的`hbase`配置文件               |
| `hbase-webapps` | `hbase`的`web ui`程序位置           |
| `lib`           | `hbase`依赖的java库                 |
| `logs`          | `hbase`的日志文件                   |

##### 2.2 `HBase`数据模型

在`HBASE`中，数据存储在具有行和列的表中。这是看起来关系数据库(RDBMS)一样，但将`HBASE`表看成是多个维度的Map结构更容易理解。

![image-20230613173106101](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab1_1/image-20230613173106101.png)![image-20230613173128784](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab1_1/image-20230613173128784.png)

###### 2.2.1 表（Table）

- `HBase`中数据都是以表形式来组织的
- `HBase`中的表由多个行组成

在`HBase WebUI`（`http://localhost:16010`中可以查看到目前`HBase`中的表）

|                                                              |
| ------------------------------------------------------------ |
| ![image-20230614091538892](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab1_1/image-20230614091538892.png) |

###### 2.2.2 行（row）

- `HBASE`中的行由一个`rowkey`（行键）和一个或多个列组成，列的值与`rowkey`、列相关联
- 行在存储时按行键按字典顺序排序
- 行键的设计非常重要，尽量让相关的行存储在一起
  - 例如：存储网站域。如行键是域，则应该将域名反转后存储(`org.apache.www`、`org.apache.mail`、`org.apache.jira`)。这样，所有Apache域都在表中存储在一起，而不是根据子域的第一个字母展开

###### 2.2.3 列（Column）

- `HBASE`中的列由列蔟（Column Family）和列标识符（Column Qualifier）组成
- 表示如下——列蔟名:列限定符名。例如：`C1:USER_ID`、`C1:SEX`

###### 2.2.4 列簇（Column Family）

![image-20230614092034702](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab1_1/image-20230614092034702.png)

- 出于性能原因，列蔟将一组列及其值组织在一起
- 每个列蔟都有一组存储属性，例如：
  - 是否应该缓存在内存中
  - 数据如何被压缩或行键如何编码等
- 表中的每一行都有相同的列蔟，但在列蔟中不存储任何内容
- 所有的列蔟的数据全部都存储在一块（文件系统`HDFS`）
- `HBase`官方建议所有的列蔟保持一样的列，并且将同一类的列放在一个列蔟中

###### 2.2.5 列标识符（Column Qualifier）

- 列蔟中包含一个个的列限定符，这样可以为存储的数据提供索引
- 列蔟在创建表的时候是固定的，但列限定符是不作限制的
- 不同的行可能会存在不同的列标识符

###### 2.2.6 单元格（Cell）

- 单元格是行、列系列和列限定符的组合
- 包含一个值和一个时间戳（表示该值的版本）
- 单元格中的内容是以二进制存储的

![image-20230614092326088](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab1_1/image-20230614092326088.png)

##### 2.3 概念模型

![image-20230614092456942](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab1_1/image-20230614092456942.png)

- 上述表格有两行、三个列蔟（`contens`、`ancho`、`people`）
- `com.cnn.www`这一行anchor列蔟两个列（`anchor:cssnsi.com`、`anchor:my.look.ca`）、`contents`列蔟有个1个列（`html`）
- `com.cnn.www`在`HBase`中有 `t3、t5、t6、t8、t9` 5个版本的数据
- `HBase`中如果某一行的列被更新的，那么最新的数据会排在最前面，换句话说同一个`rowkey`的数据是按照倒序排序的





# 实验2-1：HBase的Shell操作

## 实验概述

在使用`HBase`过程中，可以以shell的方式来维护和管理HBase。例如：执行建表语句、执行增删改查操作等等。本实验主要就是介绍Shell的常用操作和管理。

## 实验环境

- AtStudy 实训平台
- Hadoop
- `HBase`

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16871375349944235.png)

## 实验目标

学习完成本实验后，您将能够

- 掌握Shell基础操作
- 掌握Shell复杂查询操作
- 掌握常用的Shell管理命令

## 实验任务

### 任务一、Shell基础操作

#### **【任务目标】**

掌握HBase使用过程中常用的Shell命令。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_1/image-20210824110318276.png?fileid=5576678020614270928)

视频-1、Shell基础操作

##### 1.1 需求

有以下订单数据，我们想要将这样的一些数据保存到HBase中。

| 订单ID | 订单状态 | 支付金额 | 支付方式ID | 用户ID | 操作时间          | 商品分类 |
| ------ | -------- | -------- | ---------- | ------ | ----------------- | -------- |
| 001    | 已付款   | 200.5    | 1          | 001    | 2020-5-2 18:08:53 | 手机;    |

接下来，我们将使用HBase shell来进行以下操作：

1. 创建表
2. 添加数据
3. 更新数据
4. 删除数据
5. 查询数据

##### 1.2 创建表

在HBase中，所有的数据也都是保存在表中的。要将订单数据保存到HBase中，首先需要将表创建出来。

HBase的shell其实JRuby的IRB（交互式的Ruby），但在其中添加了一些HBase的命令。

启动HBase shell：

```
hbase shell复制代码
```

![image-20230614095007202](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_1/image-20230614095007202.png)

- 1、创建表

  语法：

  ```
  create '表名','列蔟名'...复制代码
  ```

  创建订单表，表名为`ORDER_INFO`，该表有一个列蔟为`C1`

  ```
  create 'ORDER_INFO','C1';复制代码
  ```

  注意：

  - create要写成小写
  - 一个表可以包含若干个列蔟

- 2、查看表

  ```
  hbase(main):005:0> list
  TABLE                                                                         
  ORDER_INFO
  1 row(s)
  Took 0.0378 seconds                                                             
  => ["ORDER_INFO"]复制代码
  ```

- 3、删除表

  要删除某个表，必须要先禁用表

  - 禁用表：`disable "表名"`
  - 删除表：`drop "表名"`

  ```
  disable "ORDER_INFO"
  drop "ORDER_INFO"复制代码
  ```

##### 1.3 CRUD操作

接下来主要介绍常用的：添加数据、查看数据、更新数据、删除数据操作

###### 1.3.1 添加数据

接下来，我们需要往订单表中添加以下数据。

| 订单ID | 订单状态 | 支付金额  | 支付方式ID | 用户ID  | 操作时间            | 商品分类 |
| ------ | -------- | --------- | ---------- | ------- | ------------------- | -------- |
| ID     | STATUS   | PAY_MONEY | PAYWAY     | USER_ID | OPERATION_DATE      | CATEGORY |
| 000001 | 已提交   | 4070      | 1          | 4944191 | 2020-04-25 12:09:16 | 手机;    |

`HBase`中的`put`命令，可以用来将数据保存到表中。但`put`一次只能保存一个列的值。以下是`put`的语法结构：

```
put '表名','ROWKEY','列蔟名:列名','值'复制代码
```

要添加以上的数据，需要使用7次`put`操作。如下：

```
put 'ORDER_INFO','000001','C1:ID','000001'
put 'ORDER_INFO','000001','C1:STATUS','已提交'
put 'ORDER_INFO','000001','C1:PAY_MONEY',4070
put 'ORDER_INFO','000001','C1:PAYWAY',1
put 'ORDER_INFO','000001','C1:USER_ID',4944191
put 'ORDER_INFO','000001','C1:OPERATION_DATE','2020-04-25 12:09:16'
put 'ORDER_INFO','000001','C1:CATEGORY','手机;'复制代码
```

###### 1.3.2 查看数据

在`HBase`中，可以使用`get`命令来获取单独的一行数据。语法：

```
get '表名','rowkey'复制代码
```

查询指定订单ID的数据

```
get 'ORDER_INFO','000001'复制代码
```

![image-20230614100346973](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_1/image-20230614100346973.png)

在`HBase shell`中，如果在数据中出现了一些中文，默认`HBase shell`中显示出来的是十六进制编码。要想将这些编码显示为中文，我们需要在get命令后添加一个属性：{FORMATTER => 'toString'}

```
get 'ORDER_INFO','000001', {FORMATTER => 'toString'}复制代码
```

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_1/image-20210823165604377.png)**注意！**

- `{ key => value}`，这个是`Ruby`语法，表示定义一个`HASH`结构
- `get`是一个`HBase Ruby`方法，`’ORDER_INFO’、’000001’、{FORMATTER => 'toString'}`是`put`方法的三个参数
- `FORMATTER`要使用**大写**

###### 1.3.3 更新操作

需求：将订单ID为000001的状态，更改为「已付款」

同样，在HBase中，也是使用put命令来进行数据的更新，语法与之前的添加数据一模一样。

```
put 'ORDER_INFO', '000001', 'C1:STATUS', '已付款'复制代码
```

注意：

- HBase中会自动维护数据的版本
- 每当执行一次put后，都会重新生成新的时间戳

```
C1:STATUS   timestamp=1588748844082, value=已提交
C1:STATUS   timestamp=1588748952074, value=已付款
C1:STATUS   timestamp=1588748994244, value=已付款复制代码
```

###### 1.3.4 删除操作

- delete命令

  在`HBase`中，可以使用`delete`命令来将一个单元格的数据删除。语法格式如下：

  ```
  delete '表名', 'rowkey', '列蔟:列'。复制代码
  ```

  注意：此处`HBase`默认会保存多个时间戳的版本数据，所以这里的`delete`删除的是最新版本的列数据。

  ```
  # 删除指定的列
  delete 'ORDER_INFO','000001','C1:STATUS'复制代码
  ```

- deleteall命令

  deleteall命令可以将指定rowkey对应的所有列全部删除。语法：`deleteall '表名','rowkey'`

  ```
  # 删除指定的订单
  deleteall 'ORDER_INFO','000001'复制代码
  ```

- truncate命令

  truncate命令用来清空某个表中的所有数据。语法：`truncate "表名"`

  ```
  # 清空ORDER_INFO的所有数据
  truncate 'ORDER_INFO'复制代码
  ```

### 任务二、Shell查询

#### **【任务目标】**

在使用`HBase`过程中经常需要使用扫描命令，本任务主要重点就是掌握扫描命令。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_1/image-20210824110318276.png?fileid=5576678020615636855)

视频-2、Shell查询

##### 2.1 数据准备

在提供的数据集`ORDER_INFO.txt` 中，有一份这样的HBase数据集，我们需要将这些指令放到HBase中执行，将数据导入到HBase中。

可以看到这些都是一堆的put语句。那么如何才能将这些语句全部执行呢？

![image-20230614102313343](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_1/image-20230614102313343.png)

首先需要将该数据集文件上传到指定的目录中

使用以下命令执行：

```
hbase shell /opt/software/ORDER_INFO.txt复制代码
```

![image-20230614102533210](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_1/image-20230614102533210.png)

##### 2.2 扫描操作

查看ORDER_INFO表中所有的数据

###### 2.2.1 需求一：查询订单所有数据

在HBase，我们可以使用scan命令来扫描HBase中的表。语法：`scan '表名'`

```
scan 'ORDER_INFO',{FORMATTER => 'toString'}复制代码
```

注意：要避免scan一张大表！

###### 2.2.2 需求二：查询订单数据（只显示3条）

```
scan 'ORDER_INFO', {LIMIT => 3, FORMATTER => 'toString'}复制代码
```

###### 2.2.3 需求三：查询订单状态、支付方式

```
scan 'ORDER_INFO', {LIMIT => 3, COLUMNS => ['C1:STATUS', 'C1:PAYWAY'], FORMATTER => 'toString'}复制代码
```

注意：`[‘C1:STATUS’, …]`在Ruby中[]表示一个数组

###### 2.2.4 需求四：查询指定订单ID的数据并以中文展示

根据ROWKEY来查询对应的数据，ROWKEY为`02602f66-adc7-40d4-8485-76b5632b5b53`，只查询订单状态、支付方式，并以中文展示。

要查询指定ROWKEY的数据，需要使用`ROWPREFIXFILTER`，用法为：

```
scan '表名', {ROWPREFIXFILTER => 'rowkey'}复制代码
```

实现指令：

```
scan 'ORDER_INFO', {ROWPREFIXFILTER => '02602f66-adc7-40d4-8485-76b5632b5b53', COLUMNS => ['C1:STATUS', 'C1:PAYWAY'], FORMATTER => 'toString'}复制代码
```

##### 2.3 过滤器

在`HBase`中，如果要对海量的数据来进行查询，此时基本的操作是比较无力的。此时，需要借助`HBase`中的高级语法——`Filter`来进行查询。`Filter`可以根据列簇、列、版本等条件来对数据进行过滤查询。因为在`HBase`中，主键、列、版本都是有序存储的，所以借助`Filter`，可以高效地完成查询。当执行`Filter`时，`HBase`会将`Filter`分发给各个`HBase`服务器节点来进行查询。

在HBase的shell中，通过show_filters指令，可以查看到HBase中内置的一些过滤器。

```
show_filters复制代码
```

![image-20230614104019421](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_1/image-20230614104019421.png)

在HBase中过滤器比较多，这里我们介绍一下常用的过滤器，掌握其基本用法。

过滤器一般结合scan命令来使用，基本语法：

```
scan '表名', { Filter => "过滤器(比较运算符, '比较器表达式')” }复制代码
```

- 过滤器

| 名称                    | 介绍                                 |
| ----------------------- | ------------------------------------ |
| RowFilter               | 实现行键字符串的比较和过滤           |
| SingleColumnValueFilter | 在指定的列蔟和列中进行比较的值过滤器 |

- 比较运算符

| 比较运算符 | 描述     |
| ---------- | -------- |
| =          | 等于     |
| >          | 大于     |
| >=         | 大于等于 |
| <          | 小于     |
| <=         | 小于等于 |
| !=         | 不等于   |

- 比较器表达式

  基本语法：比较器类型:比较器的值

| 比较器                 | 表达式语言缩写           |
| ---------------------- | ------------------------ |
| BinaryComparator       | `binary:值`              |
| BinaryPrefixComparator | `binaryprefix:值`        |
| BitComparator          | `bit:值`                 |
| NullComparator         | `null`                   |
| RegexStringComparator  | `regexstring:正则表达式` |
| SubstringComparator    | `substring:值`           |

###### 2.3.1 需求一：使用RowFilter查询指定订单ID的数据

需求：只查询订单的ID为：`02602f66-adc7-40d4-8485-76b5632b5b53`、订单状态以及支付方式

**分析**

1. 因为要订单ID就是ORDER_INFO表的rowkey，所以，我们应该使用rowkey过滤器来过滤

2. 通过RowFilter构造器，了解其基本用法

   ```
   public RowFilter(CompareOperator op, ByteArrayCompareable rowComparator)复制代码
   ```

   - op——比较运算符
   - rowComparator——比较器

使用RowFilter查询指定订单的数据实现如下：

```
scan 'ORDER_INFO', {FILTER => "RowFilter(=,'binary:02602f66-adc7-40d4-8485-76b5632b5b53')"}复制代码
```

![image-20230614105138601](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_1/image-20230614105138601.png)

###### 2.3.2 需求二：查询状态为已付款的订单

需求：查询状态为「已付款」的订单

**分析**

1. 因为此处要指定列来进行查询，所以，我们不再使用rowkey过滤器，而是要使用列过滤器

2. 我们要针对指定列和指定值进行过滤，比较适合使用SingleColumnValueFilter过滤器，查看JAVA API

   ```
   public SingleColumnValueFilter(byte[] family,
                                  byte[] qualifier,
                                  CompareOperator op,
                                  ByteArrayComparable comparator)复制代码
   ```

   需要传入四个参数：

   - 列簇
   - 列标识（列名）
   - 比较运算符
   - 比较器

查询状态为已付款的订单，实现如下：

```
scan 'ORDER_INFO', {FILTER => "SingleColumnValueFilter('C1', 'STATUS', =, 'binary:已付款')", FORMATTER => 'toString'}复制代码
```

注意：

- 列名`STATUS`的大小写一定要对！此处使用的是大写！
- 列名写错了查不出来数据，但`HBase`不会报错，因为HBase是无模式的

###### 2.3.3 需求三：查询支付方式为1，且金额大于3000的订单

**分析**

- 此处需要使用多个过滤器共同来实现查询，多个过滤器，可以使用AND或者OR来组合多个过滤器完成查询
- 使用SingleColumnValueFilter实现对应列的查询

实现过程：

1. 查询支付方式为1

   ```
   SingleColumnValueFilter('C1', 'PAYWAY', = , 'binary:1')复制代码
   ```

2. 查询金额大于3000的订单

   ```
   SingleColumnValueFilter('C1', 'PAY_MONEY', > , 'binary:3000')复制代码
   ```

3. 组合查询

   ```
   scan 'ORDER_INFO', {FILTER => "SingleColumnValueFilter('C1', 'PAYWAY', = , 'binary:1') AND SingleColumnValueFilter('C1', 'PAY_MONEY', > , 'binary:3000')", FORMATTER => 'toString'}复制代码
   ```

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_1/image-20210823165604377.png)**注意：**

- HBase shell中比较默认都是字符串比较，所以如果是比较数值类型的，会出现不准确的情况
- 例如：在字符串比较中4000是比100000大的

### 任务三、Shell管理操作

#### **【任务目标】**

掌握一些Hbase的管理命令。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_1/image-20210824110318276.png?fileid=5576678020615640762)

视频-3、Shell管理操作

##### 3.1 **status**

例如：显示服务器状态

```
2.4.1 :062 > status
1 active master, 0 backup masters, 3 servers, 0 dead, 1.0000 average load
Took 0.0034 seconds   复制代码
```

##### 3.2 **whoami**

显示HBase当前用户，例如：

```
2.4.1 :066 > whoami
root (auth:SIMPLE)
    groups: root
Took 0.0080 seconds复制代码
```

##### 3.3 **list**

显示当前所有的表

```
2.4.1 :067 > list
TABLE                                                           
ORDER_INFO                                                        
1 row(s)
Took 0.0266 seconds                                                    
 => ["ORDER_INFO"]复制代码
```

##### 3.4 **count**

统计指定表的记录数，例如：

```
2.4.1 :070 > count 'ORDER_INFO'
66 row(s)
Took 0.0404 seconds                                                          
 => 66复制代码
```

##### 3.5 **describe**

展示表结构信息

```
2.4.1 :074 > describe 'ORDER_INFO'
Table ORDER_INFO is ENABLED                                                          
ORDER_INFO                                               
COLUMN FAMILIES DESCRIPTION                                       
{NAME => 'C1', VERSIONS => '1', EVICT_BLOCKS_ON_CLOSE => 'false', NEW_VERSION_BEHAVIOR => 'false', KEEP_DELETED_CELLS => 'FALSE', CACHE_DATA_ON_WRITE =>'false',DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', MIN_VERSIONS =>'0',REPLICATION_SCOPE => '0', BLOOMFILTER => 'ROW', CACHE_INDEX_ON_WRITE => 'false', IN_MEMORY => 'false', CACHE_BLOOMS_ON_WRITE => 'false',PREFETCH_BLOCKS_ON_OPEN =>'false', COMPRESSION => 'NONE', BLOCKCACHE => 'true', BLOCKSIZE => '65536'}         1 row(s)
Took 0.0265 seconds 复制代码
```

##### 3.6 **exists**

检查表是否存在，适用于表量特别多的情况

```
2.4.1 :075 > exists 'ORDER_INFO'
Table ORDER_INFO does exist                                           
Took 0.0050 seconds                                                      
 => true复制代码
```

##### 3.7 **is_enabled、is_disabled**

检查表是否启用或禁用

```
2.4.1 :077 > is_enabled 'ORDER_INFO'
true                                                     
Took 0.0058 seconds                                                          
 => true 
2.4.1 :078 > is_disabled 'ORDER_INFO'
false                                                            
Took 0.0085 seconds                                                             
 => 1复制代码
```

##### 3.8 **disable/enable**

禁用一张表/启用一张表

##### 3.9 **drop**

删除一张表，记得在删除表之前必须先禁用

##### 3.10 **truncate**

清空表的数据，禁用表-删除表-创建表





# 实验2-2：HBase的Java操作

## 实验概述

HBase进行数据存储，一般在业务场景中还需要采用Java程序来访问这些数据，本实验的主要任务就是使用Java来操作HBase，并了解Hbase的基础架构

## 实验环境

- AtStudy 实训平台
- Hadoop
- HBase

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/1687137600304107.png)

## 实验目标

学习完成本实验后，您将能够

- 掌握Java创建项目的过程
- 掌握Java访问HBase过程
- 了解Hbase的基础架构

## 实验任务

### 任务一、HBase Java编程

#### **【任务目标】**

掌握Java程序访问HBase的过程

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_2/image-20210824110318276.png?fileid=5576678020615634844)

视频-1、HBase Java编程

##### 1.1 需求与数据集

某某自来水公司，需要存储大量的缴费明细数据。以下截取了缴费明细的一部分内容。

| 用户id  | 姓名   | 用户地址                     | 性别 | 缴费时间   | 表示数（本次） | 表示数（上次） | 用量（立方） | 合计金额 | 查表日期   | 最迟缴费日期 |
| ------- | ------ | ---------------------------- | ---- | ---------- | -------------- | -------------- | ------------ | -------- | ---------- | ------------ |
| 4944191 | 登卫红 | 贵州省铜仁市德江县7单元267室 | 男   | 2020-05-10 | 308.1          | 283.1          | 25           | 150      | 2020-04-25 | 2020-06-09   |

因为缴费明细的数据记录非常庞大，该公司的信息部门决定使用HBase来存储这些数据。并且，他们希望能够通过Java程序来访问这些数据。

##### 1.2 项目准备

###### 1.2.1 创建Maven项目

|            |             |
| ---------- | ----------- |
| groupId    | com.atstudy |
| artifactId | hbase_op    |

![image-20230614111819680](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_2/image-20230614111819680.png)

###### 1.2.2 导入依赖

```
    <repositories><!-- 代码库 -->
        <repository>
            <id>aliyun</id>
            <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
            <releases>
                <enabled>true</enabled>
            </releases>
            <snapshots>
                <enabled>false</enabled>
                <updatePolicy>never</updatePolicy>
            </snapshots>
        </repository>
    </repositories>

    <dependencies>
        <dependency>
            <groupId>org.apache.hbase</groupId>
            <artifactId>hbase-client</artifactId>
            <version>2.1.0</version>
        </dependency>
        <dependency>
            <groupId>commons-io</groupId>
            <artifactId>commons-io</artifactId>
            <version>2.6</version>
        </dependency>
        <dependency>
            <groupId>junit</groupId>
            <artifactId>junit</artifactId>
            <version>4.12</version>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.testng</groupId>
            <artifactId>testng</artifactId>
            <version>6.14.3</version>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.1</version>
                <configuration>
                    <target>1.8</target>
                    <source>1.8</source>
                </configuration>
            </plugin>
        </plugins>
    </build>复制代码
```

###### 1.2.3 复制HBase和Hadoop配置文件

将以下三个配置文件复制到resource目录中，也可以通过本地进行资料上传。

- hbase-site.xml
  - 从Linux中进行复制：`cp /opt/module/hbase/conf/hbase-site.xml ../../resource`
- core-site.xml
  - 从Linux中进行复制：`cp /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml ../../resource`
- log4j.properties

###### 1.2.4 创建包结构和类

1. 在**test**目录创建`com.atstudy.hbase.admin.api_test`包结构
2. 创建`TableAmdinTest`类

![image-20230614112739323](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_2/image-20230614112739323.png)

###### 1.2.5 创建Hbase连接以及admin管理对象

要操作Hbase也需要建立Hbase的连接。此处我们仍然使用TestNG来编写测试。使用@BeforeTest初始化HBase连接，创建admin对象、@AfterTest关闭连接。

实现步骤：

1. 使用HbaseConfiguration.create()创建Hbase配置
2. 使用ConnectionFactory.createConnection()创建Hbase连接
3. 要创建表，需要基于Hbase连接获取admin管理对象
4. 使用admin.close、connection.close关闭连接

```
public class TableAmdinTest {

    private Configuration configuration;
    private Connection connection;
    private Admin admin;

    @BeforeTest
    public void beforeTest() throws IOException {
        configuration = HBaseConfiguration.create();
        connection = ConnectionFactory.createConnection(configuration);
        admin = connection.getAdmin();
    }

    @AfterTest
    public void afterTest() throws IOException {
        admin.close();
        connection.close();
    }
}复制代码
```

##### 1.3 Java访问HBase

###### 1.3.1 需求一：使用Java代码创建表

创建一个名为WATER_BILL的表，包含一个列蔟C1。

实现步骤：

1. 判断表是否存在

   a) 存在，则退出

2. 使用`TableDescriptorBuilder.newBuilder`构建表描述构建器

3. 使用`ColumnFamilyDescriptorBuilder.newBuilder`构建列蔟描述构建器

4. 构建列蔟描述，构建表描述

5. 创建表

```
// 创建一个名为WATER_BILL的表，包含一个列蔟C1
@Test
public void createTableTest() throws IOException {
    // 表名
    String TABLE_NAME = "WATER_BILL";
    // 列蔟名
    String COLUMN_FAMILY = "C1";

    // 1. 判断表是否存在
    if(admin.tableExists(TableName.valueOf(TABLE_NAME))) {
        return;
    }

    // 2. 构建表描述构建器
    TableDescriptorBuilder tableDescriptorBuilder = TableDescriptorBuilder.newBuilder(TableName.valueOf(TABLE_NAME));

    // 3. 构建列蔟描述构建器
    ColumnFamilyDescriptorBuilder columnFamilyDescriptorBuilder = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(COLUMN_FAMILY));

    // 4. 构建列蔟描述
    ColumnFamilyDescriptor columnFamilyDescriptor = columnFamilyDescriptorBuilder.build();

    // 5. 构建表描述
    // 添加列蔟
    tableDescriptorBuilder.setColumnFamily(columnFamilyDescriptor);
    TableDescriptor tableDescriptor = tableDescriptorBuilder.build();

    // 6. 创建表
    admin.createTable(tableDescriptor);
}复制代码
```

执行代码：

![image-20230614113455210](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_2/image-20230614113455210.png)

执行结果

![image-20230614114014862](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_2/image-20230614114014862.png)

查看表

```
hbase(main):005:0> list
TABLE                                                                            
WATER_BILL                                                                         
1 row(s)
Took 0.0378 seconds                                                                
=> ["WATER_BILL"]复制代码
```

###### 1.3.2 需求二：往表中插入一条数据

- 创建包

  1. 在 test 目录中创建 com.atstudy.hbase.data.api_test 包
  2. 创建DataOpTest类

- 初始化Hbase连接

  在@BeforeTest中初始化HBase连接，在@AfterTest中关闭Hbase连接。

  ```
  public class DataOpTest {
      private Configuration configuration;
      private Connection connection;
      
      @BeforeTest
      public void beforeTest() throws IOException {
          configuration = HBaseConfiguration.create();
          connection = ConnectionFactory.createConnection(configuration);
      }
      
      @AfterTest
      public void afterTest() throws IOException {
          connection.close();
      }
  }复制代码
  ```

- 插入姓名列数据

  在表中插入一个行，该行只包含一个列。

  | ROWKEY  | 姓名（列名：NAME） |
  | ------- | ------------------ |
  | 4944191 | 登卫红             |

  实现步骤：

  1. 使用Hbase连接获取Htable
  2. 构建ROWKEY、列蔟名、列名
  3. 构建Put对象（对应put命令）
  4. 添加姓名列
  5. 使用Htable表对象执行put操作
  6. 关闭Htable表对象

  ```
  @Test
  public void addTest() throws IOException {
      // 1.使用Hbase连接获取Htable
      TableName waterBillTableName = TableName.valueOf("WATER_BILL");
      Table waterBillTable = connection.getTable(waterBillTableName);
  
      // 2.构建ROWKEY、列蔟名、列名
      String rowkey = "4944191";
      String cfName = "C1";
      String colName = "NAME";
  
      // 3.构建Put对象（对应put命令）
      Put put = new Put(Bytes.toBytes(rowkey));
  
      // 4.添加姓名列
      put.addColumn(Bytes.toBytes(cfName)
          , Bytes.toBytes(colName)
          , Bytes.toBytes("登卫红"));
  
      // 5.使用Htable表对象执行put操作
      waterBillTable.put(put);
      // 6. 关闭表
      waterBillTable.close();
  }复制代码
  ```

- 查看HBase中的数据

  ```
  get 'WATER_BILL','4944191',{FORMATTER => 'toString'}复制代码
  ```

  ![image-20230614132148179](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_2/image-20230614132148179.png)

- 插入其他列

  | **列名**     | **说明**       | **值**                       |
  | ------------ | -------------- | ---------------------------- |
  | ADDRESS      | 用户地址       | 贵州省铜仁市德江县7单元267室 |
  | SEX          | 性别           | 男                           |
  | PAY_DATE     | 缴费时间       | 2020-05-10                   |
  | NUM_CURRENT  | 表示数（本次） | 308.1                        |
  | NUM_PREVIOUS | 表示数（上次） | 283.1                        |
  | NUM_USAGE    | 用量（立方）   | 25                           |
  | TOTAL_MONEY  | 合计金额       | 150                          |
  | RECORD_DATE  | 查表日期       | 2020-04-25                   |
  | LATEST_DATE  | 最迟缴费日期   | 2020-06-09                   |

  ```
  @Test
  public void addTest() throws IOException {
      // 1.使用Hbase连接获取Htable
      TableName waterBillTableName = TableName.valueOf("WATER_BILL");
      Table waterBillTable = connection.getTable(waterBillTableName);
  
      // 2.构建ROWKEY、列蔟名、列名
      String rowkey = "4944191";
      String cfName = "C1";
      String colName = "NAME";
      String colADDRESS = "ADDRESS";
      String colSEX = "SEX";
      String colPAY_DATE = "PAY_DATE";
      String colNUM_CURRENT = "NUM_CURRENT";
      String colNUM_PREVIOUS = "NUM_PREVIOUS";
      String colNUM_USAGE = "NUM_USAGE";
      String colTOTAL_MONEY = "TOTAL_MONEY";
      String colRECORD_DATE = "RECORD_DATE";
      String colLATEST_DATE = "LATEST_DATE";
  
      // 3.构建Put对象（对应put命令）
      Put put = new Put(Bytes.toBytes(rowkey));
  
      // 4.添加姓名列
      put.addColumn(Bytes.toBytes(cfName)
              , Bytes.toBytes(colName)
              , Bytes.toBytes("登卫红"));
      put.addColumn(Bytes.toBytes(cfName)
              , Bytes.toBytes(colADDRESS)
              , Bytes.toBytes("贵州省铜仁市德江县7单元267室"));
      put.addColumn(Bytes.toBytes(cfName)
              , Bytes.toBytes(colSEX)
              , Bytes.toBytes("男"));
      put.addColumn(Bytes.toBytes(cfName)
              , Bytes.toBytes(colPAY_DATE)
              , Bytes.toBytes("2020-05-10"));
      put.addColumn(Bytes.toBytes(cfName)
              , Bytes.toBytes(colNUM_CURRENT)
              , Bytes.toBytes("308.1"));
      put.addColumn(Bytes.toBytes(cfName)
              , Bytes.toBytes(colNUM_PREVIOUS)
              , Bytes.toBytes("283.1"));
      put.addColumn(Bytes.toBytes(cfName)
              , Bytes.toBytes(colNUM_USAGE)
              , Bytes.toBytes("25"));
      put.addColumn(Bytes.toBytes(cfName)
              , Bytes.toBytes(colTOTAL_MONEY)
              , Bytes.toBytes("150"));
      put.addColumn(Bytes.toBytes(cfName)
              , Bytes.toBytes(colRECORD_DATE)
              , Bytes.toBytes("2020-04-25"));
      put.addColumn(Bytes.toBytes(cfName)
              , Bytes.toBytes(colLATEST_DATE)
              , Bytes.toBytes("2020-06-09"));
  
      // 5.使用Htable表对象执行put操作
      waterBillTable.put(put);
  
      // 6. 关闭表
      waterBillTable.close();
  }复制代码
  ```

###### 1.3.3 需求三：查看一条数据

查询rowkey为4944191的所有列的数据，并打印出来。

实现步骤：

1. 获取HTable
2. 使用rowkey构建Get对象
3. 执行get请求
4. 获取所有单元格
5. 打印rowkey
6. 迭代单元格列表
7. 关闭表

```
@Test
public void getOneTest() throws IOException {
    // 1. 获取HTable
    TableName waterBillTableName = TableName.valueOf("WATER_BILL");
    Table waterBilltable = connection.getTable(waterBillTableName);

    // 2. 使用rowkey构建Get对象
    Get get = new Get(Bytes.toBytes("4944191"));

    // 3. 执行get请求
    Result result = waterBilltable.get(get);

    // 4. 获取所有单元格
    List<Cell> cellList = result.listCells();

    // 打印rowkey
    System.out.println("rowkey => " + Bytes.toString(result.getRow()));

    // 5. 迭代单元格列表
    for (Cell cell : cellList) {
        // 打印列蔟名
        System.out.print(Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()));
        System.out.println(" => " + Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));

    }

    // 6. 关闭表
    waterBilltable.close();
}复制代码
```

###### 1.3.4 需求四：删除一条数据

删除rowkey为4944191的整条数据。

实现步骤：

1. 获取HTable对象
2. 根据rowkey构建delete对象
3. 执行delete请求
4. 关闭表

```
// 删除rowkey为4944191的整条数据
@Test
public void deleteOneTest() throws IOException {
    // 1. 获取HTable对象
    Table waterBillTable = connection.getTable(TableName.valueOf("WATER_BILL"));

    // 2. 根据rowkey构建delete对象
    Delete delete = new Delete(Bytes.toBytes("4944191"));

    // 3. 执行delete请求
    waterBillTable.delete(delete);

    // 4. 关闭表
    waterBillTable.close();
}复制代码
```

###### 1.3.5 需求六：查询2020年6月份所有用户的用水量

在Java API中，我们也是使用scan + filter来实现过滤查询。2020年6月份其实就是从2020年6月1日到2020年6月30日的所有抄表数据。

- 准备工作

  1. 在com.atstudy.hbase.data.api_test包下创建ScanFilterTest类
  2. 使用@BeforeTest、@AfterTest构建HBase连接、以及关闭HBase连接

- 实现步骤：

  1. 获取表

  2. 构建scan请求对象

  3. 构建两个过滤器

     a) 构建两个日期范围过滤器 注意：此处请使用RECORD_DATE——抄表日期比较

     b) 构建过滤器列表

  4. 执行scan扫描请求

  5. 迭代打印result

  6. 迭代单元格列表

  7. 关闭ResultScanner（这玩意把转换成一个个的类似get的操作，注意要关闭释放资源）

  8. 关闭表

  ```
  // 查询2020年6月份所有用户的用水量数据
  @Test
  public void queryTest1() throws IOException {
      // 1. 获取表
      Table waterBillTable = connection.getTable(TableName.valueOf("WATER_BILL"));
      // 2. 构建scan请求对象
      Scan scan = new Scan();
      // 3. 构建两个过滤器
      // 3.1 构建日期范围过滤器（注意此处请使用RECORD_DATE——抄表日期比较
      SingleColumnValueFilter startDateFilter = new SingleColumnValueFilter(Bytes.toBytes("C1")
              , Bytes.toBytes("RECORD_DATE")
              , CompareOperator.GREATER_OR_EQUAL
              , Bytes.toBytes("2020-06-01"));
  
      SingleColumnValueFilter endDateFilter = new SingleColumnValueFilter(Bytes.toBytes("C1")
              , Bytes.toBytes("RECORD_DATE")
              , CompareOperator.LESS_OR_EQUAL
              , Bytes.toBytes("2020-06-30"));
  
      // 3.2 构建过滤器列表
      FilterList filterList = new FilterList(FilterList.Operator.MUST_PASS_ALL
              , startDateFilter
              , endDateFilter);
  
      scan.setFilter(filterList);
  
      // 4. 执行scan扫描请求
      ResultScanner resultScan = waterBillTable.getScanner(scan);
  
      // 5. 迭代打印result
      for (Result result : resultScan) {
          System.out.println("rowkey -> " + Bytes.toString(result.getRow()));
          System.out.println("------");
  
          List<Cell> cellList = result.listCells();
  
          // 6. 迭代单元格列表
          for (Cell cell : cellList) {
              // 打印列蔟名
              System.out.print(Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()));
              System.out.println(" => " + Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
  
          }
          System.out.println("------");
      }
  
  resultScanner.close();
  
  
      // 7. 关闭表
      waterBillTable.close();
  }复制代码
  ```

- 解决乱码问题

  因为前面我们的代码，在打印所有的列时，都是使用字符串打印的，Hbase中如果存储的是int、double，那么有可能就会乱码了。

  ```
  System.out.print(Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength()));
  System.out.println(" => " + Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));复制代码
  ```

  要解决的话，我们可以根据列来判断，使用哪种方式转换字节码。如下：

  1. NUM_CURRENT
  2. NUM_PREVIOUS
  3. NUM_USAGE
  4. TOTAL_MONEY

  这4列使用double类型展示，其他的使用string类型展示。

  ```
  String colName = Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength());
  System.out.print(colName);
  
  if(colName.equals("NUM_CURRENT")
          || colName.equals("NUM_PREVIOUS")
          || colName.equals("NUM_USAGE")
          || colName.equals("TOTAL_MONEY")) {
      System.out.println(" => " + Bytes.toDouble(cell.getValueArray(), cell.getValueOffset()));
  }
  else {
      System.out.println(" => " + Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
  }复制代码
  ```

### 任务二、HBase 基础架构

#### **【任务目标】**

了解HBase基础架构

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_2/image-20210824110318276.png?fileid=5576678020616396076)

视频-2、HBase 基础架构

##### 2.1 HBase系统架构

![image-20230614134227944](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_2/image-20230614134227944.png)

###### 2.1.2 Master Server

在HBase的Web UI中，可以查看到Master的位置。

![image-20230614135341420](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_2/image-20230614135341420.png)

- 监控RegionServer
- 处理RegionServer故障转移
- 处理元数据的变更
- 处理region的分配或移除
- 在空闲时间进行数据的负载均衡
- 通过Zookeeper发布自己的位置给客户端

###### 2.1.2 Region Server

![image-20230614140612960](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_2/image-20230614140612960.png)

- 处理分配给它的Region
- 负责存储HBase的实际数据
- 刷新缓存到HDFS
- 维护HLog
- 执行压缩
- 负责处理Region分片

RegionServer中包含了大量丰富的组件，如下：

- Write-Ahead logs
- HFile(StoreFile)
- Store
- MemStore
- Region

##### 2.2 RegionServer逻辑结构模型

![image-20230614140711282](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_2/image-20230614140711282.png)

- Region

  在HBASE中，表被划分为很多「Region」，并由Region Server提供服务

  ![image-20230614140855594](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab2_2/image-20230614140855594.png)

- Store

  Region按列蔟垂直划分为「Store」，存储在HDFS在文件中

- MemStore

  MemStore与缓存内存类似

  当往HBase中写入数据时，首先是写入到MemStore

  每个列蔟将有一个MemStore

  当MemStore存储快满的时候，整个数据将写入到HDFS中的HFile中

- StoreFile

  每当任何数据被写入HBASE时，首先要写入MemStore

  当MemStore快满时，整个排序的key-value数据将被写入HDFS中的一个新的HFile中

  写入HFile的操作是连续的，速度非常快

  物理上存储的是**HFile**

- WAL

  WAL全称为Write Ahead Log，它最大的作用就是 故障恢复

  WAL是HBase中提供的一种高并发、持久化的日志保存与回放机制

  每个业务数据的写入操作（PUT/DELETE/INCR），都会保存在WAL中

  一旦服务器崩溃，通过回放WAL，就可以实现恢复崩溃之前的数据

  物理上存储是Hadoop的**Sequence File**







# 实验3-1：微信海量存储实战（上）

## 实验概述

在微信中，每天都有数千万的用户聊天消息需要存储。而且，这些消息都是需要进行大量地保存，而读取会少很多。一般在使用的时候，大多数我们都是在发消息，而不是每时每刻查询历史消息。要存储这样海量的数据， `HBase`就非常适合了，`HBase`本身也非常适合存储这种写多读少的应用场景。本案例，将结合聊天业务背景，以`HBase`来存储海量的数据。

## 实验环境

- AtStudy 实训平台
- Hadoop
- `HBase`

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16871377863431578.png)

## 实验目标

学习完成本实验后，您将能够

- `HBase`表的设计
- `HBase`表预分区
- `ROWKEY`设计
- 实现预分区过程

## 实验任务

### 任务一、项目准备

#### **【任务目标】**

完成项目之前的准备工作，包括命名空间、列簇设计以及数据压缩。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab3_1/image-20210824110318276.png?fileid=5576678020616138199)

视频-1、项目准备

##### 1.1 数据集及项目工程创建

微信打招呼消息数据集介绍

| 字段名              | 说明           |
| ------------------- | -------------- |
| msg_time            | 消息时间       |
| sender_nickyname    | 发件人昵称     |
| sender_account      | 发件人账号     |
| sender_sex          | 发件人性别     |
| sender_ip           | 发件人IP       |
| sender_os           | 发件人系统     |
| sender_phone_type   | 发件人手机型号 |
| sender_network      | 发件人网络制式 |
| sender_gps          | 发件人GPS      |
| receiver_nickyname  | 收件人昵称     |
| receiver_ip         | 收件人IP       |
| receiver_account    | 收件人账号     |
| receiver_os         | 收件人系统     |
| receiver_phone_type | 收件人手机型号 |
| receiver_network    | 收件人网络制式 |
| receiver_gps        | 收件人GPS      |
| receiver_sex        | 收件人性别     |
| msg_type            | 消息类型       |
| distance            | 双方距离       |
| message             | 消息           |

创建`IDEA Maven·项目

|              |                   |
| ------------ | ----------------- |
| `groupId`    | `com.atstudy`     |
| `artifactId` | `weixin_chat_app` |

![image-20230614144807087](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab3_1/image-20230614144807087.png)

##### 1.2 名称空间

- 在一个项目中，需要使用`HBase`保存多张表，这些表会按照业务域来划分
- 为了方便管理，不同的业务域以名称空间（`namespace`)来划分，这样管理起来会更加容易
- 类似于Hive中的数据库，不同的数据库下可以放不同类型的表
- `HBase`默认的名称空间是「`default`」，默认情况下，创建表时表都将创建在 `default` 名称空间下
- `HBase`中还有一个命名空间「`hbase`」，用于存放系统的内建表（`namespace、meta`）

1、创建命名空间

```
create_namespace 'WEIXIN_CHAT'复制代码
```

2、查看命名空间列表

```
list_namespace复制代码
```

3、查看命名空间

```
describe_namespace 'WEIXIN_CHAT'复制代码
```

4、命名空间创建表

在命令`WEIXIN_CHAT`命名空间下创建名为：`MSG`的表，该表包含一个名为`C1`的列蔟。

```
create 'WEIXIN_CHAT:MSG','C1'	复制代码
```

注意：带有命名空间的表，使用**冒号**将命名空间和表名连接到一起。

5、删除命名空间

删除命名空间，命名空间中必须没有表，如果命名空间中有表，是无法删除的

```
drop_namespace 'WEIXIN_CHAT'复制代码
```

##### 1.3 列蔟设计

- `HBase`列蔟的数量应该越少越好
  - 两个及以上的列蔟`HBase`性能并不是很好
  - 一个列蔟所存储的数据达到`flush`的阈值时，表中所有列蔟将同时进行`flush`操作
  - 这将带来不必要的`I/O`开销，列蔟越多，对性能影响越大
- 本次项目中我们只设计一个列蔟：`C1`

##### 1.4 **版本设计**

###### 1.4.1 **说明**

此处，我们需要保存的历史聊天记录是不会更新的，一旦数据保存到`HBase`中，就不会再更新

- 无需考虑版本问题
- 本次项目中只保留一个版本即可，这样可以节省大量空间
- `HBase`默认创建表的版本为 1，故此处保持默认即可

###### 1.4.2 **查看表**

通过以下输出可以看到：

- 版本是相对于列蔟而言
- 默认列蔟的版本数为1

```
describe "WEIXIN_CHAT:MSG"复制代码
```

![image-20230614145748075](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab3_1/image-20230614145748075.png)

##### 1.5 **数据压缩**

###### 1.5.1 **压缩算法**

在`HBase`可以使用多种压缩编码，包括`LZO、SNAPPY、GZIP`。只在硬盘压缩，内存中或者网络传输中没有压缩。

| 压缩算法     | 压缩后占比 | 压缩     | 解压缩   |
| ------------ | ---------- | -------- | -------- |
| GZIP         | 13.4%      | 21 MB/s  | 118 MB/s |
| LZO          | 20.5%      | 135 MB/s | 410 MB/s |
| Zippy/Snappy | 22.2%      | 172 MB/s | 409 MB/s |

- `GZIP`的压缩率最高，但是其实CPU密集型的，对CPU的消耗比其他算法要多，压缩和解压速度也慢；
- `LZO`的压缩率居中，比`GZIP`要低一些，但是压缩和解压速度明显要比`GZIP`快很多，其中解压速度快的更多；
- `Zippy/Snappy`的压缩率最低，而压缩和解压速度要稍微比`LZO`要快一些
- **本案例采用`GZ`算法，这样可以确保的压缩比最大化，更加节省空间**

###### 1.5.2 **查看表数据压缩方式**

通过以下输出可以看出，`HBase`创建表默认是没有指定压缩算法的

```
describe "WEIXIN_CHAT:MSG"复制代码
```

![image-20230614150026291](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab3_1/image-20230614150026291.png)

###### 1.5.3 **设置数据压缩**

本案例中，我们使用`GZ`压缩算法，语法如下：

- 创建新的表，并指定数据压缩算法

  ```
  create "WEIXIN_CHAT:MSG", {NAME => "C1", COMPRESSION => "GZ"}复制代码
  ```

- 修改已有的表，并指定数据压缩算法

  ```
  alter "WEIXIN_CHAT:MSG", {NAME => "C1", COMPRESSION => "GZ"}复制代码
  ```

![image-20230614150213634](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab3_1/image-20230614150213634.png)

### 任务二、`ROWKEY`设计

#### **【任务目标】**

掌握`ROWKEY`如何设计。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab3_1/image-20210824110318276.png?fileid=5576678020615244782)

视频-2、ROWKEY设计

##### 2.1 `HBase`官方的设计原则

- 避免使用递增行键/时序数据

  如果`ROWKEY`设计的都是按照顺序递增（例如：时间戳），这样会有很多的数据写入时，负载都在一台机器上。我们尽量应当将写入大压力均衡到各个`RegionServer`

- 避免`ROWKEY`和列的长度过大

  - 在`HBase`中，要访问一个Cell（单元格），需要有`ROWKEY`、列蔟、列名，如果`ROWKEY`、列名太大，就会占用较大内存空间。所以`ROWKEY`和列的长度应该尽量短小
  - `ROWKEY`的最大长度是`64KB`，建议越短越好

- 使用long等类型比String类型更省空间

- long类型为8个字节，8个字节可以保存非常大的无符号整数，例如：18446744073709551615。如果是字符串，是按照一个字节一个字符方式保存，需要快3倍的字节数存储。

- `ROWKEY`唯一性

  - 设计`ROWKEY`时，必须保证`RowKey`的唯一性
  - 由于在`HBase`中数据存储是Key-Value形式，若向`HBase`中同一张表插入相同`RowKey`的数据，则原先存在的数据会被新的数据覆盖。

##### 2.2 避免数据热点

- 热点是指大量的客户端（client）直接访问集群的一个或者几个节点（可能是读、也可能是写）
- 大量地访问量可能会使得某个服务器节点超出承受能力，导致整个`RegionServer`的性能下降，其他的`Region`也会受影响

###### 2.2.1 预分区

默认情况，一个`HBase`的表只有一个`Region`，被托管在一个`RegionServer`中

|                                                              |
| ------------------------------------------------------------ |
| ![image-20230614165343362](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab3_1/image-20230614165343362.png) |

- 每个Region有两个重要的属性：`Start Key`、`End Key`，表示这个`Region`维护的`ROWKEY`范围
- 如果只有一个`Region`，那么`tart Key`、`End Key`都是空的，没有边界。所有的数据都会放在这个`Region`中，但当数据越来越大时，会将`Region`分裂，取一个`Mid Key`来分裂成两个`Region`
- 预分区个数 = 节点的倍数。默认`Region`的大小为`10G`，假设我们预估1年下来的大小为`10T`，则`10000G / 10G = 1000`个`Region`，所以，我们可以预设为1000个`Region`，这样，1000个`Region`将均衡地分布在各个节点上

###### 2.2.2 `ROWKEY`避免热点设计

1. 反转策略
   - 如果设计出的`ROWKEY`在数据分布上不均匀，但`ROWKEY`尾部的数据却呈现出了良好的随机性，可以考虑**将`ROWKEY`的翻转**，或者直接将尾部的bytes提前到`ROWKEY`的开头。
   - 反转策略可以使`ROWKEY`随机分布，但是牺牲了`ROWKEY`的有序性
   - 缺点：利于Get操作，但不利于Scan操作，因为数据在原`ROWKEY`上的自然顺序已经被打乱
2. 加盐策略
   - Salting（加盐）的原理是在原`ROWKEY`的**前面**添加固定长度的随机数，也就是给`ROWKEY`分配一个随机前缀使它和之间的`ROWKEY`的开头不同
   - 随机数能保障数据在所有`Regions`间的负载均衡
   - 缺点：因为添加的是随机数，基于原`ROWKEY`查询时无法知道随机数是什么，那样在查询的时候就需要去各个可能的`Regions`中查找，加盐对比读取是无力的
3. 哈希策略
   - 基于` ROWKEY`的完整或部分数据进行 Hash，而后将Hashing后的值完整替换或部分替换原`ROWKEY`的前缀部分
   - 这里说的 hash 包含 `MD5、sha1、sha256` 或 `sha512` 等算法
   - 缺点：Hashing 也不利于 Scan，因为打乱了原`RowKey`的自然顺序

### 任务三、微信打招呼数据预分区

#### **【任务目标】**

掌握微信打招呼数据的预分区过程

#### **【任务步骤】**

**视频讲解：**

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab3_1/image-20210824110318276.png?fileid=5576678020615802458)

视频-3、微信打招呼数据预分区

##### 3.1 预分区

在`HBase`中，可以通过指定start key、end key来进行分区，还可以直接指定Region的数量，指定分区的策略。

1. 指定 start key、end key来分区

   ```
   hbase> create 'ns1:t1', 'f1', SPLITS => ['10', '20', '30', '40']
   hbase> create 't1', 'f1', SPLITS => ['10', '20', '30', '40']复制代码
   ```

2. 指定分区数量、分区策略

   ```
   hbase> create 't1', 'f1', {NUMREGIONS => 15, SPLITALGO => 'HexStringSplit'}复制代码
   ```

分区策略

- `HexStringSplit`: `ROWKEY`是十六进制的字符串作为前缀的
- `DecimalStringSplit`:`ROWKEY`是10进制数字字符串作为前缀的
- `UniformSplit`: `ROWKEY`前缀完全随机

`Region`的数量可以按照数据量来预估。本次案例，我们设计为6个Region。因为`ROWKEY`我们是使用多个字段拼接，而且前缀不是完全随机的，所以需要使用`HexStringSplit`。

##### 3.2 `ROWKEY`设计

为了确保数据均匀分布在每个Region，需要以`MD5Hash`作为前缀

**`ROWKEY = MD5Hash_发件人账号_收件人账号_时间戳`**

![image-20230614170505139](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab3_1/image-20230614170505139.png)

实现过程：

```
create 'WEIXIN_CHAT:MSG', {NAME => "C1", COMPRESSION => "GZ"}, { NUMREGIONS => 6, SPLITALGO => 'HexStringSplit'}复制代码
```

执行完命令后，我们发现该表已经分为6个分区。这样将来数据就可以均匀地分布到不同的分区中了

![image-20230614170558811](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab3_1/image-20230614170558811.png)

HDFS中，也有对应的6个文件夹。

URL：`/hbase/data/MOMO_CHAT/MSG`

![image-20230614170617014](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab3_1/image-20230614170617014.png)





# 实验3-2：微信海量存储实战（下）

## 实验概述

编写数据生成器模拟数据，并提供相应的数据查询服务。

## 实验环境

- AtStudy 实训平台
- Hadoop
- `HBase`

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16871379992298071.png)

## 实验目标

学习完成本实验后，您将能够

- 模拟生成测试数据
- 掌握`ROWKEY`的构建过程
- 实现数据查询服务开发

## 实验任务

### 任务一、项目初始化

#### **【任务目标】**

准备项目环境。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab3_2/image-20210824110318276.png?fileid=5576678020615055683)

视频-1、项目初始化

##### 1.1 项目依赖

```
<repositories><!-- 代码库 -->
    <repository>
        <id>aliyun</id>
        <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
        <releases>
            <enabled>true</enabled>
        </releases>
        <snapshots>
            <enabled>false</enabled>
            <updatePolicy>never</updatePolicy>
        </snapshots>
    </repository>
</repositories>

<dependencies>
    <!-- HBase客户端 -->
    <dependency>
        <groupId>org.apache.hbase</groupId>
        <artifactId>hbase-client</artifactId>
        <version>2.1.0</version>
    </dependency>
    <!-- Xml操作相关 -->
    <dependency>
        <groupId>com.github.cloudecho</groupId>
        <artifactId>xmlbean</artifactId>
        <version>1.5.5</version>
    </dependency>
    <!-- 操作Office库 -->
    <dependency>
        <groupId>org.apache.poi</groupId>
        <artifactId>poi</artifactId>
        <version>4.0.1</version>
    </dependency>
    <!-- 操作Office库 -->
    <dependency>
        <groupId>org.apache.poi</groupId>
        <artifactId>poi-ooxml</artifactId>
        <version>4.0.1</version>
    </dependency>
    <!-- 操作Office库 -->
    <dependency>
        <groupId>org.apache.poi</groupId>
        <artifactId>poi-ooxml-schemas</artifactId>
        <version>4.0.1</version>
    </dependency>
    <!-- 操作JSON -->
    <dependency>
        <groupId>com.alibaba</groupId>
        <artifactId>fastjson</artifactId>
        <version>1.2.62</version>
    </dependency>
    <!-- phoenix core -->
    <dependency>
        <groupId>org.apache.phoenix</groupId>
        <artifactId>phoenix-core</artifactId>
        <version>5.0.0-HBase-2.0</version>
    </dependency>
    <!-- phoenix 客户端 -->
    <dependency>
        <groupId>org.apache.phoenix</groupId>
        <artifactId>phoenix-queryserver-client</artifactId>
        <version>5.0.0-HBase-2.0</version>
    </dependency>
</dependencies>


<build>
    <plugins>
        <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-compiler-plugin</artifactId>
            <version>3.1</version>
            <configuration>
                <target>1.8</target>
                <source>1.8</source>
            </configuration>
        </plugin>
    </plugins>
</build>复制代码
```

##### 1.2 拷贝配置文件

将资料中的以下几个文件拷贝到resources目录。

- `core-site.xml`
- `hbase-site.xml`
- `log4j.properties`

##### 1.3 创建包结构

|                                        |                                                         |
| -------------------------------------- | ------------------------------------------------------- |
| `com.atstudy.weixin_chat.service`      | 用于存放数据服务接口相关代码，例如：查询的API代码       |
| `com.atstudy.weixin_chat.service.impl` | 用于存放数据服务接口实现类相关代码，例如：查询的API代码 |
| `com.atstudy.weixin_chat.tool`         | 工具类                                                  |
| `com.atstudy.weixin_chat.entity`       | 存放实体类                                              |

![image-20230614172341003](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab3_2/image-20230614172341003.png)

##### 1.4 导入`ExcelReader`工具类

在资料包中有一个`ExcelReader.java`文件，`ExcelReader`工具类可以读取Excel中的数据称为`HashMap`这样，方便我们快速生成数据。

`ExcelReader`工具类主要有两个方法：

1. `readXlsx`——用于将指定路径的Excel文件中的工作簿读取为Map结构
2. `randomColumn`——随机生成某一列中的数据。

**将`ExcelReader`添加到`com.atstudy.weixin_chat.tool`包中。**

##### 1.5 创建实体类

在`com.atstudy.weixin_chat.entity`包中创建一个名为Msg的实体类，使用Java代码描述微信消息。

| **字段名**          | **说明**       |
| ------------------- | -------------- |
| msg_time            | 消息时间       |
| sender_nickyname    | 发件人昵称     |
| sender_account      | 发件人账号     |
| sender_sex          | 发件人性别     |
| sender_ip           | 发件人IP       |
| sender_os           | 发件人系统     |
| sender_phone_type   | 发件人手机型号 |
| sender_network      | 发件人网络制式 |
| sender_gps          | 发件人GPS      |
| receiver_nickyname  | 收件人昵称     |
| receiver_ip         | 收件人IP       |
| receiver_account    | 收件人账号     |
| receiver_os         | 收件人系统     |
| receiver_phone_type | 收件人手机型号 |
| receiver_network    | 收件人网络制式 |
| receiver_gps        | 收件人GPS      |
| receiver_sex        | 收件人性别     |
| msg_type            | 消息类型       |
| distance            | 双方距离       |
| message             | 消息           |

操作步骤：

1. 使用列编辑，快速复制讲义中上述的表格字段给实体类添加成员变量
2. 使用IDEA快捷键 Alt + Insert 键快速生成 getter/setter 方法，并重写`toString`方法

```
public class Msg {
    private String msg_time;
    private String sender_nickyname;
    private String sender_account;
    private String sender_sex;
    private String sender_ip;
    private String sender_os;
    private String sender_phone_type;
    private String sender_network;
    private String sender_gps;
    private String receiver_nickyname;
    private String receiver_ip;
    private String receiver_account;
    private String receiver_os;
    private String receiver_phone_type;
    private String receiver_network;
    private String receiver_gps;
    private String receiver_sex;
    private String msg_type;
    private String distance;
    private String message;

    public String getMsg_time() {
        return msg_time;
    }

    public void setMsg_time(String msg_time) {
        this.msg_time = msg_time;
    }

    public String getSender_nickyname() {
        return sender_nickyname;
    }

    public void setSender_nickyname(String sender_nickyname) {
        this.sender_nickyname = sender_nickyname;
    }

    public String getSender_account() {
        return sender_account;
    }

    public void setSender_account(String sender_account) {
        this.sender_account = sender_account;
    }

    public String getSender_sex() {
        return sender_sex;
    }

    public void setSender_sex(String sender_sex) {
        this.sender_sex = sender_sex;
    }

    public String getSender_ip() {
        return sender_ip;
    }

    public void setSender_ip(String sender_ip) {
        this.sender_ip = sender_ip;
    }

    public String getSender_os() {
        return sender_os;
    }

    public void setSender_os(String sender_os) {
        this.sender_os = sender_os;
    }

    public String getSender_phone_type() {
        return sender_phone_type;
    }

    public void setSender_phone_type(String sender_phone_type) {
        this.sender_phone_type = sender_phone_type;
    }

    public String getSender_network() {
        return sender_network;
    }

    public void setSender_network(String sender_network) {
        this.sender_network = sender_network;
    }

    public String getSender_gps() {
        return sender_gps;
    }

    public void setSender_gps(String sender_gps) {
        this.sender_gps = sender_gps;
    }

    public String getReceiver_nickyname() {
        return receiver_nickyname;
    }

    public void setReceiver_nickyname(String receiver_nickyname) {
        this.receiver_nickyname = receiver_nickyname;
    }

    public String getReceiver_ip() {
        return receiver_ip;
    }

    public void setReceiver_ip(String receiver_ip) {
        this.receiver_ip = receiver_ip;
    }

    public String getReceiver_account() {
        return receiver_account;
    }

    public void setReceiver_account(String receiver_account) {
        this.receiver_account = receiver_account;
    }

    public String getReceiver_os() {
        return receiver_os;
    }

    public void setReceiver_os(String receiver_os) {
        this.receiver_os = receiver_os;
    }

    public String getReceiver_phone_type() {
        return receiver_phone_type;
    }

    public void setReceiver_phone_type(String receiver_phone_type) {
        this.receiver_phone_type = receiver_phone_type;
    }

    public String getReceiver_network() {
        return receiver_network;
    }

    public void setReceiver_network(String receiver_network) {
        this.receiver_network = receiver_network;
    }

    public String getReceiver_gps() {
        return receiver_gps;
    }

    public void setReceiver_gps(String receiver_gps) {
        this.receiver_gps = receiver_gps;
    }

    public String getReceiver_sex() {
        return receiver_sex;
    }

    public void setReceiver_sex(String receiver_sex) {
        this.receiver_sex = receiver_sex;
    }

    public String getMsg_type() {
        return msg_type;
    }

    public void setMsg_type(String msg_type) {
        this.msg_type = msg_type;
    }

    public String getDistance() {
        return distance;
    }

    public void setDistance(String distance) {
        this.distance = distance;
    }

    public String getMessage() {
        return message;
    }

    public void setMessage(String message) {
        this.message = message;
    }

    @Override
    public String toString() {
        return JSON.toJSONString(this);
    }
}复制代码
```

![image-20230614173003299](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab3_2/image-20230614173003299.png)

### 任务二、模拟测试数据

#### **【任务目标】**

生成模拟测试数据。

#### 【任务步骤】

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab3_2/image-20210824110318276.png?fileid=5576678020616399580)

视频-2、模拟测试数据

##### 2.1 测试工具类`ExcelReader`读取测试数据集

1. 在`com.atstudy.weixin_chat.tool`包中创建一个名为`WeiXinMsgGen`类

2. 在main方法中，读取资料中的`测试数据集.xlsx `

   ```
   Map<String, List<String>> resultMap = ExcelReader.readXlsx("data/测试数据集.xlsx", "微信数据");复制代码
   ```

![image-20230615095731567](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab3_2/image-20230615095731567.png)

##### 2.2 生成数据过程

编写`getOneMsg`方法，调用`ExcelReader`工具类，随机生成一条微信消息数据

实现步骤：

1. 构建`Msg`实体类对象
2. 调用`ExcelReader`中的`randomColumn`随机生成一个列的数据
3. 注意时间使用系统当前时间

```
private static Msg getOneMsg(Map<String, List<String>> resultMap) {
    Msg msg = new Msg();
    long timestamp = new Date().getTime();
    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");

    msg.setMsg_time(sdf.format(timestamp));
    msg.setSender_nickyname(ExcelReader.randomColumn(resultMap, "sender_nickyname"));
    msg.setSender_account(ExcelReader.randomColumn(resultMap, "sender_account"));
    msg.setSender_sex(ExcelReader.randomColumn(resultMap, "sender_sex"));
    msg.setSender_ip(ExcelReader.randomColumn(resultMap, "sender_ip"));
    msg.setSender_os(ExcelReader.randomColumn(resultMap, "sender_os"));
    msg.setSender_phone_type(ExcelReader.randomColumn(resultMap, "sender_phone_type"));
    msg.setSender_network(ExcelReader.randomColumn(resultMap, "sender_network"));
    msg.setSender_gps(ExcelReader.randomColumn(resultMap, "sender_gps"));
    msg.setReceiver_nickyname(ExcelReader.randomColumn(resultMap, "receiver_nickyname"));
    msg.setReceiver_ip(ExcelReader.randomColumn(resultMap, "receiver_ip"));
    msg.setReceiver_account(ExcelReader.randomColumn(resultMap, "receiver_account"));
    msg.setReceiver_os(ExcelReader.randomColumn(resultMap, "receiver_os"));
    msg.setReceiver_phone_type(ExcelReader.randomColumn(resultMap, "receiver_phone_type"));
    msg.setReceiver_network(ExcelReader.randomColumn(resultMap, "receiver_network"));
    msg.setReceiver_gps(ExcelReader.randomColumn(resultMap, "receiver_gps"));
    msg.setReceiver_sex(ExcelReader.randomColumn(resultMap, "receiver_sex"));
    msg.setMsg_type(ExcelReader.randomColumn(resultMap, "msg_type"));
    msg.setDistance(ExcelReader.randomColumn(resultMap, "distance"));
    msg.setMessage(ExcelReader.randomColumn(resultMap, "message"));

    return msg;
}复制代码
```

##### 2.3 构建`ROWKEY`

前面我们分析得到：

```
ROWKEY = MD5Hash_发件人账号_收件人账号_消息时间戳
```

1. 其中`MD5Hash`的计算方式为：`发送人账号 + “_” + 收件人账号 + “_” + 消息时间戳 `
2. 使用`MD5Hash.getMD5AsHex`方法生成`MD5`值
3. 取`MD5`值的前8位，避免过长
4. 最后把发件人账号、收件人账号、消息时间戳和`MD5`拼接起来

实现步骤：

1. 创建`getRowkey`方法，接收`Msg`实体对象，并根据该实体对象生成byte[]的`rowkey`
2. 使用`StringBuilder`将发件人账号、收件人账号、消息时间戳使用下划线`（_）`拼接起来
3. 使用`Bytes.toBytes`将拼接出来的字符串转换为`byte[]`数组
4. 使用`MD5Hash.getMD5AsHex`生成`MD5`值，并取其前8位
5. 再将`MD5`值和之前拼接好的发件人账号、收件人账号、消息时间戳，再使用下划线拼接，转换为`Bytes`数组

```
private static byte[] getRowkey(Msg msg) throws ParseException {
    // 3. 构建ROWKEY
    // 发件人ID1反转
    StringBuilder stringBuilder = new StringBuilder(msg.getSender_account());
    stringBuilder.append("_");
    stringBuilder.append(msg.getReceiver_account());
    stringBuilder.append("_");
    // 转换为时间戳
    SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");

    stringBuilder.append(sdf.parse(msg.getMsg_time()).getTime());

    byte[] orginkey = Bytes.toBytes(stringBuilder.toString());
    // 为了避免ROWKEY过长，取前八位
    String md5AsHex = MD5Hash.getMD5AsHex(orginkey).substring(0, 8);

    return Bytes.toBytes(md5AsHex + "_" + stringBuilder.toString());
}复制代码
```

##### 2.4 数据写入`HBase`

实现步骤：

1. 获取`HBase`连接
2. 获取`HBase`表`WEIXIN:MSG`
3. 初始化操作`HBase`所需的变量（列蔟、列名）
4. 构建put请求
5. 挨个添加微信消息的所有列
6. 发起put请求

###### 2.4.1 构建`HBase`连接

1. 获取`HBase`连接

   ```
   Configuration configuration = HBaseConfiguration.create();
   Connection connection = ConnectionFactory.createConnection(configuration);复制代码
   ```

2. 获取`HBase`表

   ```
   String TABLE_NAME = "WEIXIN_CHAT:MSG";
   Table weixinChatTable = connection.getTable(TableName.valueOf(TABLE_NAME));复制代码
   ```

3. 初始化操作`HBase`所需的变量（列蔟、列名）

   ```
   String cf_name = "C1";
   String col_msg_time = "msg_time";
   String col_sender_nickyname = "sender_nickyname";
   String col_sender_account = "sender_account";
   String col_sender_sex = "sender_sex";
   String col_sender_ip = "sender_ip";
   String col_sender_os = "sender_os";
   String col_sender_phone_type = "sender_phone_type";
   String col_sender_network = "sender_network";
   String col_sender_gps = "sender_gps";
   String col_receiver_nickyname = "receiver_nickyname";
   String col_receiver_ip = "receiver_ip";
   String col_receiver_account = "receiver_account";
   String col_receiver_os = "receiver_os";
   String col_receiver_phone_type = "receiver_phone_type";
   String col_receiver_network = "receiver_network";
   String col_receiver_gps = "receiver_gps";
   String col_receiver_sex = "receiver_sex";
   String col_msg_type = "msg_type";
   String col_distance = "distance";
   String col_message = "message";复制代码
   ```

###### 2.4.2 发起put请求添加数据

- 构建put请求

- 挨个添加微信消息的所有列

- 发起put请求

  ```
  Msg msg = getOneMsg(resultMap);
  Put put = new Put(getRowkey(msg));
  
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_msg_time), Bytes.toBytes(msg.getMsg_time()));
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_sender_nickyname), Bytes.toBytes(msg.getSender_nickyname()));
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_sender_account), Bytes.toBytes(msg.getSender_account()));
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_sender_sex), Bytes.toBytes(msg.getSender_sex()));
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_sender_ip), Bytes.toBytes(msg.getSender_ip()));
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_sender_os), Bytes.toBytes(msg.getSender_os()));
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_sender_phone_type), Bytes.toBytes(msg.getSender_phone_type()));
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_sender_network), Bytes.toBytes(msg.getSender_network()));
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_sender_gps), Bytes.toBytes(msg.getSender_gps()));
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_receiver_nickyname), Bytes.toBytes(msg.getReceiver_nickyname()));
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_receiver_ip), Bytes.toBytes(msg.getReceiver_ip()));
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_receiver_account), Bytes.toBytes(msg.getReceiver_account()));
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_receiver_os), Bytes.toBytes(msg.getReceiver_os()));
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_receiver_phone_type), Bytes.toBytes(msg.getReceiver_phone_type()));
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_receiver_network), Bytes.toBytes(msg.getReceiver_network()));
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_receiver_gps), Bytes.toBytes(msg.getReceiver_gps()));
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_receiver_sex), Bytes.toBytes(msg.getReceiver_sex()));
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_msg_type), Bytes.toBytes(msg.getMsg_type()));
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_distance), Bytes.toBytes(msg.getDistance()));
  put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_message), Bytes.toBytes(msg.getMessage()));
  
  // 5. 执行put请求
  wexinChatlTable.put(put);复制代码
  ```

##### 2.5 生成`10W`条数据

1. 使用一个while循环，生成`10W`条数据
2. 注意遍历的时候打印下数据生成的进度

```
/**
 * 微信数据生成器
 */
public class WeiXinMsgGen {

    public static void main(String[] args) throws Exception {
        Map<String, List<String>> resultMap = ExcelReader.readXlsx("data/测试数据集.xlsx", "微信数据");

        // 1. 获取HBase连接
        Configuration configuration = HBaseConfiguration.create();

        Connection connection = ConnectionFactory.createConnection(configuration);

        // 2. 获取HTable
        String TABLE_NAME = "WEIXIN_CHAT:MSG";
        Table wexinChatTable = connection.getTable(TableName.valueOf(TABLE_NAME));

        String cf_name = "C1";
        String col_msg_time = "msg_time";
        String col_sender_nickyname = "sender_nickyname";
        String col_sender_account = "sender_account";
        String col_sender_sex = "sender_sex";
        String col_sender_ip = "sender_ip";
        String col_sender_os = "sender_os";
        String col_sender_phone_type = "sender_phone_type";
        String col_sender_network = "sender_network";
        String col_sender_gps = "sender_gps";
        String col_receiver_nickyname = "receiver_nickyname";
        String col_receiver_ip = "receiver_ip";
        String col_receiver_account = "receiver_account";
        String col_receiver_os = "receiver_os";
        String col_receiver_phone_type = "receiver_phone_type";
        String col_receiver_network = "receiver_network";
        String col_receiver_gps = "receiver_gps";
        String col_receiver_sex = "receiver_sex";
        String col_msg_type = "msg_type";
        String col_distance = "distance";
        String col_message = "message";

        int i = 0;
        int max = 100000;
        while(i < max) {
            Msg msg = getOneMsg(resultMap);
            Put put = new Put(getRowkey(msg));

            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_msg_time), Bytes.toBytes(msg.getMsg_time()));
            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_sender_nickyname), Bytes.toBytes(msg.getSender_nickyname()));
            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_sender_account), Bytes.toBytes(msg.getSender_account()));
            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_sender_sex), Bytes.toBytes(msg.getSender_sex()));
            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_sender_ip), Bytes.toBytes(msg.getSender_ip()));
            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_sender_os), Bytes.toBytes(msg.getSender_os()));
            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_sender_phone_type), Bytes.toBytes(msg.getSender_phone_type()));
            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_sender_network), Bytes.toBytes(msg.getSender_network()));
            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_sender_gps), Bytes.toBytes(msg.getSender_gps()));
            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_receiver_nickyname), Bytes.toBytes(msg.getReceiver_nickyname()));
            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_receiver_ip), Bytes.toBytes(msg.getReceiver_ip()));
            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_receiver_account), Bytes.toBytes(msg.getReceiver_account()));
            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_receiver_os), Bytes.toBytes(msg.getReceiver_os()));
            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_receiver_phone_type), Bytes.toBytes(msg.getReceiver_phone_type()));
            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_receiver_network), Bytes.toBytes(msg.getReceiver_network()));
            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_receiver_gps), Bytes.toBytes(msg.getReceiver_gps()));
            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_receiver_sex), Bytes.toBytes(msg.getReceiver_sex()));
            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_msg_type), Bytes.toBytes(msg.getMsg_type()));
            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_distance), Bytes.toBytes(msg.getDistance()));
            put.addColumn(Bytes.toBytes(cf_name), Bytes.toBytes(col_message), Bytes.toBytes(msg.getMessage()));

            // 5. 执行put请求
            wexinChatTable.put(put);
            System.out.println(i + " / " + max);
            ++i;
        }

        // 6. 关闭连接
        wexinChatlTable.close();
        wexinChatlTable.close();
    }

    private static byte[] getRowkey(Msg msg) throws ParseException {
        // 3. 构建ROWKEY
        // 发件人ID1反转
        StringBuilder stringBuilder = new StringBuilder(msg.getSender_account());
        stringBuilder.append("_");
        stringBuilder.append(msg.getReceiver_account());
        stringBuilder.append("_");
        // 转换为时间戳
        SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");

        stringBuilder.append(sdf.parse(msg.getMsg_time()).getTime());

        byte[] orginkey = Bytes.toBytes(stringBuilder.toString());
        // 为了避免ROWKEY过长，取前八位
        String md5AsHex = MD5Hash.getMD5AsHex(orginkey).substring(0, 8);

        return Bytes.toBytes(md5AsHex + "_" + stringBuilder.toString());
    }

    private static Msg getOneMsg(Map<String, List<String>> resultMap) {
        Msg msg = new Msg();
        long timestamp = new Date().getTime();
        SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");

        msg.setMsg_time(sdf.format(timestamp));
        msg.setSender_nickyname(ExcelReader.randomColumn(resultMap, "sender_nickyname"));
        msg.setSender_account(ExcelReader.randomColumn(resultMap, "sender_account"));
        msg.setSender_sex(ExcelReader.randomColumn(resultMap, "sender_sex"));
        msg.setSender_ip(ExcelReader.randomColumn(resultMap, "sender_ip"));
        msg.setSender_os(ExcelReader.randomColumn(resultMap, "sender_os"));
        msg.setSender_phone_type(ExcelReader.randomColumn(resultMap, "sender_phone_type"));
        msg.setSender_network(ExcelReader.randomColumn(resultMap, "sender_network"));
        msg.setSender_gps(ExcelReader.randomColumn(resultMap, "sender_gps"));
        msg.setReceiver_nickyname(ExcelReader.randomColumn(resultMap, "receiver_nickyname"));
        msg.setReceiver_ip(ExcelReader.randomColumn(resultMap, "receiver_ip"));
        msg.setReceiver_account(ExcelReader.randomColumn(resultMap, "receiver_account"));
        msg.setReceiver_os(ExcelReader.randomColumn(resultMap, "receiver_os"));
        msg.setReceiver_phone_type(ExcelReader.randomColumn(resultMap, "receiver_phone_type"));
        msg.setReceiver_network(ExcelReader.randomColumn(resultMap, "receiver_network"));
        msg.setReceiver_gps(ExcelReader.randomColumn(resultMap, "receiver_gps"));
        msg.setReceiver_sex(ExcelReader.randomColumn(resultMap, "receiver_sex"));
        msg.setMsg_type(ExcelReader.randomColumn(resultMap, "msg_type"));
        msg.setDistance(ExcelReader.randomColumn(resultMap, "distance"));
        msg.setMessage(ExcelReader.randomColumn(resultMap, "message"));

        return msg;
    }
}复制代码
```

执行效果如下：

![image-20230615103544638](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab3_2/image-20230615103544638.png)

我们也可以通过Shell命令进行查看

```
count 'WEIXIN_CHAT:MSG'复制代码
```

![image-20230615103920862](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab3_2/image-20230615103920862.png)

发现，没有`10W`数据，这是因为在生成过程中可能存在部分重复的数据！

### 任务三、数据查询服务开发

#### **【任务目标】**

提供数据查询服务功能。

#### 【任务步骤】

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab3_2/image-20210824110318276.png?fileid=5576678020615995348)

视频-3、数据查询服务开发

##### 2.1 背景

数据存储到`HBase`之后，用户可能会在某一时间按照日期来查询聊天记录。例如：用户点击某一个日期，那就需要将当天用户和另外一个用户的打招呼聊天记录查询出来。也就是需要按照以下几个字段来进行查询。

- 日期
- 发件人
- 收件人

##### 2.2 创建接口与实现类

1. 在`com.atstudy.weixin_chat.service` 包下创建`ChatMessageService`接口，该接口有一个方法为：

   ```
   List<Msg> getMessage(String date, String sender, String receiver) throws Exception
       
   void close();复制代码
   ```

   - `getMessage`表示从`HBase`中的消息记录根据日期、发送者账号、接受者账号查询数据，并返回一个List集合
   - `close`表示关闭打开的相关资源

2. 在`com.atstudy.weixin_chat.service.impl`包下创建`HBaseNativeChatMessageService`实现类，并实现`getMessage`方法

`ChatMessageService`接口

```
/**
 * 微信消息服务
 */
public interface ChatMessageService {
    List<Msg> getMessage(String date, String sender, String receiver) throws Exception;
    void close();
}复制代码
```

`HbaseNativeChatMessageService`实现类

```
/**
 * 使用HBase原生API实现数据服务
 */
public class HbaseNativeChatMessageService implements ChatMessageService {
    @Override
    public List<Msg> getMessage(String date, String sender, String receiver) throws Exception {
        return null;
    }
    @Override
    public void close() {

    }
}复制代码
```

##### 2.3 构建实现类所需对象并初始化

要使用该对象操作`HBase`，我们需要提前准备以下内容：

1. `HBase`连接
2. 日期格式化器

添加几个字段，并在构造器中初始化它们。

构造器实现：

1. 构建`HBase Connection`
2. 构建日期格式化器

```
// Hbase连接
private Connection connection;
// 日期格式化器
private SimpleDateFormat simpleDateFormat;复制代码
```

##### 2.4 实现close方法

在close方法中关闭连接池、表、连接。

```
@Override
public void close() {
    try {
        connection.close();
    } catch (IOException e) {
        e.printStackTrace();
    }
}复制代码
```

##### 2.5 实现getMessage方法

要根据日期、发件人、收件人查询消息，我们需要使用scan+filter来进行扫描。我们需要使用多个Filter组合起来进行查询。

实现步骤：

1. 构建scan对象
2. 构建用于查询时间的范围，例如：`2023-06-15 00:00:00 – 2023-06-15 23:59:59`
3. 构建查询日期的两个`Filter`，大于等于、小于等于，此处过滤单个列使用`SingleColumnValueFilter`即可。
4. 构建发件人`Filter`
5. 构建收件人`Filter`
6. 使用`FilterList`组合所有`Filter`
7. 设置`scan`对象`filter`
8. 获取`HTable`对象，并调用`getScanner`执行
9. 获取迭代器，迭代每一行，同时迭代每一个单元格

具体实现过程：

- 构建scan对象

  ```
  Scan scan = new Scan();复制代码
  ```

- 构建用于查询时间的范围，例如：`2023-06-15 00:00:00 – 2023-06-15 23:59:59`

  ```
  // 构建查询时间范围
  String startDate = date + " 00:00:00";
  String endDate = date + " 23:59:59";复制代码
  ```

- 构建查询日期的两个`Filter`，大于等于、小于等于，此处过滤单个列使用`SingleColumnValueFilter`即可。

  ```
  // 构建日期查询
  SingleColumnValueFilter startDateFilter = new SingleColumnValueFilter(Bytes.toBytes("C1")
          , Bytes.toBytes("msg_time")
          , CompareOperator.GREATER_OR_EQUAL
          , new BinaryComparator(Bytes.toBytes(startDate + "")));
  
  SingleColumnValueFilter endDateFilter = new SingleColumnValueFilter(Bytes.toBytes("C1")
          , Bytes.toBytes("msg_time")
          , CompareOperator.LESS_OR_EQUAL
          , new BinaryComparator(Bytes.toBytes(endDate + "")));复制代码
  ```

- 构建发件人`Filter`

  ```
  SingleColumnValueFilter senderFilter = new SingleColumnValueFilter(Bytes.toBytes("C1")
          , Bytes.toBytes("sender_account")
          , CompareOperator.EQUAL
          , new BinaryComparator(Bytes.toBytes(sender)));复制代码
  ```

- 构建收件人`Filter`

  ```
  SingleColumnValueFilter receiverFilter = new SingleColumnValueFilter(Bytes.toBytes("C1")
                  , Bytes.toBytes("receiver_account")
                  , CompareOperator.EQUAL
                  , new BinaryComparator(Bytes.toBytes(receiver)));复制代码
  ```

- 使用`FilterList`组合所有`Filter`

  ```
  Filter filterList = new FilterList(FilterList.Operator.MUST_PASS_ALL
          , startDateFilter
          , endDateFilter
          , senderFilter
          , receiverFilter);复制代码
  ```

- 设置`scan`对象`filter`

- 获取`HTable`对象，并调用`getScanner`执行

  ```
  scan.setFilter(filterList);
  ResultScanner scanner = tableMsg.getScanner(scan);复制代码
  ```

- 获取迭代器，迭代每一行，同时迭代每一个单元格

  ```
  Iterator<Result> iter = scanner.iterator();
  List<Msg> msgList = new ArrayList<>();
  
  while(iter.hasNext()) {
      Result result = iter.next();
      Msg msg = new Msg();
      // 遍历所有列
      while(result.advance()) {
          Cell cell = result.current();
          String columnName = Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength());
  
          if(columnName.equalsIgnoreCase("msg_time")){
              msg.setMsg_time(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
          if(columnName.equalsIgnoreCase("sender_nickyname")){
              msg.setSender_nickyname(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
          if(columnName.equalsIgnoreCase("sender_account")){
              msg.setSender_account(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
          if(columnName.equalsIgnoreCase("sender_sex")){
              msg.setSender_sex(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
          if(columnName.equalsIgnoreCase("sender_ip")){
              msg.setSender_ip(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
          if(columnName.equalsIgnoreCase("sender_os")){
              msg.setSender_os(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
          if(columnName.equalsIgnoreCase("sender_phone_type")){
              msg.setSender_phone_type(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
          if(columnName.equalsIgnoreCase("sender_network")){
              msg.setSender_network(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
          if(columnName.equalsIgnoreCase("sender_gps")){
              msg.setSender_gps(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
          if(columnName.equalsIgnoreCase("receiver_nickyname")){
              msg.setReceiver_nickyname(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
          if(columnName.equalsIgnoreCase("receiver_ip")){
              msg.setReceiver_ip(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
          if(columnName.equalsIgnoreCase("receiver_account")){
              msg.setReceiver_account(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
          if(columnName.equalsIgnoreCase("receiver_os")){
              msg.setReceiver_os(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
          if(columnName.equalsIgnoreCase("receiver_phone_type")){
              msg.setReceiver_phone_type(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
          if(columnName.equalsIgnoreCase("receiver_network")){
              msg.setReceiver_network(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
          if(columnName.equalsIgnoreCase("receiver_gps")){
              msg.setReceiver_gps(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
          if(columnName.equalsIgnoreCase("receiver_sex")){
              msg.setReceiver_sex(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
          if(columnName.equalsIgnoreCase("msg_type")){
              msg.setMsg_type(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
          if(columnName.equalsIgnoreCase("distance")){
              msg.setDistance(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
          if(columnName.equalsIgnoreCase("message")){
              msg.setMessage(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
          }
  
          msgList.add(msg);
      }
  }复制代码
  ```

- 测试

  调用查询方法，检查是否能够根据指定的条件查询出数据

  ```
  public static void main(String[] args) throws Exception{
      HBaseNativeChatMessageService hbaseNativeChatMessageService = new HBaseNativeChatMessageService ();
      List<Msg> message = hbaseNativeChatMessageService.getMessage("2023-06-15", "13504113666", "18182767005");
  
      for (Msg msg : message) {
          System.out.println(msg);
      }
  
      hbaseNativeChatMessageService.close();
  }复制代码
  ```

##### 2.6 完整代码

```
/**
 * 使用HBase原生API实现数据服务
 */
public class HBaseNativeChatMessageService implements ChatMessageService {
    private Connection connection;
    private SimpleDateFormat sdf;
    private ExecutorService executorServiceMsg;
    private Table tableMsg;

    public HBaseNativeChatMessageService() {
        try {
            Configuration cfg = HBaseConfiguration.create();
            connection = ConnectionFactory.createConnection(cfg);
            sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");

            executorServiceMsg = Executors.newFixedThreadPool(5);
            tableMsg = connection.getTable(TableName.valueOf("WEIXIN_CHAT:MSG"), executorServiceMsg);
        } catch (IOException e) {
            System.out.println("**获取HBase连接失败**");
            throw new RuntimeException(e);
        }
    }

    /**
     * 根据日期、发件人、收件人查询消息
     * @param date
     * @param sender
     * @param receiver
     * @return
     */
    @Override
    public List<Msg> getMessage(String date, String sender, String receiver) throws Exception {
        if(connection == null) throw new RuntimeException("未初始化HBase连接！");

        // 构建scan对象
        Scan scan = new Scan();
        // 构建Start ROWKEY
        String startDate = date + " 00:00:00";
        String endDate = date + " 23:59:59";

        // 构建Filter
        // 构建日期查询
        SingleColumnValueFilter startDateFilter = new SingleColumnValueFilter(Bytes.toBytes("C1")
                , Bytes.toBytes("msg_time")
                , CompareOperator.GREATER_OR_EQUAL
                , new BinaryComparator(Bytes.toBytes(startDate + "")));

        SingleColumnValueFilter endDateFilter = new SingleColumnValueFilter(Bytes.toBytes("C1")
                , Bytes.toBytes("msg_time")
                , CompareOperator.LESS_OR_EQUAL
                , new BinaryComparator(Bytes.toBytes(endDate + "")));

        SingleColumnValueFilter senderFilter = new SingleColumnValueFilter(Bytes.toBytes("C1")
                , Bytes.toBytes("sender_account")
                , CompareOperator.EQUAL
                , new BinaryComparator(Bytes.toBytes(sender)));

        SingleColumnValueFilter receiverFilter = new SingleColumnValueFilter(Bytes.toBytes("C1")
                , Bytes.toBytes("receiver_account")
                , CompareOperator.EQUAL
                , new BinaryComparator(Bytes.toBytes(receiver)));


        Filter filterList = new FilterList(FilterList.Operator.MUST_PASS_ALL
                , startDateFilter
                , endDateFilter
                , senderFilter
                , receiverFilter);
        scan.setFilter(filterList);
        ResultScanner scanner = tableMsg.getScanner(scan);
        Iterator<Result> iter = scanner.iterator();
        List<Msg> msgList = new ArrayList<>();

        while(iter.hasNext()) {
            Result result = iter.next();
            Msg msg = new Msg();
            // 遍历所有列
            while(result.advance()) {
                Cell cell = result.current();
                String columnName = Bytes.toString(cell.getQualifierArray(), cell.getQualifierOffset(), cell.getQualifierLength());

                if(columnName.equalsIgnoreCase("msg_time")){
                    msg.setMsg_time(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }
                if(columnName.equalsIgnoreCase("sender_nickyname")){
                    msg.setSender_nickyname(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }
                if(columnName.equalsIgnoreCase("sender_account")){
                    msg.setSender_account(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }
                if(columnName.equalsIgnoreCase("sender_sex")){
                    msg.setSender_sex(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }
                if(columnName.equalsIgnoreCase("sender_ip")){
                    msg.setSender_ip(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }
                if(columnName.equalsIgnoreCase("sender_os")){
                    msg.setSender_os(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }
                if(columnName.equalsIgnoreCase("sender_phone_type")){
                    msg.setSender_phone_type(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }
                if(columnName.equalsIgnoreCase("sender_network")){
                    msg.setSender_network(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }
                if(columnName.equalsIgnoreCase("sender_gps")){
                    msg.setSender_gps(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }
                if(columnName.equalsIgnoreCase("receiver_nickyname")){
                    msg.setReceiver_nickyname(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }
                if(columnName.equalsIgnoreCase("receiver_ip")){
                    msg.setReceiver_ip(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }
                if(columnName.equalsIgnoreCase("receiver_account")){
                    msg.setReceiver_account(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }
                if(columnName.equalsIgnoreCase("receiver_os")){
                    msg.setReceiver_os(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }
                if(columnName.equalsIgnoreCase("receiver_phone_type")){
                    msg.setReceiver_phone_type(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }
                if(columnName.equalsIgnoreCase("receiver_network")){
                    msg.setReceiver_network(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }
                if(columnName.equalsIgnoreCase("receiver_gps")){
                    msg.setReceiver_gps(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }
                if(columnName.equalsIgnoreCase("receiver_sex")){
                    msg.setReceiver_sex(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }
                if(columnName.equalsIgnoreCase("msg_type")){
                    msg.setMsg_type(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }
                if(columnName.equalsIgnoreCase("distance")){
                    msg.setDistance(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }
                if(columnName.equalsIgnoreCase("message")){
                    msg.setMessage(Bytes.toString(cell.getValueArray(), cell.getValueOffset(), cell.getValueLength()));
                }

                msgList.add(msg);
            }
        }

        return msgList;
    }

    public void close() {
        try {
            connection.close();
            tableMsg.close();
            executorServiceMsg.shutdown();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args) throws Exception{
        HBaseNativeChatMessageService hbaseNativeChatMessageService = new HBaseNativeChatMessageService();
        List<Msg> message = hbaseNativeChatMessageService.getMessage("2023-06-15", "13504113666", "18182767005");

        for (Msg msg : message) {
            System.out.println(msg);
        }

        hbaseNativeChatMessageService.close();
    }
}复制代码
```





# 实验4-1：`HBase`“加速器”之`Phoenix`

## 实验概述

`Hbase`默认只支持对行键的索引，那么如果要针对其它的列来进行查询，就只能全表扫描，虽然可以通过scan + filter组合来进行查询的，但查询地效率不高，因为要进行顺序全表扫描而没有其他索引。如果数据量较大，只能在客户端（client）来进行处理，如果要传输到Client大量的数据，然后交由客户端处理，这会面临网络传输压力和客户端处理压力。而且查询数据都是`HBase Java API`，使用起来不是很方便。

如果有一种SQL引擎，通过SQL语句来查询数据会更加方便。

此时，使用`Apache Phoenix`就可以解决我们上述问题。

## 实验环境

- AtStudy 实训平台
- Hadoop
- `HBase`
- Phoenix

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16871380533274798.png)

## 实验目标

学习完成本实验后，您将能够

- Phoenix常用命令
- Phoenix视图的创建
- 掌握基于SQL的接口开发

## 实验任务

### 任务一、Phoenix基本使用

#### **【任务目标】**

掌握Phoenix的基本使用，主要是常用命令

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab4_1/image-20210824110318276.png?fileid=5576678020750902373)

视频-1、Phoenix基本使用

##### 1.1 认识Phoenix

###### 1.1.1 简介

![image-20230615111637102](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab4_1/image-20230615111637102.png)

Apache Phoenix让Hadoop中支持低延迟`OLTP`和业务操作分析。

- 提供标准的SQL以及完备的ACID事务支持

- 通过利用`HBase`作为存储，让`NoSQL`数据库具备通过有模式的方式读取数据，我们可以使用SQL语句来操作`HBase`，例如：创建表、以及插入数据、修改数据、删除数据等。

- Phoenix提升

  ```
  HBase
  ```

  性能

  - Phoenix将SQL查询编译为本机`HBase`扫描
  - 确定scan的key的最佳`startKey`和`endKey`
  - 行键加盐保证分配均匀，负载均衡
  - ....

Apache Phoenix可以很好地与其他的Hadoop组件整合在一起，例如：Spark、Hive、Flume以及MapReduce。

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab4_1/image-20210823170539971.png)问题：有了`HBase + Phoenix`，是不是，将来做数仓（`OLAP`）就可以不用`Hadoop + Hive`了？

> 千万不要这么以为，`HBase + Phoenix`是否适合做`OLAP`取决于`HBase`的定位。Phoenix只是在`HBase`之上构建了SQL查询引擎（注意：我称为SQL查询引擎，并不是像MapReduce、Spark这种大规模数据计算引擎）。`HBase`的定位是在高性能随机读写，`Phoenix`可以使用SQL快插查询`HBase`中的数据，但数据操作底层是必须符合`HBase`的存储结构，例如：必须要有`ROWKEY`、必须要有列蔟。因为有这样的一些限制，绝大多数公司不会选择`HBase + Phoenix`来作为数据仓库的开发。而是用来快速进行海量数据的随机读写。这方面，`HBase + Phoenix`有很大的优势。

###### 1.1.2 具体使用

启动Phoenix客户端，连接Phoenix Server

> 注意：第一次启动Phoenix连接`HBase`会稍微慢一点。

```
[root@1d7212a0a479 ~]# cd /opt/module/phoenix/bin/
[root@1d7212a0a479 bin]# ./sqlline.py localhost:2181
.....
0: jdbc:phoenix:localhost:2181> !table   # 输入!table查看Phoenix中的表复制代码
```

查看`HBase`的Web UI，可以看到Phoenix在`SYSTEM`命名空间下创建了一些表，而且该系统表加载了大量的协处理器。

![image-20230615113700742](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab4_1/image-20230615113700742.png)

![image-20230615113612622](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab4_1/image-20230615113612622.png)

##### 1.2 快速入门

本次的DEMO，我们沿用之前的订单数据集。我们将使用Phoenix来创建表，并进行数据增删改查操作。

![image-20230615113811099](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab4_1/image-20230615113811099.png)

| 列名           | 说明       |
| -------------- | ---------- |
| id             | 订单ID     |
| status         | 订单状态   |
| money          | 支付金额   |
| pay_way        | 支付方式ID |
| user_id        | 用户ID     |
| operation_time | 操作时间   |
| category       | 商品分类   |

###### 1.2.1 建表

在Phoenix中，我们可以使用类似于MySQL DDL的方式快速创建表。例如：

```
CREATE TABLE IF NOT EXISTS 表名 (
   ROWKEY名称 数据类型 PRIMARY KEY
    列蔟名.列名1 数据类型 NOT NULL,
    列蔟名.列名2 数据类型 NOT NULL,
    列蔟名.列名3 数据类型);复制代码
```

订单明细建表语句：

```
create table if not exists ORDER_DTL(
    ID varchar primary key,
    C1.STATUS varchar,
    C1.MONEY float,
    C1.PAY_WAY integer,
    C1.USER_ID varchar,
    C1.OPERATION_TIME varchar,
    C1.CATEGORY varchar
);复制代码
```

通过`HBase`的Web UI，我们可以看到Phoenix帮助我们自动在`HBase`中创建了一张名为 `ORDER_DTL `的表格，可以看到里面添加了很多的协处理器。

```
'ORDER_DTL', {TABLE_ATTRIBUTES => {coprocessor$1 => '|org.apache.phoenix.coprocessor.ScanRegionObserver|805306366|', coprocessor$2 => '|org.apache.phoenix.coprocessor.UngroupedAggregateRegionObserver|805306366|', coprocessor$3 => '|org.apache.phoenix.coprocessor.GroupedAggregateRegionObserver|805306366|', coprocessor$4 => '|org.apache.phoenix.coprocessor.ServerCachingEndpointImpl|805306366|', coprocessor$5 => '|org.apache.phoenix.hbase.index.Indexer|805306366|index.builder=org.apache.phoenix.index.PhoenixIndexBuilder,org.apache.hadoop.hbase.index.codec.class=org.apache.phoenix.index.PhoenixIndexCodec'}}, {NAME => '0', VERSIONS => '1', EVICT_BLOCKS_ON_CLOSE => 'false', NEW_VERSION_BEHAVIOR => 'false', KEEP_DELETED_CELLS => 'FALSE', CACHE_DATA_ON_WRITE => 'false', DATA_BLOCK_ENCODING => 'FAST_DIFF', TTL => 'FOREVER', MIN_VERSIONS => '0', REPLICATION_SCOPE => '0', BLOOMFILTER => 'NONE', CACHE_INDEX_ON_WRITE => 'false', IN_MEMORY => 'false', CACHE_BLOOMS_ON_WRITE => 'false', PREFETCH_BLOCKS_ON_OPEN => 'false', COMPRESSION => 'NONE', BLOCKCACHE => 'true', BLOCKSIZE => '65536'}复制代码
```

同时，我们也看到这个表格默认只有一个Region，也就是没有分区的。

![image-20230615114046844](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab4_1/image-20230615114046844.png)

###### 1.2.2 查看表信息

```
!desc ORDER_DTL复制代码
```

注意：`一定要加上 !`

###### 1.2.3 删除表语法

```
drop table if exists ORDER_DTL;复制代码
```

###### 1.2.4 大小写问题

在`HBase`中，如果在列蔟、列名没有添加双引号。Phoenix会自动转换为大写。

```
create table if not exists ORDER_DTL(
    id varchar primary key,
    C1.status varchar,
    C1.money double,
    C1.pay_way integer,
    C1.user_id varchar,
    C1.operation_time varchar,
    C1.category varchar
);复制代码
```

![image-20230615114241164](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab4_1/image-20230615114241164.png)

如果要将列的名字改为小写，需要使用双引号，如下：

```
drop table if exists ORDER_DTL;
create table if not exists ORDER_DTL(
    "id" varchar primary key,
    C1."status" varchar,
    C1."money" float,
    C1."pay_way" integer,
    C1."user_id" varchar,
    C1."operation_time" varchar,
    C1."category" varchar
);复制代码
```

![image-20230615114329875](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab4_1/image-20230615114329875.png)

注意：一旦加了小写，后面都得任何应用该列的地方都得使用双引号，否则将报以下错误：

```
Error: ERROR 504 (42703): Undefined column. columnName=ORDER_DTL.ID复制代码
```

###### 1.2.5 插入数据

在Phoenix中，插入并**不是**使用`insert`来实现的。而是 「`upsert` 」命令。它的功能为`insert + update`，与`HBase`中的`put`相对应。如果不存在则插入，否则更新。列表是可选的，如果不存在，值将按模式中声明的顺序映射到列。这些值必须计算为常量。

```
upsert into 表名(列蔟列名, xxxx, ) VALUES(XXX, XXX, XXX)复制代码
```

插入一条数据：

| 订单ID | 订单状态 | 支付金额  | 支付方式ID | 用户ID  | 操作时间            | 商品分类 |
| ------ | -------- | --------- | ---------- | ------- | ------------------- | -------- |
| ID     | STATUS   | PAY_MONEY | PAYWAY     | USER_ID | OPERATION_DATE      | CATEGORY |
| 000001 | 已提交   | 4070      | 1          | 4944191 | 2020-04-25 12:09:16 | 手机;    |

```
UPSERT INTO ORDER_DTL VALUES('000001', '已提交', 4070, 1, '4944191', '2020-04-25 12:09:16', '手机;');复制代码
```

###### 1.2.6 修改数据

在Phoenix中，更新数据也是使用UPSERT。语法格式如下：

```
UPSERT INTO 表名(列名, …) VALUES(对应的值, …);复制代码
```

将ID为'000001'的订单状态修改为已付款。

```
UPSERT INTO ORDER_DTL("id", C1."status") VALUES ('000001', '已付款');复制代码
```

##### 1.3 查询数据

与标准SQL一样，Phoenix也是使用select语句来实现数据的查询。

- 查询所有数据

```
SELECT * FROM ORDER_DTL; #复制代码
```

- 根据ID查询数据

```
SELECT * FROM ORDER_DTL WHERE "id" = '000001';复制代码
```

- 分页查询

  导入测试数据

  ```
  UPSERT INTO "ORDER_DTL" VALUES('000002','已提交',4070,1,'4944191','2020-04-25 12:09:16','手机;');
  UPSERT INTO "ORDER_DTL" VALUES('000003','已完成',4350,1,'1625615','2020-04-25 12:09:37','家用电器;;电脑;');
  UPSERT INTO "ORDER_DTL" VALUES('000004','已提交',6370,3,'3919700','2020-04-25 12:09:39','男装;男鞋;');
  UPSERT INTO "ORDER_DTL" VALUES('000005','已付款',6370,3,'3919700','2020-04-25 12:09:44','男装;男鞋;');
  UPSERT INTO "ORDER_DTL" VALUES('000006','已提交',9380,1,'2993700','2020-04-25 12:09:41','维修;手机;');
  UPSERT INTO "ORDER_DTL" VALUES('000007','已付款',9380,1,'2993700','2020-04-25 12:09:46','维修;手机;');
  UPSERT INTO "ORDER_DTL" VALUES('000008','已完成',6400,2,'5037058','2020-04-25 12:10:13','数码;女装;');
  UPSERT INTO "ORDER_DTL" VALUES('000009','已付款',280,1,'3018827','2020-04-25 12:09:53','男鞋;汽车;');
  UPSERT INTO "ORDER_DTL" VALUES('000010','已完成',5600,1,'6489579','2020-04-25 12:08:55','食品;家用电器;');
  UPSERT INTO "ORDER_DTL" VALUES('000011','已付款',5600,1,'6489579','2020-04-25 12:09:00','食品;家用电器;');
  UPSERT INTO "ORDER_DTL" VALUES('000012','已提交',8340,2,'2948003','2020-04-25 12:09:26','男装;男鞋;');
  UPSERT INTO "ORDER_DTL" VALUES('000013','已付款',8340,2,'2948003','2020-04-25 12:09:30','男装;男鞋;');
  UPSERT INTO "ORDER_DTL" VALUES('000014','已提交',7060,2,'2092774','2020-04-25 12:09:38','酒店;旅游;');
  UPSERT INTO "ORDER_DTL" VALUES('000015','已提交',640,3,'7152356','2020-04-25 12:09:49','维修;手机;');
  UPSERT INTO "ORDER_DTL" VALUES('000016','已付款',9410,3,'7152356','2020-04-25 12:10:01','维修;手机;');
  UPSERT INTO "ORDER_DTL" VALUES('000017','已提交',9390,3,'8237476','2020-04-25 12:10:08','男鞋;汽车;');
  UPSERT INTO "ORDER_DTL" VALUES('000018','已提交',7490,2,'7813118','2020-04-25 12:09:05','机票;文娱;');
  UPSERT INTO "ORDER_DTL" VALUES('000019','已付款',7490,2,'7813118','2020-04-25 12:09:06','机票;文娱;');
  UPSERT INTO "ORDER_DTL" VALUES('000020','已付款',5360,2,'5301038','2020-04-25 12:08:50','维修;手机;');
  UPSERT INTO "ORDER_DTL" VALUES('000021','已提交',5360,2,'5301038','2020-04-25 12:08:53','维修;手机;');
  UPSERT INTO "ORDER_DTL" VALUES('000022','已取消',5360,2,'5301038','2020-04-25 12:08:58','维修;手机;');
  UPSERT INTO "ORDER_DTL" VALUES('000023','已付款',6490,0,'3141181','2020-04-25 12:09:22','食品;家用电器;');
  UPSERT INTO "ORDER_DTL" VALUES('000024','已付款',3820,1,'9054826','2020-04-25 12:10:04','家用电器;;电脑;');
  UPSERT INTO "ORDER_DTL" VALUES('000025','已提交',4650,2,'5837271','2020-04-25 12:08:52','机票;文娱;');
  UPSERT INTO "ORDER_DTL" VALUES('000026','已付款',4650,2,'5837271','2020-04-25 12:08:57','机票;文娱;');复制代码
  ```

  使用limit和offset可以快速进行分页。

  limit表示每页多少条记录，offset表示从第几条记录开始查起。

  ```
  -- 第一页
  select * from ORDER_DTL limit 10 offset 0;
  -- 第二页
  -- offset从10开始
  select * from ORDER_DTL limit 10 offset 10;
  -- 第三页
  select * from ORDER_DTL limit 10 offset 20;复制代码
  ```

  ![image-20230615114743719](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab4_1/image-20230615114743719.png)



### 任务二、Phoenix视图

#### **【任务目标】**

掌握Phoenix中视图的使用。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab4_1/image-20210824110318276.png?fileid=5576678020751097072)

视频-2、Phoenix视图

##### 2.1 建立视图

因为我们之前已经创建了`WEIXIN_CHAT:MSG` 表，而且数据添加的方式都是以PUT方式原生`API`来添加的。故此时，我们不再需要再使用Phoenix创建新的表，而是使用Phoenix中的**视图**，通过视图来建立与`HBase`表之间的映射，从而实现数据快速查询。

可以在现有的`HBase`或`Phoenix`表上创建一个视图。表、列蔟和列名必须与现有元数据完全匹配，否则会出现异常。当创建视图后，就可以使用SQL查询视图，和操作Table一样。

```
-- 映射HBase中的表
CREATE VIEW "my_hbase_table"
    ( k VARCHAR primary key, "v" UNSIGNED_LONG) default_column_family='a';

-- 映射Phoenix中的表
CREATE VIEW my_view ( new_col SMALLINT )
    AS SELECT * FROM my_table WHERE k = 100;

-- 映射到一个SQL查询
CREATE VIEW my_view_on_view
    AS SELECT * FROM my_view WHERE new_col > 70;复制代码
```

##### 2.2 1.1.1 建立`WEIXIN_CHAT:MSG`的视图

考虑以下几个问题：

1. 视图如何映射到`HBase`的表？

   视图的名字必须是：`命名空间.表名`

2. 视图中的列如何映射到`HBase`的列蔟和列？

   列名必须是：`列蔟.列名`

3. 视图中的类如何映射到`HBase`的`ROWKEY`？

   指定某个列为primary key，自动映射`ROWKEY`

```
# 创建WEIXIN_CHAT:MSG视图
create view if not exists "WEIXIN_CHAT"."MSG" (
    "pk" varchar primary key,
    "C1"."msg_time" varchar,
    "C1"."sender_nickyname" varchar,
    "C1"."sender_account" varchar,
    "C1"."sender_sex" varchar,
    "C1"."sender_ip" varchar,
    "C1"."sender_os" varchar,
    "C1"."sender_phone_type" varchar,
    "C1"."sender_network" varchar,
    "C1"."sender_gps" varchar,
    "C1"."receiver_nickyname" varchar,
    "C1"."receiver_ip" varchar,
    "C1"."receiver_account" varchar,
    "C1"."receiver_os" varchar,
    "C1"."receiver_phone_type" varchar,
    "C1"."receiver_network" varchar,
    "C1"."receiver_gps" varchar,
    "C1"."receiver_sex" varchar,
    "C1"."msg_type" varchar,
    "C1"."distance" varchar,
    "C1"."message" varchar
);复制代码
```

尝试查询一条数据

```
SELECT * FROM "WEIXIN_CHAT"."MSG" LIMIT 1;复制代码
```

![image-20230615133011416](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab4_1/image-20230615133011416.png)

如果发现数据能够正常展示，说明视图映射已经成功。

> 注意：因为列名中有小写，需要用引号将字段名包含起来

### 任务三、SQL查询接口开发

#### **【任务目标】**

基于Phoenix显示接口开发。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/04_hbase/lab4_1/image-20210824110318276.png?fileid=5576678020751056482)

视频-3、SQL查询接口开发

##### 3.1 需求

需求：根据日期、发送人账号、接收人账号查询历史消息

编写SQL语句

```
-- 查询对应日期的数据（只展示出来5条）
SELECT * FROM "WEIXIN_CHAT"."MSG" T 
WHERE substr("msg_time", 0, 10) = '2023-06-12'
    AND T."sender_account" = '13504113666'
    AND T."receiver_account" = '18182767005' LIMIT 100;复制代码
```

##### 3.2 具体实现

1. 编写`PhoenixChatMessageService`实现`ChatMessageService`接口

2. 在构造器中创建JDBC连接

   a) JDBC驱动为：`PhoenixDriver.class.getName()`

   b) JDBC连接URL为：`jdbc:phoenix:localhost:2181`

3. 基于JDBC实现`getMessage`查询

4. 在close方法中

```
public class PhoenixChatMessageService implements ChatMessageService {
    private Connection connection;

    public PhoenixChatMessageService() {
        try {
            Class.forName(PhoenixDriver.class.getName());
            connection = DriverManager.getConnection("jdbc:phoenix:localhost:2181");
        } catch (ClassNotFoundException e) {
            throw new RuntimeException("加载Phoenix驱动失败!");
        } catch (SQLException e) {
            throw new RuntimeException("获取Phoenix JDBC连接失败!");
        }
    }

    @Override
    public List<Msg> getMessage(String date, String sender, String receiver) throws Exception {
        PreparedStatement ps = connection.prepareStatement(
                "SELECT * FROM WEIXIN_CHAT.MSG T WHERE substr(\"msg_time\", 0, 10) = ? "
                        + "AND T.\"sender_account\" = ? "
                        + "AND T.\"receiver_account\" = ? ");

        ps.setString(1, date);
        ps.setString(2, sender);
        ps.setString(3, receiver);

        ResultSet rs = ps.executeQuery();
        List<Msg> msgList = new ArrayList<>();

        while(rs.next()) {
            Msg msg = new Msg();
            msg.setMsg_time(rs.getString("msg_time"));
            msg.setSender_nickyname(rs.getString("sender_nickyname"));
            msg.setSender_account(rs.getString("sender_account"));
            msg.setSender_sex(rs.getString("sender_sex"));
            msg.setSender_ip(rs.getString("sender_ip"));
            msg.setSender_os(rs.getString("sender_os"));
            msg.setSender_phone_type(rs.getString("sender_phone_type"));
            msg.setSender_network(rs.getString("sender_network"));
            msg.setSender_gps(rs.getString("sender_gps"));
            msg.setReceiver_nickyname(rs.getString("receiver_nickyname"));
            msg.setReceiver_ip(rs.getString("receiver_ip"));
            msg.setReceiver_account(rs.getString("receiver_account"));
            msg.setReceiver_os(rs.getString("receiver_os"));
            msg.setReceiver_phone_type(rs.getString("receiver_phone_type"));
            msg.setReceiver_network(rs.getString("receiver_network"));
            msg.setReceiver_gps(rs.getString("receiver_gps"));
            msg.setReceiver_sex(rs.getString("receiver_sex"));
            msg.setMsg_type(rs.getString("msg_type"));
            msg.setDistance(rs.getString("distance"));
            msgList.add(msg);
        }
        return msgList;
    }

    @Override
    public void close() {
        try {
            connection.close();
        } catch (SQLException e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args) throws Exception {
        ChatMessageService chatMessageService = new PhoenixChatMessageService();
        List<Msg> message = chatMessageService.getMessage("2023-06-12", "13504113666", "18182767005");

        for (Msg msg : message) {
            System.out.println(msg);
        }

        chatMessageService.close();
    }
}
```





