# Flink笔记

## 一、Flink简介

### 1、Spark Streaming和Flink的区别

- **Spark Streaming**

  - **微批处理（Micro-batching）**：Spark Streaming 将实时数据流分成小批次（通常每批次几百毫秒到几秒），然后以批处理的方式处理这些数据。每个微批次数据被看作一个 RDD（弹性分布式数据集），由 Spark 的核心引擎处理。
  - **延迟**：由于微批处理的方式，延迟通常较大，通常在秒级。
  - **优势：**
    编程原语丰富，编程简单
    框架封装层级较高，封装性好
    可以共用批处理处理逻辑，兼容性好
    基于Spark，可以无缝内嵌Spark其他子项目，如Spark Sql，MLlib等
    **劣势：**
    微批处理，时间延迟大
    稳定性相对较差
    机器性能消耗较大

- **Flink**

  - **真正的流处理（True Stream Processing）**：Flink 采用事件驱动模型，逐条处理事件，不分批次。Flink 的处理模型更接近实时，延迟通常在毫秒级。
  - **低延迟**：由于逐条处理数据，Flink 的延迟比 Spark Streaming 更低，适合对延迟要求较高的场景。
  - **优势：**
    Flink流处理为先的方法可提供低延迟，高吞吐率，近乎逐项处理的能力
    Flink的很多组件是自行管理的
    通过多种方式对工作进行分析进而优化任务
    提供了基于Web的调度视图

- **主流实时计算框架对比**

  |                 | **模型**       | **API** | **保证次数**  | **容错机制**   | **延时** | **吞吐量** | **批流统一** | **业务模式** | **易用性** |
  | --------------- | -------------- | :-----: | ------------- | -------------- | -------- | ---------- | ------------ | ------------ | ---------- |
  | Storm           | Native         | 组合式  | At-least-once | Record ACKs    | ★★★      | ★          | 不支持       | 需要其他框架 | ★          |
  | Spark Streaming | Mirco-batching | 声明式  | Exectly-once  | RDD Checkpoint | ★        | ★★★        | 支持         | 需要其他框架 | ★★         |
  | Apache Flink    | Native         | 组合式  | Exectly-once  | Checkpoint     | ★★★      | ★★★        | 支持         | 需要其他框架 | ★★         |

### 2、什么是Flink

AApache Flink 是**一个实时计算框架和分布式处理引擎，用于在无边界和有边界数据流上进行有状态的计算**。Flink 能在所有常见集群环境中运行，并能以内存速度和任意规模进行计算。

![image-20240527214723092](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202405272147162.png)

这个图展示了一个典型的数据处理架构，强调了 Apache Flink 的应用场景和功能。让我们从左到右分步骤详细解释这个图。

**数据输入**

图的左侧列出了一些数据来源：

- **Transactions（交易）**
- **Logs（日志）**
- **IoT（物联网）**
- **Clicks（点击）**
- **...**（其他数据源）

这些数据源可以生成**实时事件（Real-time Events）**或存储在**数据库、文件系统、KV-Store**中。

**数据处理框架**

中间部分是核心，展示了 Apache Flink 如何处理这些数据：

1. **Event-driven Applications（事件驱动应用）**
   - 处理实时事件。
   - Flink 可以处理来自各种实时数据源的事件，以构建实时反应的应用。

2. **Streaming Pipelines（流处理管道）**
   - 流处理工作流。
   - Flink 提供了流处理管道，处理连续不断的数据流，支持复杂的事件处理、数据变换和聚合。

3. **Stream & Batch Analytics（流和批分析）**
   - 同时支持流处理和批处理分析。
   - Flink 能够在同一个系统中处理流数据和批数据，提供统一的分析平台。

**资源与存储**

Flink 的处理依赖于各种资源和存储系统：

- **Resources（资源）**：
  - **K8s（Kubernetes）**
  - **Yarn**
  - **Mesos**
  - 这些资源管理平台用于管理和调度 Flink 集群中的计算资源。

- **Storage（存储）**：
  - **HDFS（Hadoop Distributed File System）**
  - **S3（Amazon Simple Storage Service）**
  - **NFS（Network File System）**
  - 这些存储系统用于保存 Flink 处理过程中使用和生成的数据。

**数据输出**

图的右侧展示了 Flink 处理后的数据去向：

- **Application（应用）**：处理后的数据可以直接驱动应用程序。
- **Event Log（事件日志）**：处理后的事件数据可以记录在事件日志中，用于审计或进一步分析。
- **Database, File System, KV-Store（数据库、文件系统、键值存储）**：结果数据可以保存到不同的存储系统中。

**数据流动过程**

数据从左侧的数据源开始，经过 Flink 的处理框架（事件驱动应用、流处理管道、流和批分析），最终输出到应用、事件日志或存储系统中。在这个过程中，Flink 依赖资源管理平台（如 Kubernetes、Yarn、Mesos）来调度计算资源，并使用各种存储系统（如 HDFS、S3、NFS）来保存数据。

**总结**

这张图展示了 Flink 在处理实时数据和批数据方面的强大能力。它可以接收多种数据源，通过流处理管道和事件驱动应用实时处理数据，并能同时支持流和批处理分析。处理后的数据可以应用于不同的场景，包括驱动应用程序、记录事件日志和保存到各种存储系统中。Flink 的架构依赖于资源管理平台和分布式存储系统，确保高效、可靠地处理大量数据。

### 3、Flink的特性

1. 支持高吞吐、低延迟、高性能的流处理
2. 支持带有事件时间的窗口（Window）操作
3. 支持有状态计算的Exactly-once语义
4. 支持高度灵活的窗口（Window）操作，支持基于time、count、session，以及data-driven的窗口操作
5. 支持具有反压功能的持续流模型
6. 支持基于轻量级分布式快照（Snapshot）实现的容错
7. 一个运行时同时支持Batch on Streaming处理和Streaming处理
8. Flink在JVM内部实现了自己的内存管理，避免了出现oom
9. 支持迭代计算
10. 支持程序自动优化：避免特定情况下Shuffle、排序等昂贵操作，中间结果有必要进行缓存

### 4、Flink的组件栈

![image-20240527211118225](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202405272111344.png)

**Apache Flink的组件栈：**
**Deployment层：**
Apache Flink可以部署在单JVM,，独立集群，Yarn，K8S等集群中运行。
**Runtime层：**
针对不同的执行环境，Flink 提供了一套统一的分布式作业执行引擎，也就是 Flink Runtime 这层。
Flink Runtime 层采用了标准 master-slave 的结构，master（JobManager ）负责管理整个集群中的资源和作业；Slave（TaskManager）负责提供具体的资源并实际执行作业。
API层&Libraries层：
**API层**
API蹭其中包含 DataStream API（应用于有界/无界数据流场景）和 DataSet API（应用于有界数据集场景）两部分。Core APIs 提供的流式 API（Fluent API）为数据处理提供了通用的模块组件，例如各种形式的用户自定义转换（transformations）、联接（joins）、聚合（aggregations）、窗口（windows）和状态（state）操作等。此层API 中处理的数据类型在每种编程语言中都有其对应的类。Process Function 这类底层抽象和 DataStream API 的相互集成使得用户可以选择使用更底层的抽象 API来实现自己的需求。DataSet API 还额外提供了一些原语，比如循环/迭代（loop/iteration）操作。
**Libraries层**
Table API是以表（Table）为中心的声明式编程（DSL）API，例如在流式数据场景下，它可以表示一张正在动态改变的表。Table API遵循（扩展）关系模型：即表拥有 schema（类似于关系型数据库中的 schema），
SQL这层抽象在语义和程序表达式上都类似于 Table API，但是其程序实现都是 SQL查询表达式。SQL 抽象与 Table API 抽象之间的关联是非常紧密的，并且 SQL 查询语句可以在 Table API中定义的表上执行。

### 5、Flink 中的 API

**Flink 为流式/批式处理应用程序的开发提供了不同级别的抽象。**

![image-20240527211622737](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202405272116792.png)

- Flink API 最底层的抽象为**有状态实时流处理**。其抽象实现是 [Process Function](https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/dev/datastream/operators/process_function/)，并且 **Process Function** 被 Flink 框架集成到了 [DataStream API](https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/dev/datastream/overview/) 中来为我们使用。它允许用户在应用程序中自由地处理来自单流或多流的事件（数据），并提供具有全局一致性和容错保障的*状态*。此外，用户可以在此层抽象中注册事件时间（event time）和处理时间（processing time）回调方法，从而允许程序可以实现复杂计算。

- Flink API 第二层抽象是 **Core APIs**。实际上，许多应用程序不需要使用到上述最底层抽象的 API，而是可以使用 **Core APIs** 进行编程：其中包含 [DataStream API](https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/dev/datastream/overview/)（应用于有界/无界数据流场景）和 [DataSet API](https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/dev/dataset/overview/)（应用于有界数据集场景）两部分。Core APIs 提供的流式 API（Fluent API）为数据处理提供了通用的模块组件，例如各种形式的用户自定义转换（transformations）、联接（joins）、聚合（aggregations）、窗口（windows）和状态（state）操作等。此层 API 中处理的数据类型在每种编程语言中都有其对应的类。

  *Process Function* 这类底层抽象和 *DataStream API* 的相互集成使得用户可以选择使用更底层的抽象 API 来实现自己的需求。*DataSet API* 还额外提供了一些原语，比如循环/迭代（loop/iteration）操作。

- Flink API 第三层抽象是 **Table API**。**Table API** 是以表（Table）为中心的声明式编程（DSL）API，例如在流式数据场景下，它可以表示一张正在动态改变的表。[Table API](https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/dev/table/overview/) 遵循（扩展）关系模型：即表拥有 schema（类似于关系型数据库中的 schema），并且 Table API 也提供了类似于关系模型中的操作，比如 select、project、join、group-by 和 aggregate 等。Table API 程序是以声明的方式定义*应执行的逻辑操作*，而不是确切地指定程序*应该执行的代码*。尽管 Table API 使用起来很简洁并且可以由各种类型的用户自定义函数扩展功能，但还是比 Core API 的表达能力差。此外，Table API 程序在执行之前还会使用优化器中的优化规则对用户编写的表达式进行优化。

  表和 *DataStream*/*DataSet* 可以进行无缝切换，Flink 允许用户在编写应用程序时将 *Table API* 与 *DataStream*/*DataSet* API 混合使用。

- Flink API 最顶层抽象是 **SQL**。这层抽象在语义和程序表达式上都类似于 *Table API*，但是其程序实现都是 SQL 查询表达式。[SQL](https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/dev/table/overview/#sql) 抽象与 Table API 抽象之间的关联是非常紧密的，并且 SQL 查询语句可以在 *Table API* 中定义的表上执行。

### 6、Flink的流批处理

- 流处理的代码举例

~~~Java
package com.shujia.core;

import org.apache.flink.api.common.typeinfo.Types;
import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.DataStream;


import org.apache.flink.streaming.api.datastream.KeyedStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;

public class Demo1StreamWordCount {
    public static void main(String[] args) throws Exception {
        //加载flink环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        //设置任务的并行度；一个并行度相当于一个task
        env.setParallelism(2);

        //设置延迟时间，默认的时间是200毫秒,单位是毫秒
        env.setBufferTimeout(100);

        //读取数据
        DataStream<String> dataStream = env.socketTextStream("master", 12345);

        //计算每个单词的个数
        DataStream<Tuple2<String, Integer>> wordDS = dataStream.map((word) -> Tuple2.of(word, 1), Types.TUPLE(Types.STRING, Types.INT));

        //通过keyBy进行分组
        KeyedStream<Tuple2<String, Integer>, String> keyByDS = wordDS.keyBy((kv) -> kv.f0);

        //通过sum进行聚合
        DataStream<Tuple2<String, Integer>> wordCountDS = keyByDS.sum(1);

        //打印数据
        wordCountDS.print();

        //触发程序执行
        env.execute();

    }
}
~~~

- flink持续流处理的模型

![flink持续流模型](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202405272216587.png)

- 批处理的代码

~~~java
package com.shujia.core;

import org.apache.flink.api.common.RuntimeExecutionMode;
import org.apache.flink.api.common.typeinfo.Types;

import org.apache.flink.api.java.tuple.Tuple2;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.datastream.KeyedStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;


public class Demo2BatchWordCount {
    public static void main(String[] args) throws Exception {

        //加载flink环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        //设置任务的并行度；一个并行度相当于一个task
        env.setParallelism(2);

        //设置延迟时间，默认的时间是200毫秒,单位是毫秒
        env.setBufferTimeout(100);

        /*
         *处理模式
         * RuntimeExecutionMode.BATCH：批处理模式（MapReduce模型）
         * 1、输出最终结果
         * 2、批处理模式只能用于处理有界流
         *
         * RuntimeExecutionMode.STREAMING：流处理模式（持续流模型）
         * 1、输出连续结果
         * 2、流处理模式，有界流核无界流都可以处理
         */
        //设置数据的处理模式
        env.setRuntimeMode(RuntimeExecutionMode.BATCH);

        //读取数据
        DataStream<String> dataStream = env.readTextFile("flink/data/words.txt");


        //计算每个单词的个数
        DataStream<Tuple2<String, Integer>> wordDS = dataStream.map((word) -> Tuple2.of(word, 1), Types.TUPLE(Types.STRING, Types.INT));

        //通过keyBy进行分组
        KeyedStream<Tuple2<String, Integer>, String> keyByDS = wordDS.keyBy((kv) -> kv.f0);

        //通过sum进行聚合
        DataStream<Tuple2<String, Integer>> wordCountDS = keyByDS.sum(1);

        //打印数据
        wordCountDS.print();

        //触发程序执行
        env.execute();


    }
}
~~~

## 二、Data Sources

### 1、分为四类

> **Flink 在流处理和批处理上的 source 大概有 4 类：**
> 基于本地集合的 source、
> 基于文件的 source、
> 基于网络套接字的 source、
> 自定义的 source。自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API、Apache NiFi 等，当然你也可以定义自己的 source。

### 2、具体介绍

#### 2.1、基于本地集合的source

- `fromCollection(Collection)` - 从 Java Java.util.Collection 创建数据流。集合中的所有元素必须属于同一类型。

~~~Java
 //创建flink环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        //创建List集合
        ArrayList<String> wordsList = new ArrayList<>();

        //添加集合元素
        wordsList.add("java");
        wordsList.add("java");
        wordsList.add("flink");
        wordsList.add("flink");
        wordsList.add("hadoop");
        wordsList.add("hadoop");

        /*
         *基于集合的source  ---  有界流
         */

        //读取数据
        DataStream<String> wordDS = env.fromCollection(wordsList);

        //打印结果
        wordDS.print();

        //启动任务
        env.execute();
~~~

#### 2.2、基于文件的source

- `readTextFile(path)` - 读取文本文件，例如遵守 TextInputFormat 规范的文件，逐行读取并将它们作为字符串返回。

- `readFile(fileInputFormat, path)` - 按照指定的文件输入格式读取（一次）文件。

- `readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo)` - 这是前两个方法内部调用的方法。它基于给定的 `fileInputFormat` 读取路径 `path` 上的文件。根据提供的 `watchType` 的不同，source 可能定期（每 `interval` 毫秒）监控路径上的新数据（watchType 为 `FileProcessingMode.PROCESS_CONTINUOUSLY`），或者处理一次当前路径中的数据然后退出（watchType 为 `FileProcessingMode.PROCESS_ONCE`)。使用 `pathFilter`，用户可以进一步排除正在处理的文件。

  *实现：*

  在底层，Flink 将文件读取过程拆分为两个子任务，即 *目录监控* 和 *数据读取*。每个子任务都由一个单独的实体实现。监控由单个**非并行**（并行度 = 1）任务实现，而读取由多个并行运行的任务执行。后者的并行度和作业的并行度相等。单个监控任务的作用是扫描目录（定期或仅扫描一次，取决于 `watchType`），找到要处理的文件，将它们划分为 *分片*，并将这些分片分配给下游 reader。Reader 是将实际获取数据的角色。每个分片只能被一个 reader 读取，而一个 reader 可以一个一个地读取多个分片。

  *重要提示：*

  1. 如果 `watchType` 设置为 `FileProcessingMode.PROCESS_CONTINUOUSLY`，当一个文件被修改时，它的内容会被完全重新处理。这可能会打破 “精确一次” 的语义，因为在文件末尾追加数据将导致重新处理文件的**所有**内容。
  2. 如果 `watchType` 设置为 `FileProcessingMode.PROCESS_ONCE`，source 扫描**一次**路径然后退出，无需等待 reader 读完文件内容。当然，reader 会继续读取数据，直到所有文件内容都读完。关闭 source 会导致在那之后不再有检查点。这可能会导致节点故障后恢复速度变慢，因为作业将从最后一个检查点恢复读取。

~~~java
//加载flink环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        /*
         * 老版本读取数据
         */
        //读取数据
        DataStream<String> wordDs = env.readTextFile("flink/data/words.txt");

        //打印数据
//        wordDs.print();

        /*
         * 新版本读取数据的方式,可以读取有界流和无界流
         */
        //读取有界流
//        FileSource<String> fileSource = FileSource.forRecordStreamFormat(
//                new TextLineInputFormat("UTF-8"),
//                new Path("flink/data/words.txt")
//        ).build();
//
//        DataStreamSource<String> fileSourceDS = env.fromSource(fileSource, WatermarkStrategy.noWatermarks(), "fileSourceDS");
//
//        fileSourceDS.print();

        //读取无界流
        FileSource<String> fileSource = FileSource.forRecordStreamFormat(
                //指定读取数据的编码
                new TextLineInputFormat("UTF-8"),
                //指定读取数据的路径
                new Path("flink/data/stu")
                )
                ////每隔一段时间读取目录下新的文件，构建无界流
                .monitorContinuously(Duration.ofSeconds(5))
                .build();

        DataStream<String> fileSourceDS = env.fromSource(fileSource, WatermarkStrategy.noWatermarks(), "fileSourceDS");

        DataStream<Tuple2<String, Integer>> clazzDS = fileSourceDS.map(lines -> Tuple2.of(lines.split(",")[4], 1), Types.TUPLE(Types.STRING, Types.INT));

        KeyedStream<Tuple2<String, Integer>, String> keyByDS = clazzDS.keyBy(kv -> kv.f0);
        DataStream<Tuple2<String, Integer>> countDS = keyByDS.sum(1);

        countDS.print();

        //启动任务
        env.execute();
~~~

#### 2.3、基于套接字的source

- 通过 nc -lk post(端口号) 启动一个输入流
- `socketTextStream` - 从套接字读取。元素可以由分隔符分隔。

~~~java
  //加载flink环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        //设置任务的并行度；一个并行度相当于一个task
        env.setParallelism(2);

        //设置延迟时间，默认的时间是200毫秒,单位是毫秒
        env.setBufferTimeout(100);

        //读取数据
        DataStream<String> dataStream = env.socketTextStream("master", 12345);
~~~

#### 2.4、基于自定义的source

- `addSource` - 关联一个新的 source function。

~~~Java
public class Demo3MySource {
    public static void main(String[] args) throws Exception{
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        //读取自定义数据
        DataStreamSource<Integer> sourceFunctionDS = env.addSource(new MySource());

        //打印结果
        sourceFunctionDS.print();

        //启动任务
        env.execute();
    }
}

/**
 * 自定义source
 * 实现SourceFunction接口，实现接口中的run方法
 */
class MySource implements SourceFunction<Integer> {

    /**
     * flink启动的时候会执行一次，再run方法中读取外部的数据，将数据发送到下游
     */
    @Override
    public void run(SourceContext<Integer> ctx) throws Exception {
        while (true) {
            ctx.collect(100);
            Thread.sleep(100);
        }

    }
    //cancel方法再任务取消的时候执行，一般用于回收资源
    @Override
    public void cancel() {

    }
}
~~~

**可以通过自定义的source连接到MySQL数据库等**

~~~java
public class Demo4MySQLSource {
    public static void main(String[] args) throws Exception{
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        //读取自定义数据
        DataStreamSource<Student> sourceFunctionDS = env.addSource(new MySQLSource());

        sourceFunctionDS.print();

        env.execute();

    }
}

@Data
@AllArgsConstructor
class Student {
    private int id;
    private String name;
    private int age;
    private String gender;
    private String clazz;
}

/**
 * 自定义source 读取mysql中的数据
 * 练习：自定义source 读取redis中的数据
 */
class MySQLSource implements SourceFunction<Student> {

    //run方法在任务启动的时候执行一次
    @Override
    public void run(SourceContext<Student> ctx) throws Exception {
        //加载驱动
        Class.forName("com.mysql.jdbc.Driver");

        //创建连接对象
        Connection conn = DriverManager.getConnection("jdbc:mysql://master:3306/bigdata29?useSSL=false", "root", "123456");

        //获取操作对象
        PreparedStatement statement = conn.prepareStatement("select * from students");

        //执行SQL语句
        ResultSet resultSet = statement.executeQuery();
        while (resultSet.next()) {
            int id = resultSet.getInt(1);
            String name = resultSet.getString(2);
            int age = resultSet.getInt(3);
            String gender = resultSet.getString(4);
            String clazz = resultSet.getString(5);

            //封装为学生对象
            Student student = new Student(id, name, age, gender, clazz);

            //将数据发送到下游
            ctx.collect(student);
        }


        //关闭资源
        statement.close();
        conn.close();

    }

    @Override
    public void cancel() {

    }
}
~~~

## 三、DataStream Transformations（算子）

### 1、什么是算子

- 用户通过算子能将一个或多个 DataStream 转换成新的 DataStream，在应用程序中可以将多个数据转换算子合并成一个复杂的数据流拓扑。

### 2、数据流转换

#### 2.1、Map

- **DataStream → DataStream** [#](https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/dev/datastream/operators/overview/#datastream-rarr-datastream)

- 输入一个元素同时输出一个元素。下面是将输入流中元素数值加倍的 map function：

~~~java
public class Demo1Map {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream<String> linesDS = env.socketTextStream("master", 12345);

        //匿名内部类的方式
//        SingleOutputStreamOperator<Tuple2<String, Integer>> wordDS = linesDS.map(new MapFunction<String, Tuple2<String, Integer>>() {
//
//            @Override
//            public Tuple2<String, Integer> map(String value) throws Exception {
//                return Tuple2.of(value, 1);
//            }
//        });

        //声明式写法
        DataStream<Tuple2<String, Integer>> wordDS = linesDS.map(word -> Tuple2.of(word, 1), Types.TUPLE(Types.STRING, Types.INT));

        KeyedStream<Tuple2<String, Integer>, String> keyByDS = wordDS.keyBy(kv -> kv.f0);

        DataStream<Tuple2<String, Integer>> countDS = keyByDS.sum(1);

        countDS.print();

        env.execute();

    }
}
~~~

#### 2.2、FlatMap

- **DataStream → DataStream** [#](https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/dev/datastream/operators/overview/#datastream-rarr-datastream-1)

- 输入一个元素同时产生零个、一个或多个元素。下面是将句子拆分为单词的 flatmap function：

~~~java
public class Demo2FlatMap {
    public static void main(String[] args) throws Exception{
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream<String> linesDS = env.socketTextStream("master", 12345);

        //匿名内部类方式
//        SingleOutputStreamOperator<Tuple2<String, Integer>> flatMapDS = linesDS.flatMap(new FlatMapFunction<String, Tuple2<String, Integer>>() {
//
//            @Override
//            public void flatMap(String line, Collector<Tuple2<String, Integer>> out) throws Exception {
//                String[] words = line.split(" ");
//                for (String word : words) {
//                    out.collect(Tuple2.of(word, 1));
//
//                }
//            }
//        });


        //声明式要指定数据类型
        SingleOutputStreamOperator<Tuple2<String, Integer>>flatMapDS = linesDS.flatMap((line, out) -> {
            String[] words = line.split(" ");
            for (String word : words) {
                out.collect(Tuple2.of(word, 1));
            }
        }, Types.TUPLE(Types.STRING,Types.INT));

        KeyedStream<Tuple2<String, Integer>, String> keyByDS = flatMapDS.keyBy(kv -> kv.f0);

        SingleOutputStreamOperator<Tuple2<String, Integer>> countDS = keyByDS.sum(1);

        countDS.print();

        env.execute();
    }
}
~~~

#### 2.3、Filter

- **DataStream → DataStream** [#](https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/dev/datastream/operators/overview/#datastream-rarr-datastream-2)

- 为每个元素执行一个布尔 function，并保留那些 function 输出值为 true 的元素。下面是过滤掉零值的 filter：

~~~java
public class Demo3Filter {
    public static void main(String[] args) throws Exception{
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStreamSource<String> studentDS = env.readTextFile("flink/data/students.csv");

        //匿名内部类方式
        SingleOutputStreamOperator<String> filterDS = studentDS.filter(new FilterFunction<String>() {
            @Override
            public boolean filter(String line) throws Exception {
                return "文科一班".equals(line.split(",")[4]);
            }
        });

        //声明式
//        SingleOutputStreamOperator<String> filterDS = studentDS.filter(line -> "文科一班".equals(line.split(",")[4]));

        filterDS.print();

        env.execute();


    }
}
~~~

#### 2.4、KeyBy

- **DataStream → KeyedStream [#](https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/dev/datastream/operators/overview/#datastream-rarr-keyedstream)**

- 在逻辑上将流划分为不相交的分区。具有相同 key 的记录都分配到同一个分区。在内部， *keyBy()* 是通过哈希分区实现的。有多种[指定 key ](https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/dev/datastream/fault-tolerance/state/#keyed-datastream)的方式。
- 以下情况，一个类**不能作为 key**：
  1. 它是一种 POJO 类，但没有重写 hashCode() 方法而是依赖于 Object.hashCode() 实现。
  2. 它是任意类的数组。

~~~java
public class Demo4KeyBy {
    public static void main(String[] args) throws Exception{
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStreamSource<String> linesDS = env.socketTextStream("master", 12345);

        //匿名内部类方式
        KeyedStream<String, String> keyByDS = linesDS.keyBy(new KeySelector<String, String>() {
            @Override
            public String getKey(String word) throws Exception {

                return word;
            }
        });

        keyByDS.print();

        //lambda表达式
        //keyBy:将相同的key发送到同一个task中
//        KeyedStream<String, String> keyByDS = linesDS.keyBy(word -> word);

        env.execute();
    }
}
~~~

#### 2.5、Reduce

- **KeyedStream → DataStream [#](https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/dev/datastream/operators/overview/#keyedstream-rarr-datastream)**

- 在相同 key 的数据流上“滚动”执行 reduce。将当前元素与最后一次 reduce 得到的值组合然后输出新值。

~~~java
public class Demo5Reduce {
    public static void main(String[] args) throws Exception{
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream<String> streamSource = env.readTextFile("flink/data/words.txt");

        DataStream<String> flatMapDS = streamSource.flatMap((FlatMapFunction<String, String>) (line, out) -> {
            String[] words = line.split("\\|");
            for (String word : words) {
                out.collect(word);
            }
        },Types.STRING);;


        DataStream<Tuple2<String, Integer>> mapDS = flatMapDS.map(word -> Tuple2.of(word, 1), Types.TUPLE(Types.STRING, Types.INT));

        KeyedStream<Tuple2<String, Integer>, String> keyByDS = mapDS.keyBy(kv -> kv.f0);
        //匿名内部类
//        DataStream<Tuple2<String, Integer>> reduceDS = keyByDS.reduce(new ReduceFunction<Tuple2<String, Integer>>() {
//            @Override
//            public Tuple2<String, Integer> reduce(Tuple2<String, Integer> kv1, Tuple2<String, Integer> kv2) throws Exception {
//                return Tuple2.of(kv1.f0, kv1.f1 + kv2.f1);
//            }
//        });
        //声明式
        SingleOutputStreamOperator<Tuple2<String, Integer>> reduceDS = keyByDS.reduce((kv1, kv2) -> Tuple2.of(kv1.f0, kv1.f1 + kv2.f1));

        reduceDS.print();

        env.execute();

    }
}
~~~

#### 2.6、Window

- **KeyedStream → WindowedStream [#](https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/dev/datastream/operators/overview/#keyedstream-rarr-windowedstream)**

- 可以在已经分区的 KeyedStreams 上定义 Window。Window 根据某些特征（例如，最近 5 秒内到达的数据）对每个 key Stream 中的数据进行分组。请参阅 [windows](https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/dev/datastream/operators/windows/) 获取有关 window 的完整说明。

~~~java
public class Demo6Window {
    public static void main(String[] args) throws Exception {
        //加载环境
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        //读取数据
        DataStream<String> streamSource = env.socketTextStream("master", 12345);

        //转换kv
        DataStream<Tuple2<String, Integer>> mapDS = streamSource.map(word -> Tuple2.of(word, 1), Types.TUPLE(Types.STRING, Types.INT));

        //分组
        KeyedStream<Tuple2<String, Integer>, String> keyByDS = mapDS.keyBy(kv -> kv.f0);

        //设置滑动窗口
        //SlidingProcessingTimeWindows:滑动的处理时间窗口
        WindowedStream<Tuple2<String, Integer>, String, TimeWindow> windowDS = keyByDS.window(
                //传入窗口的大小以及滑动的距离
                SlidingProcessingTimeWindows.of(Time.seconds(15),Time.seconds(5))
        );

        //计算每个单词的个数
        DataStream<Tuple2<String, Integer>> countDS = windowDS.sum(1);

        //打印结果
        countDS.print();

        //启动任务
        env.execute();


    }
}
~~~

#### 2.7、Union

- **DataStream* → DataStream [#](https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/dev/datastream/operators/overview/#datastream-rarr-datastream-3)**

- 将两个或多个数据流联合来创建一个包含所有流中数据的新流。注意：如果一个数据流和自身进行联合，这个流中的每个数据将在合并后的流中出现两次。

~~~Java
public class Demo7Union {
    public static void main(String[] args) throws Exception{
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStreamSource<String> streamSource = env.socketTextStream("master", 12345);

        DataStreamSource<String> streamSource1 = env.socketTextStream("master", 8888);

        /*
         * union:合并两个DS
         * 在数据层面并没有合并，只是在逻辑层面合并了
         */
        DataStream<String> unionDS = streamSource.union(streamSource1);

        unionDS.print();

        env.execute();
    }
}
~~~

#### 2.8、ProcessFunction

- `ProcessFunction`是一种低级流处理操作，可以访问所有（非循环）流应用程序的基本构建块

~~~Java
public class Demo8ProcessFunction {
    public static void main(String[] args) throws Exception{
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStreamSource<String> streamSource = env.socketTextStream("master", 12345);

        /*
         * process算子是flink的底层算子吗，可以代替map flatMap filter算子
         */
        SingleOutputStreamOperator<Tuple2<String, Integer>> processDS = streamSource.process(new ProcessFunction<String, Tuple2<String, Integer>>() {

            /**
             * processElement:相当于flatMap。每一条数据执行一次，可以返回一条也可以返回多条
             * line：一行数据
             * ctx：上下文对象（代表flink的执行环境）
             * out： 用于将数据发送到下游
             */
            @Override
            public void processElement(String line, ProcessFunction<String, Tuple2<String, Integer>>.Context ctx, Collector<Tuple2<String, Integer>> out) throws Exception {
                String[] words = line.split(" ");
                for (String word : words) {
                    Tuple2<String, Integer> tuple2 = Tuple2.of(word, 1);
                    out.collect(tuple2);
                }
            }
        });

        processDS.print();

        env.execute();

    }
}
~~~

## 四、Data Sinks

### 1、概述

- 在 Apache Flink 中，数据接收器（Data Sinks）用于将处理后的数据发送到外部系统

### 2、分为四大类

- 写入文件、
  打印出来、
  写入 socket 、
  自定义的 sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 sink。

### 3、具体介绍

- 主要介绍常用的两类

#### 3.1、FileSink

```java
public class Demo1FileSink {
    public static void main(String[] args) throws Exception{
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStreamSource<String> wordDs = env.socketTextStream("master", 12345);

        FileSink<String> fileSink = FileSink
                .<String>forRowFormat(new Path("flink/data/words"), new SimpleStringEncoder<>("UTF-8"))
                //指定滚动策略
                .withRollingPolicy(
                        DefaultRollingPolicy.builder()
                                //包含了至少10秒的数据量
                                .withRolloverInterval(Duration.ofSeconds(10))
                                //从没接收延时10秒之外的新纪录
                                .withInactivityInterval(Duration.ofSeconds(10))
                                //文件大小已经达到 1MB（写入最后一条记录之后）
                                .withMaxPartSize(MemorySize.ofMebiBytes(1))
                                .build()
                ).build();

        wordDs.sinkTo(fileSink);

        env.execute();
    }
}
```

#### 3.2、自定义Sink

~~~java 
public class Demo2MySink {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStreamSource<String> wordsDs = env.socketTextStream("master", 12345);

        wordsDs.addSink(new MySink());

        env.execute();
    }
}

//自定义sink
class MySink implements SinkFunction<String> {
    @Override
    public void invoke(String value, Context context) throws Exception {
        //自定义数据sink的位置
        System.out.println("自定义sink:" + value);
    }
}
~~~

- 通过自定义Sink将数据保存到MySQL数据库中

~~~Java
public class Demo3MySQLSink {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream<Student> studentDS = env.addSource(new MySQLSource());

        DataStream<Tuple2<String, Integer>> genderDS = studentDS.map(stu -> Tuple2.of(stu.getGender(), 1), Types.TUPLE(Types.STRING, Types.INT));
        KeyedStream<Tuple2<String, Integer>, String> keyByDS = genderDS.keyBy(kv -> kv.f0);
        SingleOutputStreamOperator<Tuple2<String, Integer>> sumDS = keyByDS.sum(1);

        sumDS.addSink(new MySQLSink());

        env.execute();

    }
}


@Data
@AllArgsConstructor
class Student {
    private int id;
    private String name;
    private int age;
    private String gender;
    private String clazz;
}

/**
 * 自定义source 读取mysql中的数据
 *
 */
class MySQLSource implements SourceFunction<Student> {

    //run方法在任务启动的时候执行一次
    @Override
    public void run(SourceContext<Student> ctx) throws Exception {
        //加载驱动
        Class.forName("com.mysql.jdbc.Driver");

        //创建连接对象
        Connection conn = DriverManager.getConnection("jdbc:mysql://master:3306/bigdata29?useSSL=false&characterEncoding=UTF-8", "root", "123456");

        //获取操作对象
        PreparedStatement statement = conn.prepareStatement("select * from students");

        //执行SQL语句
        ResultSet resultSet = statement.executeQuery();
        while (resultSet.next()) {
            int id = resultSet.getInt(1);
            String name = resultSet.getString(2);
            int age = resultSet.getInt(3);
            String gender = resultSet.getString(4);
            String clazz = resultSet.getString(5);

            //封装为学生对象
            Student student = new Student(id, name, age, gender, clazz);

            //将数据发送到下游
            ctx.collect(student);
        }


        //关闭资源
        statement.close();
        conn.close();

    }

    @Override
    public void cancel() {

    }
}

class MySQLSink extends RichSinkFunction<Tuple2<String, Integer>> {
    private Connection conn;
    private PreparedStatement statement;

    @Override
    public void open(Configuration parameters) throws Exception {
        //加载驱动
        Class.forName("com.mysql.jdbc.Driver");

        //创建连接对象
        conn = DriverManager.getConnection("jdbc:mysql://master:3306/bigdata29?useSSL=false&characterEncoding=UTF-8", "root", "123456");

        //获取操作对象
        statement = conn.prepareStatement("replace into gender_count values(?,?)");
    }

    @Override
    public void close() throws Exception {
        statement.close();
        conn.close();
    }

    @Override
    public void invoke(Tuple2<String, Integer> kv, Context context) throws Exception {
        statement.setString(1, kv.f0);
        statement.setInt(2, kv.f1);
        statement.execute();
    }

}
~~~

## 五、Flink的集群搭建

### 1、Standalone(独立集群)

#### 1.1、上传解压配置环境变量

~~~shell
# 解压
tar -xvf flink-1.15.2-bin-scala_2.12.tgz 

# 配置环境变量
vim /etc/profile

export FLINK_HOME=/usr/local/soft/flink-1.15.2
export PATH=$PATH:$FLINK_HOME/bin

source /etc/profile
~~~

#### 1.2、修改配置文件

- flink-conf.yaml

~~~shell
jobmanager.rpc.address: master
jobmanager.bind-host: 0.0.0.0
taskmanager.bind-host: 0.0.0.0
taskmanager.host: localhost # noe1和node2需要单独修改
taskmanager.numberOfTaskSlots: 4
rest.address: master
rest.bind-address: 0.0.0.0
~~~

- masters

~~~shell
master:8081
~~~

- workers

~~~shell
node1
node2
~~~

#### 1.3、同步到所有节点

~~~shell
scp -r flink-1.15.2 node1:`pwd`
scp -r flink-1.15.2 node2:`pwd`

# 修改node1和node2中地taskmanager.host
taskmanager.host: node1
taskmanager.host: node2
~~~

#### 1.4、启动Flink独立集群

~~~shell
start-cluster.sh

# stop-cluster.sh

# flink web ui
http://master:8081
~~~

#### 1.5、提交任务

- 将代码打包上传到服务器提交

```shell
flink run -c com.shujia.core.Demo1StreamWordCount flink-1.0.jar
```

- 在flink web ui中直接提交

### 2、Flink on yarn

> flink on yarn模式:将flink地任务提交到yarn上运行

#### 2.1、整合到yarn

~~~shell
# 在环境变量中配置HADOOP_CLASSSPATH

vim /etc/profile

export HADOOP_CLASSPATH=`hadoop classpath`

source /etc/profile
~~~

#### 2.2、Flink on yarn部署模式

> 部署模式总共有三种

- **Application Mode**

> 1、将任务提交到yarn上运行，yarn会为每一个flink地任务启动一个jobmanager和一个或者多个taskmanasger
> 2、代码main函数不再本地运行，dataFlow不再本地构建，如果代码报错在本地看不到详细地错误日志

~~~shell 
flink run-application -t yarn-application -c com.shujia.core.Demo1StreamWordCount flink-1.0.jar

# 查看yarn的日志
yarn logs -applicationId application_1717039073374_0001
~~~

- **Per-Job Cluster Mode**

> 1、将任务提交到yarn上运行，yarn会为每一个flink地任务启动一个jobmanager和一个或者多个taskmanasger
> 2、代码地main函数在本地启动，在本地构建dataflow，再将dataflow提交给jobmanager,如果代码报错再本地可以烂到部分错误日志

~~~shell
flink run -t yarn-per-job -c com.shujia.core.Demo1StreamWordCount flink-1.0.jar
~~~

- **Session Mode**

> 1、先再yarn中启动一个jobmanager,  不启动taskmanager
> 2、提交任务地时候再动态申请taskmanager
> 3、所有使用session模式提交的任务共享同一个jobmanager
> 4、类似独立集群，只是集群在yarn中启动了，可以动态申请资源
> 5、一般用于测试

~~~shell 
# 1、先启动会话集群
yarn-session.sh -d

# 2、在提交任务
flink run -t yarn-session -Dyarn.application.id=application_1717068381486_0003  -c com.shujia.core.Demo1StreamWordCount flink-1.0.jar

# 在网页中直接提交
~~~

## 六、Flink的架构及其工作原理

### 1、解析Flink集群

>  Flink 运行时由两种类型的进程组成：一个 *JobManager* 和一个或者多个 *TaskManager*。

![image-20240530202535396](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202405302025493.png)

1. **Flink Program**：
   - **Program Code**：这是用户编写的Flink程序，包含数据处理的逻辑。
   - **Optimizer / Graph Builder**：当用户提交Flink程序时，首先由客户端（Client）解析程序并构建一个数据流图（Dataflow Graph）。
2. **Client**：
   - **Actor System**：Flink的Client通过Actor System与JobManager进行通信。
   - **Dataflow Graph**：客户端生成的数据流图（Dataflow Graph）会被提交给JobManager。
3. **JobManager**：
   - **Actor System**：JobManager也通过Actor System与Client和TaskManager进行通信。
   - **Dataflow Graph**：JobManager接收到数据流图后，负责将其转换为实际的执行图。
   - **Scheduler**：调度器负责分配任务，并决定在集群中的哪个TaskManager上运行哪些任务。
   - **Checkpoint Coordinator**：负责协调检查点操作，以确保在出现故障时能够从最近的检查点恢复。
4. **TaskManager**：
   - **Task Slots**：每个TaskManager拥有多个任务槽（Task Slot），每个任务槽可以独立执行一个任务（Task）。
   - **Memory & I/O Manager**：管理内存和I/O操作，以确保高效的数据处理。
   - **Network Manager**：管理网络通信，确保TaskManager之间的数据流传输顺畅。
   - **Actor System**：与JobManager和其他TaskManager进行通信。

### 2、TaskSlot和并行度

#### 2.1、TaskSlot

- ##### Slot 是指 TaskManager 最大能并发执行的能力

> Task Slot 是 Flink 中用于执行并行任务的物理资源单元。它们可以被视为一个 TaskManager（任务管理器）上的资源分配单位。
>
> 每个 Task Slot 可以运行一个并行任务，但也可以配置多个 Task Slot 来支持更高的并行度。
>
> Task Slot 通常与计算资源相关联，比如一个 CPU 核心、一段内存等。

#### 2.2、Parallelism(并行度)

- **parallelism 是指 TaskManager 实际使用的并发能力**

> Parallelism 是指在同一时间处理数据的任务数量。在 Flink 中，它可以应用于整个作业（全局并行度）或单个算子（局部并行度）。
>
> Flink 作业的并行度可以在作业提交时指定，也可以在算子级别进行设置。通过设置并行度，可以控制作业的性能和资源利用率。

#### 2.3、设置并行度的方式

- 在代码中设置:env.setParallelism(2) 

- 在提交任务是通过参数设置 -p    (推荐使用)
- 在配置文件中统一设置
- 每一个算子可以单独设置并行度

> 算子设置并行度 > env 设置并行度 > 配置文件默认并行度。

#### 2.4、Flink的共享资源

> 1、flink需要资源的数量和task数量无关
> 2、一个并行度对应一个资源（slot）
> 3、上游task的下游task共享同一个资源

#### 2.5、并行度设置的原则

> 1、实时计算的任务并行度取决于数据的吞吐量
> 2、聚合计算（有shuffle）的代码一个并行度大概一秒可以处理10000条数据左右
> 3、非聚合计算是，一个并行度大概一秒可以处理10万条左右

### 3、Event Time（事件时间） and Processing Time （处理时间）

![image-20240530214512533](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202405302145855.png)

#### 3.1、Processing Time

> 处理时间是指执行相应操作的机器的系统时间。

~~~java 
public class Demo4ProcTime {
    public static void main(String[] args) throws Exception {

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        DataStream<String> wordsDS = env.socketTextStream("master", 8888);

        //转换成kv
        DataStream<Tuple2<String, Integer>> kvDS = wordsDS
                .map(word -> Tuple2.of(word, 1), Types.TUPLE(Types.STRING, Types.INT));

        //按照单词分组
        KeyedStream<Tuple2<String, Integer>, String> keyByDS = kvDS.keyBy(kv -> kv.f0);

        //划分窗口
        //TumblingProcessingTimeWindows:股东的处理时间窗口
        WindowedStream<Tuple2<String, Integer>, String, TimeWindow> windowDS = keyByDS
                .window(TumblingProcessingTimeWindows.of(Time.seconds(5)));

        //统计单词的数量
        DataStream<Tuple2<String, Integer>> countDS = windowDS.sum(1);

        countDS.print();

        env.execute();
    }
}
~~~

#### 3.2、Event Time

> 事件时间是每个单独事件在其产生设备上发生的时间。这个时间通常在记录进入Flink之前嵌入到记录中，并且可以从每个记录中提取事件时间戳。在事件时间中，时间的进展取决于数据，而不是任何操作机器的系统时间（例如：window系统时间）。事件时间程序必须指定如何生成事件时间水位线，这是在事件时间中表示进程的机制。

#### 3.3、水位线

> 水位线等于最新一条数据的时间戳，在Flink中测量事件时间进展的机制是水位线。水位线作为数据流的一部分，并携带时间戳t。水位线(t)声明事件时间在该流中已经达到时间t，这意味着流中不应该再有时间戳t <= t的元素(即时间戳比水位线更早或等于水位线的事件)。这种情况只对于数据流是按照事件中的时间戳顺序排列的

![image-20240530222001513](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202405302220657.png)

> 但是，数据流可能是无序的，这会导致watermark 一旦越过窗口结束的 timestamp，小于watermark 的事件时间不能触发窗口，从而导致数据丢失，可以设置一些参数让水位线延迟，使小于水位线的数据可以再次触发窗口

- 水位线的对齐

> 1、当上游有多个task时，下游task会取上游task水位线的最小值，如果数据量小。水位线就很难对齐，窗口就不会触发计算

~~~Java
public class Demo5EventTime {
    public static void main(String[] args) throws Exception {

        /*
         * 事件时间：数据中有一个时间字段，使用数据的时间字段触发计算，代替真实的时间，可以反应数据真实发生的顺序，计算更有意义
         */

        /*
java,1685433130000
java,1685433131000
java,1685433132000
java,1685433134000
java,1685433135000
java,1685433137000
java,1685433139000
java,1685433140000
java,1685433170000
         */
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        /*
         *水位线对齐
         * 1、当上游有多个task时，下游task会取上游task水位线的最小值，如果数据量小。水位线就很难对齐，窗口就不会触发计算
         */

        env.setParallelism(1);

        DataStream<String> linesDS = env.socketTextStream("master", 8888);

        //解析数据
        DataStream<Tuple2<String, Long>> tsDS = linesDS.map(line -> {
            String[] split = line.split(",");
            String word = split[0];
            long ts = Long.parseLong(split[1]);
            return Tuple2.of(word, ts);
        }, Types.TUPLE(Types.STRING, Types.LONG));

        /*
         * 指定时间字段和水位线生成策略
         */
        DataStream<Tuple2<String, Long>> assDS = tsDS
                .assignTimestampsAndWatermarks(
                        WatermarkStrategy
                                //指定水位线生产策略，水位线等于最新一条数据的时间戳，如果数据乱序可能会丢失数据
                                //.<Tuple2<String, Long>>forMonotonousTimestamps()
                                //水位线前移时间（数据最大乱序时间）
                                .<Tuple2<String, Long>>forBoundedOutOfOrderness(Duration.ofSeconds(5))
                                //指定时间字段
                                .withTimestampAssigner((event, ts) -> event.f1)
                );


        /*
         *每隔5秒统计单词的数量
         */
        DataStream<Tuple2<String, Integer>> kvDS = assDS
                .map(kv -> Tuple2.of(kv.f0, 1), Types.TUPLE(Types.STRING, Types.INT));

        KeyedStream<Tuple2<String, Integer>, String> keyByDS = kvDS
                .keyBy(kv -> kv.f0);

        //TumblingEventTimeWindows:滚动的事件时间窗口
        WindowedStream<Tuple2<String, Integer>, String, TimeWindow> windowDS = keyByDS
                .window(TumblingEventTimeWindows.of(Time.seconds(5)));

        windowDS.sum(1).print();

        env.execute();

    }
}
~~~

### 4、窗口

> 窗口总共分为三大类：
>
> 1、Time Window
>
> 2、Count Window
>
> 3、Session Window

#### 4.1、Time Window

> 时间窗口分为四种：
>
> 1、滚动处理时间窗口（TumblingProcessingTimeWindows）
>
> 2、滚动事件时间窗口（TumblingEventTimeWindows）
>
> 3、滑动处理时间窗口（SlidingProcessingTimeWindows）
>
> 4、滑动事件时间窗口（SlidingEventTimeWindows）

- **滚动处理时间窗口（TumblingProcessingTimeWindows）**

> 滚动窗口的大小是固定的，且各自范围之间不重叠。时间是根据系统时间来计算的

~~~Java
 public static void main(String[] args) throws Exception{
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStreamSource<String> streamSource = env.socketTextStream("master", 12345);

        SingleOutputStreamOperator<Tuple2<String, Integer>> wordDS = streamSource.map(word -> Tuple2.of(word, 1), Types.TUPLE(Types.STRING, Types.INT));

        KeyedStream<Tuple2<String, Integer>, String> keyByDS = wordDS.keyBy(kv -> kv.f0);

        WindowedStream<Tuple2<String, Integer>, String, TimeWindow> windowDS = keyByDS.window(TumblingProcessingTimeWindows.of(Time.seconds(5)));

        SingleOutputStreamOperator<Tuple2<String, Integer>> countDS = windowDS.sum(1);

        countDS.print();

        env.execute();
    }
~~~

- **滚动事件时间窗口（TumblingEventTimeWindows**）

> 滚动窗口的大小是固定的，且各自范围之间不重叠。时间是根据所带的时间字段来计算的

~~~Java
public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
     	/*
         *水位线对齐
         * 1、当上游有多个task时，下游task会取上游task水位线的最小值，如果数据量小。水位线就很难对齐，窗口就不会触发计算
         *因此并行度设为1
         */

        env.setParallelism(1);

        DataStreamSource<String> streamSource = env.socketTextStream("master", 12345);

        SingleOutputStreamOperator<Tuple2<String, Long>> wordDS = streamSource.map(lines -> {
            String[] line = lines.split(",");
            String word = line[0];
            long ts = Long.parseLong(line[1]);
            return Tuple2.of(word, ts);
        }, Types.TUPLE(Types.STRING, Types.LONG));

        SingleOutputStreamOperator<Tuple2<String, Long>> eventDS = wordDS.assignTimestampsAndWatermarks(WatermarkStrategy
				//设置水位线延迟时间
                .<Tuple2<String, Long>>forBoundedOutOfOrderness(Duration.ofSeconds(2))
                    //不设置水位线的延迟                                                                       
                //.<Tuple2<String, Long>>forMonotonousTimestamps()
				//指定时间字段
                .withTimestampAssigner((event, ts) -> event.f1)

        );

        SingleOutputStreamOperator<Tuple2<String, Integer>> mapDS = eventDS.map(kv -> Tuple2.of(kv.f0, 1), Types.TUPLE(Types.STRING, Types.INT));

        KeyedStream<Tuple2<String, Integer>, String> keyByDS = mapDS.keyBy(kv -> kv.f0);

        WindowedStream<Tuple2<String, Integer>, String, TimeWindow> windowDS = keyByDS.window(TumblingEventTimeWindows.of(Time.seconds(5)));

        SingleOutputStreamOperator<Tuple2<String, Integer>> countDS = windowDS.sum(1);

        countDS.print();

        env.execute();

    }
~~~



- **滑动处理时间窗口（SlidingProcessingTimeWindows）**

> 与滚动窗口类似，滑动窗口的 assigner 分发元素到指定大小的窗口，窗口大小通过 *window size* 参数设置。 滑动窗口需要一个额外的滑动距离（*window slide*）参数来控制生成新窗口的频率,时间是根据系统时间来计算的。

~~~java
  public static void main(String[] args) throws Exception{
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStreamSource<String> streamSource = env.socketTextStream("master", 12345);

        SingleOutputStreamOperator<Tuple2<String, Integer>> lineDS = streamSource.map(line -> Tuple2.of(line, 1), Types.TUPLE(Types.STRING, Types.INT));

        KeyedStream<Tuple2<String, Integer>, String> keyByDS = lineDS.keyBy(kv -> kv.f0);

        WindowedStream<Tuple2<String, Integer>, String, TimeWindow> windowDS = keyByDS.window(SlidingProcessingTimeWindows.of(Time.seconds(15), Time.seconds(5)));

        SingleOutputStreamOperator<Tuple2<String, Integer>> countDS = windowDS.sum(1);

        countDS.print();

        env.execute();
    }
~~~

- **滑动事件时间窗口（SlidingEventTimeWindows）**

> 与滚动窗口类似，滑动窗口的 assigner 分发元素到指定大小的窗口，窗口大小通过 *window size* 参数设置。 滑动窗口需要一个额外的滑动距离（*window slide*）参数来控制生成新窗口的频率,时间是根据所带的时间字段来计算的。

~~~Java
 public static void main(String[] args) throws Exception{
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        env.setParallelism(1);

        DataStreamSource<String> streamSource = env.socketTextStream("master", 12345);

        SingleOutputStreamOperator<Tuple2<String, Long>> wordDS = streamSource.map(line -> {
            String[] words = line.split(",");
            String word = words[0];
            long ts = Long.parseLong(words[1]);
            return Tuple2.of(word, ts);
        }, Types.TUPLE(Types.STRING, Types.LONG));

        SingleOutputStreamOperator<Tuple2<String, Long>> watermarks = wordDS.assignTimestampsAndWatermarks(WatermarkStrategy
                .<Tuple2<String, Long>>forBoundedOutOfOrderness(Duration.ofSeconds(2))
                .withTimestampAssigner((event, ts) -> event.f1)

        );

        SingleOutputStreamOperator<Tuple2<String, Integer>> mapDS = watermarks.map(kv -> Tuple2.of(kv.f0, 1), Types.TUPLE(Types.STRING, Types.INT));

        KeyedStream<Tuple2<String, Integer>, String> keyByDS = mapDS.keyBy(kv -> kv.f0);

        WindowedStream<Tuple2<String, Integer>, String, TimeWindow> windowDS = keyByDS.window(SlidingEventTimeWindows.of(Time.seconds(15), Time.seconds(5)));

        SingleOutputStreamOperator<Tuple2<String, Integer>> countDS = windowDS.sum(1);

        countDS.print();

        env.execute();
    }
~~~

#### 4.2、Count Window

> 计数窗口基于元素的个数来截取数据，到达固定的个数时就触发计算并关闭窗口
>
> 下面的代码是基于同一个单词达到3个就会计算

~~~java
  public static void main(String[] args) throws Exception{
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStreamSource<String> streamSource = env.socketTextStream("master", 12345);

        SingleOutputStreamOperator<Tuple2<String, Integer>> lineDS = streamSource.map(line -> Tuple2.of(line, 1), Types.TUPLE(Types.STRING, Types.INT));

        KeyedStream<Tuple2<String, Integer>, String> keyByDS = lineDS.keyBy(kv -> kv.f0);

        WindowedStream<Tuple2<String, Integer>, String, GlobalWindow> countWindow = keyByDS.countWindow(3);

        SingleOutputStreamOperator<Tuple2<String, Integer>> countDS = countWindow.sum(1);

        countDS.print();

        env.execute();
    }
~~~

#### 4.3、Session Window

> *会话窗口*的 assigner 会把数据按活跃的会话分组。 与*滚动窗口*和*滑动窗口*不同，会话窗口不会相互重叠，且没有固定的开始或结束时间。 会话窗口在一段时间没有收到数据之后会关闭，即在一段不活跃的间隔之后。 会话窗口的 assigner 可以设置固定的会话间隔（session gap）或 用 *session gap extractor* 函数来动态地定义多长时间算作不活跃。 当超出了不活跃的时间段，当前的会话就会关闭，并且将接下来的数据分发到新的会话窗口。

~~~java
 public static void main(String[] args) throws Exception{
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStreamSource<String> streamSource = env.socketTextStream("master", 12345);

        SingleOutputStreamOperator<Tuple2<String, Integer>> lineDS = streamSource.map(line -> Tuple2.of(line, 1), Types.TUPLE(Types.STRING, Types.INT));

        KeyedStream<Tuple2<String, Integer>, String> keyByDS = lineDS.keyBy(kv -> kv.f0);

        WindowedStream<Tuple2<String, Integer>, String, TimeWindow> windowDS = keyByDS.window(ProcessingTimeSessionWindows.withGap(Time.seconds(5)));

        SingleOutputStreamOperator<Tuple2<String, Integer>> countDS = windowDS.sum(1);

        countDS.print();

        env.execute();
    }
~~~

## 七、Kafka（消息队列）

### 1、概述

> Kafka是一款分布式消息发布和订阅系统，它的特点是高性能、高吞吐量。

### 2、Kafka性能好的原因

> 1、kafka写磁盘是顺序的，所以不断的往前产生，不断的往后写
> 2、kafka还用了sendFile的0拷贝技术，提高速度
> 3、而且还用到了批量读写，一批批往里写，64K为单位

- **非零拷贝**

> 数据在内核空间和用户空间之间需要多次拷贝，增加了 CPU 和内存带宽的使用，性能较低。

![image-20240602192530907](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202406021925138.png)

- **非零拷贝**

> 通过减少数据拷贝次数，降低了 CPU 和内存带宽的使用，提高了数据传输性能。常见的实现技术包括 `sendfile()`、`mmap()` 和 DMA。

![image-20240602192933528](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202406021929616.png)

### 3、Kafka的架构

![image-20240602193223138](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202406021932224.png)

- Broker

> Kafka集群中包含的服务器，有一个或多个服务器，这种服务器被称为 Broker。
>
> Broker 端不维护数据的消费状态，提升了性能。直接使用磁盘进行存储，线性读写，速度快。避免了在JVM 内存和系统内存之间的复制，减少耗性能的创建对象和垃圾回收。

- Producer(生产者)

> 负责发布消息到Kafka Broker
>
> producer自己决定往哪个partition写消息，可以是轮询的负载均衡，或者是基于hash的partition策略

- Consumer(消费者)

> 负责从Broker 拉取（pull）数据并进行处理。

- Topic

> 每条发布到kafka集群的消息都有一个类别，这个类别被称为Topic
>
> 物理上不同Topic的消息分开存储，逻辑上一个Topic 的消息虽然保存于一个或多个Broker上但是用户只需指定消费的Topic即课生产或消费数据而不必关心数据存于何处。一个topic分成多个partition

- Partition

> Partition 是物理上的概念，每个Topic 包含一个或多个Partition。kafka分配的单位是Partition
>
> 每个partition内部消息强有序，其中的每个消息都有一个序号叫offset
> 一个partition只对应一个broker，一个broker可以管多个partition

- Consumer Group

> 每个Consumer 属于一个特定的Consumer Group
>
> 可为每个Consumer 指定Group name，若不指定group name 则属于默认的group
>
> 每条消息只可以被Consumer Goup 组中中的一个Consumer消费，但是可以指定多个Consumer Group
>
> 所以一个消息在Consumer Group 里面只可以被消费一次。已确定！

### 4、Kafka的Java API

#### 4.1、Produce

~~~Java
public static void main(String[] args) {

        Properties properties = new Properties();

        properties.setProperty("bootstrap.servers","master:9092,node2:9092,node2:9092");

        //指定key和value的数据格式
        properties.setProperty("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        properties.setProperty("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        //创建生产者
        KafkaProducer<String, String> kafkaProducer = new KafkaProducer<>(properties);

        //生产数据
        kafkaProducer.send(new ProducerRecord<>("words","java"));

        //刷新数据
        kafkaProducer.flush();

        //关闭资源
        kafkaProducer.close();


    }
~~~

#### 4.2、Consumer

~~~Java
public static void main(String[] args) {

        Properties properties = new Properties();
        //kafka 集群列表
        properties.setProperty("bootstrap.servers", "master:9092,node2:9092,node2:9092");

        //读取数据的格式
        properties.setProperty("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        properties.setProperty("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

        /*
         * earliest
         * 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
         * latest  默认
         * 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产认值生的该分区下的数据
         * none
         * topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
         *
         */
        properties.setProperty("auto.offset.reset", "earliest");
        //指定消费者组，一条数据在一个组内只消费一次
        properties.setProperty("group.id", "asddffas");
        //创建消费者
        KafkaConsumer<String, String> kafkaConsumer = new KafkaConsumer<>(properties);

        //订阅生产者
        ArrayList<String> list = new ArrayList<>();
        list.add("student_hash");
        kafkaConsumer.subscribe(list);

        while (true){
            //拉去数据
            ConsumerRecords<String, String> consumerRecords = kafkaConsumer.poll(1000);
            //获取数据的信息
            for (ConsumerRecord<String, String> consumerRecord : consumerRecords) {
                String topic = consumerRecord.topic();
                long offset = consumerRecord.offset();
                int partition = consumerRecord.partition();
                String value = consumerRecord.value();
                long timestamp = consumerRecord.timestamp();

                System.out.println(topic + "\t" + offset + "\t" + partition + "\t" + value + "\t" + timestamp);

            }
        }
    }
~~~

### 5、Flink on Kafka

#### 5.1、KafkaSource

- **起始消费位点**

  > Kafka source 能够通过位点初始化器（`OffsetsInitializer`）来指定从不同的偏移量开始消费 。内置的位点初始化器包括：

```java
KafkaSource.builder()
    // 从消费组提交的位点开始消费，不指定位点重置策略
    .setStartingOffsets(OffsetsInitializer.committedOffsets())
    // 从消费组提交的位点开始消费，如果提交位点不存在，使用最早位点
    .setStartingOffsets(OffsetsInitializer.committedOffsets(OffsetResetStrategy.EARLIEST))
    // 从时间戳大于等于指定时间戳（毫秒）的数据开始消费
    .setStartingOffsets(OffsetsInitializer.timestamp(1657256176000L))
    // 从最早位点开始消费
    .setStartingOffsets(OffsetsInitializer.earliest())
    // 从最末尾位点开始消费
    .setStartingOffsets(OffsetsInitializer.latest());
```

~~~Java
public static void main(String[] args) throws Exception{

        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();


        KafkaSource<String> source = KafkaSource.<String>builder()
                //kafka集群
                .setBootstrapServers("master:9092,node1:9092,node2:9092")
                //指定topic
                .setTopics("students")
                //指定消费者组
                .setGroupId("student-group")
                //指定读取的数据的起始点
                .setStartingOffsets(OffsetsInitializer.earliest())
                //指定读取数据的格式
                .setValueOnlyDeserializer(new SimpleStringSchema())
                .build();

        DataStreamSource<String> kafkaSourceDS = env.fromSource(source, WatermarkStrategy.noWatermarks(), "kafkaSource");

        kafkaSourceDS.print();

        env.execute();
    }
~~~

#### 5.2、KafkaSink

~~~Java
public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStreamSource<String> scoreDS = env.readTextFile("flink/data/score.txt");

        KafkaSink<String> kafkaSink = KafkaSink.<String>builder()
                .setBootstrapServers("master:9092,node1:9092,node2:9092")
                .setRecordSerializer(KafkaRecordSerializationSchema.builder()
                        .setTopic("score")
                        .setValueSerializationSchema(new SimpleStringSchema())
                        .build()
                )
                .setDeliverGuarantee(DeliveryGuarantee.AT_LEAST_ONCE)
                .build();

        scoreDS.sinkTo(kafkaSink);

        env.execute();

    }
~~~

## 八、状态与容错

### 1、Flink的状态

> 虽然数据流中的许多操作一次只查看一个单独的事件(例如事件解析器)，但有些操作记住跨多个事件的信息(例如窗口操作符)。这些操作称为有状态操作。

#### 1.1、**Keyed State**

> keyed state 接口提供不同类型状态的访问接口，这些状态都作用于当前输入数据的 key 下。换句话说，这些状态仅可在 `KeyedStream` 上使用，在Java/Scala API上可以通过 `stream.keyBy(...)` 得到 `KeyedStream`，在Python API上可以通过 `stream.key_by(...)` 得到 `KeyedStream`。
>
> 接下来，我们会介绍不同类型的状态，然后介绍如何使用他们。所有支持的状态类型如下所示：

- `ValueState<T>`: 保存一个可以更新和检索的值（如上所述，每个值都对应到当前的输入数据的 key，因此算子接收到的每个 key 都可能对应一个值）。 这个值可以通过 `update(T)` 进行更新，通过 `T value()` 进行检索。
- `ListState<T>`: 保存一个元素的列表。可以往这个列表中追加数据，并在当前的列表上进行检索。可以通过 `add(T)` 或者 `addAll(List<T>)` 进行添加元素，通过 `Iterable<T> get()` 获得整个列表。还可以通过 `update(List<T>)` 覆盖当前的列表。
- `ReducingState<T>`: 保存一个单值，表示添加到状态的所有值的聚合。接口与 `ListState` 类似，但使用 `add(T)` 增加元素，会使用提供的 `ReduceFunction` 进行聚合。
- `AggregatingState<IN, OUT>`: 保留一个单值，表示添加到状态的所有值的聚合。和 `ReducingState` 相反的是, 聚合类型可能与 添加到状态的元素的类型不同。 接口与 `ListState` 类似，但使用 `add(IN)` 添加的元素会用指定的 `AggregateFunction` 进行聚合。
- `MapState<UK, UV>`: 维护了一个映射列表。 你可以添加键值对到状态中，也可以获得反映当前所有映射的迭代器。使用 `put(UK，UV)` 或者 `putAll(Map<UK，UV>)` 添加映射。 使用 `get(UK)` 检索特定 key。 使用 `entries()`，`keys()` 和 `values()` 分别检索映射、键和值的可迭代视图。你还可以通过 `isEmpty()` 来判断是否包含任何键值对。

所有类型的状态还有一个`clear()` 方法，清除当前 key 下的状态数据，也就是当前输入元素的 key。

- 传统求wordcount的代码以及出现的问题

~~~java
public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream<String> lineDS = env.socketTextStream("master", 12345);

        KeyedStream<String, String> keyByDS = lineDS.keyBy(word -> word);

        DataStream<Tuple2<String, Integer>> countDS = keyByDS.process(new ProcessFunction<String, Tuple2<String, Integer>>() {
            //保存之前统计的结果（状态）
            //问题：同一个task中的数据共享同一个count变量
            //int count = 0;
            //问题：如果关闭这个程序或者系统坏了数据就会丢失，使用hashmap保存计算的中间结果，flink的checkpoint不会将hashmap中的数据持久化到hdfs上

            final HashMap<String, Integer> hashMap = new HashMap<>();

            /**
             * processElement方法每一条数据执行一次
             * @param word 一行数据
             * @param ctx 上下文对象,可以获取到flink的key和时间属性
             * @param out 用于将处理结果发送到下游
             */
            @Override
            public void processElement(String word, ProcessFunction<String, Tuple2<String, Integer>>.Context ctx, Collector<Tuple2<String, Integer>> out) throws Exception {
                Integer count = hashMap.getOrDefault(word, 0);
                count++;
                out.collect(Tuple2.of(word, count));
                hashMap.put(word, count);
            }
        });
        countDS.print();

        env.execute();
    }
~~~

- 修改后的状态代码

~~~Java
public static void main(String[] args) throws Exception{
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStream<String> lineDS = env.socketTextStream("master", 12345);

        KeyedStream<String, String> keyByDS = lineDS.keyBy(word -> word);

        SingleOutputStreamOperator<Tuple2<String, Integer>> countDS = keyByDS.process(new ProcessFunction<String, Tuple2<String, Integer>>() {

            ValueState<Integer> state;

            @Override
            public void open(Configuration parameters) throws Exception {
                RuntimeContext context = getRuntimeContext();
                ValueStateDescriptor<Integer> valueStateDescriptor = new ValueStateDescriptor<>("count", Types.INT, 0);
                state = context.getState(valueStateDescriptor);
            }

            @Override
            public void processElement(String word, ProcessFunction<String, Tuple2<String, Integer>>.Context ctx, Collector<Tuple2<String, Integer>> out) throws Exception {

                Integer count = state.value();
                count++;
                out.collect(Tuple2.of(word, count));

                state.update(count);


            }
        });

        countDS.print();

        env.execute();
    }
~~~

- 基于 DataStream API 实现欺诈检测（官网案例关于状态的）

> 我们先实现第一版报警程序，对于一个账户，如果出现小于 $1 美元的交易后紧跟着一个大于 $500 的交易，就输出一个报警信息。

~~~java
 public static void main(String[] args) throws Exception{
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        DataStreamSource<String> streamSource = env.socketTextStream("master", 12345);

        SingleOutputStreamOperator<User> userDS = streamSource.map(line -> {
            String[] split = line.split(",");
            String id = split[0];
            double price = Double.parseDouble(split[1]);
            return new User(id, price);
        });

        KeyedStream<User, String> keyByDS = userDS.keyBy(User::getId);

        SingleOutputStreamOperator<String> process = keyByDS.process(new ProcessFunction<User, String>() {

            ValueState<Boolean> state;

            @Override
            public void open(Configuration parameters) throws Exception {
                RuntimeContext context = getRuntimeContext();

                ValueStateDescriptor<Boolean> flag = new ValueStateDescriptor<>("flag", Types.BOOLEAN, false);

                state = context.getState(flag);
            }

            @Override
            public void processElement(User value, ProcessFunction<User, String>.Context ctx, Collector<String> out) throws Exception {
                if (state.value()) {
                    if (value.getPrice() > 500) {
                        System.out.println("报警");
                    }
                    state.update(false);
                }
                if (value.getPrice() < 1) {
                    state.update(true);
                }

            }
        });

        env.execute();

    }
@AllArgsConstructor
@Data
class User{
    private String id;
    private Double price;
}
~~~

#### 1.2、算子状态

> 算子状态（或者非 keyed 状态）是绑定到一个并行算子实例的状态。Kafka Connector 是 Flink 中使用算子状态一个很具有启发性的例子。Kafka consumer 每个并行实例维护了 topic partitions 和偏移量的 map 作为它的算子状态。
>
> 当并行度改变的时候，算子状态支持将状态重新分发给各并行算子实例。处理重分发过程有多种不同的方案。
>
> 在典型的有状态 Flink 应用中你无需使用算子状态。它大都作为一种特殊类型的状态使用。用于实现 source/sink，以及无法对 state 进行分区而没有主键的这类场景中。

#### 1.3、广播状态

> *广播状态*是一种特殊的*算子状态*。引入它的目的在于支持一个流中的元素需要广播到所有下游任务的使用情形。在这些任务中广播状态用于保持所有子任务状态相同。 该状态接下来可在第二个处理记录的数据流中访问。可以设想包含了一系列用于处理其他流中元素规则的低吞吐量数据流，这个例子自然而然地运用了广播状态。 考虑到上述这类使用情形，广播状态和其他算子状态的不同之处在于：
>
> 1. 它具有 map 格式，
> 2. 它仅在一些特殊的算子中可用。这些算子的输入为一个*广播*数据流和*非广播*数据流，
> 3. 这类算子可以拥有不同命名的*多个广播状态* 。

### 2、Checkpoint

> 可以定时将flink计算的状态持久化到hdfs中，如果任务执行失败，可以基于hdfs中保存到的状态恢复任务，保证之前的结果不丢失
>
> Flink 的 checkpoint 机制会和持久化存储进行交互，读写流与状态。一般需要：
>
> - 一个能够回放一段时间内数据的持久化数据源，例如持久化消息队列（例如 Apache Kafka、RabbitMQ、 Amazon Kinesis、 Google PubSub 等）或文件系统（例如 HDFS、 S3、 GFS、 NFS、 Ceph 等）。
> - 存放状态的持久化存储，通常为分布式文件系统（比如 HDFS、 S3、 GFS、 NFS、 Ceph 等）。

#### 2.1、开启Checkpoint的方式

- 在代码中单独开启

~~~Java
public static void main(String[] args) throws Exception{
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // 每 1000ms 开始一次 checkpoint
        env.enableCheckpointing(20000);

        // 高级选项：

        // 设置模式为精确一次 (这是默认值)
//        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);

        env.setStateBackend(new HashMapStateBackend());



        // 使用 externalized checkpoints，这样 checkpoint 在作业取消后仍就会被保留
        env.getCheckpointConfig().setExternalizedCheckpointCleanup(
                CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);

        // 开启实验性的 unaligned checkpoints
        env.getCheckpointConfig().setCheckpointStorage("hdfs://master:9000/flink/checkpoint");

        DataStream<String> streamSource = env.socketTextStream("master", 12345);

        DataStream<Tuple2<String, Integer>> wordDS = streamSource.map(line -> Tuple2.of(line, 1), Types.TUPLE(Types.STRING, Types.INT));

        KeyedStream<Tuple2<String, Integer>, String> keyByDS = wordDS.keyBy(kv -> kv.f0);

        DataStream<Tuple2<String, Integer>> countDS = keyByDS.sum(1);

        countDS.print();

        env.execute();
//      flink run -t yarn-session -p 3 -Dyarn.application.id=application_1717413381741_0001  -c com.shujia.state.Demo2Checkpoint flink-1.0.jar
       
    }
~~~

- 在配置文件中统一开启

  vim flink-conf.yaml

  ~~~shell
  execution.checkpointing.interval: 5000
  execution.checkpointing.externalized-checkpoint-retention: RETAIN_ON_CANCELLATION
  execution.checkpointing.max-concurrent-checkpoints: 1
  execution.checkpointing.min-pause: 0
  execution.checkpointing.mode: EXACTLY_ONCE
  execution.checkpointing.timeout: 10min
  execution.checkpointing.tolerable-failed-checkpoints: 0
  execution.checkpointing.unaligned: false
  state.backend: hashmap
  state.checkpoints.dir: hdfs://master:9000/flink/checkpoint
  ~~~

#### 2.2、使用checkpoint

- 第一次提交任务之间提交

~~~shell
flink run -t yarn-session -p 3 -Dyarn.application.id=application_1717413381741_0001  -c com.shujia.state.Demo2Checkpoint flink-1.0.jar
~~~

- 重启任务时基于hdfs中的快照重启

~~~shell
# -s 指定恢复任务的位置
flink run -t yarn-session -p 3 -Dyarn.application.id=application_1717413381741_0001  -c com.shujia.state.Demo2Checkpoint -s hdfs://master:9000/flink/checkpoint/303212ccf121165b0d12713d61608dc3/chk-14  flink-1.0.jar
~~~

### 3、Exactly Once

> 数据只处理一次

#### 3.1、Kafka的Exactly Once

> Kafka保证数据处理的唯一一次
>
> 1、幂等性：保证数据不重复
>
> 2、事务：保证数据不重复
>
> 3、Acks+副本：保证数据不丢失
>
> acks机制：
>
> acks=1（默认）：当主分区写入成功，就会返回成功，如果这时主分区挂了，刚写入的数据就会丢失
>
> acks=0：生产者只负责生产数据，不负责验证数据是否成功写入，可能会造成数据丢失，但是写入的性能好
>
> acks=-1或者all:生产者生产数据后必须等待数据都同步到副本之后才会返回成功，但是性能差

![image-20240603215029886](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202406032150005.png)

#### 3.2、Flink的Exactly Once

> Flink 分布式快照保存数据**计算的状态**(checkpoint)和**消费的偏移量**，保证程序重启之后不丢失状态和消费偏移量

![image-20240603214748238](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202406032147945.png)

#### 3.3、Exactly Once的代码

- 聚合运算

> 如果任务在执行过程中失败了，可以恢复到上一次成功的checkpoint的位置，保证计算的状态和消费偏移量不丢失，保证数据处理的唯一一次

~~~Java
 public static void main(String[] args) throws Exception{
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        KafkaSource<String> kafkaSource = KafkaSource.<String>builder()
                .setBootstrapServers("master:9092,node1:9092,node2:9092")
                .setTopics("words")
                .setGroupId("group-id")
                .setStartingOffsets(OffsetsInitializer.earliest())
                .setValueOnlyDeserializer(new SimpleStringSchema())
                .build();

        DataStream<String> streamSource = env.fromSource(kafkaSource, WatermarkStrategy.noWatermarks(), "kafkaSource");

        DataStream<Tuple2<String, Integer>> wordDS = streamSource.map(line -> Tuple2.of(line, 1), Types.TUPLE(Types.STRING, Types.INT));

        KeyedStream<Tuple2<String, Integer>, String> keyByDS = wordDS.keyBy(kv -> kv.f0);

        DataStream<Tuple2<String, Integer>> countDS = keyByDS.sum(1);

        countDS.print();

        env.execute();

//        flink run -t yarn-session -p 3 -Dyarn.application.id=application_1717499706759_0001  -c com.shujia.state.Demo5ExactlyOnce flink-1.0.jar
    }
~~~

- 非聚合运算

> 只利用checkpoint和偏移量不能保证数据处理是唯一一次，需要将两次的checkpoint放到一个事务中，上一次checkpoint完成时开启事务，下一次事务完成时提交事务，这样做会增加数据处理的延迟，但是保证了数据处理的唯一一次

~~~java
 public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        KafkaSource<String> kafkaSource = KafkaSource.<String>builder()
                .setBootstrapServers("master:9092,node1:9092,node2:9092")
                .setTopics("words")
                .setGroupId("group-id")
                .setStartingOffsets(OffsetsInitializer.earliest())
                .setValueOnlyDeserializer(new SimpleStringSchema())
                .build();

        DataStream<String> streamSource = env.fromSource(kafkaSource, WatermarkStrategy.noWatermarks(), "kafkaSource");

        DataStream<String> filterDS = streamSource.filter(word -> !"".equals(word));

        //flink处理事务的时间要小于Kafka中的事务时间15min
        Properties properties = new Properties();

        properties.setProperty("transaction.timeout.ms", 10 * 60 * 1000 + "");

        KafkaSink<String> kafkaSink = KafkaSink.<String>builder()
                .setBootstrapServers("master:9092,node1:9092,node2:9092")
                //同步flink处理事务的时间
                .setKafkaProducerConfig(properties)
                .setRecordSerializer(KafkaRecordSerializationSchema.builder()
                        .setTopic("filter")
                        .setValueSerializationSchema(new SimpleStringSchema())
                        .build()
                )
                //指定数据处理的语义
                .setDeliverGuarantee(DeliveryGuarantee.EXACTLY_ONCE)
                .build();

        filterDS.sinkTo(kafkaSink);

        env.execute();

    }
~~~

## 九、FlinkSql

### 1、DataStream 上的关系查询

> 下表比较了传统的关系代数和流处理与输入数据、执行和输出结果的关系。

| 关系代数 / SQL                                               | 流处理                                                     |
| ------------------------------------------------------------ | ---------------------------------------------------------- |
| 关系(或表)是有界(多)元组集合。                               | 流是一个无限元组序列。                                     |
| 对批数据(例如关系数据库中的表)执行的查询可以访问完整的输入数据。 | 流式查询在启动时不能访问所有数据，必须“等待”数据流入。     |
| 批处理查询在产生固定大小的结果后终止。                       | 流查询不断地根据接收到的记录更新其结果，并且始终不会结束。 |

### 2、动态表 & 连续查询

> *动态表* 是 Flink 的支持流数据的 Table API 和 SQL 的核心概念。与表示批处理数据的静态表不同，动态表是随时间变化的。可以像查询静态批处理表一样查询它们。查询动态表将生成一个 *连续查询* 。一个连续查询永远不会终止，结果会生成一个动态表。查询不断更新其(动态)结果表，以反映其(动态)输入表上的更改。本质上，动态表上的连续查询非常类似于定义物化视图的查询。
>
> 需要注意的是，连续查询的结果在语义上总是等价于以批处理模式在输入表快照上执行的相同查询的结果。
>
> 下图显示了流、动态表和连续查询之间的关系:

![image-20240604211250050](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202406042112373.png)

> 1. 将流转换为动态表。
> 2. 在动态表上计算一个连续查询，生成一个新的动态表。
> 3. 生成的动态表被转换回流。
>
> **注意：** 动态表首先是一个逻辑概念。在查询执行期间不一定(完全)物化动态表。

### 3、表到流的转换

- **Append-only 流：** 仅通过 `INSERT` 操作修改的动态表可以通过输出插入的行转换为流。
- **Retract 流：** retract 流包含两种类型的 message： *add messages* 和 *retract messages* 。通过将`INSERT` 操作编码为 add message、将 `DELETE` 操作编码为 retract message、将 `UPDATE` 操作编码为更新(先前)行的 retract message 和更新(新)行的 add message，将动态表转换为 retract 流。下图显示了将动态表转换为 retract 流的过程。

![image-20240604211938997](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202406042119224.png)

- **Upsert 流:** upsert 流包含两种类型的 message： *upsert messages* 和*delete messages*。转换为 upsert 流的动态表需要(可能是组合的)唯一键。通过将 `INSERT` 和 `UPDATE` 操作编码为 upsert message，将 `DELETE` 操作编码为 delete message ，将具有唯一键的动态表转换为流。消费流的算子需要知道唯一键的属性，以便正确地应用 message。与 retract 流的主要区别在于 `UPDATE` 操作是用单个 message 编码的，因此效率更高。下图显示了将动态表转换为 upsert 流的过程。

![image-20240604211959651](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202406042119749.png)

### 4、SQL命令行

~~~sql
# 启动flink集群
yarn-seesion.sh -d

# 进入sql命令行
sql-client.sh

# 1、创建表，数据源时kafka
CREATE TABLE students (
  id STRING,
  name STRING,
  age INT,
  sex STRING,
  clazz STRING
) WITH (
  'connector' = 'kafka',
  'topic' = 'students', -- 指定topic
  'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092', -- 指定kafka集群列表
  'properties.group.id' = 'testGroup', -- 指定消费者组
  'scan.startup.mode' = 'earliest-offset', -- 指定读取数据的位置
  'format' = 'csv' -- 指定数据的格式
);

# 2、编写sql进行连续查询
select 
clazz,count(1)as num
from students
group by clazz;

# 3、生产数据
kafka-console-producer.sh --broker-list master:9092,node1:9092,node2:9092 --topic students
~~~

### 5、SQL命令行打印结果模式

#### 5.1**表格模式**（table mode）默认

> 在内存中实体化结果，并将结果用规则的分页表格可视化展示出来。执行如下命令启用：

~~~SQL
SET 'sql-client.execution.result-mode' = 'table';
~~~

#### 5.2、**变更日志模式**（changelog mode）

> 不会实体化和可视化结果，而是由插入（`+`）和撤销（`-`）组成的持续查询产生结果流。

~~~SQL
SET 'sql-client.execution.result-mode' = 'changelog';
~~~

#### 5.3、**Tableau模式**（tableau mode)

> 更接近传统的数据库，会将执行的结果以制表的形式直接打在屏幕之上。具体显示的内容会取决于作业 执行模式的不同(`execution.type`)：

~~~SQL
SET 'sql-client.execution.result-mode' = 'tableau';
~~~

### 6、处理模式

#### 6.1、流处理

> 1、可以用于处理有界流和无界流
> 2、流处理模式输出连续结果
> 3、流处理模式底层时持续流模型

~~~SQL
SET 'execution.runtime-mode' = 'streaming';
~~~

#### 6.2、批处理

> 1、批处理模式只能用于处理有界流
> 2、输出最终结果
> 3、底层是MapReduce模型

~~~sql
SET 'execution.runtime-mode' = 'batch'; 
~~~

### 7、连接器

#### 7.1、Kafka

- KafkaSource

~~~SQL
-- 创建表 --- 无界流
-- TIMESTAMP(3): 时flink总的时间字段
CREATE TABLE students_kafka (
    id STRING,
    name STRING,
    age INT,
    sex STRING,
    clazz STRING,
    `event_time` TIMESTAMP(3) METADATA FROM 'timestamp',-- 获取kfka时间戳
    `partition` BIGINT METADATA VIRTUAL, -- 获取kafka数据所在的分区
    `offset` BIGINT METADATA VIRTUAL,-- 偏移量
    -- 指定时间字段和水位线生成策略
    WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND
) WITH (
    'connector' = 'kafka',
    'topic' = 'students',
    'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092',
    'properties.group.id' = 'testGroup',
    'scan.startup.mode' = 'earliest-offset',
    'format' = 'csv',
    'csv.ignore-parse-errors' ='true' -- 当有脏数据时是否跳过当前行
);

select id,name,event_time,`partition`,`offset` from students_kafka;

-- 每隔5秒统计每个班级的人数
select 
clazz,
TUMBLE_START(event_time,INTERVAL '5' SECOND) as win_start,
TUMBLE_END(event_time,INTERVAL '5' SECOND) as win_end,
count(id) as num
from 
students_kafka
group by
clazz,
-- 滚动的事件时间窗口
TUMBLE(event_time,INTERVAL '5' SECOND);
~~~

- kafka sink

~~~SQL
CREATE TABLE students_kafka_num (
    clazz STRING,
	win_start TIMESTAMP(3),
    win_end TIMESTAMP(3),
    num BIGINT
) WITH (
    'connector' = 'kafka',
    'topic' = 'clazz_num',
    'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092',
    'properties.group.id' = 'testGroup',
    'scan.startup.mode' = 'earliest-offset',
    'format' = 'csv',
    'csv.ignore-parse-errors' ='true' -- 当有脏数据时是否跳过当前行
);

insert into students_kafka_num
select 
clazz,
TUMBLE_START(event_time,INTERVAL '5' SECOND) as win_start,
TUMBLE_END(event_time,INTERVAL '5' SECOND) as win_end,
count(id) as num
from 
students_kafka
group by
clazz,
-- 滚动的事件时间窗口
TUMBLE(event_time,INTERVAL '5' SECOND);
~~~

#### 7.2、JDBC

- 整合

~~~shell
# 将依赖包上传到flink的lib目录下
flink-connector-jdbc-1.15.2.jar
mysql-connector-java-5.1.47.jar

# 依赖更新后需要重启集群才会生效
yarn application -list
yarn application -kill [appid]
yarn-session.sh -d

sql-client.sh
~~~

- MySQL Source

~~~SQL
CREATE TABLE students_mysql (
  id INT,
  name STRING,
  age INT,
  gender STRING,
  clazz STRING
) WITH (
   'connector' = 'jdbc',
   'url' = 'jdbc:mysql://master:3306/bigdata29?useUnicode=true&characterEncoding=UTF-8',
   'table-name' = 'students',
   'username'='root',
   'password'='123456'
);

-- 求每个班的人数

select 
clazz,
count(1) as num
from
students_mysql
group by 
clazz;
~~~

- MySQL Sink

> **注意**：[ERROR] Could not execute SQL statement. Reason:
> java.lang.IllegalStateException: please declare primary key for sink table when query contains update/delete record.
>
> 这个错误是这是一个Retract 流有更新和删除的作用为了数据的最终结果，要保证主键的唯一性

~~~sql
CREATE TABLE clazz_num (
  clazz STRING,
  num BIGINT,
  PRIMARY KEY (clazz) NOT ENFORCED
) WITH (
   'connector' = 'jdbc',
   'url' = 'jdbc:mysql://master:3306/bigdata29?useUnicode=true&characterEncoding=UTF-8',
   'table-name' = 'clazz_num',
   'username'='root',
   'password'='123456'
);

insert into clazz_num
select 
clazz,
count(1) as num
from
students_mysql
group by 
clazz;
~~~

#### 7.3、HDFS

- HDFS Source

~~~sql 
CREATE TABLE students_hdfs (
  id INT,
  name STRING,
  age INT,
  gender STRING,
  clazz STRING
) WITH (
   'connector' = 'filesystem',
   'path' = 'hdfs://master:9000/bigdata29/data/students.txt',
   'format' ='csv'
);

-- 求每个班的男生和女生各有多少人
select
clazz,
gender,
count(1) as num
from
students_hdfs
group by 
clazz,
gender;
~~~

- HDFS Sink

~~~sql
--仅追加的写入到HDFS中
CREATE TABLE clazz_gender_hdfs (
  id INT,
  name STRING,
  age INT,
  gender STRING,
  clazz STRING
) WITH (
   'connector' = 'filesystem',
   'path' = 'hdfs://master:9000/bigdata29/flink/data',
   'format' ='csv'
);
insert into  clazz_gender_hdfs
select id,name,age,gender,clazz from students_mysql;


---- 2、将更新更改的结果写入hdfs
CREATE TABLE clazz_gender_num_hdfs (
  clazz STRING,
  gender STRING,
  num BIGINT
) WITH (
   'connector' = 'filesystem',  -- 必选：指定连接器类型
   'path' = 'hdfs://master:9000/bigdata29/flink/sqlSink/clazz_gender_num_hdfs',--必选：指定路径
   'format' ='canal-json' -- 必选：文件系统连接器指定 format
);

insert into clazz_gender_num_hdfs 
-- 求每个班的男生和女生各有多少人
select
clazz,
gender,
count(1) as num
from
students_hdfs
group by 
clazz,
gender;

~~~

#### 7.4、HBase

- 整合

~~~shell
# 将依赖包上传到flink的lib目录下
flink-sql-connector-hbase-2.2-1.15.2.jar

# 依赖更新后需要重启集群才会生效
yarn application -list
yarn application -kill [appid]
yarn-session.sh -d

sql-client.sh
~~~

- HBase sink

~~~sql
--创建hbase表
create 'students_flink','info'
-- 创建hbase sink表
CREATE TABLE students_hbase (
 id INT, 
 info ROW<name STRING,age INT,gender STRING,clazz STRING>, -- 指定列簇中的列
 PRIMARY KEY (id) NOT ENFORCED -- 设置hbaserowkey
) WITH (
 'connector' = 'hbase-2.2',
 'table-name' = 'students_flink',
 'zookeeper.quorum' = 'master:2181,node1:2181,node2:2181'
);

insert into students_hbase
select 
id,
ROW(name,age,gender,clazz) as info
from students_mysql;
--查看结果
select * from students_hbase;
scan 'students_flink'
~~~

#### 7.5、datagen 

> 用于生成测试数据，可以用于高性能测试

~~~SQL
CREATE TABLE students_datagen (
    id STRING,
    name STRING,
    age INT,
    sex STRING,
    clazz STRING
) WITH (
    'connector' = 'datagen',
    'rows-per-second'='5', -- 指定每秒生成的数据量
    'fields.id.length'='5',
    'fields.name.length'='3',
    'fields.age.min'='1',
    'fields.age.max'='100',
    'fields.sex.length'='1',
    'fields.clazz.length'='4'
);
~~~

#### 7.6、print

> 在task manager中打印结果

~~~sql
CREATE TABLE print_table (
    id STRING,
    name STRING,
    age INT,
    sex STRING,
    clazz STRING
) WITH (
 'connector' = 'print'
);

CREATE TABLE print_table 
WITH ('connector' = 'print')
-- 应用目标表的字段创建新的
LIKE students_datagen (EXCLUDING ALL);

insert into print_table
select * from students_datagen;
~~~

#### 7.7、BlackHole

> 用于高性能测试

~~~sql
CREATE TABLE blackhole_table (
    id STRING,
    name STRING,
    age INT,
    sex STRING,
    clazz STRING
) WITH (
  'connector' = 'blackhole'
);

insert into blackhole_table
select * from students_datagen;
~~~

### 8、数据格式

#### 8.1、csv

> 数据中字段的顺序需要和建表语句字段的顺序保持一致 （顺序映射）
> 默认按照逗号分割

~~~sql
CREATE TABLE students_csv (
    id STRING,
    name STRING,
    age INT,
    sex STRING,
    clazz STRING
) WITH (
    'connector' = 'kafka',
    'topic' = 'students', -- 指定topic
    'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092', -- 指定kafka集群列表
    'properties.group.id' = 'testGroup', -- 指定消费者组
    'scan.startup.mode' = 'earliest-offset', -- 指定读取数据的位置
    'format' = 'csv', -- 指定数据的格式
    'csv.field-delimiter' = ',' ,-- 指定分隔符
    'csv.ignore-parse-errors' ='true' -- 跳过脏数据
);
~~~

#### 8.2、json

> link表中的字段和类型需要和json中保持一致（同名映射）

~~~sql
CREATE TABLE cars (
    car STRING,
    city_code STRING,
    county_code STRING,
    card BIGINT,
    camera_id STRING,
    orientation STRING,
    road_id BIGINT,
    `time` BIGINT,
    speed DOUBLE
) WITH (
    'connector' = 'kafka',
    'topic' = 'cars', -- 指定topic
    'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092', -- 指定kafka集群列表
    'properties.group.id' = 'testGroup', -- 指定消费者组
    'scan.startup.mode' = 'earliest-offset', -- 指定读取数据的位置
    'format' = 'json', -- 指定数据的格式
    'json.ignore-parse-errors' ='true'
);
~~~

#### 8.3、canal-json

> 用于保存更新更改的结果流

~~~sql
CREATE TABLE clazz_num (
    clazz STRING,
    num BIGINT
) WITH (
  'connector' = 'kafka',
  'topic' = 'clazz_num',
  'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092',
  'properties.group.id' = 'testGroup',
  'scan.startup.mode' = 'earliest-offset',
  'format' = 'canal-json'
);

insert into clazz_num
select 
clazz,
count(1) as num
from 
students_kafka
group by 
clazz;
~~~

### 9、时间属性

#### 9.1、处理时间

> 处理时间是基于机器的本地时间来处理数据，它是最简单的一种时间概念，但是它不能提供确定性。它既不需要从数据里获取时间，也不需要生成 watermark。

~~~sql
-- PROCTIME() 生成处理时间的函数
CREATE TABLE words (
    word STRING,
    proctime AS PROCTIME() -- 声明一个额外的列作为处理时间属性
) WITH (
    'connector' = 'kafka',
    'topic' = 'words',
    'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092',
    'properties.group.id' = 'testGroup',
    'scan.startup.mode' = 'earliest-offset',
    'format' = 'csv',
    'csv.ignore-parse-errors' ='true' -- 当有脏数据时是否跳过当前行
);

-- 实时统计每个单词最近5秒单词的数量

select 
word,
TUMBLE_START(proctime,INTERVAL '5' SECOND) win_start,
TUMBLE_END(proctime,INTERVAL '5' SECOND) win _end,
count(word) as num
from
words
group by 
word,
TUMBLE(proctime,INTERVAL '5' SECOND);
~~~

#### 9.2、事件时间

> 事件时间允许程序按照数据中包含的时间来处理，这样可以在有乱序或者晚到的数据的情况下产生一致的处理结果。它可以保证从外部存储读取数据后产生可以复现（replayable）的结果。
>
> 除此之外，事件时间可以让程序在流式和批式作业中使用同样的语法。在流式程序中的事件时间属性，在批式程序中就是一个正常的时间字段。
>
> 为了能够处理乱序的事件，并且区分正常到达和晚到的事件，Flink 需要从事件中获取事件时间并且产生 watermark（[watermarks](https://nightlies.apache.org/flink/flink-docs-release-1.15/zh/docs/concepts/time/)）。
>
> 事件时间属性也有类似于处理时间的三种定义方式：在DDL中定义、在 DataStream 到 Table 转换时定义、用 TableSource 定义。

~~~sql
CREATE TABLE word_event_time (
  word STRING,
  word_event_time TIMESTAMP(3),
  -- 声明 user_action_time 是事件时间属性，并且用 延迟 5 秒的策略来生成 watermark
  WATERMARK FOR word_event_time  AS word_event_time - INTERVAL '5' SECOND
) WITH (
 'connector' = 'kafka',
    'topic' = 'words_event_time',
    'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092',
    'properties.group.id' = 'testGroup',
    'scan.startup.mode' = 'earliest-offset',
    'format' = 'csv',
    'csv.ignore-parse-errors' ='true' -- 当有脏数据时是否跳过当前行
);

select 
word,
TUMBLE_START(word_event_time ,INTERVAL '5' SECOND) win_start,
TUMBLE_END(word_event_time ,INTERVAL '5' SECOND) win_end,
count(word) as num
from
word_event_time
group by 
word,
TUMBLE(word_event_time ,INTERVAL '5' SECOND);
~~~

### 10、SQL语法

#### 10.1、Hints

> 动态表选择：可以在查询表的时候动态修改表的参数配置

~~~sql
CREATE TABLE students (
  id STRING,
  name STRING,
  age INT,
  sex STRING,
  clazz STRING
) WITH (
  'connector' = 'kafka',
  'topic' = 'students', -- 指定topic
  'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092', -- 指定kafka集群列表
  'properties.group.id' = 'testGroup', -- 指定消费者组
  'scan.startup.mode' = 'earliest-offset', -- 指定读取数据的位置
  'format' = 'csv' -- 指定数据的格式
);

select * from students /*+ OPTIONS('csv.ignore-parse-errors' ='true') */;

-- latest-offset： 读取任务启动之后生产的数据
select * from students /*+ OPTIONS('csv.ignore-parse-errors' ='true','scan.startup.mode' = 'latest-offset') */;

CREATE TABLE students_hdfs_stream (
    id int,
    name STRING,
    age INT,
    gender STRING,
    clazz STRING
) WITH (
    'connector' = 'filesystem',           -- 必选：指定连接器类型
    'path' = 'hdfs://master:9000/data/students',  -- 必选：指定路径
    'format' = 'csv'                   -- 必选：文件系统连接器指定 format
);

select * from students_hdfs_stream /*+OPTIONS('source.monitor-interval' = '5000') */
~~~

#### 10.2、WITH 子句

> `WITH` 子句提供了一种用于更大查询而编写辅助语句的方法。这些编写的语句通常被称为公用表表达式，表达式可以理解为仅针对某个查询而存在的临时视图

~~~sql
with tmp as (
    select 
    id,name,age,clazz 
    from 
    students_mysql
    where age > 22
)
select * from tmp
union all
select * from tmp;
~~~

#### 10.3、SELECT WHERE

~~~sql
select * from students_hdfs_stream
where
age > 21
and gender in ('男','女');
~~~

#### 10.4、SELECT DISTINCT

> 对于流处理的问题
> 1、flink会将之前的数据保存在状态中，用于判断是否重复
> 2、如果表的数据量很大，随着时间的推移状态会越来越大，状态的数据时先保存在TM的内存中的，时间长了可能会出问题

~~~sql
select distinct * from students /*+ OPTIONS('csv.ignore-parse-errors' ='true','scan.startup.mode' = 'latest-offset') */;
~~~

#### 10.5、窗口函数

- TUMBLE（滚动窗口）

> TUMBLE函数将每个元素分配给指定窗口大小的窗口。翻滚窗口有固定的大小，不重叠。例如，假设您指定了一个大小为5分钟的滚动窗口。在这种情况下，Flink将评估当前窗口，并且每五分钟启动一个新窗口，如下图所示。

![image-20240605143013262](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202406051430475.png)

> TUMBLE函数根据时间属性字段为关系的每一行分配一个窗口。在流模式下，时间属性字段必须是事件或处理时间属性。在批处理模式下，窗口表函数的时间属性字段必须是TIMESTAMP或TIMESTAMP_LTZ类型的属性。TUMBLE的返回值是一个新的关系，它包括原关系的所有列，以及额外的名为“window_start”，“window_end”，“window_time”的3列，以表示分配的窗口。原来的时间属性“timecol”将是一个常规的时间戳列

```sql
TUMBLE(TABLE data, DESCRIPTOR(timecol), size [, offset ])
```

> - `data`：是一个表参数，可以与时间属性列有任意关系。
> - `timecol`：是一个列描述符，指示应将数据的哪些时间属性列映射到滚动窗口。
> - `size`：是指定滚动窗口宽度的持续时间。
> - `offset`：是一个可选参数，用于指定窗口起始位置的偏移量。

~~~sql
--创建bid的表
CREATE TABLE bid (
    item  STRING,
    price  DECIMAL(10, 2),
    bidtime TIMESTAMP(3),
    WATERMARK FOR bidtime AS bidtime - INTERVAL '5' SECOND
) WITH (
    'connector' = 'kafka',
    'topic' = 'bid',
    'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092',
    'properties.group.id' = 'testGroup',
    'scan.startup.mode' = 'earliest-offset',
    'format' = 'csv',
    'csv.ignore-parse-errors' ='true' -- 当有脏数据时是否跳过当前行
);
--生产数据
kafka-console-producer.sh --broker-list master:9092,node1:9092,node2:9092 --topic bid
C,4.00,2020-04-15 08:05:01
A,2.00,2020-04-15 08:07:01
D,5.00,2020-04-15 08:09:01
B,3.00,2020-04-15 08:11:01
E,1.00,2020-04-15 08:13:01
F,6.00,2020-04-15 08:17:01
-- TUMBLE:滚动窗口函数，在原表的基础上增加窗口开始时间，窗口结束时间，窗口时间
SELECT item,price,bidtime,window_start,window_end,window_time FROM TABLE(
   TUMBLE(TABLE bid, DESCRIPTOR(bidtime), INTERVAL '10' MINUTES)
);
--或者
SELECT * FROM TABLE(
   TUMBLE(TABLE bid, DESCRIPTOR(bidtime), INTERVAL '10' MINUTES));
-- 或者
SQL> SELECT * FROM TABLE(
   TUMBLE(
     DATA => TABLE Bid,
     TIMECOL => DESCRIPTOR(bidtime),
     SIZE => INTERVAL '10' MINUTES));
--每隔十分钟统计总价格
SELECT window_start, window_end, SUM(price)
 FROM TABLE(
    TUMBLE(TABLE bid, DESCRIPTOR(bidtime), INTERVAL '10' MINUTES))
 GROUP BY window_start, window_end;
~~~

- HOP（滑动窗口）

> HOP函数将元素分配给固定长度的窗口。与TUMBLE窗口函数一样，窗口的大小由窗口大小参数配置。另一个窗口滑动参数控制启动跳窗的频率。因此，如果幻灯片小于窗口大小，则跳跃窗口可以重叠。在这种情况下，元素被分配给多个窗口。跳窗也被称为“滑动窗”。
>
> 例如，您可以设置大小为10分钟的窗口，每隔5分钟滑动一次。这样，每隔5分钟就会出现一个窗口，其中包含最近10分钟内到达的事件，如下图所示。

![image-20240605145325440](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202406051453589.png)

> HOP函数分配的窗口覆盖大小间隔内的行，并根据时间属性字段移动每张幻灯片。在流模式下，时间属性字段必须是事件或处理时间属性。在批处理模式下，窗口表函数的时间属性字段必须是TIMESTAMP或TIMESTAMP_LTZ类型的属性。HOP的返回值是一个新的关系，它包含了原关系的所有列，并增加了名为“window_start”，“window_end”，“window_time”的3列，以表示分配的窗口。原始的时间属性“timecol”将是窗口TVF之后的一个常规时间戳列。

```sql
HOP(TABLE data, DESCRIPTOR(timecol), slide, size [, offset ])
```

> - `data`：是一个表参数，可以与时间属性列有任意关系。
> - `timecol`：是列描述符，指示应将数据的哪些时间属性列映射到跳跃窗口。
> - `slide`：是指定连续跳跃窗口开始之间的持续时间
> - `size`：是指定跳跃窗口宽度的持续时间。
> - `offset`：是一个可选参数，用于指定窗口起始位置的偏移量。

~~~sql
CREATE TABLE bid_proctime (
    item  STRING,
    price  DECIMAL(10, 2),
    proctime AS PROCTIME()
) WITH (
    'connector' = 'kafka',
    'topic' = 'bid_proctime',
    'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092',
    'properties.group.id' = 'testGroup',
    'scan.startup.mode' = 'earliest-offset',
    'format' = 'csv',
    'csv.ignore-parse-errors' ='true' -- 当有脏数据时是否跳过当前行
);
kafka-console-producer.sh --broker-list master:9092,node1:9092,node2:9092 --topic bid_proctime
C,4.00
A,2.00
D,5.00
B,3.00
E,1.00
F,6.00

-- HOP: 滑动窗口函数
SELECT item,price,proctime,window_start,window_end,window_time FROM TABLE(
    HOP(TABLE bid_proctime, DESCRIPTOR(proctime), INTERVAL '5' SECOND, INTERVAL '10' SECOND)
);

-- 窗口聚合
SELECT 
    window_start,
    window_end,
    avg(price) as avg_price
FROM 
    TABLE(
        HOP(TABLE bid_proctime, DESCRIPTOR(proctime), INTERVAL '5' SECOND, INTERVAL '10' SECOND)
    )
group by 
    window_start,
    window_end;
~~~

- CUMULATE (累积窗口)

> 累积窗口（Cumulative Window，也称为累积窗口或滚动累积窗口）是一种用于处理流数据的窗口类型，它允许窗口的长度随时间逐步增加，直到达到指定的最大长度。

![image-20240605152149244](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202406051521365.png)

```sql
CUMULATE(TABLE data, DESCRIPTOR(timecol), step, size)
```

> - `data`：是一个表参数，可以与时间属性列有任意关系。
> - `timecol`：是一个列描述符，指示应将数据的哪些时间属性列映射到累积窗口。
> - `step`：是指定连续累积窗口末尾之间增加的窗口大小的持续时间。
> - `size`：是指定累积窗口最大宽度的持续时间。`size`必须是的整数倍`step`。
> - `offset`：是一个可选参数，用于指定窗口起始位置的偏移量。

~~~sql
CREATE TABLE bid (
    item  STRING,
    price  DECIMAL(10, 2),
    bidtime TIMESTAMP(3),
    WATERMARK FOR bidtime AS bidtime - INTERVAL '5' SECOND
) WITH (
    'connector' = 'kafka',
    'topic' = 'bid',
    'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092',
    'properties.group.id' = 'testGroup',
    'scan.startup.mode' = 'earliest-offset',
    'format' = 'csv',
    'csv.ignore-parse-errors' ='true' -- 当有脏数据时是否跳过当前行
);
kafka-console-producer.sh --broker-list master:9092,node1:9092,node2:9092 --topic bid
C,4.00,2020-04-15 08:05:01
A,2.00,2020-04-15 08:07:01
D,5.00,2020-04-15 08:09:01
B,3.00,2020-04-15 08:11:01
E,1.00,2020-04-15 08:13:01
F,6.00,2020-04-15 08:17:01
SELECT * FROM TABLE(
    CUMULATE(TABLE bid, DESCRIPTOR(bidtime), INTERVAL '2' MINUTES, INTERVAL '10' MINUTES)
);
--每2分钟累积一次数据，直到累积时间达到10分钟
SELECT window_start, window_end, SUM(price)
  FROM TABLE(
    CUMULATE(TABLE bid, DESCRIPTOR(bidtime), INTERVAL '2' MINUTES, INTERVAL '10' MINUTES))
  GROUP BY window_start, window_end;
~~~

- 会话窗口

~~~sql
CREATE TABLE bid_proctime (
    item  STRING,
    price  DECIMAL(10, 2),
    proctime AS PROCTIME()
) WITH (
    'connector' = 'kafka',
    'topic' = 'bid_proctime',
    'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092',
    'properties.group.id' = 'testGroup',
    'scan.startup.mode' = 'earliest-offset',
    'format' = 'csv',
    'csv.ignore-parse-errors' ='true' -- 当有脏数据时是否跳过当前行
);

kafka-console-producer.sh --broker-list master:9092,node1:9092,node2:9092 --topic bid_proctime
C,4.00
C,2.00
C,5.00
C,3.00
C,1.00
C,6.00

-- 实时统计每个商品的总的金额，隔5秒没有数据开始统计
select 
    item,
    SESSION_START(proctime,INTERVAL '5' SECOND)  as session_start,
    SESSION_END(proctime,INTERVAL '5' SECOND)  as session_end,
    sum(price) as sum_price
from 
    bid_proctime
group by
    item,
    SESSION(proctime,INTERVAL '5' SECOND);
~~~

#### 10.6、GROUP BY

~~~sql
-- 出啊关键datagen source表
CREATE TABLE words_datagen (
    word STRING
) WITH (
    'connector' = 'datagen',
    'rows-per-second'='50000', -- 指定每秒生成的数据量
    'fields.word.length'='5'
);

CREATE TABLE blackhole_table (
    word STRING,
    num BIGINT
) WITH (
  'connector' = 'blackhole'
);


-- 分组聚合需要将之前的计算结果保存在状态中，
-- 如果状态无限增长，会导致checkpoint时间拉长，如果checkpoint超时失败了，也会导致任务失败
insert into blackhole_table
select 
    word,
    count(1)as num
from 
    words_datagen /*+ OPTIONS('fields.word.length'='7') */
group by 
    word;
~~~

#### 10.7、OVER

- sum max min avg count

~~~sql
CREATE TABLE `order` (
    order_id  STRING,
    amount  DECIMAL(10, 2),
    product STRING,
    order_time TIMESTAMP(3),
    WATERMARK FOR order_time AS order_time - INTERVAL '5' SECOND
) WITH (
    'connector' = 'kafka',
    'topic' = 'order',
    'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092',
    'properties.group.id' = 'testGroup',
    'scan.startup.mode' = 'earliest-offset',
    'format' = 'csv',
    'csv.ignore-parse-errors' ='true' -- 当有脏数据时是否跳过当前行
);
kafka-console-producer.sh --broker-list master:9092,node1:9092,node2:9092 --topic order
1,4.00,001,2020-04-15 08:05:01
2,2.00,001,2020-04-15 08:07:01
3,5.00,001,2020-04-15 08:09:01
4,3.00,001,2020-04-15 08:11:01
5,1.00,001,2020-04-15 08:13:01
6,6.00,001,2020-04-15 08:17:01
6,6.00,001,2020-04-15 08:20:01
6,6.00,001,2020-04-15 08:21:01
6,10.00,001,2020-04-15 08:21:02
6,11.00,001,2020-04-15 08:21:03
6,12.00,001,2020-04-15 08:21:04

-- 1、实时统计每个商品的累计总金额，将总金额放在每一条数据的后面
-- 流处理的问题
-- a、sum over必须按照时间升序排序，因为数据时一条一套过来的，只能做累加求和，不能做全局求和
-- b、只能按照时间升序排序，如果按照其他的字段排序，每来一条数据都需要重新排序，计算代价太大，影响性能
select 
    order_id,
    amount,
    product,
    order_time,
    sum(amount) over(
        partition by product  
        order by order_time
    )
from 
`order`
;

/**
RANGE 间隔
RANGE ORDER BY 列的值上定义了一个间隔，在 Flink 中，该间隔始终是时间属性。以下 RANGE 间隔定义所有时间属性比当前行小最多 30 分钟的行都包含在聚合中。
RANGE BETWEEN INTERVAL '30' MINUTE PRECEDING AND CURRENT ROW
*/
-- 2、实时统计每个商品的累计总金额，将总金额放在每一条数据的后面，只统计最近10分钟的数据
select 
    order_id,
    amount,
    product,
    order_time,
    sum(amount) over(
        partition by product  
        order by order_time
        -- 统计10分钟前到当前行的数据
        RANGE BETWEEN INTERVAL '10' MINUTES PRECEDING AND CURRENT ROW
    )
from 
    `order`
;
/**
行间隔 #
间隔ROWS是基于计数的间隔。它精确定义了聚合中包含多少行。以下ROWS间隔定义了当前行和当前行之前的 10 行（因此总共 11 行）包含在聚合中。
ROWS BETWEEN 10 PRECEDING AND CURRENT ROW
*/
-- 3、实时统计每个商品的累计总金额，将总金额放在每一条数据的后面，计算最近5条数据
select 
    order_id,
    amount,
    product,
    order_time,
    sum(amount) over(
        partition by product  
        order by order_time
        -- 从前4条数据到当前行
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
from 
    `order`
;


-- 4、实时统计每个商品的最大金额，将总金额放在每一条数据的后面，计算最近5条数据
select 
    order_id,
    amount,
    product,
    order_time,
    max(amount) over(
        partition by product  
        order by order_time
        -- 从前4条数据到当前行
        ROWS BETWEEN 4 PRECEDING AND CURRENT ROW
    )
from 
    `order`
;

~~~

#### 10.8、TOP-N

- row_number

~~~sql
-- 如果只是增加排名，只能按照时间字段升序排序
select 
    order_id,
    amount,
    product,
    order_time,
    row_number() over(partition by product order by order_time) as r
from 
    `order`
;

-- 实时统计每个商品金额最高的前两个商品  -- TOPN
-- 去完topn之后需要计算的排名的数据较少了，计算代价降低了
select * 
from (
    select 
        order_id,
        amount,
        product,
        order_time,
        row_number() over(partition by product order by amount desc) as r
    from 
        `order`
)
where r <= 2;
~~~

#### 10.9、ORDER BY

~~~sql
-- 子流处理模式中，order by 需要按照时间字段升序排序
select * from 
`order`
order by 
order_time,amount

-- 加上limit ,计算代价就不高了，就可以按照普通字段进行排序了
select * from 
`order`
order by 
amount
limit 2;
~~~

#### 10.10、模式检测（CEP）

- 案例1

> 我们先实现第一版报警程序，对于一个账户，如果出现小于 $1 美元的交易后紧跟着一个大于 $500 的交易，就输出一个报警信息。

~~~sql
CREATE TABLE tran (
    id  STRING,
    amount  DECIMAL(10, 2),
    proctime as PROCTIME()
) WITH (
    'connector' = 'kafka',
    'topic' = 'tran',
    'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092',
    'properties.group.id' = 'testGroup',
    'scan.startup.mode' = 'latest-offset',
    'format' = 'csv',
    'csv.ignore-parse-errors' ='true' -- 当有脏数据时是否跳过当前行
);
kafka-console-producer.sh --broker-list master:9092,node1:9092,node2:9092 --topic tran
1,4.00
1,2.00
1,5.00
1,0.90
1,600.00
1,4.00
1,2.00
1,0.10
1,200.00
1,700.00
-- MATCH_RECOGNIZE(模式检测)
-- 在数据流上对数据进行匹配，当数满足我们定义的规则后，返回匹配的结果

-- 我们先实现第一版报警程序，对于一个账户，如果出现小于 $1 美元的交易后紧跟着一个大于 $500 的交易，就输出一个报警信息。
SELECT *
FROM tran
    MATCH_RECOGNIZE (
      PARTITION BY id -- 分组字段
      ORDER BY proctime -- 排序字段，只能按照时间字段升序排序
      MEASURES -- 相当于select
        A.amount as min_amount,
        A.proctime as min_proctime,
        B.amount as max_amount,
        B.proctime as max_proctime
      PATTERN (A B) -- 定义规则
      DEFINE -- 定义条件
        A as amount < 1,
        B as amount > 500
    ) AS T
  
  -- 我们先实现第一版报警程序，对于一个账户，如果出现小于 $1 美元的交易后紧跟着一个大于 $500 的交易，就输出一个报警信，两次事件需要在10秒内出现
  
SELECT *
FROM tran
    MATCH_RECOGNIZE (
      PARTITION BY id -- 分组字段
      ORDER BY proctime -- 排序字段，只能按照时间字段升序排序
      MEASURES -- 相当于select
        A.amount as min_amount,
        A.proctime as min_proctime,
        B.amount as max_amount,
        B.proctime as max_proctime
      PATTERN (A B)  WITHIN INTERVAL '5' SECOND -- 定义规则，增加事件约束，需要在5秒内匹配出结果
      DEFINE -- 定义条件
        A as amount < 1,
        B as amount > 500
    ) AS T;
-- 我们先实现第一版报警程序，对于一个账户，如果连续出现三次出现小于 $1 美元的交易后紧跟着一个大于 $500 的交易，就输出一个报警信息

SELECT *
FROM tran
    MATCH_RECOGNIZE (
      PARTITION BY id -- 分组字段
      ORDER BY proctime -- 排序字段，只能按照时间字段升序排序
      MEASURES -- 相当于select
        A.amount as a_amount, -- 获取最后一条

        min(A.amount) as min_a_amount, -- 取最小的
        max(A.amount) as max_a_amount, -- 取最大的

        sum(A.amount) as sum_a_amount, -- 求和
        avg(A.amount) as avg_a_amount, -- 平均

        FIRST(A.amount) AS first_a_amount, -- 取前面第一条
        LAST(A.amount) AS LAST_a_amount, -- 取后面第一条

        B.amount as b_amount
      PATTERN (A{3} B) -- 定义规则
      DEFINE -- 定义条件
        A as amount < 1,
        B as amount > 500
    ) AS T;
 
1,0.90
1,0.10
1,0.20
1,600.00
~~~

- 案例2

> 找出一个单一股票价格不断下降的时期

~~~sql
CREATE TABLE ticker (
    symbol  STRING,
    rowtime  TIMESTAMP(3), -- 时间字段
    price  DECIMAL(10, 2) ,
    tax  DECIMAL(10, 2),
    -- 指定时间字段和水位线生成策略
    WATERMARK FOR rowtime AS rowtime
) WITH (
    'connector' = 'kafka',
    'topic' = 'ticker',
    'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092',
    'properties.group.id' = 'testGroup',
    'scan.startup.mode' = 'latest-offset',
    'format' = 'csv',
    'csv.ignore-parse-errors' ='true' -- 当有脏数据时是否跳过当前行
);
kafka-console-producer.sh --broker-list master:9092,node1:9092,node2:9092 --topic ticker
ACME,2024-06-04 10:00:00,12,1
ACME,2024-06-04 10:00:01,17,2
ACME,2024-06-04 10:00:02,19,1
ACME,2024-06-04 10:00:03,21,3
ACME,2024-06-04 10:00:04,25,2
ACME,2024-06-04 10:00:05,18,1
ACME,2024-06-04 10:00:06,15,1
ACME,2024-06-04 10:00:07,14,2
ACME,2024-06-04 10:00:08,24,2
ACME,2024-06-04 10:00:09,25,2
ACME,2024-06-04 10:00:10,19,1

-- 找出一个单一股票价格不断下降的时期
select * from 
ticker
MATCH_RECOGNIZE (
      PARTITION BY symbol -- 分组字段
      ORDER BY rowtime -- 排序字段，只能按照时间字段升序排序
      MEASURES -- 相当于select
        A.price as a_price,
        FIRST(B.price) as FIRST_b_price,
        LAST(B.price) as last_b_price,
    	C.price as c_price
      AFTER MATCH SKIP PAST LAST ROW -- 从当前匹配成功止呕的下一行开始匹配
      PATTERN (A B+ C) -- 定义规则
      DEFINE -- 定义条件
        -- 如果时第一个B，就和A比较，如果时后面的B，就和前一个B比较
        B as (LAST(B.price,1)is null and B.price < A.price) or B.price < LAST(B.price,1),
        C as C.price > LAST(B.price)
    ) AS T;
~~~

#### 10.11、Joins

- Regular Joins

> 和hive sql中的join是一样的,

~~~sql
CREATE TABLE students (
  id STRING,
  name STRING,
  age INT,
  sex STRING,
  clazz STRING
) WITH (
  'connector' = 'kafka',
  'topic' = 'students', -- 指定topic
  'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092', -- 指定kafka集群列表
  'properties.group.id' = 'testGroup', -- 指定消费者组
  'scan.startup.mode' = 'latest-offset', -- 指定读取数据的位置
  'format' = 'csv' -- 指定数据的格式
);

kafka-console-producer.sh --broker-list master:9092,node1:9092,node2:9092 --topic students
1500100001,施笑槐,22,女,文科六班
1500100002,吕金鹏,24,男,文科六班
1500100003,单乐蕊,22,女,理科六班
1500100004,葛德曜,24,男,理科三班
1500100005,宣谷芹,22,女,理科五班
1500100006,边昂雄,21,男,理科二班
1500100007,尚孤风,23,女,文科六班

CREATE TABLE scores (
  sid STRING,
  cid STRING,
  score INT
) WITH (
  'connector' = 'kafka',
  'topic' = 'scores', -- 指定topic
  'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092', -- 指定kafka集群列表
  'properties.group.id' = 'testGroup', -- 指定消费者组
  'scan.startup.mode' = 'latest-offset', -- 指定读取数据的位置
  'format' = 'csv' -- 指定数据的格式
);
kafka-console-producer.sh --broker-list master:9092,node1:9092,node2:9092 --topic scores
1500100001,1000001,98
1500100001,1000002,5
1500100001,1000003,137
1500100001,1000004,29
1500100001,1000005,85
1500100001,1000006,52
1500100002,1000001,139
1500100002,1000002,102

-- inner jion(内连接)
select 
a.id,a.name,b.sid,b.score
from 
students as a
inner join
scores as b
on a.id=b.sid;
--外连接
-- left  join (左连接)
select 
a.id,a.name,b.sid,b.score
from 
students as a
left join
scores as b
on a.id=b.sid;

-- full join (全连接)
select 
a.id,a.name,b.sid,b.score
from 
students as a
full join
scores as b
on a.id=b.sid;

-- 常规的关联方式，会将两个表的数据一直保存在状态中，时间长了，状态会越来越大，导致任务执行失败
-- 状态有效期，状态在flink中保存的事件，状态保留多久需要根据实际业务分析
SET 'table.exec.state.ttl' = '10000'; 
~~~

- ### Interval Joins

> Interval Join 用于在两个流之间进行时间间隔内的 Join。它允许你指定一个时间间隔，在这个时间间隔内匹配流中的元素。

~~~sql
CREATE TABLE students_proctime (
    id STRING,
    name STRING,
    age INT,
    sex STRING,
    clazz STRING,
    proctime AS PROCTIME()
) WITH (
  'connector' = 'kafka',
  'topic' = 'students', -- 指定topic
  'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092', -- 指定kafka集群列表
  'properties.group.id' = 'testGroup', -- 指定消费者组
  'scan.startup.mode' = 'latest-offset', -- 指定读取数据的位置
  'format' = 'csv' -- 指定数据的格式
);

kafka-console-producer.sh --broker-list master:9092,node1:9092,node2:9092 --topic students
1500100001,施笑槐,22,女,文科六班
1500100002,吕金鹏,24,男,文科六班
1500100003,单乐蕊,22,女,理科六班
1500100004,葛德曜,24,男,理科三班
1500100005,宣谷芹,22,女,理科五班
1500100006,边昂雄,21,男,理科二班
1500100007,尚孤风,23,女,文科六班

CREATE TABLE scores_proctime (
    sid STRING,
    cid STRING,
    score INT,
    proctime AS PROCTIME()
) WITH (
  'connector' = 'kafka',
  'topic' = 'scores', -- 指定topic
  'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092', -- 指定kafka集群列表
  'properties.group.id' = 'testGroup', -- 指定消费者组
  'scan.startup.mode' = 'latest-offset', -- 指定读取数据的位置
  'format' = 'csv' -- 指定数据的格式
);
kafka-console-producer.sh --broker-list master:9092,node1:9092,node2:9092 --topic scores
1500100001,1000001,98
1500100001,1000002,5
1500100001,1000003,137
1500100001,1000004,29
1500100001,1000005,85
1500100001,1000006,52
1500100002,1000001,139
1500100002,1000002,102


select a.id,a.name,b.sid,b.score from 
students_proctime a, scores_proctime b
where a.id=b.sid
-- a表的时间需要在b表时间10秒内
and (
	a.proctime BETWEEN b.proctime - INTERVAL '10' SECOND AND b.proctime
    or b.proctime BETWEEN a.proctime - INTERVAL '10' SECOND AND a.proctime
);
~~~

- ### Temporal Joins

> 时态表是一个随时间变化的表，在Flink中也称为动态表。时态表中的行与一个或多个时态周期相关联，所有Flink表都是时态的(动态的)。时态表包含一个或多个版本表快照，它可以是一个不断变化的历史表，用于跟踪变化(例如:数据库变更日志，包含所有快照)或一个变化的维度表，它具体化了变化(例如:包含最新快照的数据库表)。

~~~sql
CREATE TABLE orders (
    order_id    STRING,
    price       DECIMAL(32,2),
    currency    STRING,
    order_time  TIMESTAMP(3),
    WATERMARK FOR order_time AS order_time --设置水位线
) WITH (
  'connector' = 'kafka',
  'topic' = 'orders', -- 指定topic
  'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092', -- 指定kafka集群列表
  'properties.group.id' = 'testGroup', -- 指定消费者组
  'scan.startup.mode' = 'latest-offset', -- 指定读取数据的位置
  'format' = 'csv' -- 指定数据的格式
);

kafka-console-producer.sh --broker-list master:9092,node1:9092,node2:9092 --topic orders
o_001,1,EUR,2024-06-06 12:00:00
o_002,100,EUR,2024-06-06 12:00:07
o_003,200,EUR,2024-06-06 12:00:16
o_004,10,EUR,2024-06-06 12:00:21
o_005,20,EUR,2024-06-06 12:00:25

-- 汇率表
CREATE TABLE currency_rates (
    currency STRING,
    conversion_rate DECIMAL(32, 2),
    update_time TIMESTAMP(3),
    WATERMARK FOR update_time AS update_time,--设置水位线
    PRIMARY KEY(currency) NOT ENFORCED -- 主键，区分不同的汇率
) WITH (
  'connector' = 'kafka',
  'topic' = 'currency_rates1', -- 指定topic
  'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092', -- 指定kafka集群列表
  'properties.group.id' = 'testGroup', -- 指定消费者组
  'scan.startup.mode' = 'earliest-offset', -- 指定读取数据的位置
  'format' = 'canal-json' -- 指定数据的格式
);

insert into currency_rates
values
('EUR',0.12,TIMESTAMP'2024-06-06 12:00:00'),
('EUR',0.11,TIMESTAMP'2024-06-06 12:00:09'),
('EUR',0.15,TIMESTAMP'2024-06-06 12:00:17'),
('EUR',0.14,TIMESTAMP'2024-06-06 12:00:23');

kafka-console-consumer.sh --bootstrap-server  master:9092,node1:9092,node2:9092 --from-beginning --topic currency_rates

-- 使用常规关联方式关联时态表只能关联到最新的数据
select 
a.price,a.order_time,b.conversion_rate,b.update_time
from 
orders as a
join
currency_rates as b
on a.currency=b.currency;

-- 时态表join
-- FOR SYSTEM_TIME AS OF a.order_time: 使用a表的时间到b表中查询对应时间段的数据
select 
a.price,a.order_time,b.conversion_rate,b.update_time
from 
orders as a
join
currency_rates FOR SYSTEM_TIME AS OF a.order_time as b 
on a.currency=b.currency;
~~~

- Lookup Joins

> 用于流表关联维度表
> 流表：动态表
> 维度表：不怎么变化的变，维度表的数据一般可以放在hdfs或者mysql

~~~sql
CREATE TABLE scores (
    sid INT,
    cid STRING,
    score INT,
    proctime AS PROCTIME()
) WITH (
  'connector' = 'kafka',
  'topic' = 'scores', -- 指定topic
  'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092', -- 指定kafka集群列表
  'properties.group.id' = 'testGroup', -- 指定消费者组
  'scan.startup.mode' = 'latest-offset', -- 指定读取数据的位置
  'format' = 'csv' -- 指定数据的格式
);
kafka-console-producer.sh --broker-list master:9092,node1:9092,node2:9092 --topic scores
1500100001,1000001,98
1500100002,1000002,5
1500100001,1000003,137

CREATE TABLE students (
    id INT,
    name STRING,
    age INT,
    gender STRING,
    clazz STRING
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:mysql://master:3306/bigdata29',
    'table-name' = 'student',
    'username' ='root',
    'password' = '123456',
    'lookup.cache.max-rows' = '1000', -- 最大缓存行数
    'lookup.cache.ttl' ='10000' -- 缓存过期时间
);

--1、使用常规关联方式
-- 维表的数据只在任务启动的时候读取一次，后面不再实时读取，
-- 只能关联到任务启动时读取的数据
select a.sid,a.score,b.id,b.name from
scores as a
left join
students  as b
on a.sid=b.id;

-- lookup join
-- 当流表每来一条数据时，使用关联字段到维表的数据源中查询
-- 每一次都需要查询数据库，性能会降低
select a.sid,a.score,b.id,b.name from
scores as a
left join
students FOR SYSTEM_TIME AS OF a.proctime as b
on a.sid=b.id;
~~~

#### 10.12、整合hive

- 整合

~~~shell
# 上传依赖到flink的lib目录下
flink-sql-connector-hive-3.1.2_2.12-1.15.2.jar

# 重启flink集群
yarn application -list
yarn application -kill XXX
yarn-session.sh -d

sql-client.sh
~~~

- hive catalog

> catalog--->database--->table---->字段---->数据 
> catalog是数据库上面的一个概念，一个cataloglog中可以i有多个database, 
> catalog就是flink抽象的元数据层
>
> default_catalog：是flink默认的元数据，将元数据保存在jobmanager的内存中

~~~sql
-- 1、启动hive的元数据服务
nohup hive --service metastore &

-- 2、创建hive catalog
 CREATE CATALOG hive_catalog WITH (
  'type' = 'hive',
  'hive-conf-dir' = '/usr/local/soft/hive-3.1.2/conf'
);

show catalogs;
--3、切换catalog 
use catalog hive_catalog;
-- 查询hive中的表
select * from hive_catalog.bigdata29.students;


-- 创建数据库
create database flink;

-- flink可以查询hive的表，hive不能查询flink创建的动态表
-- 在hive cagalog 中保存flink的动态表
CREATE TABLE students_csv (
    id STRING,
    name STRING,
    age INT,
    sex STRING,
    clazz STRING
) WITH (
    'connector' = 'kafka',
    'topic' = 'students', -- 指定topic
    'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092', -- 指定kafka集群列表
    'properties.group.id' = 'testGroup', -- 指定消费者组
    'scan.startup.mode' = 'earliest-offset', -- 指定读取数据的位置
    'format' = 'csv', -- 指定数据的格式
    'csv.field-delimiter' = ',' ,-- 指定分隔符
    'csv.ignore-parse-errors' ='true' -- 跳过脏数据
);
~~~

- hive function

~~~sql
-- 加载hive函数
LOAD MODULE hive WITH ('hive-version' = '3.1.2');

select split('java,flink',',');

CREATE TABLE lines (
    line STRING
) WITH (
    'connector' = 'kafka',
    'topic' = 'lines', -- 指定topic
    'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092', -- 指定kafka集群列表
    'properties.group.id' = 'testGroup', -- 指定消费者组
    'scan.startup.mode' = 'earliest-offset', -- 指定读取数据的位置
    'format' = 'csv', -- 指定数据的格式
    'csv.field-delimiter' = '|' ,-- 指定分隔符
    'csv.ignore-parse-errors' ='true' -- 跳过脏数据
);

kafka-console-producer.sh --broker-list master:9092,node1:9092,node2:9092 --topic lines
java,java,flink


select 
word,count(1) as num
from 
lines,
lateral table(explode(split(line,','))) t(word)
group by 
word;
~~~

#### 10.13、Checkpoint

- 编写sql文件

> vim word_count.sql

~~~sql
-- 1、创建source表
CREATE TABLE lines (
    line STRING
) WITH (
    'connector' = 'kafka',
    'topic' = 'lines', -- 指定topic
    'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092', -- 指定kafka集群列表
    'properties.group.id' = 'testGroup', -- 指定消费者组
    'scan.startup.mode' = 'earliest-offset', -- 指定读取数据的位置
    'format' = 'csv', -- 指定数据的格式
    'csv.field-delimiter' = '|' ,-- 指定分隔符
    'csv.ignore-parse-errors' ='true' -- 跳过脏数据
);
-- 创建sink表
CREATE TABLE print_table (
    word STRING,
    num BIGINT
) WITH (
 'connector' = 'print'
);

-- 加载hive函数
LOAD MODULE hive WITH ('hive-version' = '3.1.2');

-- 执行sql
insert into print_table
select 
word,count(1) as num
from 
lines,
lateral table(explode(split(line,','))) t(word)
group by 
word;
~~~

- 执行SQL文件

~~~shell
-- 第一次直接提交任务
sql-client.sh -f word_count.sql
~~~

- 失败重启

~~~shell
-- 基于hdfs中保存的快照重启任务

-- 在inert into 语句的前面增加
SET 'execution.savepoint.path' = 'hdfs://master:9000/flink/checkpoint/d915e6278f156a9278156e67105f914e/chk-36';

-- 重启任务
sql-client.sh -f word_count.sql
~~~

#### 10.14、一个表被多次使用的时候

> vim student.sql

~~~sql
CREATE TABLE students_csv (
    id STRING,
    name STRING,
    age INT,
    sex STRING,
    clazz STRING
) WITH (
    'connector' = 'kafka',
    'topic' = 'students', -- 指定topic
    'properties.bootstrap.servers' = 'master:9092,node1:9092,node2:9092', -- 指定kafka集群列表
    'properties.group.id' = 'testGroup', -- 指定消费者组
    'scan.startup.mode' = 'earliest-offset', -- 指定读取数据的位置
    'format' = 'csv', -- 指定数据的格式
    'csv.field-delimiter' = ',' ,-- 指定分隔符
    'csv.ignore-parse-errors' ='true' -- 跳过脏数据
);

-- 创建sink表
CREATE TABLE clazz_num (
    clazz STRING,
    num BIGINT
) WITH (
 'connector' = 'print'
);

CREATE TABLE sex_num (
    sex STRING,
    num BIGINT
) WITH (
 'connector' = 'print'
);


-- 执行一组sql,如果多个sql中使用了同一张表，flink只会读取一次
EXECUTE STATEMENT SET 
BEGIN
    insert into clazz_num
    select 
    clazz,
    count(1) as num
    from 
    students_csv 
    group by 
    clazz;

    insert into sex_num
    select 
    sex,
    count(1) as num
    from 
    students_csv 
    group by 
    sex;
END;
~~~

#### 10.15、反压

> 如果你看到一个 task 发生 **反压警告**（例如： `High`），意味着它生产数据的速率比下游 task 消费数据的速率要快。 在工作流中数据记录是从上游向下游流动的（例如：从 Source 到 Sink）。反压沿着相反的方向传播，沿着数据流向上游传播。

- 测试反压

~~~sql
-- 出啊关键datagen source表
CREATE TABLE words_datagen (
    word STRING
) WITH (
    'connector' = 'datagen',
    'rows-per-second'='50000', -- 指定每秒生成的数据量
    'fields.word.length'='5'
);

CREATE TABLE blackhole_table (
    word STRING,
    num BIGINT
) WITH (
  'connector' = 'blackhole'
);

-- 反压发生情况
--1、单词太多，状态太大导致反压
insert into blackhole_table
select 
    word,
    count(1)as num
from 
    words_datagen /*+ OPTIONS('fields.word.length'='6') */
group by 
    word;


--2、数据量太大导致反压
insert into blackhole_table
select 
    word,
    count(1)as num
from 
    words_datagen /*+ OPTIONS('fields.word.length'='5','rows-per-second'='400000') */
group by 
    word;
~~~

- 解决反压的方法

  - 增加资源

  ~~~sql
  -- 1、增加Taskmanager的内存
  -- 启动汲取设置tm的内存
  yarn-session.sh -tm 6G -d
  
  -- 2、增加并行度
  SET 'parallelism.default' = '8';
  ~~~

  - 预聚合

  ~~~sql
  -- 开启微批处理
  set 'table.exec.mini-batch.enabled' ='true';
  set 'table.exec.mini-batch.allow-latency' = '5 s';
  set 'table.exec.mini-batch.size' ='100000';
  
  -- 开启预聚合
  set 'table.optimizer.agg-phase-strategy' ='TWO_PHASE';
  ~~~

  



# 实验1-1：初始Flink

## 实验概述

Flink 是 Apache 基金会旗下的一个开源大数据处理框架，如今已被很多人认为是大数据实时处理的方向和未来，许多公司也都在招聘和储备掌握 Flink 技术的人才，本实验主要任务就是认识Flink，了解Flink的特性！

## 实验环境

- AtStudy 实训平台

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16792747823003060.png)

## 实验目标

学习完成本实验后，您将能够

- 了解Flink的起源
- 了解为什么需要Flink
- 了解Flink和Spark之前的差异

## 实验任务

### 任务一、Apache Flink源起

#### **【任务目标】**

了解Flink的发展史。

#### **【任务步骤】**

##### 1.1 Apache Flink

在当代数据量激增的时代，各种业务场景都有大量的业务数据产生，对于这些不断产生的数据应该如何进行有效的处理，成为当下大多数公司所面临的问题。

随着雅虎对hadoop的开源，越来越多的大数据处理技术开始涌入人们的视线，例如目前比较流行的大数据处理引擎Apache Spark，基本上已经取代了MapReduce成为当前大数据处理的标准。但是随着数据的不断增长，新技术的不断发展，人们逐渐意识到对实时数据处理的重要性。相对于传统的数据处理模式，流式数据处理有着更高的处理效率和成本控制能力。Flink 就是近年来在开源社区不断发展的技术中的能够同时支持高吞吐、低延迟、高性能的分布式处理框架。

Flink 起源于一个叫作 `Stratosphere` 的项目，它是由 3 所地处柏林的大学和欧洲其他一些大学共同进行的研究项目，由柏林工业大学的教授沃克尔·马尔科（`Volker Markl`）领衔开发。2014年 4 月，`Stratosphere` 的代码被复制并捐赠给了` Apache` 软件基金会，Flink 就是在此基础上被重新设计出来的。

2014 年 8 月，Flink 第一个版本 0.6 正式发布（至于 0.5 之前的版本，那就是在Stratosphere 名下的了）。与此同时 Fink 的几位核心开发者创办了 Data Artisans 公司，主要做 Fink 的商业应用，帮助企业部署大规模数据处理解决方案。

![image-20230307102450346](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_1/image-20230307102450346.png)

- 2014 年 12 月，Flink 项目完成了孵化，一跃成为 Apache 软件基金会的顶级项目。
- 2015 年 4 月，Flink 发布了里程碑式的重要版本 0.9.0，很多国内外大公司也正是从这时开始关注、并参与到 Flink 社区建设的。
- 2019 年 1 月，长期对 Flink 投入研发的阿里巴巴，以 9000 万欧元的价格收购了 Data Artisans 公司；之后又将自己的内部版本 Blink 开源，继而与 8 月份发布的 Flink 1.9.0版本进行了合并。自此之后，Flink 被越来越多的人所熟知，成为当前最火的新一代大数据处理框架

由此可见，Flink 从真正起步到火爆，只不过几年时间。在这短短几年内，Flink 从最初的第一个稳定版本 0.9，到目前已经发布到了 1.16.0，这期间不断有新功能新特性加入。从一开始，Flink 就拥有一个非常活跃的社区，而且一直在快速成长。

Flink 的具体定位是：Apache Flink 是一个框架和分布式处理引擎，如图所示，用于对无界和有界数据流进行有状态计算。Flink 被设计在所有常见的集群环境中运行，以内存执行速度和任意规模来执行计算

![image-20230307103413997](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_1/image-20230307103413997.png)

##### 1.2 名称由来

起源于德国的科研项目（`Stratosphere`），在德语中，flink 一词表示快速和灵巧。项目采用一只松鼠的彩色图案作为 logo，这不仅因为松鼠具有快速和灵巧的特点，还因为柏林的松鼠有一种迷人的红棕色。而 Flink 的松鼠 logo 拥有可爱的尾巴，尾巴的颜色与 Apache 软件基金会的 logo 颜色相呼应，也就是说，这是一只 Apache 风格的松鼠。

![image-20230307101747367](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_1/image-20230307101747367.png)

官网地址：`flink.apache.org`

##### 1.3 Flink应用

Flink 是一个大数据流处理引擎，它可以为不同的行业提供大数据实时处理的解决方案。随着 Flink 的快速发展完善，如今在世界范围许多公司都可以见到 Flink 的身影。

![image-20230307110527513](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_1/image-20230307110527513.png)

可以看到，各种行业的众多公司都在使用 Flink，那到底他们用 Flink 来实现什么需求呢？换句话说，什么的场景最适合 Flink 大显身手呢？

1. 电商和市场营销

举例：实时数据报表、广告投放、实时推荐

在电商行业中，网站点击量是统计`PV、UV`的重要来源，也是如今“流量经济”的最主要数据指标。很多公司的营销策略，比如广告的投放，也是基于点击量来决定的。另外，在网站上提供给用户的实时推荐，往往也是基于当前用户的点击行为做出的。我们需要的是直接处理数据流，而 Flink 就可以做到这一点。

1. 物联网（`IOT`）

举例：传感器实时数据采集和显示、实时报警，交通运输业

物联网是流数据被普遍应用的领域。各种传感器不停获得测量数据，并将它们以流的形式传输至数据中心。而数据中心会将数据处理分析之后，得到运行状态或者报警信息，实时地显示在监控屏幕上。所以在物联网中，低延迟的数据传输和处理，以及准确的数据分析通常很关键。

1. 物流配送和服务业

举例：订单状态实时更新、通知信息推送

在很多服务型应用中，都会涉及订单状态的更新和通知的推送。这些信息基于事件触发，不均匀地连续不断生成，处理之后需要及时传递给用户。这也是非常典型的数据流的处理。

1. 银行和金融业

举例：实时结算和通知推送，实时检测异常行为

银行和金融业是另一个典型的应用行业。在全球化经济中，能够提供 24 小时服务变得越来越重要。现在交易和报表都会快速准确地生成，我们跨行转账也可以做到瞬间到账，还可以接到实时的推送通知。这就需要我们能够实时处理数据流。

### 任务二、数据架构演变

#### **【任务目标】**

我们已经了解，Flink 的主要应用场景，就是处理大规模的数据流。那为什么一定要用 Flink呢？数据处理还有没有其他的方式？要解答这个疑惑，我们就需要了解数据架构的演变过程。

#### **【任务步骤】**

##### 2.1 流处理和批处理

数据处理有不同的方式。

对于具体应用来说，有些场景数据是一个一个来的，是一组有序的数据序列，我们把它叫作“数据流”；而有些场景的数据，本身就是一批同时到来，是一个有限的数据集，这就是批量数据（有时也直接叫数据集）。容易想到，处理数据流，当然应该“来一个就处理一个”，这种数据处理模式就叫作**流处理**；因为这种处理是即时的，所以也叫实时处理。与之对应，处理批量数据自然就应该一批读入、一起计算，这种方式就叫作**批处理**，也叫作离线处理。

在 IT 应用场景中，数据的生成，一般都是流式的。企业的绝大多数应用程序，都是在不停地接收用户请求、记录用户行为和系统日志，或者持续接收采集到的状态信息。所以数据会在不同的时间持续生成，形成一个有序的数据序列——这就是典型的数据流。

所以流数据更真实地反映了我们的生活方式。真实场景中产生的，一般都是数据流。显然，对于流式数据，用流处理是最好、也最合理的方式。

但我们知道，传统的数据处理架构并不是这样。无论是关系型数据库、还是数据仓库，都倾向于先“收集数据”，然后再进行处理。为什么不直接用流处理的方式呢？这是因为，分布式批处理在架构上更容易实现。想想生活中发消息聊天的例子，我们就很容易理解了：如果来一条消息就立即处理，“微信秒回”，这样做一定会很受人欢迎；但是这要求自己必须时刻关注新消息，这会耗费大量精力，工作效率会受到很大影响。如果隔一段时间查一下新消息，做个“批处理”，压力明显就小多了。当然，这样的代价就是可能无法及时处理有些消息，造成一定的后果。

想要弄清楚流处理的发展演变，我们先要了解传统的数据处理架构。

##### 2.2 传统事务处理

IT 互联网公司往往会用不同的应用程序来处理各种业务。比如内部使用的企业资源规划（`ERP`）系统、客户关系管理（`CRM`）系统，还有面向客户的 `Web` 应用程序。这些系统一般都会进行分层设计："计算层"就是应用程序本身，用于数据计算和处理；而“存储层”往往是传统的关系型数据库，用于数据存储。

![image-20230307112649810](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_1/image-20230307112649810.png)

这就是传统的“事务处理”架构。系统所处理的连续不断的事件，其实就是一个数据流。而对于每一个事件，系统都在收到之后进行相应的处理，这也是符合流处理的原则的。所以可以说，传统的事务处理，就是最基本的流处理架构

##### 2.3 有状态的流处理

我们可以把需要的额外数据保存成一个“状态”，然后针对这条数据进行处理，并且更新状态。在传统架构中，这个状态就是保存在数据库里的。这就是所谓的“有状态的流处理”。为了加快访问速度，我们可以直接将状态保存在本地内存，如图所示。当应用收到一个新事件时，它可以从状态中读取数据，也可以更新状态。而当状态是从内存中读写的时候，这就和访问本地变量没什么区别了，实时性可以得到极大的提升

![image-20230307115031636](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_1/image-20230307115031636.png)

##### 2.4 Lambda 架构

以 Storm 为代表的第一代分布式开源流处理器，主要专注于具有毫秒延迟的事件处理，特点就是一个字“快”；而对于准确性和结果的一致性，是不提供内置支持的，因为结果有可能取决于到达事件的时间和顺序。另外，第一代流处理器通过检查点来保证容错性，但是故障恢复的时候，即使事件不会丢失，也有可能被重复处理——所以无法保证“exactly-once”。

与批处理器相比，可以说第一代流处理器牺牲了结果的准确性，用来换取更低的延迟。而批处理器恰好反过来，牺牲了实时性，换取了结果的准确。

我们自然想到，如果可以让二者做个结合，不就可以同时提供快速和准确的结果了吗？正是基于这样的思想，Lambda 架构被设计出来，如图所示。我们可以认为这是第二代流处理架构，但事实上，它只是第一代流处理器和批处理器的简单合并。

![image-20230307115417240](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_1/image-20230307115417240.png)

Lambda 架构主体是传统批处理架构的增强。它的“批处理层”（Batch Layer）就是由传统的批处理器和存储组成，而“实时层”（Speed Layer）则由低延迟的流处理器实现。数据到达之后，两层处理双管齐下，一方面由流处理器进行实时处理，另一方面写入批处理存储空间，等待批处理器批量计算。流处理器快速计算出一个近似结果，并将它们写入“流处理表”中。而批处理器会定期处理存储中的数据，将准确的结果写入批处理表，并从流处理表中删除不准确的结果。最终，应用程序会合并流处理表和批处理表中的结果，并展示出来。Lambda 架构的优点非常明显，它兼具了批处理器和第一代流处理器的特点，同时保证了低延迟和结果的准确性。而它的缺点同样非常明显。首先，Lambda 架构本身就很难建立和维护；而且，它需要我们对一个应用程序，做出两套语义上等效的逻辑实现，因为批处理和流处理是两套完全独立的系统，它们的 API 也完全不同。为了实现一个应用，付出了双倍的工作量，这对程序员显然不够友好

##### 2.5 新一代流处理器

之前的分布式流处理架构，都有明显的缺陷，人们也一直没有放弃对流处理器的改进和完善。终于，在原有流处理器的基础上，新一代分布式开源流处理器诞生了。为了与之前的系统区分，我们一般称之为第三代流处理器，代表当然就是 Flink。

第三代流处理器通过巧妙的设计，完美解决了乱序数据对结果正确性所造成的影响。这一代系统还做到了精确一次（exactly-once）的一致性保障，是第一个具有一致性和准确结果的开源流处理器。另外，先前的流处理器仅能在高吞吐和低延迟中二选一，而新一代系统能够同时提供这两个特性。所以可以说，这一代流处理器仅凭一套系统就完成了 Lambda 架构两套系统的工作，它的出现使得 Lambda 架构黯然失色。

除了低延迟、容错和结果准确性之外，新一代流处理器还在不断添加新的功能，例如高可用的设置，以及与资源管理器（如 YARN 或 Kubernetes）的紧密集成等等。

那么Flink为代表的新一代流处理器具体有哪些特点呢？

### 任务三、这就是Flink

#### **【任务目标】**

Flink 是第三代分布式流处理器，它的功能丰富而强大，了解Flink的特性。

#### **【任务步骤】**

##### 3.1 Flink 的特性

Flink 区别与传统数据处理框架的特性如下。

- 高吞吐和低延迟。每秒处理数百万个事件，毫秒级延迟。
- 结果的准确性。Flink 提供了事件时间（event-time）和处理时间（processing-time）语义。对于乱序事件流，事件时间语义仍然能提供一致且准确的结果。
- 精确一次（exactly-once）的状态一致性保证。
- 可以连接到最常用的存储系统，如 Apache Kafka、Apache Cassandra、Elasticsearch、JDBC、Kinesis 和（分布式）文件系统，如 HDFS 和 S3。
- 高可用。本身高可用的设置，加上与 K8s，YARN 和 Mesos 的紧密集成，再加上从故障中快速恢复和动态扩展任务的能力，Flink 能做到以极少的停机时间实现 7×24 全天候运行。
- 能够更新应用程序代码并将作业（jobs）迁移到不同的 Flink 集群，而不会丢失应用程序的状态。

##### 3.2 分层 API

除了上述这些特性之外，Flink 还是一个非常易于开发的框架，因为它拥有易于使用的分层 API，整体 API 分层如下：

![image-20230307131852041](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_1/image-20230307131852041.png)

##### 3.3 Flink vs Spark

谈到大数据处理引擎，不能不提 Spark。Apache Spark 是一个通用大规模数据分析引擎。它提出的内存计算概念让大家耳目一新，得以从 Hadoop 繁重的 MapReduce 程序中解脱出来，可以说是划时代的大数据处理框架。除了计算速度快、可扩展性强，Spark 还为批处理（Spark SQL）、流处理（Spark Streaming）、机器学习（Spark MLlib）、图计算（Spark GraphX）提供了统一的分布式数据处理平台，整个生态经过多年的蓬勃发展已经非常完善。

然而正在大家认为 Spark 已经如日中天、即将一统天下之际，Flink 如一颗新星异军突起，使得大数据处理的江湖再起风云。很多读者在最初接触都会有这样的疑问：想学习一个大数据处理框架，到底选择 Spark，还是 Flink 呢？

这就需要我们了解两者的主要区别，理解它们在不同领域的优势

###### 3.3.1 数据处理架构不同

从根本上说，Spark 和 Flink 采用了完全不同的数据处理方式。可以说，两者的世界观是截然相反的。

Spark 以批处理为根本，并尝试在批处理之上支持流计算；在 Spark 的世界观中，万物皆批次，离线数据是一个大批次，而实时数据则是由一个一个无限的小批次组成的。所以对于流处理框架 Spark Streaming 而言，其实并不是真正意义上的“流”处理，而是“微批次”（micro-batching）处理。

![image-20230307132157416](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_1/image-20230307132157416.png)

而 Flink 则认为，流处理才是最基本的操作，批处理也可以统一为流处理。在 Flink 的世界观中，万物皆流，实时数据是标准的、没有界限的流，而离线数据则是有界限的流。

1. 无界数据流（Unbounded Data Stream）

所谓无界数据流，就是有头没尾，数据的生成和传递会开始但永远不会结束，如下图所示。我们无法等待所有数据都到达，因为输入是无界的，永无止境，数据没有“都到达”的时候。所以对于无界数据流，必须连续处理，也就是说必须在获取数据后立即处理。在处理无界流时，为了保证结果的正确性，我们必须能够做到按照顺序处理数据。

1. 有界数据流（Bounded Data Stream）

对应的，有界数据流有明确定义的开始和结束，如下图所示，所以我们可以通过获取所有数据来处理有界流。处理有界流就不需要严格保证数据的顺序了，因为总可以对有界数据集进行排序。有界流的处理也就是批处理。

![image-20230307133333823](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_1/image-20230307133333823.png)

正因为这种架构上的不同，Spark 和 Flink 在不同的应用领域上表现会有差别。一般来说，Spark 基于微批处理的方式做同步总有一个“攒批”的过程，所以会有额外开销，因此无法在流处理的低延迟上做到极致。在低延迟流处理场景，Flink 已经有明显的优势。而在海量数据的批处理领域，Spark 能够处理的吞吐量更大，加上其完善的生态和成熟易用的 API，目前同样优势比较明显。

###### 3.3.2 数据模型和运行架构不同

Spark 和 Flink 在底层实现最主要的差别就在于数据模型不同。

Spark 底层数据模型是弹性分布式数据集（RDD），Spark Streaming 进行微批处理的底层接口 DStream，实际上处理的也是一组组小批数据 RDD 的集合。可以看出，Spark 在设计上本身就是以批量的数据集作为基准的，更加适合批处理的场景。

而 Flink 的基本数据模型是数据流（DataFlow），以及事件（Event）序列。Flink 基本上是完全按照 Google 的 DataFlow 模型实现的，所以从底层数据模型上看，Flink 是以处理流式数据作为设计目标的，更加适合流处理的场景。

数据模型不同，对应在运行处理的流程上，自然也会有不同的架构。Spark 做批计算，需要将任务对应的 DAG 划分阶段（Stage），一个完成后经过 shuffle 再进行下一阶段的计算。而Flink 是标准的流式执行模式，一个事件在一个节点处理完后可以直接发往下一个节点进行处理。

###### 3.3.3 抉择

通过分析，我们已经可以看出，Spark 和 Flink 可以说目前是各擅胜场，批处理领域 Spark 称王，而在流处理方面 Flink 当仁不让。具体到项目应用中，不仅要看是流处理还是批处理，还需要在延迟、吞吐量、可靠性，以及开发容易度等多个方面进行权衡。

如果在工作中需要从 Spark 和 Flink 这两个主流框架中选择一个来进行实时流处理，我们更加推荐使用 Flink，主要的原因有：

- Flink 的延迟是毫秒级别，而 Spark Streaming 的延迟是秒级延迟。
- Flink 提供了严格的精确一次性语义保证。
- Flink 的窗口 API 更加灵活、语义更丰富。
- Flink 提供事件时间语义，可以正确处理延迟数据。
- Flink 提供了更加灵活的可对状态编程的 API。

基于以上特点，使用 Flink 可以解放程序员, 加快编程效率, 把本来需要程序员花大力气手动完成的工作交给框架完成。当然，在海量数据的批处理方面，Spark 还是具有明显的优势。而且 Spark 的生态更加成熟，也会使其在应用中更为方便。相信随着 Flink 的快速发展和完善，这方面的差距会越来越小。

另外，Spark 2.0 之后新增的 Structured Streaming 流处理引擎借鉴 `DataFlow` 进行了大量优化，同样做到了低延迟、时间正确性以及精确一次性语义保证；Spark 2.3 以后引入的连续处理（Continuous Processing）模式，更是可以在至少一次语义保证下做到 1 毫秒的延迟。而 Flink自 1.9 版本合并 Blink 以来，在 SQL 的表达和批处理的能力上同样有了长足的进步。





# 实验1-2：Flink快速体验

## 实验概述

Flink的应用程序和传统的MR程序编程过程有什么不同以及Flink的流式是如何进行词频统计的，开发完Flink的应用后在实战环境下如何进行部署，这些都是我们快速体验中需要掌握和了解的，这也为我们后面深入了解Flink原理架构做好前期准备。

## 实验环境

- AtStudy 实训平台
- Flink1.13
- Hadoop3

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/1679275331544773.png)

## 实验目标

学习完成本实验后，您将能够

- 掌握Flink的程序结构
- 掌握Flink流式统计词频的经典案例
- 掌握Flink在实际中的部署过程
- 了解Flink的执行过程

## 实验任务

### 任务一、Flink快速上手

#### **【任务目标】**

开发我们的第一个Flink应用程序。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009251658886)

视频-1、Flink快速上手

##### 1.1 创建工程

我们开始开发自己的第一个 Flink 程序了，首先我们要做的，就是在 IDEA 中搭建一个 Flink 项目的骨架。我们会使用 Java 项目中常见的 Maven来进行依赖管理。

- 创建工程

  打开 IntelliJ IDEA，创建一个 Maven 工程，

![image-20230307150035121](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307150035121.png)

![image-20230307150129402](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307150129402.png)

 将这个 Maven 工程命名为 FlinkTutorial

![image-20230307150324837](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307150324837.png)

 设置这个 Maven 工程所在存储路径，并点击 Finish，Maven 工程即创建成功。

- 添加项目依赖

  在项目的 pom 文件中，增加`<properties>`标签设置属性，然后增加`<denpendencies>`标签引入需要的依赖。我们需要添加的依赖最重要的就是 Flink 的相关组件，包括 `flink-scala`、`flink-streaming-scala`，以及` flink-clients`（客户端，也可以省略）。

  ```
  复制代码
  ```

xml <flink.version>1.13.0</flink.version> <target.java.version>1.8</target.java.version> <scala.binary.version>2.12</scala.binary.version> org.apache.flink flink-scala_${scala.binary.version} ${flink.version} org.apache.flink flink-streaming-scala_${scala.binary.version} ${flink.version} org.apache.flink flink-clients_${scala.binary.version} ${flink.version}

```
> 提示：在pom文件中添加好依赖后，需要稍微等待一会，因为需要在线下载依赖包！

之后，我们可以在`Maven Projects`中看到我们添加的依赖内容

![image-20230307151023107](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307151023107.png)

- 添加Scala项目支持

选中项目，右键，`Add Framework Support `

![image-20230307152516260](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307152516260.png)

![image-20230307152605870](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307152605870.png)

选中`Scala`完成，项目的支持。

##### 1.2 编写代码

搭好项目框架，接下来就是我们的核心工作——往里面填充代码。我们会用一个最简单的示例来说明 Flink 代码怎样编写：统计一段文字中，每个单词出现的频次。

我们首先在 src/main 路径下新建一个源码目录 scala，在这个目录下新建一个包，命名为 `com.atstudy.chapter01`，在这个包下我们将编写 Flink入门的 WordCount 程序。

###### 1.2.1  批处理

对于批处理而言，输入的应该是收集好的数据集。这里我们可以将要统计的文字，写入一个文本文档，然后读取这个文件处理数据就可以了。

- 在工程根目录下新建一个` input` 文件夹，并在下面创建文本文件 `words.txt`

- 在 `words.txt` 中输入一些文字，例如：
复制代码
```

tex hello world hello flink hello scala

```
- 在 `com.atstudy.chapter01` 包下新建 Scala 的单例对象（object）`BatchWordCount`，在静态 main 方法中编写测试代码。

我们进行单词频次统计的基本思路是：先逐行读入文件数据，然后将每一行文字拆分成单词；接着按照单词分组，统计每组数据的个数，就是对应单词的频次。

具体代码实现如下：

```scala
package com.atstudy.chapter01

import org.apache.flink.api.scala._

object BatchWordCount {
    def main(args: Array[String]): Unit = {
        //创建执行环境并配置并行度
        val env = ExecutionEnvironment.getExecutionEnvironment
        
        //读取文本文件
        val lineDS:DataSet[String] = env.readTextFile("input/words.txt")
        
        //对数据进行格式转换
        val wordAndOne:DataSet[(String,Int)] = lineDS.flatMap(_.split(" ")).map(r => (r, 1))
        
        //对数据进行分组
        val wordAndOneUG = wordAndOne.groupBy(0)
       
        //对分组数据进行聚合
        val sum = wordAndOneUG.sum(1)
        
        //打印结果
        sum.print
    }
}复制代码
```

- 运行程序，控制台会打印出结果：

  ```
  scala
  (scala,1)
  (flink,1)
  (world,1)
  (hello,3)复制代码
  ```

  可以看到，我们将文档中的所有单词的频次，全部统计出来，以二元组的形式在控制台打印输出了。

  需要注意的是，这种代码的实现方式，是基于 DataSet API 的，也就是我们对数据的处理转换，是看作数据集来进行操作的。事实上 Flink 本身是流批统一的处理架构，批量的数据集本质上也是流，没有必要用两套不同的 API 来实现。所以从 Flink 1.12 开始，官方推荐的做法是直接使用 DataStream API，在提交任务时通过将执行模式设为 BATCH 来进行批处理：

  ```
  $ bin/flink run -Dexecution.runtime-mode=BATCH BatchWordCount.jar复制代码
  ```

  这样，DataSet API 就已经处于“软弃用”（soft deprecated）的状态，在实际应用中我们只要维护一套 DataStream API 就可以了。这里只是为了方便大家理解，我们依然用 DataSet API做了批处理的实现。

###### 1.2.2 流处理

我们已经知道，用 DataSet API 可以很容易地实现批处理；与之对应，流处理当然可以用DataStream API 来实现。对于 Flink 而言，流才是整个处理逻辑的底层核心，所以流批统一之后的 DataStream API 更加强大，可以直接处理批处理和流处理的所有场景。

下面我们就针对不同类型的输入数据源，用具体的代码来实现流处理。

1. 读取文件

   我们同样试图读取文档 `words.txt` 中的数据，并统计每个单词出现的频次。整体思路与之前的批处理非常类似，代码模式也基本一致。

   - 在 `com.atstudy.chapter01` 包下新建 Scala 的单例对象 `BoundedStreamWordCount`，在静态 main 方法中编写测试代码。具体代码实现如下：

   ```
   package com.atstudy.chapter01
   import org.apache.flink.streaming.api.scala._
   
   object BoundedStreamWordCount {
       def main(args: Array[String]): Unit = {
           // 创建流执行环境
           val env = StreamExecutionEnvironment.getExecutionEnvironment
   
           // 读取文件获取数据流
           val lineDS = env.readTextFile("input/words.txt")
           
           //对数据流执行转换操作
           val wordAndOne = lineDS.flatMap(_.split(" ")).map(data => (data, 1))
           
           //对数据进行分组
           val wordAndOneKS = wordAndOne.keyBy(_._1)
           
           //对分组数据进行聚合
           val result = wordAndOneKS.sum(1)
           
           //打印结果
           result.print()
          
           //执行任务
           env.execute()
       }
   }复制代码
   ```

   - 运行程序，控制台输出结果如下：

   ```
   3> (world,1)
   2> (hello,1)
   4> (flink,1)
   2> (hello,2)
   2> (hello,3)
   1> (scala,1)复制代码
   ```

   我们可以看到，这与批处理的结果是完全不同的。批处理针对每个单词，只会输出一个最终的统计个数；而在流处理的打印结果中，“hello”这个单词每出现一次，都会有一个频次统计数据输出。这就是流处理的特点，数据逐个处理，每来一条数据就会处理输出一次。我们通过打印结果，可以清晰地看到单词“hello”数量增长的过程。

2. 读取文本流

   在实际的生产环境中，真正的数据流其实是无界的，有开始却没有结束，这就要求我们需要保持一个监听事件的状态，持续地处理捕获的数据。

   为了模拟这种场景，我们就不再通过读取文件来获取数据了，而是监听数据发送端主机的指定端口，统计发送来的文本数据中出现过的单词的个数。具体实现上，我们只要对`BoundedStreamWordCount` 代码中读取数据的步骤稍做修改，就可以实现对真正无界流的处理。

   - 将 `BoundedStreamWordCount` 代码中读取文件数据的 `readTextFile()`方法，替换成读取`socket`文本流的方法 `socketTextStream()`方法。具体代码实现如下：

   ```
   package com.atstudy.chapter01
   import org.apache.flink.streaming.api.scala._
   
   object StreamWordCount {
       def main(args: Array[String]): Unit = {
           val env = StreamExecutionEnvironment.getExecutionEnvironment
           
           //通过主机名和端口号读取 socket 文本流
           val lineDS = env.socketTextStream("localhost", 7777)
           
           // 进行转换计算
           val result = lineDS
                .flatMap(data => data.split(" ")) // 用空格切分字符串
               .map((_, 1)) // 切分以后的单词转换成一个元组
                  .keyBy(_._1) // 使用元组的第一个字段进行分组
               .sum(1) // 对分组后的数据的第二个字段进行累加
           
           // 打印计算结果
           result.print()
           // 执行程序
           env.execute()
       }
   }
   （复制代码
   ```

   - 然后，打开一个终端窗口，执行下列命令，发送数据进行测试：

   ```
   nc -lk 7777复制代码
   ```

   ![image-20230307163949205](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307163949205.png)

   - 启动 StreamWordCount 程序

     我们会发现程序启动之后没有任何输出、也不会退出。这是正常的——因为 Flink 的流处理是事件驱动的，当前程序会一直处于监听状态，只有接收到数据才会执行任务、输出统计结果。

   - 从终端窗口发送数据：

   ```
   hello flink
   hello world
   hello scala复制代码
   ```

   ![image-20230307164137558](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307164137558.png)

   - 可以看到控制台输出结果如下：

   ```
   2> (hello,1)
   4> (flink,1)
   3> (world,1)
   2> (hello,2)
   2> (hello,3)
   1> (scala,1)复制代码
   ```

   我们会发现，输出的结果与之前读取文件的流处理非常相似。而且可以非常明显地看到，每输入一条数据，就有一次对应的输出。具体对应关系是：输入“hello flink”，就会输出两条统计结果（flink,1）和（hello,1）；之后再输入“hello world”，同样会将 hello 和 world 的个数统计输出，hello 的个数会对应增长为 2。

那么，到目前为止，我们基本了解了Flink应用的开发过程。

##### 1.3 总结

本任务主要实现一个 Flink 开发的入门程序——词频统计 `WordCount`。通过批处理和流处理两种不同模式的实现，可以对 Flink 的API 风格和编程方式有所熟悉，并且更加深刻地理解批处理和流处理的不同。另外，通过读取有界数据（文件）和无界数据（socket 文本流）进行流处理的比较，我们也可以更加直观地体会到 Flink 流处理的方式和特点。

这是我们 Flink 长征路上的第一步，是后续学习的基础。有了这番初体验，想必大家会发现 Flink 提供了非常易用的API，基于它进行开发并不是难事。之后我们会逐步深入展开，为大家打开 Flink 神奇世界的大门。

### 任务二、安装部署

#### **【任务目标】**

掌握Flink应用在实战中的部署过程，了解Flink执行流程。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009251186545)

视频-2、安装部署

##### 2.1 Flink的执行过程

我们在集成开发环境里编写 Flink 代码，然后运行测试。但是：对于读取文本流的流处理程序，运行之后其实并不会去直接执行代码中定义好的操作——因为这时还没有数据；只有在输入数据之后，才会触发分词转换、分组统计的一系列处理操作。可明明我们的代码顺序执行，会调用到 flatMap、keyBy 和 sum 等一系列处理方法，这是怎么回事呢？

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210823161648091.png)**这涉及 Flink 作业提交运行的原理！**

我们编写的代码，对应着在 Flink 集群上执行的一个`作业`；所以我们在本地执行代码，其实是先模拟启动一个 Flink 集群，然后将作业提交到集群上，创建好要执行的任务等待数据输入。

这里需要提到 Flink 中的几个关键组件：客户端（`Client`）、作业管理器（`JobManager`）和任务管理器（`TaskManager`）。我们的代码，实际上是由客户端获取并做转换，之后提交给`JobManger` 的。所以 `JobManager` 就是 Flink 集群里的“领导者”，对作业进行中央调度管理；而它获取到要执行的作业后，会进一步处理转换，然后分发任务给众多的 `TaskManager`。这里的 `TaskManager`，就是真正“干活的人”，数据的处理操作都是它们来做的。

![image-20230307170628963](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307170628963.png)

##### 2.2 Flink本地启动

Flink 是典型的 `Master-Slave` 架构的分布式数据处理框架，其中 `Master` 角色对应着`JobManager`，`Slave` 角色则对应 `TaskManager`。实际生产环境下，一般会搭建多台服务器形成集群环境，比如在其中一台服务器上启动`JobManager`进程，另外几台服务器启动`TaskManger`进程。

由于我们服务器环境限制，只能采用本地部署方式，也就是在一台服务器上启动`JobManager`和`TaskManager`进程。

本地部署非常简单，直接解压安装包就可以使用，不用进行任何配置，一般用来做一些简单的测试。

具体安装步骤如下：

1. 下载安装包

   在`/opt/software`目录下，下载 `1.13.0` 版本安装包 `flink-1.13.0-bin-scala_2.12.tgz`，注意此处选用对应 `scala` 版本为 `scala 2.12` 的安装包。

   执行下载命令：

   ```
   wget https://archive.apache.org/dist/flink/flink-1.13.0/flink-1.13.0-bin-scala_2.12.tgz复制代码
   ```

   ![image-20230307172307855](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307172307855.png)

   > 下载过程可能会稍微有点慢，需要稍稍等待一会！100%表示下载完成
   >
   > ![image-20230307212028340](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307212028340.png)

2. 解压

   在服务器的`/opt/software`目录下，执行解压命令，解压至当前目录。

   ```
   # tar -zxvf flink-1.13.0-bin-scala_2.12.tgz -C /opt/module/
   flink-1.13.0/
   flink-1.13.0/log/
   flink-1.13.0/LICENSE
   flink-1.13.0/lib/
   ……复制代码
   ```

3. 启动

   进入解压后的目录，执行启动命令，并查看进程。

   ```
   # cd flink-1.13.0/
   # bin/start-cluster.sh 
   Starting cluster.
   Starting standalonesession daemon on host 1d7212a0a4479.
   Starting taskexecutor daemon on host 1d7212a0a4479.
   
   # jps
   10369 StandaloneSessionClusterEntrypoint
   10680 TaskManagerRunner
   10717 Jps复制代码
   ```

4. 访问 Web UI

   启动成功后，访问 `http://localhost:8081`，可以对 flink 集群和任务进行监控管理，如图所示。

![image-20230307212356041](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307212356041.png)

1. 关闭集群

   如果想要让 Flink 集群停止运行，可以执行以下命令：

   ```
   复制代码
   ```

shell

# bin/stop-cluster.sh

Stopping taskexecutor daemon (pid: 10680) on host 1d7212a0a4479. Stopping standalonesession daemon (pid: 10369) on host 1d7212a0a4479.

```
##### 2.3 应用部署

在上一个任务中，我们已经编写了词频统计的批处理和流处理的示例程序，现在既然已经有了真正的本地环境，那接下来我们就要把作业提交上去执行了。

这里我们将以流处理的程序为例，演示如何将任务提交到集群中进行执行。具体步骤如下。

###### 2.3.1 程序打包

- 为方便自定义结构和定制依赖，我们可以引入插件 `maven-assembly-plugin` 进行打包。在 `FlinkTutorial` 项目的 `pom.xml` 文件中添加打包插件的配置，具体如下：

  ```xml
  <build>
   <plugins>
       <plugin>
           <groupId>org.apache.maven.plugins</groupId>
           <artifactId>maven-assembly-plugin</artifactId>
           <version>3.0.0</version>
           <configuration>
               <descriptorRefs>
                   <descriptorRef>jar-with-dependencies</descriptorRef>
               </descriptorRefs>
           </configuration>
           <executions>
               <execution>
                   <id>make-assembly</id>
                   <phase>package</phase>
                   <goals>
                       <goal>single</goal>
                   </goals>
               </execution>
           </executions>
       </plugin>
   </plugins>
  </build>复制代码
```

- 插件配置完毕后，可以使用 IDEA 的 Maven 工具执行 package 命令，出现如下提示即表示打包成功。

```
[INFO]	-------------------------------------------------------------------------
[INFO]	BUILD SUCCESS
[INFO]	-------------------------------------------------------------------------
[INFO]	Total time: 21.665 s
[INFO]	Finished at: 2023-03-07T17:21:26+08:00
[INFO]	Final Memory: 141M/770M
[INFO]	-------------------------------------------------------------------------复制代码
```

打包完成后， 在 target 目录下即可找到所需 JAR 包， JAR 包会有两个， `FlinkTutorial-1.0-SNAPSHOT.jar `和 `FlinkTutorial-1.0-SNAPSHOT-jar-with-dependencies.jar`，因为集群中已经具备任务运行所需的所有依赖，所以建议使用 `FlinkTutorial-1.0-SNAPSHOT.jar`。

![image-20230307220302007](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307220302007.png)

###### 2.3.2 在`Web UI`上提交作业

- 任务打包完成后，我们打开 Flink 的`WEB UI`页面，在右侧导航栏点击`Submit New Job`，然后点击按钮`+ Add New`，选择要上传运行的 JAR 包，如图所示：

  ![image-20230307223149489](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307223149489.png)

  上传完成后，如图所示：

  ![image-20230307223455958](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307223455958.png)

- 点击该JAR 包，出现任务配置页面，进行相应配置。

  主要配置程序入口主类的全类名，任务运行的并行度，任务运行所需的配置参数和保存点路径等，配置完成后，即可点击按钮`Submit`，将任务提交到集群运行。

  ![image-20230307224437931](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307224437931.png)

  在`Submit`前，需要先开启一个终端窗口，执行如下命令：

  ```
  # nc -lk 7777 复制代码
  ```

  ![image-20230307224548536](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307224548536.png)

  然后就可以`Submit`提交作业了。

  ![image-20230307224934771](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307224934771.png)

- 任务提交成功之后，可点击左侧导航栏的`Running Jobs`查看程序运行列表情况， 如图所示。

  ![image-20230307225007623](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307225007623.png)

- 这个时候，我可以在终端的命令窗口中输入字符串内容：

  ![image-20230307225403215](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307225403215.png)

- 在`Task Managers`中我们可以日志内容的输出

  ![image-20230307225531942](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307225531942.png)

  ![image-20230307225557492](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307225557492.png)

- 点击该任务，可以查看任务运行的具体情况，也可以通过点击`Cancel Job`结束任务运行，如图所示。

  ![image-20230307230008522](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307230008522.png)

###### 2.3.3 命令行提交作业

除了通过 WEB UI 界面提交任务之外，也可以直接通过命令行来提交任务。这里为方便起见，我们可以先把 jar 包拷贝到目录 `flink-1.13.0` 下

- 首先需要启动集群。

  ```
  # bin/start-cluster.sh	复制代码
  ```

- 在终端中执行以下命令启动`netcat`

  ```
  # nc -lk 7777复制代码
  ```

- 进入到Flink 的安装路径下，在命令行使用 `flink run` 命令提交作业。

  ```
  # ./bin/flink run -m localhost:8081 -c com.atstudy.chapter01.StreamWordCount ./FlinkTutorial-1.0-SNAPSHOT.jar 复制代码
  ```

  ![image-20230307230722326](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307230722326.png)

  > `–m` ：指定了提交到的 JobManager
  >
  > `-c`： 指定了入口类

- 在浏览器中打开Web UI，`http://localhost:8081` 查看应用执行情况，如图所示。

  用 netcat 输入数据。

  ![image-20230307231148148](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307231148148.png)

  可以在 TaskManager 的标准输出（Stdout）看到对应的统计结果。

  ![image-20230307231118842](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307231118842.png)

- 在 log 日志中，也可以查看执行结果，需要找到执行该数据任务的TaskManager 节点查看日志。

  ![image-20230307231433601](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20230307231433601.png)





# 实验2-1：Flink运行架构及原理

## 实验概述

在深入学习Flink之前，还是有必要研究一下Flink的运行架构及原理，这也是企业筛选人才的必选考题。

## 实验环境

- AtStudy 实训平台
- Flink1.13

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16792754034316155.png)

## 实验目标

学习完成本实验后，您将能够

- 了解Flink整体系统架构
- 了解作业的提交流程
- 掌握Flink架构中的一些核心概念

## 实验任务

### 任务一、系统架构

#### **【任务目标】**

了解Flink系统架构，掌握核心组件的作用。

#### **【任务步骤】**

**视频讲解：**

##### 1.1 整体构成

Flink 的运行时架构中，最重要的就是两大组件：作业管理器（`JobManger`）和任务管理器（`TaskManager`）。对于一个提交执行的作业，`JobManager` 是真正意义上的“管理者”（`Master`），负责管理调度，所以在不考虑高可用的情况下只能有一个；而 `TaskManager` 是“工作者”（`Worker、Slave`），负责执行任务处理数据，所以可以有一个或多个。

Flink 的作业提交和任务处理时的系统如图所示：

![image-20230308092340059](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab2_1/image-20230308092340059.png)

这里首先要说明一下“客户端”。其实客户端并不是处理系统的一部分，它只负责作业的提交。`JobManager `和 `TaskManagers` 可以以不同的方式启动：

- 作为独立（`Standalone`）集群的进程，直接在机器上启动
- 在容器中启动
- 由资源管理平台调度启动，比如 `YARN、K8S`

这其实就对应着不同的部署方式。

`TaskManager` 启动之后，`JobManager` 会与它建立连接，并将作业图（`JobGraph`）转换成可执行的“执行图”（`ExecutionGraph`）分发给可用的` TaskManager`，然后就由 `TaskManager` 具体执行任务。接下来，我们就具体介绍一下 `JobManger` 和 `TaskManager `在整个过程中扮演的角色。

##### 1.2 作业管理器

`JobManager` 是一个 Flink 集群中任务管理和调度的核心，是控制应用执行的主进程。也就是说，每个应用都应该被唯一的 `JobManager` 所控制执行。`JobManger` 又包含 3 个不同的组件。

1. `JobMaster`

   `JobMaster `是` JobManager` 中最核心的组件，负责处理单独的作业（job）。`JobMaster `和具体的 job 是一一对应的，多个 job 可以同时运行在一个 Flink 集群中, 每个 job 都有一个自己的`JobMaster`。

   在作业提交时，`JobMaste`r 会先接收到要执行的应用。`JobMaster` 会把`JobGraph`转换成一个物理层面的数据流图，这个图被叫作“执行图”（`ExecutionGraph`），它包含了所有可以并发执行的任务。`JobMaster` 会向资源管理器（`ResourceManager`）发出请求，申请执行任务必要的资源。一旦它获取到了足够的资源，就会将执行图分发到真正运行它们的 `TaskManager` 上。而在运行过程中，`JobMaster` 会负责所有需要中央协调的操作，比如说检查点（`checkpoints`）的协调。

2. 资源管理器（`ResourceManager`）

   `ResourceManager` 主要负责资源的分配和管理，在 Flink 集群中只有一个。所谓“资源”，主要是指 `TaskManager `的任务槽（`task slots`）。任务槽就是 Flink 集群中的资源调配单元，包含了机器用来执行计算的一组 CPU 和内存资源。每一个任务（task）都需要分配到一个 slot 上执行。

   Flink 的 `ResourceManager`，针对不同的环境和资源管理平台（比如 Standalone 部署，或者YARN），有不同的具体实现。在 Standalone 部署时，因为 `TaskManager `是单独启动的（没有Per-Job 模式），所以 `ResourceManager` 只能分发可用 `TaskManager` 的任务槽，不能单独启动新`TaskManager`。而在有资源管理平台时，就不受此限制。当新的作业申请资源时，`ResourceManager `会将有空闲槽位的 `TaskManager `分配给` JobMaster`。如果 `ResourceManager `没有足够的任务槽，它还可以向资源提供平台发起会话，请求提供启动 `TaskManager` 进程的容器。另外，`ResourceManager` 还负责停掉空闲的 `TaskManager`，释放计算资源。

3. 分发器（`Dispatcher`）

   `Dispatcher `主要负责提供一个 REST 接口，用来提交作业，并且负责为每一个新提交的作业启动一个新的 `JobMaster` 组件。`Dispatcher `也会启动一个 `Web UI`，用来方便地展示和监控作业执行的信息。`Dispatcher` 在架构中并不是必需的，在不同的部署模式下可能会被忽略掉。

##### 1.3 任务管理器

`TaskManager` 是 Flink 中的工作进程，负责数据流的具体计算任务（task）。Flink 集群中必须至少有一个 `TaskManager`；当然由于分布式计算的考虑，通常会有多个 `TaskManager `运行，每一个`TaskManager`都包含了一定数量的任务槽（task slots）。Slot 是资源调度的最小单位，slots的数量限制了 `TaskManager` 能够并行处理的任务数量。

启动之后，`TaskManager` 会向资源管理器注册它的 `slots`；收到资源管理器的指令后，`TaskManager `就会将一个或者多个槽位提供给 `JobMaster` 调用，`JobMaster` 就可以分配任务来执行了。

在执行过程中，`TaskManager` 可以缓冲数据，还可以跟其他运行同一应用的` TaskManager`交换数据。

### 任务二、作业提交流程

#### **【任务目标】**

了解作业提交到Flink后如何执行的，掌握整个提流程。

#### **【任务步骤】**

##### 2.1 部署模式

Flink作业提交流程和具体的部署模式是直接相关联的，而Flink应用如何部署，在一些应用场景中，对于集群资源分配和占用的方式，可能会有特定的需求。Flink 为各种场景提供了不同的部署模式，主要有以下三种：

- 会话模式（Session Mode）
- 单作业模式（Per-Job Mode）
- 应用模式（Application Mode）

它们的区别主要在于：集群的生命周期以及资源的分配方式；以及应用的 main 方法到底在哪里执行——客户端（Client）还是` JobManager`。

###### 2.1.1 会话模式

会话模式其实最符合常规思维。我们需要先启动一个集群，保持一个会话，在这个会话中通过客户端提交作业，如图所示。集群启动时所有资源就都已经确定，所以所有提交的作业会竞争集群中的资源。

![image-20230308095811917](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab2_1/image-20230308095811917.png)

会话模式比较适合于单个规模小、执行时间短的大量作业。

###### 2.1.2 单作业模式（Per-Job Mode）

会话模式因为资源共享会导致很多问题，所以为了更好地隔离资源，我们可以考虑为每个提交的作业启动一个集群，这就是所谓的单作业（Per-Job）模式，如图所示。

![image-20230308095907761](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab2_1/image-20230308095907761.png)

需要注意的是，Flink 本身无法直接这样运行，所以单作业模式一般需要借助一些资源管理平台来启动集群，比如 `YARN、Kubernetes`。

###### 2.1.3 应用模式（Application Mode）

前面提到的两种模式下，应用代码都是在客户端上执行，然后由客户端提交给` JobManager`的。但是这种方式客户端需要占用大量网络带宽，去下载依赖和把二进制数据发送给`JobManager`；加上很多情况下我们提交作业用的是同一个客户端，就会加重客户端所在节点的资源消耗。

所以解决办法就是，我们不要客户端了，直接把应用提交到` JobManger` 上运行。而这也就代表着，我们需要为每一个提交的应用单独启动一个` JobManager`，也就是创建一个集群。这个`JobManager`只为执行这一个应用而存在，执行结束之后`JobManager`也就关闭了，这就是所谓的应用模式，如图所示。

![image-20230308100048043](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab2_1/image-20230308100048043.png)

应用模式与单作业模式，都是提交作业之后才创建集群；单作业模式是通过客户端来提交的，客户端解析出的每一个作业对应一个集群；而应用模式下，是直接由 `JobManager` 执行应用程序的，并且即使应用包含了多个作业，也只创建一个集群。

##### 2.2 提交流程

Flink 的提交流程，随着部署模式、资源管理平台的不同，会有不同的变化。首先我们从一个高层级的视角，来做一下抽象提炼，看一看作业提交时宏观上各组件是怎样交互协作的。

![image-20230308101415728](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab2_1/image-20230308101415728.png)

如图所示，具体步骤如下：

1. 一般情况下，由客户端（`App`）通过分发器提供的REST接口，将作业提交给`JobManager`。
2. 由分发器启动 `JobMaster`，并将作业（包含` JobGraph`）提交给 `JobMaster`。
3. `JobMaster` 将 `JobGraph` 解析为可执行的 `ExecutionGraph`，得到所需的资源数量，然后向资源管理器请求任务槽资源（slots）。
4. 资源管理器判断当前是否由足够的可用资源；如果没有，启动新的 `TaskManager`。
5. `TaskManager` 启动之后，向 `ResourceManager` 注册自己的可用任务槽（slots）。
6. 资源管理器通知`TaskManager`为新的作业提供 slots。
7. `TaskManager` 连接到对应的` JobMaster`，提供 slots。
8. `JobMaster` 将需要执行的任务分发给 `TaskManager`。
9. `TaskManager` 执行任务，互相之间可以交换数据。

如果部署模式不同，或者集群环境不同（例如 Standalone、YARN、K8S 等），其中一些步骤可能会不同或被省略，也可能有些组件会运行在同一个 `JVM` 进程中。

是否依赖于第三资源管理平台，我们将部分方式分为独立模式（Standalone）和YARN模式。

###### 2.2.1 独立模式（Standalone）

在独立模式（`Standalone`）下，只有会话模式和应用模式两种部署方式。两者整体来看流程是非常相似的： `TaskManager `都需要手动启动，所以当`ResourceManager`收到` JobMaster` 的请求时，会直接要求`TaskManager`提供资源。而` JobMaster` 的启动时间点，会话模式是预先启动，应用模式则是在作业提交时启动。提交的整体流程如图所示:

![image-20230308102453584](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab2_1/image-20230308102453584.png)

###### 2.2.2 YARN模式

选择YARN作为资源管理平台，是业内采用的常规方式，针对不同的部署模式，作业提交流程也有所区分：

- 会话（Session）模式

  在会话模式下，我们需要先启动一个 YARN session，这个会话会创建一个 Flink 集群。

  

  这里只启动了` JobManager`，而 `TaskManager` 可以根据需要动态地启动。在` JobManager` 内部，由于还没有提交作业，所以只有` ResourceManager` 和 `Dispatcher `在运行，如上图所示

  接下来就是真正提交作业的流程：

  

  1. 客户端通过 REST 接口，将作业提交给分发器。
  2. 分发器启动 `JobMaster`，并将作业（包含 `JobGraph`）提交给 `JobMaster`。
  3. `JobMaster` 向资源管理器请求资源（slots）。
  4. 资源管理器向 YARN 的资源管理器请求 container 资源。
  5. YARN 启动新的 `TaskManager `容器。
  6. `TaskManager` 启动之后，向 Flink 的资源管理器注册自己的可用任务槽。
  7. 资源管理器通知 `TaskManager` 为新的作业提供 slots。
  8. `TaskManager `连接到对应的` JobMaster`，提供 slots。
  9. `JobMaster` 将需要执行的任务分发给 `TaskManager`，执行任务

- 单作业（Per-Job）模式

  在单作业模式下，Flink 集群不会预先启动，而是在提交作业时，才启动新的` JobManager`。具体流程如图所示。

  

  1. 客户端将作业提交给 YARN 的资源管理器，这一步中会同时将 Flink 的 Jar 包和配置上传到 HDFS，以便后续启动 Flink 相关组件的容器。
  2. YARN 的资源管理器分配容器（container）资源，启动 Flink `JobManager`，并将作业提交给 `JobMaster`。这里省略了 Dispatcher 组件。
  3. `JobMaster` 向资源管理器请求资源（slots）。
  4. 资源管理器向 YARN 的资源管理器请求容器（container）。
  5. YARN 启动新的` TaskManager` 容器。
  6. `TaskManager `启动之后，向 Flink 的资源管理器注册自己的可用任务槽。
  7. 资源管理器通知` TaskManager` 为新的作业提供 slots。
  8. `TaskManager` 连接到对应的` JobMaster`，提供 slots。
  9. `JobMaster` 将需要执行的任务分发给 `TaskManager`，执行任务。

- 应用（Application）模式

  应用模式与单作业模式的提交流程非常相似，只是初始提交给 YARN 资源管理器的不再是具体的作业，而是整个应用。一个应用中可能包含了多个作业，这些作业都将在 Flink 集群中启动各自对应的` JobMaster`。

### 任务三、核心概念

#### **【任务目标】**

我们现在已经了解 Flink 运行时的核心组件和整体架构，也明白了不同场景下作业提交的具体流程。但有些细节还需要进一步思考：一个具体的作业，是怎样从我们编写的代码，转换成 `TaskManager` 可以执行的任务的呢？`JobManager` 收到提交的作业，又是怎样确定总共有多少任务、需要多少资源呢？接下来我们就从一些重要概念入手，对这些问题做详细的展开讲解。

#### **【任务步骤】**

##### 3.1 数据流图（`Dataflow Graph`）

Flink 是流式计算框架。它的程序结构，其实就是定义了一连串的处理操作，每一个数据输入之后都会依次调用每一步计算。在 Flink 代码中，我们定义的每一个处理转换操作都叫作“**算子**”（Operator）。

所有的 Flink 程序都可以归纳为由三部分构成：`Source`、`Transformation` 和 `Sink`。

- `Source` 表示“源算子”，负责读取数据源。
- `Transformation` 表示“转换算子”，利用各种算子进行处理加工。
- `Sink` 表示“下沉算子”，负责数据的输出。

在运行时，Flink 程序会被映射成所有算子按照逻辑顺序连接在一起的一张图，这被称为“逻辑数据流”（`logical dataflow`），或者叫“数据流图”（`dataflow graph`）。在数据流图中，可以清楚地看到 `Source、Transformation、Sink` 三部分。

![image-20230308105434456](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab2_1/image-20230308105434456.png)

##### 3.2 并行度（Parallelism）

1. 并行子任务和并行度

   把一个算子操作，“复制”多份到多个节点，数据来了之后就可以到其中任意一个执行。这样一来，一个算子操作就被拆分成了多个并行的“子任务”（subtasks），再将它们分发到不同节点，就真正实现了并行计算。

   在 Flink 执行过程中，每一个算子（operator）可以包含一个或多个子任务（operator subtask），这些子任务在不同的线程、不同的物理机或不同的容器中完全独立地执行。

   

   一个特定算子的子任务（subtask）的个数被称之为其并行度（parallelism）。这样，包含并行子任务的数据流，就是并行数据流，它需要多个分区（stream partition）来分配并行任务。一般情况下，一个流程序的并行度，可以认为就是其所有算子中最大的并行度。一个程序中，不同的算子可能具有不同的并行度。

   如上图所示，当前数据流中有 Source、map()、keyBy()/window()/apply()、Sink 四个算子，除最后 Sink，其他算子的并行度都为 2。整个程序包含了 7 个子任务，至少需要 2 个分区来并行执行。我们可以说，这段流处理程序的并行度就是 2。

2. 并行度的设置

   在 Flink 中，可以用不同的方法来设置并行度，它们的有效范围和优先级别也是不同的。

   （1）代码中设置

   我们在代码中，可以很简单地在算子后跟着调用 `setParallelism()`方法，来设置当前算子的并行度：

   ```
   stream.map((_,1)).setParallelism(2)复制代码
   ```

   这种方式设置的并行度，只针对当前算子有效。另外，我们也可以直接调用执行环境的 `setParallelism()`方法，全局设定并行度：

   ```
   env.setParallelism(2)复制代码
   ```

   这样代码中所有算子，默认的并行度就都为 2 了。我们一般不会在程序中设置全局并行度，因为如果在程序中对全局并行度进行硬编码，会导致无法动态扩容。这里要注意的是，由于 `keyBy()`方法返回的不是算子，所以无法对` keyBy()`设置并行度。

   （2）提交作业时设置

   在使用` flink run` 命令提交作业时，可以增加`-p` 参数来指定当前应用程序执行的并行度，它的作用类似于执行环境的全局设置：

   ```
   #bin/flink run –p 2 –c com.atstudy.wc.StreamWordCount ./FlinkTutorial-1.0-SNAPSHOT.jar复制代码
   ```

   如果我们直接在` Web UI` 上提交作业，也可以在对应输入框中直接添加并行度。

   （3）配置文件中设置

   我们还可以直接在集群的配置文件` flink-conf.yaml` 中直接更改默认并行度：`parallelism.default: 2`

   这个设置对于整个集群上提交的所有作业有效，初始值为 1。无论在代码中设置、还是提交时的`-p` 参数，都不是必须的。所以，在没有指定并行度的时候，就会采用配置文件中的集群默认并行度

##### 3.3 作业图（`JobGraph`）与执行图（`ExecutionGraph`）

由 Flink 程序直接映射成的数据流图（`dataflow graph`），也被称为逻辑流图（`logical StreamGraph`）。到具体执行环节时，Flink 需要进一步将逻辑流图进行解析，转换为物理执行图。

在这个转换过程中，有几个不同的阶段，会生成不同层级的图，其中最重要的就是作业图（`JobGraph`）和执行图（`ExecutionGraph`）。Flink 中任务调度执行的图，按照生成顺序可以分成四层：

```
逻辑流图（StreamGraph）→ 作业图（JobGraph）→ 执行图（ExecutionGraph）→ 物理图（Physical Graph）
```

![image-20230308112847810](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab2_1/image-20230308112847810.png)

1. 逻辑流图（`StreamGraph`）

   这是根据用户通过` DataStream API` 编写的代码生成的最初的 DAG 图，用来表示程序的拓扑结构。这一步一般在客户端完成。

2. 作业图（`JobGraph`）

   `StreamGraph` 经过优化后生成的就是作业图（`JobGraph`），这是提交给` JobManager` 的数据结构，确定了当前作业中所有任务的划分。主要的优化为: 将多个符合条件的节点链接在一起合并成一个任务节点，形成算子链，这样可以减少数据交换的消耗。`JobGraph `一般也是在客户端生成的，在作业提交时传递给 `JobMaster`。

3. 执行图（`ExecutionGraph`）

   `JobMaster` 收到` JobGraph` 后，会根据它来生成执行图（`ExecutionGraph`）。`ExecutionGraph`是 `JobGraph` 的并行化版本，是调度层最核心的数据结构。

4. 物理图（`Physical Graph`）

   `JobMaster `生成执行图后， 会将它分发给 `TaskManager`；各个` TaskManager` 会根据执行图部署任务，最终的物理执行过程也会形成一张“图”，一般就叫作物理图（`Physical Graph`）。这只是具体执行层面的图，并不是一个具体的数据结构

##### 3.4 任务（Tasks）和任务槽（Task Slots）

1. 任务槽（Task Slots）

   Flink 中每一个 worker(也就是 `TaskManager`)都是一个 JVM 进程，它可以启动多个独立的线程，来并行执行多个子任务（subtask）。为了控制并发量，我们需要在 `TaskManager` 上对每个任务运行所占用的资源做出明确的划分，这就是所谓的任务槽（`task slots`）。每个任务槽（`task slot`）其实表示了 `TaskManager `拥有计算资源的一个固定大小的子集。这些资源就是用来独立执行一个子任务的。

![image-20230308113209964](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab2_1/image-20230308113209964.png)

1. 任务槽数量的设置

   我们可以通过集群的配置文件来设定` TaskManager` 的 slots 数量：

   ```
   taskmanager.numberOfTaskSlots: 8复制代码
   ```

   通过调整 slots 的数量，我们就可以控制子任务之间的隔离级别。

   需要注意的是，slots 目前仅仅用来隔离内存，不会涉及 CPU 的隔离。

2. 任务对任务槽的共享

   

   默认情况下，Flink 允许子任务共享 slots。如图所示，只要属于同一个作业，那么对于不同任务节点的并行子任务，就可以放到同一个 slot 上执行。

   如果希望某个算子对应的任务完全独占一个 slot，或者只有某一部分算子共享 slots，我们也可以通过设置“slot 共享组”（`SlotSharingGroup`）手动指定：

   ```
   .map((_,1)).slotSharingGroup(“1”);复制代码
   ```

   这样，只有属于同一个 slot 共享组的子任务，才会开启 slots 共享；不同组之间的任务是完全隔离的，必须分配到不同的 slots 上。

3. 任务槽和并行度的关系

   slots 和并行度确实都跟程序的并行执行有关，但两者是完全不同的概念。简单来说，task slots 是 静 态 的 概 念 ， 是 指 `TaskManager` 具 有 的 并 发 执 行 能 力 ， 可 以 通 过 参 数`taskmanager.numberOfTaskSlots `进行配置；而并行度（parallelism）是动态概念，也就是`TaskManager` 运行程序时实际使用的并发能力，可以通过参数 `parallelism.default` 进行配置。

![image-20230308113427184](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab2_1/image-20230308113427184.png)

![image-20230308113435924](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab2_1/image-20230308113435924.png)

![image-20230308113444700](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab2_1/image-20230308113444700.png)

![image-20230308113453071](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab2_1/image-20230308113453071.png)

![image-20230308113500944](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab2_1/image-20230308113500944.png)

##### 3.5 总结

我们在之前Flink快速入门的基础上，深入介绍了 Flink 的系统架构和不同组件，并进一步针对不同的部署模式详细讲述了作业提交和任务处理的流程。此外，通过展开讲解架构中的一些重要概念，解答了 Flink 任务调度的核心问题，并对分布式流处理架构的设计做了思考分析。

本章内容不仅是 Flink 架构知识的学习，更是分布式处理思想的入门。我们可以通过 Flink这样一个经典框架的学习，触摸到分布式架构的底层原理。

Flink 流处理架构设计还涉及事件时间、状态管理以及检查点等重要概念，保证分布式流处理系统的低延迟、时间正确性和状态一致性。我们将在后面的实验中对这些内容做详细展开。



# 实验3-1：DataStream API基础（上）

## 实验概述

基于前面实验内容，我们已经对 Flink 的编程方式有了基本的认识。接下来，将详细了解用于 Flink 程序开发的API，其实就是对 DataStream 的各种转换。

Flink 有非常灵活的分层API 设计，其中的核心层就是 DataStream/DataSet API。由于新版本已经实现了流批一体，DataSet API 将被弃用，官方推荐统一使用 DataStream API 处理流数据和批数据，所以首先需要介绍的就是DataStream API，本实验主要是介绍执行环境和源算子的相关内容。

## 实验环境

- AtStudy 实训平台
- Flink1.13

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16792779117206429.png)

## 实验目标

学习完成本实验后，您将能够

- 掌握Flink应用执行环境构建
- 掌握Flink获取各种数据源的渠道
- 掌握Flink和Kafka之间的关联

## 实验任务

### 任务一、执行环境

#### **【任务目标】**

了解流批执行环境的创建。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_1/image-20210824110318276.png?fileid=3270835009250657686)

视频-1、执行环境

##### 1.1 DataStream介绍

DataStream（数据流）本身是 Flink 中一个用来表示数据集合的类（Class），我们编写的 Flink 代码其实就是基于这种数据类型的处理，所以这套核心API 就以DataStream 命名。对于批处理和流处理，我们都可以用这同一套 API 来实现。

DataStream 在用法上有些类似于常规的 Java 集合，但又有所不同。我们在代码中往往并不关心集合中具体的数据，而只是用 API 定义出一连串的操作来处理它们；这就叫作数据流的“转换”（transformations）。

一个 Flink 程序，其实就是对 DataStream 的各种转换。具体来说，代码基本上都由以下几部分构成，如图所示：

![image-20230308133535949](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_1/image-20230308133535949.png)

- 获取执行环境（execution environment）
- 读取数据源（source）
- 定义基于数据的转换操作（transformations）
- 定义计算结果的输出位置（sink）
- 触发程序执行（execute）

其中，获取环境和触发执行，都可以认为是针对执行环境的操作。所以我们就先从执行环境、数据源（source）、转换操作（transformation）、输出（sink）四大部分，对常用的 DataStream API 做基本介绍。

##### 1.2 创建执行环境

执行环境（Execution Environment），Flink 程序可以在各种上下文环境中运行：我们可以在本地 JVM 中执行程序，也可以提交到远程集群上运行。

编写 Flink 程序的第一步 ， 就是创建执行环境 。 我们要获取的执行环境 ， 是`StreamExecutionEnvironment` 类的对象，这是所有 Flink 程序的基础。在代码中创建执行环境的方式，就是调用这个类的静态方法，具体有以下三种。

1. **getExecutionEnvironment**

   最简单的方式，就是直接调用 getExecutionEnvironment 方法。它会根据当前运行的上下文直接得到正确的结果；也就是说，这个方法会根据当前运行的方式，自行决定该返回什么样的运行环境。

   ```
   //此处的 env 是 StreamExecutionEnvironment 对象
   val env = StreamExecutionEnvironment.getExecutionEnvironment复制代码
   ```

   这种“智能”的方式不需要我们额外做判断，用起来简单高效，是最常用的一种创建执行环境的方式。

2. **createLocalEnvironment**

   这个方法返回一个本地执行环境。可以在调用时传入一个参数，指定默认的并行度；如果不传入，则默认并行度就是本地的 CPU 核心数。

   ```
   //此处的 localEnvironment 是 StreamExecutionEnvironment 对象
   val localEnvironment = StreamExecutionEnvironment.createLocalEnvironment()复制代码
   ```

3. **createRemoteEnvironment**

   这个方法返回集群执行环境。需要在调用时指定 JobManager 的主机名和端口号，并指定要在集群中运行的 Jar 包。

   ```
   //此处的 remoteEnv 是 StreamExecutionEnvironment 对象
   val remoteEnv = StreamExecutionEnvironment
   .createRemoteEnvironment("host", // JobManager 主机名
                            1234, // JobManager 进程端口号
                            "path/to/jarFile.jar" // 提交给 JobManager 的 JAR 包
                           )复制代码
   ```

   在获取到程序执行环境后，我们还可以对执行环境进行灵活的设置。

##### 1.3 执行模式(Execution Mode)

在之前的 Flink 版本中，批处理的执行环境与流处理类似，是调用类 ExecutionEnvironment的静态方法，并返回它的对象：

```
// 批处理环境
val batchEnv = ExecutionEnvironment.getExecutionEnvironment

// 流处理环境
val env = StreamExecutionEnvironment.getExecutionEnvironment复制代码
```

基于 ExecutionEnvironment 读入数据创建的数据集合，就是 DataSet；对应的调用的一整套转换方法，就是 DataSet API。这些我们在第二章的批处理 WordCount 程序中已经有了基本了解。

而从 1.12.0 版本起，Flink 实现了 API 上的流批统一。DataStream API 新增了一个重要特性：可以支持不同的“执行模式”（execution mode），通过简单的设置就可以让一段 Flink 程序在流处理和批处理之间切换。这样一来，DataSet API 也就没有存在的必要了。

- 流执行模式（STREAMING）

  这是 DataStream API 最经典的模式，一般用于需要持续实时处理的无界数据流。默认情况下，程序使用的就是 STREAMING 执行模式。

- 批执行模式（BATCH）

  专门用于批处理的执行模式, 这种模式下，Flink 处理作业的方式类似于 MapReduce 框架。对于不会持续计算的有界数据，我们用这种模式处理会更方便。

- 自动模式（AUTOMATIC）

  在这种模式下，将由程序根据输入数据源是否有界，来自动选择执行模式。

由于 Flink 程序默认是 STREAMING 模式，我们这里重点介绍一下 BATCH 模式的配置。

主要有两种方式：

- 通过命令行配置

  ```
  bin/flink run -Dexecution.runtime-mode=BATCH ...复制代码
  ```

  在提交作业时，增加 execution.runtime-mode 参数，指定值为 BATCH。

- 通过代码配置

  ```
  val env = StreamExecutionEnvironment.getExecutionEnvironment
  env.setRuntimeMode(RuntimeExecutionMode.BATCH)复制代码
  ```

在代码中，直接基于执行环境调用 setRuntimeMode 方法，传入 BATCH 模式。

有了执行环境，我们就可以构建程序的处理流程了：基于环境读取数据源，进而进行各种转换操作，最后输出结果到外部系统。

需要注意的是，写完输出（sink）操作并不代表程序已经结束。因为当 main()方法被调用时，其实只是定义了作业的每个执行操作，然后添加到数据流图中；这时并没有真正处理数据——因为数据可能还没来。Flink 是由事件驱动的，只有等到数据到来，才会触发真正的计算， 这也被称为“延迟执行”或“懒执行”（lazy execution）。

所以我们需要显式地调用执行环境的 execute() 方法，来触发程序执行。execute()方法将一直等待作业完成，然后返回一个执行结果（JobExecutionResult）。

```
env.execute()复制代码
```

### 任务二、源算子

#### **【任务目标】**

掌握各种数据源的获取过程。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_1/image-20210824110318276.png?fileid=3270835009251472989)

视频-2、源算子

##### 2.1 源算子

![image-20230308145529306](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_1/image-20230308145529306.png)

创建环境之后，就可以构建数据处理的业务逻辑了，想要处理数据，先得有数据，所以首要任务就是把数据读进来。

Flink 可以从各种来源获取数据，然后构建 DataStream 进行转换处理。一般将数据的输入来源称为数据源，而读取数据的算子就是源算子（Source）。所以，Source 就是我们整个处理程序的输入端。

Flink 代码中通用的添加 Source 的方式，是调用执行环境的 addSource()方法：

```
//通过调用 addSource()方法可以获取 DataStream 对象
DataStream<String> stream = env.addSource(...);复制代码
```

方法传入一个对象参数，需要实现 SourceFunction 接口，返回一个 DataStream。

为了更好地理解，我们先构建一个实际应用场景。比如网站的访问操作，可以抽象成一个三元组（用户名，用户访问的 url，用户访问 url 的时间戳），所以在这里，我们可以创建一个类 Event，将用户行为包装成它的一个对象。Event 包含了以下一些字段：

| **字段名** | **数据类型** | **说明**              |
| ---------- | ------------ | --------------------- |
| user       | String       | 用户名                |
| url        | String       | 用户访问的 url        |
| timestamp  | Long         | 用户访问 url 的时间戳 |

```
package com.atstudy.chapter03

case class Event(user: String, url: String, timestamp: Long)复制代码
```

##### 2.2 从集合中读取数据

最简单的读取数据的方式，就是在代码中直接创建一个集合，然后调用执行环境的fromCollection 方法进行读取。这相当于将数据临时存储到内存中，形成特殊的数据结构后，作为数据源使用，一般用于测试。

```
package com.atguigu.chapter03

// 导入必要的包以及隐式类型转换
import org.apache.flink.streaming.api.scala._

object SourceCollection {
    def main(args: Array[String]): Unit = {
        // 获取流执行环境
        val env = StreamExecutionEnvironment.getExecutionEnvironment
        // 设置并行任务的数量为 1
        env.setParallelism(1)
        // 创建包含点击事件的列表
        val clicks = List(
            Event("Mary", "./home", 1000L),
            Event("Bob", "./cart",2000L)
        )
        // 将列表作为流输入
        val stream = env.fromCollection(clicks)
        stream.print()
        env.execute()
    }
}复制代码
```

我们也可以不构建集合，直接将元素列举出来，调用 fromElements 方法进行读取数据：

```
val stream2 = env.fromElements(
    Event("Mary", "./home", 1000L), 
    Event("Bob", "./cart", 2000L)
)复制代码
```

##### 2.3 从文件读取数据

真正的实际应用中，自然不会直接将数据写在代码中。通常情况下，我们会从存储介质中获取数据，一个比较常见的方式就是读取日志文件。这也是批处理中最常见的读取方式。

```
val stream = env.readTextFile("clicks.csv")复制代码
```

说明:

- 参数可以是目录，也可以是文件；

- 路径可以是相对路径，也可以是绝对路径；

- 相对路径是从系统属性 `user.dir` 获取路径:在 IDEA 下是 project 的根目录, standalone模式下是集群节点根目录；

- 也可以从 HDFS 目录下读取, 使用路径 `hdfs://...`, 由于 Flink 没有提供 hadoop 相关依赖, 需要 pom 中添加相关依赖:

  ```
  <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-client</artifactId>
      <version>2.7.5</version>
      <scope>provided</scope>
  </dependency>复制代码
  ```

##### 2.4 从 Socket 读取数据

不论从集合还是文件，我们读取的其实都是有界数据。在流处理的场景中，数据往往是无界的。一个简单的例子，就是我们之前用到的读取 socket 文本流。这种方式由于吞吐量小、稳定性较差，一般也是用于测试。

```
val stream = env.socketTextStream("localhost", 7777)复制代码
```

##### 2.5 从 Kafka 读取数据

Kafka 作为分布式消息传输队列，是一个高吞吐、易于扩展的消息系统。而消息队列的传输方式，恰恰和流处理是完全一致的。所以可以说 Kafka 和 Flink 天生一对，是当前处理流式数据的双子星。在如今的实时流处理应用中，由 Kafka 进行数据的收集和传输，Flink 进行分析计算，这样的架构已经成为众多企业的首选，如图所示：

![image-20230308152716351](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_1/image-20230308152716351.png)

Flink 官方提供了连接工具` flink-connector-kafka`，直接帮我们实现了一个消费者`FlinkKafkaConsumer`，它就是用来读取 Kafka 数据的 `SourceFunction`。

所以想要以 Kafka 作为数据源获取数据，我们只需要引入 Kafka 连接器的依赖。Flink 官方提供的是一个通用的 Kafka 连接器，它会自动跟踪最新版本的 Kafka 客户端。目前最新版本只支持` 0.10.0` 版本以上的 Kafka。

```
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-kafka_${scala.binary.version}</artifactId>
    <version>${flink.version}</version>
</dependency>复制代码
```

然后调用 `env.addSource()`，传入` FlinkKafkaConsumer` 的对象实例就可以了

```
package com.atstudy.chapter03

import org.apache.flink.api.common.serialization.SimpleStringSchema
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer
import java.util.Properties

object SourceKafkaTest {
    def main(args: Array[String]): Unit = {
        val env = StreamExecutionEnvironment.getExecutionEnvironment
        env.setParallelism(1)
        //使用 Java 配置类保存与 Kafka 连接的必要配置
        val properties = new Properties();
        properties.setProperty("bootstrap.servers", "localhost:9092")
        properties.setProperty("group.id", "consumer-group")
 properties.setProperty("key.deserializer","org.apache.kafka.common.serialization.StringDeserializer") properties.setProperty("value.deserializer","org.apache.kafka.common.serialization.StringDeserializer")
        properties.setProperty("auto.offset.reset", "latest")
        //创建一个 FlinkKafkaConsumer 对象，传入必要参数，从 Kafka 中读取数据
        val stream = env
        .addSource(new FlinkKafkaConsumer[String](
            "clicks",
            new SimpleStringSchema(),
            properties
        ))
        stream.print("kafka")
        env.execute()
    }
}复制代码
```

创建 FlinkKafkaConsumer 时需要传入三个参数：

- 第一个参数 topic，定义了从哪些主题中读取数据。可以是一个 topic，也可以是 topic列表，还可以是匹配所有想要读取的 topic 的正则表达式。当从多个 topic 中读取数据时，Kafka 连接器将会处理所有 topic 的分区，将这些分区的数据放到一条数据流中去。
- 第二个参数是一个 `DeserializationSchema` 或者 `KeyedDeserializationSchema`。Kafka 消息被存储为原始的字节数据，所以需要反序列化成 Java 或者 Scala 对象。上面代码中使用的 `SimpleStringSchema`，是一个内置的 `DeserializationSchema`，它只是将字节数组简单地反序列化成字符串。`DeserializationSchema` 和 `KeyedDeserializationSchema` 是公共接口，所以我们也可以自定义反序列化逻辑。
- 第三个参数是一个 `Properties` 对象，设置了 Kafka 客户端的一些属性。

接下来启动`zookeeper`和`kafka`服务。

![image-20230308160322409](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_1/image-20230308160322409.png)

创建kafka的生产者发送数据：

```
# ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic clicks复制代码
```

![image-20230308160726957](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_1/image-20230308160726957.png)

我可以看到，在Flink应用中已经接收到了Kafka发送的数据

![image-20230308160903527](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_1/image-20230308160903527.png)

##### 2.6 自定义源

接下来我们创建一个自定义的数据源，实现 SourceFunction 接口。主要重写两个关键方法：run()和 cancel()。

- run()方法：使用运行时上下文对象（SourceContext）向下游发送数据；
- cancel()方法：通过标识位控制退出循环，来达到中断数据源的效果。

```
package com.atstudy.chapter03

import org.apache.flink.streaming.api.functions.source.SourceFunction
import org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext
import java.util.Calendar
import scala.util.Random

// 实现 SourceFunction 接口，接口中的泛型是自定义数据源中的类型
class ClickSource extends SourceFunction[Event] {
    // 标志位，用来控制循环的退出
    var running = true

    //重写 run 方法，使用上下文对象 sourceContext 调用 collect 方法
    override def run(ctx: SourceContext[Event]): Unit = {
        // 实例化一个随机数发生器
        val random = new Random
        // 供随机选择的用户名的数组
        val users = Array("Mary", "Bob", "Alice", "Cary")
        // 供随机选择的 url 的数组
        val urls = Array("./home", "./cart", "./fav", "./prod?id=1", "./prod?id=2")
        //通过 while 循环发送数据，running 默认为 true，所以会一直发送数据
        while (running) {
            // 调用 collect 方法向下游发送数据
            ctx.collect(
                Event(
                    users(random.nextInt(users.length)), // 随机选择一个用户名
                    urls(random.nextInt(urls.length)), // 随机选择一个 url
                    Calendar.getInstance.getTimeInMillis // 当前时间戳
                )
            )
            // 隔 1 秒生成一个点击事件，方便观测
            Thread.sleep(1000)
        }
    }
    //通过将 running 置为 false 终止数据发送循环
    override def cancel(): Unit = running = false
}复制代码
```

这个数据源，我们后面会频繁使用，之后的代码若涉及 ClickSource()数据源，使用上面的代码就可以了。

下面的代码我们来读取一下自定义的数据源。有了自定义的 Source，接下来只要调用addSource()就可以了：

```
env.addSource(new ClickSource)复制代码
```

下面是完整的代码：

```
package com.atstudy.chapter03
// 导入隐式类型转换，必须导入！
import org.apache.flink.streaming.api.scala._

object SourceCustom {
    def main(args: Array[String]): Unit = {
        val env = StreamExecutionEnvironment.getExecutionEnvironment
        env.setParallelism(1)
        // 使用自定义的数据源
        val stream = env.addSource(new ClickSource)
        stream.print()
        env.execute()
    }
}复制代码
```





# 实验3-2：DataStream API基础（下）

## 实验概述

基于前面的实验继续介绍DataStream API相关内容，这种重点是介绍转换和输出的相关操作！

## 实验环境

- AtStudy 实训平台
- Flink1.13
- Kafka
- MySQL

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16792779731718625.png)

## 实验目标

学习完成本实验后，您将能够

- 掌握转换过程中各种算子的使用
- 掌握Flink向Kafka的输出方式
- 掌握Flink向MySQL的输出方式

## 实验任务

### 任务一、转换算子

#### **【任务目标】**

数据源读入数据之后，就可以使用各种转换算子，将一个或多个 DataStream 转换为新的 DataStream，而一个 Flink 程序的核心，其实就是所有的转换操作，它们决定了处理的业务逻辑，本任务的主要目标就是掌握各种转换算子的使用。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_2/image-20210824110318276.png?fileid=3270835009251728950)

视频-1、转换算子

##### 1.1 基本转换算子

数据源获取数据之后，接下来就是进行各种业务处理，这就是转换操作的重点了：

![image-20230308163033928](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_2/image-20230308163033928.png)

首先可以进行基本的转换过程，主要可以有：

- 映射转换(`Map`)
- 过滤转换(`Filter`)
- 扁平映射(`flatMap`)

那么接下来，我们进行一一介绍：

1. 映射（`map`）

   map()是大家非常熟悉的大数据操作算子，主要用于将数据流中的数据进行转换，形成新的数据流。简单来说，就是一个“一一映射”，消费一个元素就产出一个元素，如图所示:

   

   我们只需要基于DataStrema调用map()方法就可以进行转换处理。方法需要传入的参数是接口MapFunction 的实现；返回值类型还是 DataStream，不过泛型（流中的元素类型）可能改变。

   下面的代码用不同的方式，实现了提取 Event 中的 user 字段的功能

   ```
   package com.atstudy.chapter03
   
   import org.apache.flink.api.common.functions.MapFunction
   import org.apache.flink.streaming.api.scala._
   
   object TransMap {
       def main(args: Array[String]): Unit = {
           val env = StreamExecutionEnvironment.getExecutionEnvironment
           env.setParallelism(1)
           val stream = env.fromElements(
               Event("Mary", "./home", 1000L),
               Event("Bob", "./cart", 2000L)
           )
           //1.使用匿名函数的方式提取 user 字段
           stream.map(_.user).print()
           //2.使用调用外部类的方式提取 user 字段
           stream.map(new UserExtractor).print()
           env.execute()
       }
       // 显式的实现 MapFunction 接口
       class UserExtractor extends MapFunction[Event, String] {
           override def map(value: Event): String = value.user
       }
   }复制代码
   ```

   上面代码中，MapFunction 实现类的泛型类型，与输入数据类型和输出数据的类型有关。在实现 MapFunction 接口的时候，需要指定两个泛型，分别是输入事件和输出事件的类型，还需要重写一个 map()方法，定义从一个输入事件转换为另一个输出事件的具体逻辑。

   查看源码可以发现，基于 DataStream[T]调用 map()方法，得到的是一个新的 DataStream[R]，泛型由 T 转变为 R，这也正对应着 MapFunction 的输入输出类型。

2. 过滤（filter)

   filter()转换操作，顾名思义是对数据流执行一个过滤，通过一个布尔条件表达式设置过滤条件，对于每一个流内元素进行判断，若为 true 则元素正常输出，若为 false 则元素被过滤掉，如图所示：

   

   进行 filter()转换之后的新数据流的数据类型与原数据流是相同的。filter()转换需要传入的参数需要实现 FilterFunction 接口，而 FilterFunction 内要实现 filter()方法，就相当于一个返回布尔类型的条件表达式。

   下面的代码会将数据流中用户 Mary 的浏览行为过滤出来。

   ```
   package com.atstudy.chapter03
   
   import org.apache.flink.api.common.functions.FilterFunction
   import org.apache.flink.streaming.api.scala._
   
   object TransFilter {
       def main(args: Array[String]): Unit = {
           val env = StreamExecutionEnvironment.getExecutionEnvironment
           env.setParallelism(1)
           val stream = env
           .fromElements(
               Event("Mary", "./home", 1000L),
               Event("Bob", "./cart", 2000L)
           )
           //过滤出用户名是 Mary 的数据
           stream.filter(_.user.equals("Mary")).print()
           stream.filter(new UserFilter).print()
           env.execute()
       }
       class UserFilter extends FilterFunction[Event] {
           override def filter(value: Event): Boolean = value.user.equals("Mary")
       }
   }复制代码
   ```

3. 扁平映射（flatMap)

   flatMap()操作又称为扁平映射，主要是将数据流中的整体（一般是集合类型）拆分成一个一个的个体使用。消费一个元素，可以产生 0 到多个元素。flatMap()可以认为是“扁平化”（flatten）和“映射”（map）两步操作的结合，也就是先按照某种规则对数据进行打散拆分，再对拆分后的元素做转换处理，如图所示。我们此前 WordCount 程序的第一步分词操作，就用到了 flatMap()。

   

   同 map()一样，flatMap()也可以使用 Lambda 表达式或者 FlatMapFunction 接口实现类的方式来进行传参，返回值类型取决于所传参数的具体逻辑，可以与原数据流的数据类型相同，也可以不同。

   flatMap()的使用非常灵活，可以对结果进行任意输出，下面就是一个例子：

   ```
   package com.atstudy.chapter03
   
   import org.apache.flink.api.common.functions.FlatMapFunction
   import org.apache.flink.streaming.api.scala._
   import org.apache.flink.util.Collector
   
   object TransFlatmapTest {
       def main(args: Array[String]): Unit = {
           val env = StreamExecutionEnvironment.getExecutionEnvironment
           env.setParallelism(1)
           val stream = env.fromElements(
               Event("Mary", "./home", 1000L),
               Event("Bob", "./cart", 2000L)
           )
           stream.flatMap(new MyFlatMap).print()
           env.execute()
       }
       
       //自定义 FlatMapFunction
       class MyFlatMap extends FlatMapFunction[Event, String] {
           override def flatMap(value:Event, out: Collector[String]): Unit = {
               //如果是 Mary 的点击事件，则向下游发送 1 次，如果是 Bob 的点击事件，则向下游发送 2 次
               if (value.user.equals("Mary")) {
                   out.collect(value.user)
               } else if (value.user.equals("Bob")) {
                   out.collect(value.user)
                   out.collect(value.user)
               }
           }
       }
   }复制代码
   ```

   运行结果如下所示：

   ```
   Mary
   Bob
   Bob复制代码
   ```

##### 1.2 聚合算子（Aggregation）

1. 按键分区（`keyBy`）

   对于 Flink 而言，DataStream 是没有直接进行聚合的 API 的。因为我们对海量数据做聚合肯定要进行分区并行处理，这样才能提高效率。所以在 Flink 中，要做聚合，需要先进行分区；这个操作就是通过 keyBy()来完成的。

   keyBy()是聚合前必须要用到的一个算子。keyBy()通过指定键（key），可以将一条流从逻辑上划分成不同的分区（partitions）。这里所说的分区，其实就是并行处理的子任务，也就对应着任务槽（task slots）。基于不同的 key，流中的数据将被分配到不同的分区中去，如图所示；这样一来，所有具有相同的 key 的数据，都将被发往同一个分区，那么下一步算子操作就将会在同一个 slot中进行处理了。

   

   在内部，是通过计算 key 的哈希值（hash code），对分区数进行取模运算来实现的。所以这里 key 如果是 POJO 类的话，必须要重写 hashCode()方法。

   keyBy() 方法需要传入一个参数，这个参数指定了一个或一组 key。有很多不同的方法来指定 key：比如对于 Tuple 数据类型，可以指定字段的位置或者多个位置的组合；对于 POJO 类型或 Scala 的样例类，可以指定字段的名称（String）；另外，还可以传入 Lambda 表达式或者实现一个键选择器（KeySelector），用于说明从数据中提取 key 的逻辑。

   我们可以以 id 作为 key 做一个分区操作，代码实现如下：

   ```
   package com.atstudy.chapter03
   import org.apache.flink.streaming.api.scala._
   
   object TransKeyBy {
       def main(args: Array[String]): Unit = {
           val env = StreamExecutionEnvironment.getExecutionEnvironment
           env.setParallelism(1)
           val stream = env
           .fromElements(
               Event("Mary", "./home", 1000L),
               Event("Bob", "./cart", 2000L)
           )
           //指定 Event 的 user 属性作为 key
           val keyedStream = stream.keyBy(_.user)
           keyedStream.print()
           env.execute()
       }
   }复制代码
   ```

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_2/image-20210823165604377.png)**注意！**

> Flink 的keyBy本质上并不是将相同key的元素集合到一个集合元素里面，而是将相同key的元素散列到一个子任务中，而并不改变原来的元素数据结构。

1. 简单聚合

   有了按键分区的数据流 KeyedStream，我们就可以基于它进行聚合操作了。Flink 为我们内置实现了一些最基本、最简单的聚合 API，主要有以下几种：

   - sum()：在输入流上，对指定的字段做叠加求和的操作。
   - min()：在输入流上，对指定的字段求最小值。
   - max()：在输入流上，对指定的字段求最大值。
   - minBy()：与 min() 类似，在输入流上针对指定字段求最小值。不同的是，min()只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而 minBy()则会返回包含字段最小值的整条数据。
   - maxBy()：与 max()类似，在输入流上针对指定字段求最大值。两者区别与min()/minBy()完全一致。

   简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称。

   对于元组类型的数据，同样也可以使用这两种方式来指定字段。需要注意的是，元组中字段的名称，是以`_1、_2、_3、…`来命名的。

   例如，下面就是对元组数据流进行聚合的测试:

   ```
   package com.atstudy.chapter03
   import org.apache.flink.streaming.api.scala._
   
   object TransTupleAggregation {
       def main(args: Array[String]): Unit = {
           val env = StreamExecutionEnvironment.getExecutionEnvironment
           env.setParallelism(1)
           val stream = env.fromElements(
               ("a", 1), ("a", 3), ("b", 3), ("b", 4)
           )
           stream.keyBy(_._1).sum(1).print() //对元组的索引 1 位置数据求和
           stream.keyBy(_._1).sum("_2").print() //对元组的第 2 个位置数据求和
           stream.keyBy(_._1).max(1).print() //对元组的索引 1 位置求最大值
           stream.keyBy(_._1).max("_2").print() //对元组的第 2 个位置数据求最大值
           stream.keyBy(_._1).min(1).print() //对元组的索引 1 位置求最小值
           stream.keyBy(_._1).min("_2").print() //对元组的第 2 个位置数据求最小值
           stream.keyBy(_._1).maxBy(1).print() //对元组的索引 1 位置求最大值
           stream.keyBy(_._1).maxBy("_2").print() //对元组的第 2 个位置数据求最大值
           stream.keyBy(_._1).minBy(1).print() //对元组的索引 1 位置求最小值
           stream.keyBy(_._1).minBy("_2").print() //对元组的第 2 个位置数据求最小值
           env.execute()
       }
   }复制代码
   ```

   而如果数据流的类型是样例类，那么就只能通过字段名称来指定，不能通过位置来指定了。

   ```
   package com.atstudy.chapter05
   import org.apache.flink.streaming.api.scala._
   
   object TransAggregationCaseClass {
       def main(args: Array[String]): Unit = {
           val env = StreamExecutionEnvironment.getExecutionEnvironment
           env.setParallelism(1)
           val stream = env.fromElements(
               Event("Mary", "./home", 1000L),
               Event("Bob", "./cart", 2000L)
           )
           // 使用 user 作为分组的字段，并计算最大的时间戳
           stream.keyBy(_.user).max("timestamp").print()
           env.execute()
       }
   }复制代码
   ```

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_2/image-20210823170539971.png)**重点提示**

> - max：取指定字段的当前的最大值，如果有多个字段，其他非比较字段，以第一条为准。
> - maxBy：取指定字段的当前的最大值，如果有多个字段，其他字段以最大值那条数据为准。

1. 归约聚合（reduce）

   与简单聚合类似，reduce()操作也会将 KeyedStream 转换为 DataStream。它不会改变流的元素数据类型，所以输出类型和输入类型是一样的。

   调用 KeyedStream 的 reduce()方法时，需要传入一个参数，实现 ReduceFunction 接口。接口在源码中的定义如下：

   ```
   public interface ReduceFunction<T> extends Function, Serializable {
       T reduce(T value1, T value2) throws Exception;
   }复制代码
   ```

   ReduceFunction 接口里需要实现 reduce()方法，这个方法接收两个输入事件，经过转换处理之后输出一个相同类型的事件；所以，对于一组数据，我们可以先取两个进行合并，然后再将合并的结果看作一个数据、再跟后面的数据合并，最终会将它“简化”成唯一的一个数据，这也就是 reduce“归约”的含义。在流处理的底层实现过程中，实际上是将中间“合并的结果”作为任务的一个状态保存起来的；之后每来一个新的数据，就和之前的聚合状态进一步做归约。

   下面我们来看一个稍复杂的例子。

   我们将数据流按照用户 id 进行分区，然后用一个 reduce()算子实现 sum()的功能，统计每个用户访问的频次；进而将所有统计结果分到一组，用另一个 reduce()算子实现 maxBy()的功能，记录所有用户中访问频次最高的那个，也就是当前访问量最大的用户是谁。

   ```
   package com.atstudy.chapter03
   import org.apache.flink.streaming.api.scala._
   
   object TransReduce {
       def main(args: Array[String]): Unit = {
           val env = StreamExecutionEnvironment.getExecutionEnvironment
           env.setParallelism(1)
           val stream:DataStream[Event] = env.fromElements(
               Event("Mary", "./home", 1000L),
               Event("Bob", "./cart", 2000L),
               Event("Alice", "./index", 3000L),
               Event("Mary", "./prod?id=2", 4000L),
               Event("Mary", "./prod?id=3", 5000L),
               Event("Mary", "./prod?id=4", 6000L),
           )
           
         stream.map(r => (r.user, 1L))//按照用户名进行分组
               .keyBy(_._1)
               //计算每个用户的访问频次
               .reduce((r1, r2) => (r1._1, r1._2 + r2._2))
               //将所有数据都分到同一个分区
               .keyBy(_ => true)
               //通过 reduce 实现 max 功能，计算访问频次最高的用户
               .reduce((r1, r2) => if (r1._2 > r2._2) r1 else r2)
               .print()
           env.execute()
       }
   }复制代码
   ```

   reduce()同简单聚合算子一样，也要针对每一个 key 保存状态。因为状态不会清空，所以我们需要将 reduce()算子作用在一个有限 key 的流上。

##### 1.3 自定义函数

Flink 的 DataStream API 编程风格其实是一致的：基本上都是基于 DataStream 调用一个方法，表示要做一个转换操作；方法需要传入一个参数，这个参数都是需要实现一个接口。

这些接口有一个共同特点：全部都以`算子操作名称 + Function` 命名，例如源算子需要实现 `SourceFunction 接口`，map 算子需要实现 `MapFunction` 接口，reduce()算子需要实现`ReduceFunction` 接口。我们不仅可以通过自定义函数类或者匿名类来实现接口，也可以直接传入 Lambda 表达式。这就是所谓的用户自定义函数（`user-defined function，UDF`）。接下来我们就对这几种编程方式做一个梳理总结。

1. 函数类（`Function Classes`）

   对于大部分操作而言，都需要传入一个用户自定义函数（UDF），实现相关操作的接口，来完成处理逻辑的定义。Flink 暴露了所有 UDF 函数的接口，具体实现方式为接口或者抽象类，例如 `MapFunction、FilterFunction、ReduceFunction` 等。所以最简单直接的方式，就是自定义一个函数类，实现对应的接口。下面例子实现了 `FilterFunction` 接口，用来筛选 url 中包含“home”的内容：

   ```
   package com.atstudy.chapter03
   
   import org.apache.flink.api.common.functions.FilterFunction
   import org.apache.flink.streaming.api.scala._
   
   object TransFunctionUDFTest {
       def main(args: Array[String]): Unit = {
           val env = StreamExecutionEnvironment.getExecutionEnvironment
           env.setParallelism(1)
           val clicks = env
           .fromElements(
               Event("Mary", "./home", 1000L),
               Event("Bob", "./cart", 2000L)
           )
           //通过传入自定义 FilterFunction 实现过滤
           val stream1 = clicks.filter(new FlinkFilter)
           stream1.print()
           env.execute()
       }
       
       //自定义 FilterFunction 函数类
       class FlinkFilter extends FilterFunction[Event] {
           override def filter(value: Event): Boolean = value.url.contains("home")
       }
   }复制代码
   ```

   当然还可以通过匿名类来实现 `FilterFunction` 接口：

   ```
   val filterdStream = stream.filter(new FilterFunction[Event] {
       override def filter(value: Event): Boolean = value.url.contains("home")
   })复制代码
   ```

   为了类可以更加通用，我们还可以将用于过滤的关键字“home”抽象出来作为类的属性，调用构造方法时传进去

   ```
   stream.filter(new KeywordFilter("home")).print()
   
   //自定义 FilterFunction 函数类，将需要用到的过滤参数作为类的构造参数传入
   class KeywordFilter(keyword: String) extends FilterFunction[Event] {
       override def filter(value: Event): Boolean = value.url.contains(keyword)
   }复制代码
   ```

   对于 Scala 这样的函数式编程语言，更为简单的写法是直接传入一个 Lambda 表达式：

   ```
   stream.filter(_.url.contains("home")).print()复制代码
   ```

   这样我们用一行代码就可以搞定，显得更加简洁明晰。

2. 富函数类（`Rich Function Classes`）

   “富函数类”也是 DataStream API 提供的一个函数类的接口，所有的 Flink 函数类都有其Rich 版本。富函数类一般是以抽象类的形式出现的。例如：`RichMapFunction、RichFilterFunction、RichReduceFunction` 等。

   与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。典型的生命周期方法有：

   - open()方法，是 `Rich Function` 的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如 map()或者 filter()方法被调用之前，open()会首先被调用。所以像文件 IO 流的创建，数据库连接的创建，配置文件的读取等等这样一次性的工作，都适合在 open()方法中完成。

   - close()方法，是生命周期中的最后一个调用的方法，类似于解构方法。一般用来做一些清理工作。需要注意的是，这里的生命周期方法，对于一个并行子任务来说只会调用一次；而对应的，实际工作方法，例如 `RichMapFunction` 中的 map()，在每条数据到来后都会触发一次调用。

     来看一个例子：

     ```
     package com.atstudy.chapter03
     
     import org.apache.flink.api.common.functions.RichMapFunction
     import org.apache.flink.configuration.Configuration
     import org.apache.flink.streaming.api.scala._
     
     object RichFunctionTest{
         def main(args: Array[String]): Unit = {
             val env = StreamExecutionEnvironment.getExecutionEnvironment
             env.setParallelism(2)
             env.fromElements(
                 Event("Mary", "./home", 1000L),
                 Event("Bob", "./cart", 2000L),
                 Event("Alice", "./prod?id=1", 5 * 1000L),
                 Event("Cary", "./home", 60 * 1000L)
             )
             .map(new RichMapFunction[Event, Long]() {
                 //在任务生命周期开始时会执行 open 方法，在控制台打印对应语句
                 override def open(parameters: Configuration): Unit = {
                     println("索引为 " + getRuntimeContext.getIndexOfThisSubtask + " 的任务开始")
                 }
                 // 将点击事件转换成长整型的时间戳输出
                 override def map(value: Event): Long = value.timestamp
                 //在任务声明周期结束时会执行 close 方法，在控制台打印对应语句
                 override def close(): Unit = {
                     println("索引为 " + getRuntimeContext.getIndexOfThisSubtask + " 的任务结束")
                 }
             }).print()
             env.execute()
         }
     }复制代码
     ```

     输出结果是：

     ```
     索引为 0 的任务开始
     索引为 1 的任务开始
     1> 1000
     2> 2000
     2> 60000
     1> 5000
     索引为 0 的任务结束
     索引为 1 的任务结束复制代码
     ```

     一个常见的应用场景就是，如果我们希望连接到一个外部数据库进行读写操作，推荐的最佳实践如下：

     ```
     class MyFlatMap extends RichFlatMapFunction[IN,OUT]{
     
         override def open(parameters: Configuration): Unit = {
             // 做一些初始化工作
             // 例如建立一个和 MySQL 的连接
         }
         override def flatMap(value: IN, out: Collector[OUT]): Unit = {
             // 对数据库进行读写
         }
         override def close(): Unit = {
             // 清理工作，关闭和 MySQL 数据库的连接。
     
         }
     }复制代码
     ```

     另外，富函数类提供了 getRuntimeContext()方法（我们在本节的第一个例子中使用了一下），可以获取到运行时上下文的一些信息，例如程序执行的并行度，任务名称，以及状态（state）。这使得我们可以大大扩展程序的功能，特别是对于状态的操作，使得 Flink 中的算子具备了处理复杂业务的能力。

### 任务二、输出算子

#### **【任务目标】**

业务逻辑处理完成后接下来就是输出结果，本任务的重点就是介绍各种输出算子的使用。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_2/image-20210824110318276.png?fileid=3270835009250949786)

视频-2、输出算子

##### 2.1 输出算子

Flink 的 DataStream API 专门提供了向外部写入数据的方法：addSink。与 addSource 类似，addSink 方法对应着一个“Sink”算子，主要就是用来实现与外部系统连接、并将数据提交写入的；Flink 程序中所有对外的输出操作，一般都是利用 Sink 算子完成的。与 Source 算子非常类似，除去一些 Flink 预实现的 Sink，一般情况下 Sink 算子的创建是通过调用 DataStream 的 addSink()方法实现的。

```
stream.addSink(new SinkFunction(…))复制代码
```

addSource 的参数需要实现一个 SourceFunction 接口；类似地，addSink 方法同样需要传入一个参数，实现的是 SinkFunction 接口。在这个接口中只需要重写一个方法 invoke()，用来将指定的值写入到外部系统中。这个方法在每条数据记录到来时都会调用：

```
default void invoke(IN value, Context context) throws Exception复制代码
```

当然，SinkFuntion 多数情况下同样并不需要我们自己实现。Flink 官方提供了一部分的框架的 Sink 连接器。如图所示，列出了 Flink 官方目前支持的第三方系统连接器：

![image-20230309114527751](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_2/image-20230309114527751.png)

我们可以看到，像 Kafka 之类流式系统，Flink 提供了完美对接，Source/Sink 两端都能连接，可读可写；而对于 Elasticsearch、文件系统（FileSystem）、JDBC 等数据存储系统，则只提供了输出写入的 Sink 连接器。

##### 2.2 输出到文件

最简单的输出方式，当然就是写入文件了。对应着读取文件作为输入数据源，Flink 本来也有一些非常简单粗暴的输出到文件的预实现方法：如 writeAsText()、writeAsCsv()，可以直接将输出结果保存到文本文件或 Csv 文件。目前这些简单的方法已经要被弃用。

Flink 为此专门提供了一个流式文件系统的连接器：`StreamingFileSink`，它继承自抽象类`RichSinkFunction`，而且集成了 Flink 的检查点（checkpoint）机制，用来保证精确一次（exactly once）的一致性语义。

`StreamingFileSink` 为批处理和流处理提供了一个统一的 Sink，它可以将分区文件写入 Flink支持的文件系统。它可以保证精确一次的状态一致性，大大改进了之前流式文件输出的方式。它的主要操作是将数据写入（buckets），每个桶中的数据都可以分割成一个个大小有限的分区文件。

`StreamingFileSink` 支持行编码（Row-encoded）和批量编码（Bulk-encoded，比如 Parquet）格式。这两种不同的方式都有各自的构建器（builder），调用方法也非常简单，可以直接调用`StreamingFileSink` 的静态方法：

- 行编码：`StreamingFileSink.forRowFormat(basePath,rowEncoder)。`
- 批量编码：`StreamingFileSink.forBulkFormat(basePath,bulkWriterFactory)。`

在创建行或批量编码 Sink 时，我们需要传入两个参数，用来指定存储桶的基本路径（basePath）和数据的编码逻辑（rowEncoder 或 bulkWriterFactory）。

下面我们就以行编码为例，将一些测试数据直接写入文件：

```
package com.atstudy.chapter03

import org.apache.flink.api.common.serialization.SimpleStringEncoder
import org.apache.flink.core.fs.Path
import org.apache.flink.streaming.api.functions.sink.filesystem.StreamingFileSink
import org.apache.flink.streaming.api.functions.sink.filesystem.rollingpolicies.DefaultRollingPolicy
import org.apache.flink.streaming.api.scala._
import java.util.concurrent.TimeUnit

object SinkToFileTest {
    def main(args: Array[String]): Unit = {
        val env = StreamExecutionEnvironment.getExecutionEnvironment
        env.setParallelism(4)
        val stream = env.fromElements(
            Event("Mary", "./home", 1000L),
            Event("Bob", "./cart", 2000L),
            Event("Alice", "./prod?id=100", 3000L),
            Event("Alice", "./prod?id=200", 3500L),
            Event("Bob", "./prod?id=2", 2500L),
            Event("Alice", "./prod?id=300", 3600L),
            Event("Bob", "./home", 3000L),
            Event("Bob", "./prod?id=1", 2300L),
            Event("Bob", "./prod?id=3", 3300L)
        )
        val fileSink = StreamingFileSink.forRowFormat(
            new Path("./output"),
            new SimpleStringEncoder[String]("UTF-8")
        )
        //通过.withRollingPolicy()方法指定“滚动策略”
        .withRollingPolicy(
            DefaultRollingPolicy.builder()
            .withRolloverInterval(TimeUnit.MINUTES.toMillis(15))
            .withInactivityInterval(TimeUnit.MINUTES.toMillis(5))
            .withMaxPartSize(1024 * 1024 * 1024)
            .build()
        ).build
        stream.map(_.toString).addSink(fileSink)
        env.execute()
    }
}复制代码
```

这里我们创建了一个简单的文件 Sink，通过 withRollingPolicy()方法指定了一个“滚动策略”。上面的代码设置了在以下 3 种情况下，我们就会滚动分区文件：

- 至少包含 15 分钟的数据
- 最近 5 分钟没有收到新的数据
- 文件大小已达到 1 GB

##### 2.3 输出到 Kafka

Flink 官方为 Kafka 提供了 Source 和 Sink 的连接器，我们可以用它方便地从 Kafka 读写数据。Flink 与 Kafka 的连接器提供了端到端的精确一次（exactly once）语义保证，这在实际项目中是最高级别的一致性保证。

- 添加 Kafka 连接器依赖

  由于我们已经测试过从 Kafka 数据源读取数据，连接器相关依赖已经引入，这里就不重复介绍了。

- 启动 Kafka

- 编写输出到 Kafka 的示例代码

  我们可以直接将用户行为数据保存为文件 clicks.csv,读取后不做转换直接写入 Kafka，主题（topic）命名为“clicks”。

  ```
  package com.atstudy.chapter03
  
  import org.apache.flink.api.common.serialization.SimpleStringSchema
  import org.apache.flink.streaming.api.scala._
  import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer
  import java.util.Properties
  
  object SinkToKafka {
      def main(args: Array[String]): Unit = {
          val env = StreamExecutionEnvironment.getExecutionEnvironment
          env.setParallelism(1)
          val properties = new Properties()
          properties.put("bootstrap.servers", "localhost:9092")
          val stream = env.readTextFile("input/clicks.csv")
          stream.addSink(new FlinkKafkaProducer[String](
              "clicks",
              new SimpleStringSchema(),
              properties
          ))
          env.execute()
      }
  }复制代码
  ```

  这里我们可以看到， addSink() 方法传入的参数是一个 FlinkKafkaProducer 。

  FlinkKafkaProducer 继承了抽象类 TwoPhaseCommitSinkFunction，这是一个实现了“两阶段提交”的 RichSinkFunction。两阶段提交提供了 Flink 向 Kafka 写入数据的事务性保证，能够真正做到精确一次（exactly once）的状态一致性。

- 运行代码，在 Linux 主机启动一个消费者, 查看是否收到数据

  ```
  bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic clicks复制代码
  ```

##### 2.4 输出到 MySQL（JDBC）

写入数据到 MySQL 的测试步骤如下。

- 添加依赖

  ```
  <dependency>
      <groupId>org.apache.flink</groupId>
      <artifactId>flink-connector-jdbc_${scala.binary.version}</artifactId>
      <version>${flink.version}</version>
  </dependency>
  <dependency>
      <groupId>mysql</groupId>
      <artifactId>mysql-connector-java</artifactId>
      <version>5.1.47</version>
  </dependency>复制代码
  ```

- 启动 MySQL，在 test 库下建表 clicks。

  ```
  create table clicks(
      user varchar(20) not null,
      url varchar(100) not null
  );复制代码
  ```

- 编写输出到 MySQL 的示例代码。

  ```
  package com.atstudy.chapter03
  
  import org.apache.flink.connector.jdbc.{JdbcConnectionOptions,JdbcExecutionOptions, JdbcSink, JdbcStatementBuilder}
  import org.apache.flink.streaming.api.scala._
  import java.sql.PreparedStatement
  
  object SinkToMySQL {
      def main(args: Array[String]): Unit = {
          val env = StreamExecutionEnvironment.getExecutionEnvironment
          env.setParallelism(1)
          val stream = env.fromElements(
              Event("Mary", "./home", 1000L),
              Event("Bob", "./cart", 2000L),
              Event("Alice", "./prod?id=100", 3000L),
              Event("Alice", "./prod?id=200", 3500L),
              Event("Bob", "./prod?id=2", 2500L),
              Event("Alice", "./prod?id=300", 3600L),
              Event("Bob", "./home", 3000L),
              Event("Bob", "./prod?id=1", 2300L),
              Event("Bob", "./prod?id=3", 3300L)
          )
          stream.addSink(
              JdbcSink.sink(
                  "INSERT INTO clicks (user, url) VALUES (?, ?)",
                  new JdbcStatementBuilder[Event] {
                      override def accept(t: PreparedStatement, u: Event): Unit = {
                          t.setString(1, u.user)
                          t.setString(2, u.url)
                      }
                  },
                  new JdbcConnectionOptions.JdbcConnectionOptionsBuilder()
                  .withUrl("jdbc:mysql://localhost:3306/test")
                  .withDriverName("com.mysql.jdbc.Driver")
                  .withUsername("root")
                  .withPassword("123456")
                  .build()
              )
          )
          env.execute ()
      }
  }复制代码
  ```

- 运行代码，用客户端连接 MySQL，查看是否成功写入数据。

  ```
  mysql> select * from clicks;
  +------+--------------+
  | user | url          |
  +------+--------------+
  | Mary | ./home       |
  | Alice| ./prod?id=300|
  | Bob  | ./prod?id=3  |
  +------+--------------+
  3 rows in set (0.00 sec)复制代码
  ```

##### 2.5 总结

到目前为止，我们已经将常用的 DataStream API 做基本介绍。进行了一个完整的实现，从`Source->Transformation->Sink`，整体还需要多熟悉才能游刃有余，为后期更高阶的API实验做好准备。

![image-20230309131328545](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_2/image-20230309131328545.png)





# 实验3-3：时间和窗口

## 实验概述

我们已经了解了基本 API 的用法，熟悉了 DataStream 进行简单转换、聚合的一些操作。除此之外，Flink 还提供了丰富的转换算子，可以用于更加复杂的处理场景。

在流数据处理应用中，一个很重要、也很常见的操作就是窗口计算。所谓的“窗口”，一般就是划定的一段时间范围，也就是“时间窗”；对在这范围内的数据进行处理，就是所谓的窗口计算。所以窗口和时间往往是分不开的。接下来我们就深入了解一下 Flink 中的时间语义和窗口的应用

## 实验环境

- AtStudy 实训平台
- Flink1.13

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16799808914755595.png)

## 实验目标

学习完成本实验后，您将能够

- 掌握Flink中时间语义
- 区分事件时间和处理时间的异同
- 掌握水位线的作用

## 实验任务

### 任务一、时间和水位线

#### **【任务目标】**

掌握时间的概念和水位线的作用。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_3/image-20210824110318276.png?fileid=3270835009252003995)

视频-1、时间和水位线

##### 1.1 从星球大战说起

![image-20230309150349730](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_3/image-20230309150349730.png)

《星球大战》是一部经典的科幻电影，在 1977 拍摄上映之后就引起了巨大的反响，票房爆棚好评如潮。我们知道，但凡一部商业电影叫好又叫座，那十有八九都是要拍续集的——于是 6 年内又上映了两部续集，这就是当时轰动一时的星战三部曲。好 IP 总是要反复拿来用，所以十几年后又有了星战前传三部曲，到了 2015 年之后又以每年一部的频率继续拍摄后传和外传。而星战系列的命名也很有趣，是按照故事时间线的发展来的：经典三部曲是系列的四、五、六部，之后是前传一、二、三，2015 年开始的后传就从第七部算起了。

![image-20230309141317009](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_3/image-20230309141317009.png)

如图所示，我们会发现，看电影其实就是处理影片中数据的过程，所以影片的上映时间就相当于“**处理时间**”；而影片的数据就是所描述的故事，它所发生的背景时间就相当于“**事件时间**”。

现在我们考虑一下，作为没有看过星战的新影迷，如果想要入坑一览，该选择什么样的观影顺序呢？这就要看我们具体的需求了：如果你是剧情党，重点想看一个完整的故事，那最好的选择无疑就是按照系列的编号，沿着故事发展的时间线来看；而如果你是特效党，更想体验炫目的视觉效果和时代技术的发展，那就按照电影的拍摄顺序来观看，不过剧情可能就需要多脑补一下了。

所以，两种时间语义都有各自的用途，适用于不同的场景。

##### 1.2 Flink中时间定义

时间本身就有着“流”的特性，它可以用来判断事件发生的先后以及间隔；所以如果我们想要划定窗口来收集数据，一般就需要基于时间。

![image-20230309142257543](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_3/image-20230309142257543.png)

流式数据处理的过程，如图所示，在事件发生之后，生成的数据被收集起来，首先进入分布式消息队列，然后被 Flink 系统中的 Source 算子读取消费，进而向下游的转换算子（窗口算子）传递，最终由窗口算子进行计算处理。

很明显，这里有两个非常重要的时间点：一个是数据产生的时间，我们把它叫作“事件时间”（Event Time）；另一个是数据真正被处理的时刻，叫作“处理时间”（Processing Time）。我们所定义的窗口操作，到底是以那种时间作为衡量标准，就是所谓的“时间语义”（Notions of Time）。由于分布式系统中网络传输的延迟和时钟漂移，处理时间相对事件发生的时间会有所滞后。

###### 1.2.1 处理时间

处理时间的概念非常简单，就是指执行处理操作的机器的系统时间。

在这种时间语义下处理窗口非常简单粗暴，不需要各个节点之间进行协调同步，也不需要考虑数据在流中的位置，简单来说就是“我的地盘听我的”。所以处理时间是最简单的时间语义。

###### 1.2.2 事件时间

事件时间，是指每个事件在对应的设备上发生的时间，也就是数据生成的时间。

数据一旦产生，这个时间自然就确定了，所以它可以作为一个属性嵌入到数据中。这其实就是这条数据记录的“时间戳”（`Timestamp`）。

在事件时间语义下，我们对于时间的衡量，就不看任何机器的系统时间了，而是依赖于数据本身。但是由于分布式系统中网络传输延迟的不确定性，实际应用中我们要面对的数据流往往是乱序的。在这种情况下，就不能简单地把数据自带的时间戳当作时钟了，而需要用另外的标志来表示事件时间进展，在 Flink 中把它叫作事件时间的“水位线”（`Watermarks`）。

##### 1.3 水位线

在介绍事件时间语义时，我们提到了“水位线”的概念，已经知道了它其实就是用来度量事件时间的。那么水位线具体有什么含义，又跟数据的时间戳有什么关系呢？接下来我们就来深入探讨一下这个流处理中的核心概念。

###### 1.3.1 事件时间和窗口

在实际应用中，一般会采用事件时间语义。而水位线，就是基于事件时间提出的概念。所以在介绍水位线之前，我们首先来梳理一下事件时间和窗口的关系。

一个数据产生的时刻，就是流处理中事件触发的时间点，这就是“事件时间”，一般都会以时间戳的形式作为一个字段记录在数据里。这个时间就像商品的“生产日期”一样，一旦产生就是固定的，印在包装袋上，不会因为运输辗转而变化。如果我们想要统计一段时间内的数据，需要划分时间窗口，这时只要判断一下时间戳就可以知道数据属于哪个窗口了。

![image-20230309152215174](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_3/image-20230309152215174.png)

> 事件时间语义下窗口的开启和关闭（窗口时间区间为左闭右开，即包含 8 点不包含 9 点）

**但是**

在事件时间语义下，我们不依赖系统时间，而是基于数据自带的时间戳去定义了一个时钟， 用来表示当前时间的进展。于是每个并行子任务都会有一个自己的逻辑时钟，它的前进是靠数据的时间戳来驱动的。

但在分布式系统中，这种驱动方式又会有一些问题。因为数据本身在处理转换的过程中会变化，如果遇到窗口聚合这样的操作，其实是要攒一批数据才会输出一个结果，那么下游的数据就会变少，时间进度的控制就不够精细了。另外，数据向下游任务传递时，一般只能传输给一个子任务（除广播外），这样其他的并行子任务的时钟就无法推进了。例如一个时间戳为 9点整的数据到来，当前任务的时钟就已经是 9 点了；处理完当前数据要发送到下游，如果下游任务是一个窗口计算，并行度为 3，那么接收到这个数据的子任务，时钟也会进展到 9 点，9 点结束的窗口就可以关闭进行计算了；而另外两个并行子任务则时间没有变化，不能进行窗口计算。

那怎么办呢？

###### 1.3.2 什么是水位线

我们可以把时钟也以数据的形式传递出去，告诉下游任务当前时间的进展；而且这个时钟的传递不会因为窗口聚合之类的运算而停滞。一种简单的想法是，在数据流中加入一个时钟标记，记录当前的事件时间；这个标记可以直接广播到下游，当下游任务收到这个标记，就可以更新自己的时钟了。由于类似于水流中用来做标志的记号，在 Flink 中，这种用来衡量事件时间（Event Time）进展的标记，就被称作“水位线”（Watermark）。

具体实现上，水位线可以看作一条特殊的数据记录，它是插入到数据流中的一个标记点，主要内容就是一个时间戳，用来指示当前的事件时间。而它插入流中的位置，就应该是在某个数据到来之后；这样就可以从这个数据中提取时间戳，作为当前水位线的时间戳了。

![image-20230309151113797](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_3/image-20230309151113797.png)

1. 有序流中的水位线

   在理想状态下，数据应该按照它们生成的先后顺序、排好队进入流中；而在实际应用中，如果当前数据量非常大，可能会有很多数据的时间戳是相同的，这时每来一条数据就提取时间戳、插入水位线就做了大量的无用功。所以为了提高效率，一般会每隔一段时间生成一个水位线，这个水位线的时间戳，就是当前最新数据的时间戳，如下图所示。所以这时的水位线，其实就是有序流中的一个周期性出现的时间标记。

   ![image-20230309154340214](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_3/image-20230309154340214.png)

2. 乱序流中的水位线

   在分布式系统中，数据在节点间传输，会因为网络传输延迟的不确定性，导致顺序发生改变，这就是所谓的“乱序数据”。

   ![image-20230309154412398](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_3/image-20230309154412398.png)

   对于连续数据流，我们插入新的水位线时，要先判断一下时间戳是否比之前的大，否则就不再生成新的水位线，如下图所示。也就是说，只有数据的时间戳比当前时钟大，才能推动时钟前进，这时才插入水位线。

   ![image-20230309154553857](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_3/image-20230309154553857.png)

   如果考虑到大量数据同时到来的处理效率，我们同样可以周期性地生成水位线。这时只需要保存一下之前所有数据中的最大时间戳，需要插入水位线时，就直接以它作为时间戳生成新的水位线，如下图所示

   ![image-20230309155032336](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_3/image-20230309155032336.png)

   为了让窗口能够正确收集到迟到的数据，我们可以等上 2 秒；也就是用当前已有数据的最大时间戳减去 2 秒，就是要插入的水位线的时间戳，如下图所示。

   ![image-20230309155201539](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_3/image-20230309155201539.png)

   如果仔细观察就会看到，这种“等 2 秒”的策略其实并不能处理所有的乱序数据。所以我们可以试着多等几秒，也就是把时钟调得更慢一些。最终的目的，就是要让窗口能够把所有迟到数据都收进来，得到正确的计算结果。对应到水位线上，其实就是要保证，当前时间已经进展到了这个时间戳，在这之后不可能再有迟到数据来了。

   下面是一个示例，我们可以使用周期性的方式生成正确的水位线。

   ![image-20230309155317082](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_3/image-20230309155317082.png)

3. 水位线特征

   现在我们可以知道，水位线就代表了当前的事件时间时钟，而且可以在数据的时间戳基础上加一些延迟来保证不丢数据，这一点对于乱序流的正确处理非常重要。

   我们可以总结一下水位线的特性：

   - 水位线是插入到数据流中的一个标记，可以认为是一个特殊的数据
   - 水位线主要的内容是一个时间戳，用来表示当前事件时间的进展
   - 水位线是基于数据的时间戳生成的
   - 水位线的时间戳必须单调递增，以确保任务的事件时间时钟一直向前推进
   - 水位线可以通过设置延迟，来保证正确处理乱序数据
   - 一个水位线 Watermark(t)，表示在当前流中事件时间已经达到了时间戳 t, 这代表 t 之前的所有数据都到齐了，之后流中不会出现时间戳 t’ ≤ t 的数据

   水位线是 Flink 流处理中保证结果正确性的核心机制，它往往会跟窗口一起配合，完成对乱序数据的正确处理。

###### 1.3.3 如何生成水位线

1. 生成水位线的总体原则

   如果我们希望计算结果能更加准确，那可以将水位线的延迟设置得更高一些，等待的时间越长，自然也就越不容易漏掉数据。不过这样做的代价是处理的实时性降低了，我们可能为极少数的迟到数据增加了很多不必要的延迟。

   如果我们希望处理得更快、实时性更强，那么可以将水位线延迟设得低一些。这种情况下，可能很多迟到数据会在水位线之后才到达，就会导致窗口遗漏数据，计算结果不准确。对于这些 “漏网之鱼”，Flink 另外提供了窗口处理迟到数据的方法，我们会在后面介绍。当然，如果我们对准确性完全不考虑、一味地追求处理速度，可以直接使用处理时间语义，这在理论上可以得到最低的延迟。

   所以 Flink 中的水位线，其实是流处理中对低延迟和结果正确性的一个权衡机制，而且把控制的权力交给了程序员，我们可以在代码中定义水位线的生成策略。

2. Flink内置的水位线

   Flink 提供了内置的水位线生成器（`WatermarkGenerator`），不仅开箱即用简化了编程，而且也为我们自定义水位线策略提供了模板。

   这两个生成器可以通过调用 `WatermarkStrateg`y 的静态辅助方法来创建。它们都是周期性生成水位线的，分别对应着处理有序流和乱序流的场景

   - 有序流

     对于有序流，主要特点就是时间戳单调增长（`Monotonously Increasing Timestamps`），所以永远不会出现迟到数据的问题。这是周期性生成水位线的最简单的场景，直接调用`WatermarkStrategy.forMonotonousTimestamps()`方法就可以实现。简单来说，就是直接拿当前最大的时间戳作为水位线就可以了。

     ```
     stream.assignTimestampsAndWatermarks(
         WatermarkStrategy.forMonotonousTimestamps[Event]()
         .withTimestampAssigner(
             new SerializableTimestampAssigner[Event] {
                 override def extractTimestamp(element: Event, recordTimestamp: Long): 
                 Long = element.timestamp
             }
         )
     )复制代码
     ```

     上面代码中我们调用 `withTimestampAssigner()`方法，将数据中的 `timestamp` 字段提取出来，作为时间戳分配给数据元素；然后用内置的有序流水位线生成器构造出了生成策略。这样，提取出的数据时间戳，就是我们处理计算的事件时间。

     这里需要注意的是，时间戳和水位线的单位，必须都是毫秒。

   - 乱序流

     由于乱序流中需要等待迟到数据到齐，所以必须设置一个固定量的延迟时间（`Fixed Amount of Lateness`）。这时生成水位线的时间戳，就是当前数据流中最大的时间戳减去延迟的结果，相当于把表调慢，当前时钟会滞后于数据的最大时间戳。调用 `WatermarkStrategy.forBoundedOutOfOrderness()`方法就可以实现。这个方法需要传入一个 `maxOutOfOrderness` 参数，表示“最大乱序程度”，它表示数据流中乱序数据时间戳的最大差值；如果我们能确定乱序程度，那么设置对应时间长度的延迟，就可以等到所有的乱序数据了。

     ```
     package com.atstudy.chapter03
     
     import java.time.Duration
     import org.apache.flink.api.common.eventtime.{SerializableTimestampAssigner,WatermarkStrategy}
     import org.apache.flink.streaming.api.scala._
     
     object OutOfOrdernessTest {
         def main(args: Array[String]): Unit = {
             val env = StreamExecutionEnvironment.getExecutionEnvironment
             env.addSource(new ClickSource)
             //插入水位线的逻辑
             .assignTimestampsAndWatermarks(
                 //针对乱序流插入水位线，延迟时间设置为 5s
                 WatermarkStrategy.forBoundedOutOfOrderness[Event](Duration.ofSeconds(5))
                 .withTimestampAssigner(
                     new SerializableTimestampAssigner[Event] {
                         // 指定数据中的哪一个字段是时间戳
                         override def extractTimestamp(element: Event, recordTimestamp: Long): 
                         Long = element.timestamp
                     }
                 )
             )
             .print()
             env.execute()
         }
     }复制代码
     ```

     上面代码中，我们同样提取了 `timestamp` 字段作为时间戳，并且以 5 秒的延迟时间创建了处理乱序流的水位线生成器。

     事实上，有序流的水位线生成器本质上和乱序流是一样的，相当于延迟设为 0 的乱序流水位线生成器，两者完全等同：

     ```
     WatermarkStrategy.forMonotonousTimestamps()
     WatermarkStrategy.forBoundedOutOfOrderness(Duration.ofSeconds(0))复制代码
     ```

### 任务二、窗口计算

#### **【任务目标】**

在 Flink 中，提供了非常丰富的窗口操作，在本任务中需要掌握各种窗口的使用，这是Flink的重点也是难点。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_3/image-20210824110318276.png?fileid=3270835009252580720)

视频-2、窗口计算

##### 2.1 窗口概念

Flink 是一种流式计算引擎，主要是来处理无界数据流的，数据源源不断、无穷无尽。想要更加方便高效地处理无界流，一种方式就是将无限数据切割成有限的“数据块”进行处理，这就是所谓的“窗口”（Window）。

![image-20230309171730009](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_3/image-20230309171730009.png)

这里注意为了明确数据划分到哪一个窗口，定义窗口都是包含起始时间、不包含结束时间的，用数学符号表示就是一个左闭右开的区间。

对于处理时间下的窗口而言，这样理解似乎没什么问题。然而如果我们采用事件时间语义，就会有些令人费解了。由于有乱序数据，我们需要设置一个延迟时间来等所有数据到齐。比如上面的例子中，我们可以设置延迟时间为 2 秒，如下图所示，这样 0~10 秒的窗口会在时间戳为 12 的数据到来之后，才真正关闭计算输出结果，这样就可以正常包含迟到的 9 秒数据了。

![image-20230309171753066](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_3/image-20230309171753066.png)

但是这样一来，0~10 秒的窗口不光包含了迟到的 9 秒数据，连 11 秒和 12 秒的数据也包含进去了。我们为了正确处理迟到数据，结果把早到的数据划分到了错误的窗口——最终结果都是错误的。

所以在 Flink 中，窗口其实并不是一个“框”，流进来的数据被框住了就只能进这一个窗口。相比之下，我们应该把窗口理解成一个“桶”，如下图所示。在 Flink 中，窗口可以把流切割成有限大小的多个“存储桶”（bucket)；每个数据都会分发到对应的桶中，当到达窗口结束时间时，就对每个桶中收集的数据进行计算处理。

![image-20230309171815679](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_3/image-20230309171815679.png)

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_3/image-20210823165604377.png)**注意！**

> 窗口并不是静态准备好的，而是动态创建——当有落在这个窗口区间范围的数据达到时，才创建对应的窗口

##### 2.2 窗口的分类

Window 可以分成两类：

- CountWindow：按照固定的个数，来截取一段数据集，这种窗口叫作“计数窗口”，与时间无关。
- TimeWindow：是按照时间段去截取数据，这种窗口就叫作“时间窗口”

![image-20230309172520373](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_3/image-20230309172520373.png)

对于 TimeWindow，可以根据窗口实现原理的不同分成三类：滚动窗口（Tumbling Window）、滑动窗口（Sliding Window）和会话窗口（Session Window）。

1. 滚动窗口（Tumbling Windows）

   将数据依据固定的窗口长度对数据进行切片。

   **特点：时间对齐，窗口长度固定，没有重叠。**

   滚动窗口有固定的大小，是一种对数据进行“均匀切片”的划分方式。窗口之间没有重叠，也不会有间隔，是“首尾相接”的状态。如果我们把多个窗口的创建，看作一个窗口的运动，那就好像它在不停地向前“翻滚”一样。

   滚动窗口可以基于时间定义，也可以基于数据个数定义；需要的参数只有一个，就是窗口的大小（window size）。比如我们可以定义一个长度为 1 小时的滚动时间窗口，那么每个小时就会进行一次统计；或者定义一个长度为 10 的滚动计数窗口，就会每 10 个数进行一次统计，窗口的创建如下图所示：

   

   适用场景：适合做 BI 统计等（做每个时间段的聚合计算）。

2. 滑动窗口（Sliding Windows）

   滑动窗口是固定窗口的更广义的一种形式，滑动窗口由固定的窗口长度和滑动间隔组成。

   **特点：时间对齐，窗口长度固定，可以有重叠。**

   滑动窗口分配器将元素分配到固定长度的窗口中，与滚动窗口类似，窗口的大小由窗口大小参数来配置，另一个窗口滑动参数控制滑动窗口开始的频率。因此，滑动窗口如果滑动参数小于窗口大小的话，窗口是可以重叠的，在这种情况下元素会被分配到多个窗口中。

   例如，你有 10 分钟的窗口和 5 分钟的滑动，那么每个窗口中 5 分钟的窗口里包含着上个 10 分钟产生的数据，如下图所示：

   

   适用场景：对最近一个时间段内的统计（求某接口最近 5min 的失败率来决定是否要报警）。

3. 会话窗口（Session Windows）

   由一系列事件组合一个指定时间长度的 timeout 间隙组成，类似于 web 应用的session，也就是一段时间没有接收到新数据就会生成新的窗口。

   **特点：时间无对齐。**

   session 窗口分配器通过 session 活动来对元素进行分组，session 窗口跟滚动窗口和滑动窗口相比，不会有重叠和固定的开始时间和结束时间的情况，相反，当它在一个固定的时间周期内不再收到元素，即非活动间隔产生，那个这个窗口就会关闭。一个 session 窗口通过一个 session 间隔来配置，这个 session 间隔定义了非活跃周期的长度，当这个非活跃周期产生，那么当前的 session 将关闭并且后续的元素将被分配到新的 session 窗口中去。

   

##### 2.3 窗口API

###### 2.3.1 按键分区（Keyed）和非按键分区（Non-Keyed）

在定义窗口操作之前，首先需要确定，到底是基于按键分区（Keyed）的数据流 `KeyedStream`来开窗，还是直接在没有按键分区的 DataStream 上开窗。也就是说，在调用窗口算子之前，是否有 `keyBy()`操作。

（1）按键分区窗口（Keyed Windows）

经过按键分区 `keyBy()` 操作后，数据流会按照 key 被分为多条逻辑流（logical streams），这就是 `KeyedStream`。基于`KeyedStream` 进行窗口操作时, 窗口计算会在多个并行子任务上同时执行。相同 key 的数据会被发送到同一个并行子任务，而窗口操作会基于每个 key 进行单独的处理。所以可以认为，每个 key上都定义了一组窗口，各自独立地进行统计计算。

在代码实现上，我们需要先对DataStream调用`keyBy()`进行按键分区，然后再调用window()定义窗口。

```
stream
  .keyBy(...)
  .window(...)复制代码
```

（2）非按键分区（Non-Keyed Windows）

如果没有进行 `keyBy()`，那么原始的 DataStream 就不会分成多条逻辑流。这时窗口逻辑只能在一个任务（task）上执行，就相当于并行度变成了 1。所以在实际应用中一般不推荐使用这种方式。在代码中，直接基于 DataStream 调用` windowAll()`定义窗口。

```
stream.windowAll(...)复制代码
```

这里需要注意的是，对于非按键分区的窗口操作，手动调大窗口算子的并行度也是无效的，`windowAll` 本身就是一个非并行的操作。

（3）代码中窗口 API 的调用

有了前置的基础，接下来我们就可以真正在代码中实现一个窗口操作了。简单来说，窗口操作主要有两个部分：`窗口分配器（Window Assigners`）和`窗口函数（Window Functions）`。

```
stream.keyBy(<key selector>)
 .window(<window assigner>)
 .aggregate(<window function>)复制代码
```

其中`.window()`方法需要传入一个窗口分配器，它指明了窗口的类型；而后面的`.aggregate()`方法传入一个窗口函数作为参数，它用来定义窗口具体的处理逻辑。

另外，在实际应用中，一般都需要并行执行任务，非按键分区很少用到，所以我们之后都以按键分区窗口为例。

###### 2.3.2 窗口分配器（Window Assigners）

定义窗口分配器（Window Assigners）是构建窗口算子的第一步，它的作用就是定义数据应该被“分配”到哪个窗口。而窗口分配数据的规则，其实就对应着不同的窗口类型。所以可以说，窗口分配器其实就是在指定窗口的类型。窗口分配器最通用的定义方式，就是调用 window()方法，具体使用分配器如下：

| 时间窗口         | 分配器                                                       | 说明                                  |
| ---------------- | ------------------------------------------------------------ | ------------------------------------- |
| 滚动处理时间窗口 | `TumblingProcessingTimeWindows.of(Time.seconds(5))`          | 长度为 5 秒的滚动窗口                 |
| 滑动处理时间窗口 | `SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))` | 长度为 10 秒、滑动步长为 5 秒的滑动窗 |
| 处理时间会话窗口 | `ProcessingTimeSessionWindows.withGap(Time.seconds(10))`     | 会话超时时间为 10 秒的会话窗口        |
| 滚动事件时间窗口 | `TumblingEventTimeWindows.of(Time.seconds(5))`               | 同上                                  |
| 滑动事件时间窗口 | `SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))` | 同上                                  |
| 事件时间会话窗口 | `EventTimeSessionWindows.withGap(Time.seconds(10))`          | 同上                                  |

| 计数窗口 | 分配器                                 | 说明                                                         |
| -------- | -------------------------------------- | ------------------------------------------------------------ |
| 滚动     | `stream.keyBy(...).countWindow(10)`    | 长度为 10 的滚动计数窗口，当窗口中元素数量达到 10 的时候，就会触发计算执行并关闭窗口 |
| 滑动     | `stream.keyBy(...).countWindow(10，3)` | 长度为 10、步长为 3 的滑动计数窗口。每个窗口统计 10 个数据，每隔 3 个数据就统计输出一次结果 |

###### 2.3.3 窗口函数

定义了窗口分配器，我们只是知道了数据属于哪个窗口，可以将数据收集起来了；至于收集起来到底要做什么，其实还完全没有头绪。所以在窗口分配器之后，必须再接上一个定义窗口如何进行计算的操作，这就是所谓的“窗口函数”（window functions）。

窗口函数定义了要对窗口中收集的数据做的计算操作，根据处理的方式可以分为两类：增量聚合函数和全窗口函数。

- 增量聚合函数（incremental aggregation functions）

典型的增量聚合函数有两个：`ReduceFunction` 和 `AggregateFunction`。

（1）归约函数（`ReduceFunction`）

和之前聚合算子中介绍过 reduce 的用法类似，只是将窗口中收集到的数据两两进行归约。

```
package com.atstudy.chapter03

import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows
import org.apache.flink.streaming.api.windowing.time.Time
object WindowReduceExample {
    def main(args: Array[String]): Unit = {
        val env = StreamExecutionEnvironment.getExecutionEnvironment
        env.setParallelism(1)
        val stream = env.addSource(new ClickSource)
        // 数据源中的时间戳是单调递增的，所以使用下面的方法，只需要抽取时间戳就好了
        // 等同于最大延迟时间是 0 毫秒
        // env.addSource(new ClickSource).assignAscendingTimestamps(_.timestamp)
        .assignTimestampsAndWatermarks(
            WatermarkStrategy.forMonotonousTimestamps[Event]()
            .withTimestampAssigner(
                new SerializableTimestampAssigner[Event] {
                    override def extractTimestamp(element: Event, recordTimestamp: Long): 
                    Long = element.timestamp
                }
            )
        )
        stream.map(data => (data.user, 1L))
        // 使用用户名对数据流进行分组
        .keyBy(_._1)
        // 设置 5 秒钟的滚动事件时间窗口
        .window(TumblingEventTimeWindows.of(Time.seconds(5)))
        // 保留第一个字段，针对第二个字段进行聚合
        .reduce((state, data) => (state._1, state._2 + data._2))
        .print()
        env.execute()
    }
}复制代码
```

运行结果如下：

```
(Bob,1)
(Alice,2)
(Mary,2)
……复制代码
```

（2）聚合函数（`AggregateFunction`）

`ReduceFunction` 可以解决大多数归约聚合的问题，但是这个接口有一个限制，就是聚合状态的类型、输出结果的类型都必须和输入数据类型一样。`AggregateFunction` 在源码中的定义如下：

```
public interface AggregateFunction<IN, ACC, OUT> extends Function, Serializable 
{
    ACC createAccumulator();
    ACC add(IN value, ACC accumulator);
    OUT getResult(ACC accumulator);
    ACC merge(ACC a, ACC b);
}复制代码
```

`AggregateFunction` 可以看作是 `ReduceFunction` 的通用版本，这里有三种类型：输入类型（IN）、累加器类型（ACC）和输出类型（OUT）。输入类型 IN 就是输入流中元素的数据类型；累加器类型 ACC 则是我们进行聚合的中间状态类型；而输出类型当然就是最终计算结果的类型了。

`AggregateFunction` 接口中有四个方法：

- `createAccumulator()`：创建一个累加器，这就是为聚合创建了一个初始状态，每个聚合任务只会调用一次
- `add()`：将输入的元素添加到累加器中。这就是基于聚合状态，对新来的数据进行进一步聚合的过程。方法传入两个参数：当前新到的数据 value，和当前的累加器accumulator；返回一个新的累加器值，也就是对聚合状态进行更新。每条数据到来之后都会调用这个方法。
- `getResult()`：从累加器中提取聚合的输出结果。
- `merge()`：合并两个累加器，并将合并后的状态作为一个累加器返回。

下面来看一个具体例子。我们计算一下 `PV/UV` 这个比值，来表示“人均重复访问量”，也就是平均每个用户会访问多少次页面，这在一定程度上代表了用户的黏度。

```
package com.atstudy.chapter03
import org.apache.flink.api.common.functions.AggregateFunction
import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows
import org.apache.flink.streaming.api.windowing.time.Time

object AggregateFunctionExample {
    def main(args: Array[String]): Unit = {
        val env = StreamExecutionEnvironment.getExecutionEnvironment
        env.setParallelism(1)

        env.addSource(new ClickSource).assignAscendingTimestamps(_.timestamp)
        // 通过为每条数据分配同样的 key，来将数据发送到同一个分区
        .keyBy(_ => "key")
        .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(2)))
        .aggregate(new AvgPv)
        .print()
        env.execute()
    }
    
    //Event:输入类型
    //(Set[String],Double):(表示所有的独立uv, pv个数）
    //Double：pv/uv的结果
    class AvgPv extends AggregateFunction[Event, (Set[String], Double), Double] {
        // 创建空累加器，类型是元组，元组的第一个元素类型为 Set 数据结构，用来对用户名进行去重
        // 第二个元素用来累加 pv 操作，也就是每来一条数据就加一
        override def createAccumulator(): (Set[String], Double) = (Set[String](), 0L)
       
        // 累加规则
        override def add(value: Event, accumulator: (Set[String], Double)): 
        (Set[String], Double) = (accumulator._1 + value.user, accumulator._2 + 1L)
        
        // 获取窗口关闭时向下游发送的结果
        override def getResult(accumulator: (Set[String], Double)): Double = 
        accumulator._2 / accumulator._1.size
        
        // merge 方法只有在事件时间的会话窗口时，才需要实现，这里无需实现。
        override def merge(a: (Set[String], Double), b: (Set[String], Double)): 
        (Set[String], Double) = ???
    }
}复制代码
```

输出结果如下：

```
1.0
1.6666666666666667
……复制代码
```

- 全窗口函数

与增量聚合函数不同，全窗口函数需要先收集窗口中的数据，并在内部缓存起来，等到窗口要输出结果的时候再取出数据进行计算，`ProcessWindowFunction`就是常用的一个全窗口函数。

`WindowedStream` 调用 process()方法，传入一个 `ProcessWindowFunction `的实现类，下面以一个电商网站统计每小时 UV 的例子：

```
package com.atstudy.chapter03

import org.apache.flink.streaming.api.scala._
import org.apache.flink.streaming.api.scala.function.ProcessWindowFunction
import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.streaming.api.windowing.windows.TimeWindow
import org.apache.flink.util.Collector
import java.sql.Timestamp
import scala.collection.mutable.Set

object UvCountByWindowExample {
    def main(args: Array[String]): Unit = {
        val env = StreamExecutionEnvironment.getExecutionEnvironment
        env.setParallelism(1)
        env.addSource(new ClickSource)
        .assignAscendingTimestamps(_.timestamp)
        // 为所有数据都指定同一个 key，可以将所有数据都发送到同一个分区
        .keyBy(_ => "key")
        .window(TumblingEventTimeWindows.of(Time.seconds(10)))
        .process(new UvCountByWindow)
        .print()
        env.execute()
    }
    // 自定义窗口处理函数
    //第一个参数：Event表示输入类型
    //第二个参数：String表示输出类型
    //第三个参数：当前key的类型
    //第四个参数：窗口类型
    class UvCountByWindow extends ProcessWindowFunction[Event, String, String,TimeWindow] {
        override def process(key: String, context: Context, elements: Iterable[Event], 
                             out: Collector[String]): Unit = {
            // 初始化一个 Set 数据结构，用来对用户名进行去重
            var userSet = Set[String]()
            // 将所有用户名进行去重
            elements.foreach(userSet += _.user)
            // 结合窗口信息，包装输出内容
            val windowStart = context.window.getStart
            val windowEnd = context.window.getEnd
            out.collect(" 窗 口 ： " + new Timestamp(windowStart) + "~" + new 
                        Timestamp(windowEnd) + "的独立访客数量是：" + userSet.size)
        }
    }
}复制代码
```

输出结果形式如下：

```
窗口：...~...的独立访客数量是：2
窗口：...~...的独立访客数量是：3
……复制代码
```

- 增量聚合和全窗口结合

  **总结：**

  - 增量聚合函数处理计算会更高效。增量聚合相当于把计算量“均摊”到了窗口收集数据的过程中，自然就会比全窗口聚合更加高效、输出更加实时。
  - 而全窗口函数的优势在于提供了更多的信息，可以认为是更加“通用”的窗口操作，窗口计算更加灵活，功能更加强大。

  所以在实际应用中，我们往往希望兼具这两者的优点，把它们结合在一起使用。

  我们之前在调用 `WindowedStream` 的 reduce()和 aggregate()方法时，只是简单地直接传入了一个 `ReduceFunction` 或 `AggregateFunction` 进行增量聚合。除此之外，其实还可以传入第二个参数：一个全窗口函数，可以是 `ProcessWindowFunction`。

  > 这样调用的处理机制是：基于第一个参数（增量聚合函数）来处理窗口数据，每来一个数据就做一次聚合；等到窗口需要触发计算时，则调用第二个参数（全窗口函数）的处理逻辑输出结果。需要注意的是，这里的全窗口函数就不再缓存所有数据了，而是直接将增量聚合函数的结果拿来当作了 Iterable 类型的输入。一般情况下，这时的可迭代集合中就只有一个元素了。

  例如：统计 10 秒钟的 url 浏览量，每 5 秒钟更新一次；另外为了更加清晰地展示，还应该把窗口的起始结束时间一起输出。

  ```
  package com.atstudy.chapter03
  import org.apache.flink.api.common.functions.AggregateFunction
  import org.apache.flink.streaming.api.scala._
  import org.apache.flink.streaming.api.scala.function.ProcessWindowFunction
  import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows
  import org.apache.flink.streaming.api.windowing.time.Time
  import org.apache.flink.streaming.api.windowing.windows.TimeWindow
  import org.apache.flink.util.Collector
  
  object UrlViewCountExample {
      def main(args: Array[String]): Unit = {
          val env = StreamExecutionEnvironment.getExecutionEnvironment
          env.setParallelism(1)
          env.addSource(new ClickSource)
          .assignAscendingTimestamps(_.timestamp)
          // 使用 url 作为 key 对数据进行分区
          .keyBy(_.url)
          .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5)))
          // 注意这里调用的是 aggregate 方法
          // 增量聚合函数和全窗口聚合函数结合使用
          .aggregate(new UrlViewCountAgg, new UrlViewCountResult)
          .print()
          env.execute()
      }
      
      class UrlViewCountAgg extends AggregateFunction[Event, Long, Long] {
          override def createAccumulator(): Long = 0L
          // 每来一个事件就加一
          override def add(value: Event, accumulator: Long): Long = accumulator + 1L
          // 窗口闭合时发送的计算结果
          override def getResult(accumulator: Long): Long = accumulator
          override def merge(a: Long, b: Long): Long = ???
      }
      
      class UrlViewCountResult extends ProcessWindowFunction[Long, UrlViewCount, String, TimeWindow] {
          // 迭代器中只有一个元素，是增量聚合函数在窗口闭合时发送过来的计算结果
          override def process(key: String, context: Context, elements: Iterable[Long], out: Collector[UrlViewCount]): Unit = {
              out.collect(UrlViewCount(
                  key,
                  elements.iterator.next(),
                  context.window.getStart,
                  context.window.getEnd
              ))
          }
      }
      
      case class UrlViewCount(url: String, count: Long, windowStart: Long,windowEnd: Long)
  }复制代码
  ```

  > 这里我们为了方便处理，单独定义了一个样例类 UrlViewCount 来表示聚合输出结果的数据类型，包含了 url、浏览量以及窗口的起始结束时间。用一个 AggregateFunction 来实现增量聚合，每来一个数据就计数加一；得到的结果交给 ProcessWindowFunction，结合窗口信息包装成我们想要的 UrlViewCount，最终输出统计结果。

##### 2.4 **其它可选** API

- `.trigger()` —— 触发器

  定义 window 什么时候关闭，触发计算并输出结果

- `.evitor()` —— 移除器

  定义移除某些数据的逻辑

- `.allowedLateness()` —— 允许处理迟到的数据

- `.sideOutputLateData()` —— 将迟到的数据放入侧输出流

- `.getSideOutput()` —— 获取侧输出流





# 实验3-4：状态编程

## 实验概述

在流处理中，数据是连续不断到来和处理的。每个任务进行计算处理时，可以基于当前数据直接转换得到输出结果；也可以依赖一些其他数据。这些由一个任务维护，并且用来计算输出结果的所有数据，就叫作这个任务的状态，本实验的主要任务就是介绍关于状态的编程内容。

## 实验环境

- AtStudy 实训平台
- Flink1.13

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16792783345942595.png)

## 实验目标

学习完成本实验后，您将能够

- 了解什么是状态算子及状态分类
- 掌握键控状态的使用
- 了解Flink的容错机制

## 实验任务

### 任务一、状态介绍

#### **【任务目标】**

了解Flink中的状态算子及状态的分类。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_4/image-20210824110318276.png?fileid=3270835009335901383)

视频-1、状态介绍

##### 1.1 有状态算子

在 Flink 中，算子任务可以分为无状态和有状态两种情况。

无状态的算子任务只需要观察每个独立事件，根据当前输入的数据直接转换输出结果，如下图所示。例如，可以将一个字符串类型的数据拆分开作为元组输出；也可以对数据做一些计算，比如每个代表数量的字段加 1。我们之前讲到的基本转换算子，如` map()、filter()、flatMap()`，计算时不依赖其他数据，就都属于无状态的算子。

![image-20230313101357798](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_4/image-20230313101357798.png)

而有状态的算子任务，则除当前数据之外，还需要一些其他数据来得到计算结果。这里的“其他数据”，就是所谓的状态（state），最常见的就是之前到达的数据，或者由之前数据计算出的某个结果。比如，做求和（sum）计算时，需要保存之前所有数据的和，这就是状态；窗口算子中会保存已经到达的所有数据，这些也都是它的状态。

![image-20230313101450533](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_4/image-20230313101450533.png)

如上图所示为有状态算子的一般处理流程，具体步骤如下：

1. 算子任务接收到上游发来的数据；
2. 获取当前状态；
3. 根据业务逻辑进行计算，更新状态；
4. 得到计算结果，输出发送到下游任务。

##### 1.2 状态分类

Flink 内置的很多算子，数据源 source，数据存储 sink 都是有状态的，流中的数据都是 buffer records，会保存一定的元素或者元数据。例如: `ProcessWindowFunction`会缓存输入流的数据，`ProcessFunction` 会保存设置的定时器信息等等。在 Flink 中，状态始终与特定算子相关联。总的来说，有两种类型的状态：

- 算子状态（`operator state`）
- 键控状态（`keyed state`）

###### 1.2.1 算子状态

状态作用范围限定为当前的算子任务实例，也就是只对当前并行子任务实例有效。这就意味着对于一个并行子任务，占据了一个“分区”，它所处理的所有数据都会访问到相同的状态，状态对于同一任务而言是共享的，如图所示。

![image-20230313102420447](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_4/image-20230313102420447.png)

Flink 为算子状态提供三种基本数据结构：

- 列表状态（`List state`）：将状态表示为一组数据的列表。

- 联合列表状态（`Union list state`）

  也将状态表示为数据的列表。它与常规列表状态的区别在于，在发生故障时，或者从保存点（`savepoint`）启动应用程序时如何恢复。

- 广播状态（`Broadcast state`）

  如果一个算子有多项任务，而它的每项任务状态又都相同，那么这种特殊情况最适合应用广播状态。

###### 1.2.2 键控状态

键控状态是根据输入数据流中定义的键（key）来维护和访问的。Flink 为每个键值维护一个状态实例，并将具有相同键的所有数据，都分区到同一个算子任务中，这个任务会维护和处理这个 key 对应的状态。当任务处理一条数据时，它会自动将状态的访问范围限定为当前数据的 key。因此，具有相同 key 的所有数据都会访问相同的状态。`Keyed State` 很类似于一个分布式的 `key-value map` 数据结构，只能用于 `KeyedStream`（`keyBy` 算子处理之后）。如图所示。

![image-20230313102755877](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_4/image-20230313102755877.png)

Flink 的 Keyed State 支持以下数据类型：

- `ValueState[T]`保存单个的值，值的类型为 T。
  - get 操作: `ValueState.value()`
  - set 操作: `ValueState.update(value: T)`
- `ListState[T]`保存一个列表，列表里的元素的数据类型为 T。基本操作如下：
  - `ListState.add(value: T)`
  - `ListState.addAll(values: java.util.List[T])`
  - `ListState.get()返回 Iterable[T]`
  - `ListState.update(values: java.util.List[T])`
- `MapState[K, V]`保存 Key-Value 对。
  - `MapState.get(key: K)`
  - `MapState.put(key: K, value: V)`
  - `MapState.contains(key: K)`
  - `MapState.remove(key: K)`
- `ReducingState[T]`：归约状态
- `AggregatingState[I, O]`： 聚合状态

> 提示：相当于算子状态，键控状态需要考虑key的隔离特性，所以键控状态更丰富，应用场景也比较多，所以，后面我们会重点介绍键控状态。

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_4/image-20210823165604377.png)**注意！**

> - 即使是 `map()、filter()` 这样无状态的基本转换算子，我们也可以通过富函数类给它们“追加”Keyed State，或者实现 `CheckpointedFunction` 接口来定义 `Operator State`；从这个角度讲，Flink 中所有的算子都可以是有状态的，不愧是“有状态的流处理”。
> - 无论是 Keyed State 还是 Operator State，它们都是在本地实例上维护的，也就是说每个并行子任务维护着对应的状态，算子的子任务之间状态不共。

### 任务二、键控状态

#### **【任务目标】**

掌握键控状态的使用过程。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_4/image-20210824110318276.png?fileid=3270835009336643794)

视频-2、键控状态

##### 2.1 键控状态概念和特点

在实际应用中，我们一般都需要将数据按照某个 key 进行分区，然后再进行计算处理；所以最为常见的状态类型就是 `Keyed State`。之前介绍到`keyBy`之后的聚合、窗口计算，算子所持有的状态，都是 `Keyed State`。

键控状态（Keyed State）顾名思义，是任务按照键（key）来访问和维护的状态。它的特点非常鲜明，就是以 key 为作用范围进行隔离。

在进行按键分区之后，具有相同键的所有数据，都会分配到同一个并行子任务中；所以如果当前任务定义了状态，Flink 就会在当前并行子任务实例中，为每个键值维护一个状态的实例。于是当前任务就会为分配来的所有数据，按照 key 维护和处理对应的状态。

在底层，`Keyed State` 类似于一个分布式的映射（map）数据结构，所有的状态会根据 key 保存成键值对（key-value）的形式。这样当一条数据到来时，任务就会自动将状态的访问范围限定为当前数据的 key，从 map 存储中读取出对应的状态值。所以具有相同 key 的所有数据都会到访问相同的状态，而不同 key 的状态之间是彼此隔离的。需要注意，使用 `Keyed State `必须基于 `KeyedStream`。没有进行 `keyBy` 分区的 `DataStream`，即使转换算子实现了对应的富函数类，也不能通过运行时上下文访问` Keyed State`。

##### 2.2 键控状态描述器

在 Flink 中，状态始终是与特定算子相关联的；算子在使用状态前首先需要“注册”，其实就是告诉 Flink 当前上下文中定义状态的信息，这样运行时的 Flink 才能知道算子有哪些状态。

状态的注册，主要是通过“状态描述器”（`StateDescriptor`）来实现的。状态描述器中最重要的内容，就是状态的名称（name）和类型（type）。

以 `ValueState` 为例，我们可以定义值状态描述器如下：

```
val descriptor = new ValueStateDescriptor[Long](
    "my state", // 状态名称
    classOf[Long] // 状态类型
)复制代码
```

这里我们定义了一个叫作“my state”的长整型 `ValueState` 的描述器。

代码中完整的操作是，首先定义出状态描述器；然后调用 `getRuntimeContext()`方法获取运行时上下文；继而调用 `RuntimeContext` 的获取状态的方法，将状态描述器传入，就可以得到对应的状态了。

接下来，我们重点分析`ValueState`的具体使用。

##### 2.3 `ValueState`状态案例

顾名思义，状态中只保存一个“值”（value）。`ValueState<T>`本身是一个接口，源码中定义如下：

```
public interface ValueState<T> extends State {
    T value() throws IOException;
    void update(T value) throws IOException;

}复制代码
```

这里的 T 是泛型，表示状态的数据内容可以是任何具体的数据类型。如果想要保存一个长整型值作为状态，那么类型就是 `ValueState<Long>`。

我们可以在代码中读写值状态，实现对于状态的访问和更新。

- T value()：获取当前状态的值；
- update(T value)：对状态进行更新，传入的参数 value 就是要覆写的状态值。

在具体使用时，为了让运行时上下文清楚到底是哪个状态，我们还需要创建一个“状态描述器”（`StateDescriptor`）来提供状态的基本信息。例如源码中，`ValueState` 的状态描述器构造方法如下：

```
public ValueStateDescriptor(String name, Class<T> typeClass) {
    super(name, typeClass, null);
}复制代码
```

这里需要传入状态的名称和类型——这跟我们声明一个变量时做的事情完全一样。有了这个描述器，运行时环境就可以获取到状态的控制句柄（handler）了。

案例说明：我们经常需要根据用户 id 来进行分流，然后分别统计每个用户的 pv 数据，由于我们并不想每次 pv 加一，就将统计结果发送到下游去，所以这里我们注册了一个定时器，用来隔一段时间发送 pv 的统计结果，这样对下游算子的压力不至于太大。具体实现方式是定义一个用来保存定时器时间戳的值状态变量。当定时器触发并向下游发送数据以后，便清空储存定时器时间戳的状态变量，这样当新的数据到来时，发现并没有定时器存在，就可以注册新的定时器了，注册完定时器之后将定时器的时间戳继续保存在状态变量中。

```
package com.atstudy.chapter03
import org.apache.flink.api.common.state.ValueStateDescriptor
import org.apache.flink.streaming.api.functions.KeyedProcessFunction
import org.apache.flink.streaming.api.scala._
import org.apache.flink.util.Collector

object PeriodicPvExample {
   def main(args: Array[String]): Unit = {
       val env = StreamExecutionEnvironment.getExecutionEnvironment
       env.setParallelism(1)
       env.addSource(new ClickSource)
           .assignAscendingTimestamps(_.timestamp)
           .keyBy(_.user) // 按照用户分组
           .process(new PeriodicPvResult) // 自定义 KeyedProcessFunction 进行处理
           .print()
           env.execute()
   }
   
   // 注册定时器，周期性输出 pv
   class PeriodicPvResult extends KeyedProcessFunction[String, Event, String] {
       // 懒加载值状态变量，用来储存当前 pv 数据
       lazy val countState = getRuntimeContext.getState(
           new ValueStateDescriptor[Long]("count", classOf[Long])
       )
       // 懒加载状态变量，用来储存发送 pv 数据的定时器的时间戳
       lazy val timerTsState = getRuntimeContext.getState(
           new ValueStateDescriptor[Long]("timer-ts", classOf[Long])
       )
   
       override def processElement(value: Event, ctx: KeyedProcessFunction[String, Event, String]#Context, out: Collector[String]): Unit = {
           // 更新 count 值
           val count = countState.value()
           countState.update(count + 1)
           
           // 如果保存发送 pv 数据的定时器的时间戳的状态变量为 0L，则注册一个 10 秒后的定时器
           if (timerTsState.value() == 0L) {
               // 注册定时器
               ctx.timerService.registerEventTimeTimer(value.timestamp + 10 * 1000L)
               // 将定时器的时间戳保存在状态变量中
               timerTsState.update(value.timestamp + 10 * 1000L)
           }
       }
       
       override def onTimer(timestamp: Long, ctx: KeyedProcessFunction[String, Event, String]#OnTimerContext, out: Collector[String]): Unit = {
           // 定时器触发，向下游输出当前统计结果
           out.collect("用户 " + ctx.getCurrentKey + " 的 pv 是：" + countState.value())
           // 清空保存定时器时间戳的状态变量，这样新数据到来时又可以注册定时器了
           timerTsState.clear()
       }
   }
}复制代码
```

### 任务三、容错机制

#### **【任务目标】**

在 Flink 的状态管理机制中，很重要的一个功能就是对状态进行持久化（persistence）保存，这样就可以在发生故障后进行重启恢复，这也就是容错机制，本任务的重点是理解如何通过检查点来实现容错机制。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_4/image-20210824110318276.png?fileid=3270835009336656368)

视频-3、容错机制

##### 3.1 检查点（Checkpoint）

在 Flink 中，有一套完整的容错机制（fault tolerance）来保证故障后的恢复，其中最重要的就是检查点（checkpoint）。

有状态流应用中的检查点（checkpoint），其实就是所有任务的状态在某个时间点的一个快照（一份拷贝）。简单来讲，就是一次“存盘”，让我们之前处理数据的进度不要丢掉。在一个流应用程序运行时，Flink 会定期保存检查点，在检查点中会记录每个算子的 id 和状态；如果发生故障，Flink 就会用最近一次成功保存的检查点来恢复应用的状态，重新启动处理流程，就如同“读档”一样。

默认情况下，检查点是被禁用的，需要在代码中手动开启。直接调用执行环境的`.enableCheckpointing()` 方法就可以开启检查点。

```
val env = StreamExecutionEnvironment.getEnvironment
env.enableCheckpointing(1000)复制代码
```

这里传入的参数是检查点的间隔时间，单位为毫秒。

检查点是 Flink 容错机制的核心。这里所谓的“检查”，其实是针对故障恢复的结果而言的：故障恢复之后继续处理的结果，应该与发生故障前完全一致，我们需要“检查”结果的正确性。所以，有时又会把 checkpoint 叫作“一致性检查点”。

##### 3.2 精确一次状态一致性

精确一次状态一致性：就是如何通过检查点真正做到数据不丢失也能不重复计算数据的过程，这也是我们容错机制的最佳状态。

###### 3.2.1 策略

什么时候进行检查点的保存呢？最理想的情况下，我们应该“随时”保存，也就是每处理完一个数据就保存一下当前的状态；这样如果在处理某条数据时出现故障，我们只要回到上一个数据处理完之后的状态，然后重新处理一遍这条数据就可以。

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_4/image-20210823161648091.png)**随时存档，适合吗？**

> “随时存档”确实恢复起来方便，可是需要我们不停地做存档操作。如果每处理一条数据就进行检查点的保存，当大量数据同时到来时，就会耗费很多资源来频繁做检查点，数据处理的速度就会受到影响。所以更好的方式是，每隔一段时间去做一次存档，这样既不会影响数据的正常处理，也不会有太大的延迟。在 Flink 中，检查点的保存是周期性触发的。

所以检查点作为应用状态的一份“存档”，其实就是所有任务状态在同一时间点的一个“快照”（snapshot），它的触发是周期性的。具体来说，当每隔一段时间检查点保存操作被触发时，就把每个任务当前的状态复制一份，按照一定的逻辑结构放在一起持久化保存起来，就构成了**检查点**。

我们保存状态的策略是：当所有任务都恰好处理完一个相同的输入数据的时候，将它们的状态保存下来。

###### 3.2.2 生成检查点

检查点的保存，最关键的就是要等所有任务将“同一个数据”处理完毕。下面我们通过一个具体的例子，来详细描述一下检查点具体的保存过程。

回忆一下我们最初实现的统计词频的程序——WordCount。这里为了方便，我们直接从数据源读入已经分开的一个个单词，例如这里输入的就是：

```
hello
world
hello
flink
hello
world
hello复制代码
```

对应的代码就可以简化为：

```
val wordCountStream = env.addSource(...)
    .map((_,1))
    .keyBy(_._1)
    .sum(1)复制代码
```

源（Source）任务从外部数据源读取数据，并记录当前的偏移量，作为算子状态（Operator State）保存下来。然后将数据发给下游的 map 任务，它会将一个单词转换成(word, count)二元组，初始 count 都是 1，也就是(“hello”, 1)、(“world”, 1)这样的形式；这是一个无状态的算子任务。进而以 word 作为键（key）进行分区，调用 sum()方法就可以对 count 值进行求和统计了；sum()算子会把当前求和的结果作为按键分区状态（Keyed State）保存下来。最后得到的就是当前单词的频次统计(word, count)，如图所示

![image-20230313133710478](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_4/image-20230313133710478.png)

当我们需要保存检查点（checkpoint）时，就是在所有任务处理完同一条数据后，对状态做个快照保存下来。例如上图中，已经处理了三个数据：“hello”“world”“hello”，所以我们会看到 Source 算子的偏移量为 3；后面的 sum()算子处理完第三条数据“hello”之后，此时已经有 2 个“hello”和 1 个“world”，所以对应的状态为“hello”→ 2，“world”→ 1（这里`KeyedState` 底层会以 key-value 形式存储）。此时所有任务都已经处理完了前三个数据，所以我们可以把当前的状态保存成一个检查点，写入外部存储中。一般情况下，我们会将检查点写入持久化的分布式文件系统。

###### 3.2.3 检查点恢复

在运行流处理程序时，Flink 会周期性地保存检查点。当发生故障时，就需要找到最近一次成功保存的检查点来恢复状态。

例如在上节的 WordCount 示例中，我们处理完三个数据后保存了一个检查点。之后继续运行，又正常处理了一个数据“flink”，在处理第五个数据“hello”时发生了故障，如图 10-3所示。

![image-20230313133908889](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_4/image-20230313133908889.png)

这里 Source 任务已经处理完毕，所以偏移量为 5；map 任务也处理完成了。而 sum 任务在处理中发生了故障，此时状态并未保存。接下来就需要从检查点来恢复状态了。具体的步骤为：

（1）重启应用

遇到故障之后，第一步当然就是重启。我们将应用重新启动后，所有任务的状态会清空，如下图所示。

![image-20230313133929363](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_4/image-20230313133929363.png)

（2）读取检查点，重置状态

找到最近一次保存的检查点，从中读出每个算子任务状态的快照，分别填充到对应的状态中。这样，Flink 内部所有任务的状态，就恢复到了保存检查点的那一时刻，也就是刚好处理完第三个数据的时候，如下图所示。这里 key 为“flink”并没有数据到来，所以初始为 0。

![image-20230313134002437](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_4/image-20230313134002437.png)

（3）重放数据

从检查点恢复状态后还有一个问题：如果直接继续处理数据，那么保存检查点之后、到发生故障这段时间内的数据，也就是第 4、5 个数据（“flink”“hello”）就相当于丢掉了；这会造成计算结果的错误。

为了不丢数据，我们应该从保存检查点后开始重新读取数据，这可以通过 Source 任务向外部数据源重新提交偏移量（offset）来实现，如下图所示。

![image-20230313134025050](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_4/image-20230313134025050.png)

这样，整个系统的状态已经完全回退到了检查点保存完成的那一时刻。

（4）继续处理数据

接下来，我们就可以正常处理数据了。首先是重放第 4、5 个数据，然后继续读取后面的数据，如下图所示。

![image-20230313134038528](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab3_4/image-20230313134038528.png)

当处理到第 5 个数据时，就已经追上了发生故障时的系统状态。之后继续处理，就好像没有发生过故障一样；我们既没有丢掉数据也没有重复计算数据，这就保证了计算结果的正确性。

在分布式系统中，这叫作实现了“精确一次”（exactly-once）的状态一致性保证。





# 实验4-1：TableAPI&SQL快速上手

## 实验概述

在 Flink 提供的多层级 API 中，核心是 DataStream API，底层则是所谓的处理函数（process function），可以访问事件的时间信息、注册定时器、自定义状态，进行有状态的流处理。DataStream API 和处理函数比较通用，有了这些 API，理论上我们就可以实现所有场景的需求了。不过在企业实际应用中，往往会面对大量类似的处理逻辑，所以一般会将底层 API 包装成更加具体的应用级接口。怎样的接口风格最容易让大家接收呢？作为大数据工程师，我们最为熟悉的数据统计方式，当然就是写 SQL 了。

关系型数据库中的 SQL 我们已经非常熟悉了，Flink 同样提供了对于“表”处理的支持，这就是更高层级的应用 API，在 Flink 中被称为Table API 和 SQL。Table API 顾名思义，就是基于“表”（Table）的一套 API，它是内嵌在 Java、Scala 等语言中的一种声明式领域特定语言（DSL），也就是专门为处理表而设计的；在此基础上，Flink 还基于 Apache Calcite 实现了对 SQL 的支持。这样一来，我们就可以在 Flink 程序中直接写 SQL 来实现处理需求了。

## 实验环境

- AtStudy 实训平台
- Flink1.13

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16800721336812648.png)

## 实验目标

学习完成本实验后，您将能够

- 掌握TableAPI及SQL的使用
- 掌握表和流执行的相互转换
- 掌握TableAPI及SQL中的聚合操作

## 实验任务

### 任务一、快速上手

#### **【任务目标】**

如果我们对关系型数据库和 SQL 非常熟悉，那么 Table API 和 SQL 的使用其实非常简单：只要得到一个“表”（Table），然后对它调用 Table API，或者直接写 SQL 就可以了。接下来我们就以一个例子上手，初步了解一下这种高层级 API 的使用方法。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab4_1/image-20210824110318276.png?fileid=3270835009336560532)

视频-1、快速上手

##### 1.1 添加依赖

TableAPI和SQL，在 Flink 中这两种 API 被集成在一起，SQL 执行的对象也是 Flink 中的表（Table），所以我们一般会认为它们是一体的。Flink 是批流统一的处理框架，无论是批处理（DataSet API）还是流处理（DataStream API），在上层应用中都可以直接使用 Table API 或者 SQL 来实现；这两种 API 对于一张表执行相同的查询操作，得到的结果是完全一样的，我们主要还是以流处理应用为例进行讲解。

![image-20230313224602419](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab4_1/image-20230313224602419.png)

所以，在使用TableAPI&SQL之前先需要引入依赖：

```
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-table-api-scala-bridge_${scala.binary.version}</artifactId>
    <version>${flink.version}</version>
</dependency>复制代码
```

这里的依赖是一个 Scala 的“桥接器”（bridge），主要就是负责 Table API 和下层 DataStream API 的连接支持。

如果我们希望在本地的集成开发环境（IDE）里运行 Table API 和 SQL，还需要引入以下依赖：

```
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-table-planner-blink_${scala.binary.version}</artifactId>
    <version>${flink.version}</version>
</dependency>复制代码
```

这里主要添加的依赖是一个“计划器”（planner），它是 Table API 的核心组件，负责提供运行时环境，并生成程序的执行计划。这里我们用到的是新版的 blink planner。由于 Flink 安装包的 lib 目录下会自带 planner，所以在生产集群环境中提交的作业不需要打包这个依赖。

另外，如果想实现自定义的数据格式来做序列化，可以引入下面的依赖：

```
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-table-common</artifactId>
    <version>${flink.version}</version>
</dependency>复制代码
```

##### 1.2 上手案例

有了基本的依赖，接下来我们就可以尝试在 Flink 代码中使用 Table API 和 SQL 了。比如，我们可以自定义一些 Event 类型（包含了 user、url 和 timestamp 三个字段，）的用户访问事件，作为输入的数据源；而后从中提取 url 地址和用户名 user 两个字段作为输出。

如果使用 DataStream API，我们可以直接读取数据源后，用一个简单转换算子 map()来做字段的提取。而这个需求直接写 SQL 的话，实现会更加简单：

```
select url, user from EventTable;复制代码
```

这里我们把流中所有数据组成的表叫作 EventTable。在 Flink 代码中直接对这个表执行上面的 SQL，就可以得到想要提取的数据了。

```
package com.atstudy.chapter04
import com.atstudy.chapter03.Event
import org.apache.flink.streaming.api.scala._
import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment

object TableExample {
    def main(args: Array[String]): Unit = {
        // 获取流执行环境
        val env = StreamExecutionEnvironment.getExecutionEnvironment
        env.setParallelism(1)
        // 读取数据源
        val eventStream = env
        .fromElements(
            Event("Alice", "./home", 1000L),
            Event("Bob", "./cart", 1000L),
            Event("Alice", "./prod?id=1", 5 * 1000L),
            Event("Cary", "./home", 60 * 1000L),
            Event("Bob", "./prod?id=3", 90 * 1000L),
            Event("Alice", "./prod?id=7", 105 * 1000L)
        )
        // 获取表环境
        val tableEnv = StreamTableEnvironment.create(env)
        
        // 将数据流转换成表
        val eventTable = tableEnv.fromDataStream(eventStream)
        
        // 用执行 SQL 的方式提取数据
        val visitTable = tableEnv.sqlQuery("select url, user from " + eventTable)
        
        // 将表转换成数据流，打印输出
        tableEnv.toDataStream(visitTable).print()
        
        // 执行程序
        env.execute()
    }
}复制代码
```

这里我们需要创建一个“表环境”（TableEnvironment），然后将数据流（DataStream）转换成一个表（Table）；之后就可以执行 SQL 在这个表中查询数据了。查询得到的结果依然是一个表，把它重新转换成流就可以打印输出了。

代码执行的结果如下：

```
+I[./home, Alice]
+I[./cart, Bob]
+I[./prod?id=1, Alice]
+I[./home, Cary]
+I[./prod?id=3, Bob]
+I[./prod?id=7, Alice]复制代码
```

可以看到，我们将原始的 Event 数据转换成了(url，user)这样类似二元组的类型。每行输出前面有一个“+I”标志，这是表示每条数据都是“插入”（Insert）到表中的新增数据。

Table 是 Table API 中的核心接口类，对应着我们熟悉的“表”的概念。基于 Table 我们也可以调用一系列查询方法直接进行转换，这就是所谓 Table API 的处理方式：

```
// 用 Table API 方式提取数据
val clickTable2 = eventTable.select($("url"), $("user"))复制代码
```

这里的$符号是 Table API 中定义的“表达式”类 Expressions 中的一个静态方法，传入一个字段名称，就可以指代数据中对应字段，这个方法需要使用如下的方式进行手动导入。

```
import org.apache.flink.table.api.Expressions.$ 复制代码
```

##### 1.3 API介绍

在 Flink 中，Table API 和 SQL 可以看作联结在一起的一套 API，这套 API 的核心概念就是“表”（Table）。在我们的程序中，输入数据可以定义成一张表；然后对这张表进行查询，就可以得到新的表，这相当于就是流数据的转换操作；最后还可以定义一张用于输出的表，负责将处理结果写入到外部系统。

整体结构如下：

```
// 创建表环境
val tableEnv = ...;

// 创建输入表，连接外部系统读取数据
tableEnv.executeSql("CREATE TEMPORARY TABLE inputTable ... WITH ( 'connector' = ... )")

// 注册一个表，连接到外部系统，用于输出
tableEnv.executeSql("CREATE TEMPORARY TABLE outputTable ... WITH ( 'connector' = ... )")

// 执行 SQL 对表进行查询转换，得到一个新的表
val table1 = tableEnv.sqlQuery("SELECT ... FROM inputTable... ")

// 使用 Table API 对表进行查询转换，得到一个新的表
val table2 = tableEnv.from("inputTable").select(...)

// 将得到的结果写入输出表
val tableResult = table1.executeInsert("outputTable")复制代码
```

###### 1.3.1 表定义

具体创建表的方式，有通过连接器（connector）和虚拟表（virtual tables）两种：

- 连接器

  这种方式创建表，主要是针对与一些外部系统，像文件、数据库、Kafka等，创建的时候需要提供一个连接器（connector），然后定义出对应的表结构。

  ```
  tableEnv.executeSql("CREATE [TEMPORARY] TABLE MyTable ... WITH ( 'connector' = ... )")复制代码
  ```

  > 当我们在表环境中读取这张表，连接器就会从外部系统读取数据并进行转换；而当我们向这张表写入数据，连接器就会将数据输出（Sink）到外部系统中。

  例如读取一个文件：

  ```
  tableEnv.executeSql("CREATE TABLE MyTable (
                         uid INT,
                      name STRING
                     ) with ('connector' = 'filesystem', 'path'='input/word.txt', 'format' = 'csv')")复制代码
  ```

  > 注意：由于这里我们使用csv的格式文件，所以需要引入对应的依赖
  >
  > ```
  > <dependency>
  >     <groupId>org.apache.flink</groupId>
  >     <artifactId>flink-csv</artifactId>
  >     <version>${flink.version}</version>
  > </dependency>复制代码
  > ```

  ![image-20230314123654323](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab4_1/image-20230314123654323.png)

- 虚拟表

  ```
  // 将数据流转换成表
  val eventTable = tableEnv.fromDataStream(eventStream)
  
  // 创建一个虚拟表
  tableEnv.createTemporaryView("mytable1", eventTable)
  //注意：这里的第一个参数"mytable1"是注册的表名，而第二个参数 eventTable 是 Table 对象。
  
  // 用执行 SQL 的方式提取数据
  val visitTable = tableEnv.sqlQuery("select url, user from mytable")
  
  // 再次创建一个虚拟表
  tableEnv.createTemporaryView("mytable2", visitTable)复制代码
  ```

  注册为虚拟表之后，我们就又可以在 SQL 中直接使用 NewTable 进行查询转换了。不难看到，通过虚拟表可以非常方便地让 SQL 分步骤执行得到中间结果，这为代码编写提供了很大的便利。

###### 1.3.2 查询

创建好了表，接下来自然就是对表进行查询转换了。对一个表的查询（Query）操作，就对应着流数据的转换（Transformation）处理。

Flink 为我们提供了两种查询方式：SQL 和 Table API

```
tableEnv.createTemporaryView("EventTable", eventTable)

// SQL方式：查询用户 Alice 的点击事件，并提取表中前两个字段
val aliceVisitTable = tableEnv.sqlQuery("SELECT user, url FROM EventTable WHERE user = 'Alice' ")复制代码
```

由于Table API是基于Table的实例进行调用的，因此我们首先要得到表的对象。基于环境中已注册的表，可以通过表环境的 from()方法非常容易地得到一个 Table 对象：

```
//Table API方式：
val envTable = tableEnv.from("EventTable")
val aliceVisitTable = envTable.where($("url").isEqual("./home")).select($("url"), $("user"))复制代码
```

###### 1.3.3 输出

输出一张表最直接的方法，就是调用 Table 的方法 executeInsert()方法将一个Table 写入到注册过的表中，方法传入的参数就是注册的表名

```
// 注册表，用于输出数据到外部系统
tableEnv.executeSql("CREATE TABLE ouputTable (
                       user STRING,
                    url STRING
                   ) with ('connector' = 'filesystem', 'path'='input/output', 'format' = 'csv')")

// 将结果表写入已注册的输出表中
aliceVisitTable.executeInsert("ouputTable")复制代码
```

##### 1.4 流表互换

###### 1.4.1 表转换成流

（1）调用 toDataStream()方法

```
val aliceVisitTable = tableEnv.sqlQuery(
    "SELECT user, url " +
    "FROM EventTable " +
    "WHERE user = 'Alice' "
)

// 将表转换成数据流
tableEnv.toDataStream(aliceVisitTable).print()复制代码
```

（2）调用 toChangelogStream()方法

如果是聚合相关的操作，toDataStream就会报错，这个时候可以选择toChangelogStream

```
val urlCountTable = tableEnv.sqlQuery(
 "SELECT user, COUNT(url) " +
 "FROM EventTable " +
 "GROUP BY user "
 )

// 将表转换成更新日志流
tableEnv.toChangelogStream(urlCountTable).print()复制代码
```

###### 1.4.2 流转换成表

（1）调用 fromDataStream()方法

```
val env = StreamExecutionEnvironment.getExecutionEnvironment
// 获取表环境
val tableEnv = StreamTableEnvironment.create(env)
// 读取数据源
val eventStream = env.addSource(...)
// 将数据流转换成表
val eventTable = tableEnv.fromDataStream(eventStream)复制代码
```

（2）调用 createTemporaryView()方法

```
tableEnv.createTemporaryView("EventTable", eventStream);复制代码
```

### 任务二、聚合查询

#### **【任务目标】**

在 SQL 中，一个很常见的功能就是对某一列的多条数据做一个合并统计，得到一个或多个结果值；比如求和、最大最小值、平均值等等，这种操作叫作聚合（Aggregation）查询，这里主要是掌握分组聚合和窗口聚合及TopN经典聚合过程。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab4_1/image-20210824110318276.png?fileid=3270835009336832421)

视频-2、聚合查询

##### 2.1 分组聚合

SQL 中一般所说的聚合我们都很熟悉，主要是通过内置的一些聚合函数来实现的，比如SUM()、MAX()、MIN()、AVG()以及 COUNT()。它们的特点是对多条输入数据进行计算，得到一个唯一的值，属于“多对一”的转换。比如我们可以通过下面的代码计算输入数据的个数：

```
val eventCountTable = tableEnv.sqlQuery("select COUNT(*) from EventTable")复制代码
```

而更多的情况下，我们可以通过 GROUP BY 子句来指定分组的键（key），从而对数据按照某个字段做一个分组统计。例如之前我们举的例子，可以按照用户名进行分组，统计每个用户点击 url 的次数：

```
val eventCountTable = tableEnv.sqlQuery("SELECT user, COUNT(url) as cnt FROM EventTable GROUP BY user")复制代码
```

这种聚合方式，就叫作“分组聚合”（group aggregation）。

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab4_1/image-20210823165604377.png)**注意！**

> 在流处理中，分组聚合是一个持续查询，而且还是一个更新查询，得到的是一个动态表；每当流中有一个新的数据到来时，都会导致结果表的更新操作。因此，想要将结果表转换成流或输出到外部系统，必须采用撤回流（retract stream）或更新插入流（upsert stream）的编码方式；如果在代码中直接转换成DataStream 打印输出，需要调用 toChangelogStream()

##### 2.2 窗口聚合

在流处理中，往往需要将无限数据流划分成有界数据集，这就是所谓的“窗口”。

从 1.13 版本开始，Flink 开始使用窗口表值函数（Windowing table-valued functions，Windowing TVFs）来定义窗口。窗口表值函数是 Flink 定义的多态表函数（PTF），可以将表进行扩展后返回。表函数（table function）可以看作是返回一个表的函数。

在窗口表值函数的返回值中，除去原始表中的所有列，还增加了用来描述窗口的额外 3个列：“窗口起始点”（window_start）、“窗口结束点”（window_end）、“窗口时间”（window_time）。起始点和结束点比较好理解，这里的“窗口时间”指的是窗口中的时间属性，它的值等于`window_end - 1ms`，所以相当于是窗口中能够包含数据的最大时间戳。

目前 Flink 提供了以下几个窗口 TVF：

- 滚动窗口（Tumbling Windows）；
- 滑动窗口（Hop Windows，跳跃窗口）；
- 累积窗口（Cumulate Windows）；
- 会话窗口（Session Windows，目前尚未完全支持）。

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab4_1/image-20210823161648091.png)**累计窗口？**

> 滚动窗口和滑动窗口，可以用来计算大多数周期性的统计指标。不过在实际应用中还会遇到这样一类需求：我们的统计周期可能较长，因此希望中间每隔一段时间就输出一次当前的统计值；与滑动窗口不同的是，在一个统计周期内，我们会多次输出统计值，它们应该是不断叠加累积的。

在 Flink 的 Table API 和 SQL 中，窗口的计算是通过“窗口聚合”（window aggregation）来实现的。与分组聚合类似，窗口聚合也需要调用 SUM()、MAX()、MIN()、COUNT()一类的聚合函数，通过 GROUP BY 子句来指定分组的字段。

只不过窗口聚合时，需要将窗口信息作为分组 key 的一部分定义出来，窗口本身返回的是就是一张表，窗口需要出现在 FROM 后面，GROUP BY 后面的则是窗口新增的字段 window_start 和 window_end。

```
val result = tableEnv.sqlQuery(
    "SELECT " +
        "user, " +
        "window_end AS endT, " +
        "COUNT(url) AS cnt " +
    "FROM TABLE( " +     //窗口表函数
            "TUMBLE( TABLE EventTable, " +
            "DESCRIPTOR(ts), " +		   // ts 作为时间属性字段
            "INTERVAL '1' HOUR)) " +       //1 个小时
    "GROUP BY user, window_start, window_end "  //window_start, window_end 窗口信息
)复制代码
```

> 这里我们以 ts 作为时间属性字段、基于 EventTable 定义了 1 小时的滚动窗口，希望统计出每小时每个用户点击 url 的次数。用来分组的字段是用户名 user，以及表示窗口的window_start 和 window_end；而 TUMBLE()是表值函数，所以得到的是一个表（Table），我们的聚合查询就是在这个 Table 中进行的。

案例：我们需要统计周期为 1 小时、累积间隔为 30 分钟的累积窗口，实现如下：

```
package com.atstudy.chapter04
import com.atstudy.chapter03.Event

import org.apache.flink.streaming.api.scala._
import org.apache.flink.table.api.Expressions.$
import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment

object CumulateWindowExample {
    def main(args: Array[String]): Unit = {
        val env = StreamExecutionEnvironment.getExecutionEnvironment
        env.setParallelism(1)
        // 读取数据源，并分配时间戳、生成水位线
        val eventStream = env.fromElements(
            Event("Alice", "./home", 1000L),
            Event("Bob", "./cart", 1000L),
            Event("Alice", "./prod?id=1", 25 * 60 * 1000L),
            Event("Alice", "./prod?id=4", 55 * 60 * 1000L),
            Event("Bob", "./prod?id=5", 3600 * 1000L + 60 * 1000L),
            Event("Cary", "./home", 3600 * 1000L + 30 * 60 * 1000L),
            Event("Cary", "./prod?id=7", 3600 * 1000L + 59 * 60 * 1000L)
        ).assignAscendingTimestamps(_.timestamp)
        
        // 创建表环境
        val tableEnv = StreamTableEnvironment.create(env)
        
        // 将数据流转换成表，并指定时间属性
        val eventTable = tableEnv.fromDataStream(
            eventStream,
            $("user"),
            $("url"),
            $("timestamp").rowtime().as("ts")
        )
        
        // 为方便在 SQL 中引用，在环境中注册表 EventTable
        tableEnv.createTemporaryView("EventTable", eventTable);
        
        // 设置累积窗口，执行 SQL 统计查询
        val result = tableEnv
        .sqlQuery(
            "SELECT " +
                "user, " +
                "window_end AS endT, " +
                "COUNT(url) AS cnt " +
            "FROM TABLE( " +
                    "CUMULATE( TABLE EventTable, " + // 定义累积窗口
                    "DESCRIPTOR(ts), " +     
                    "INTERVAL '30' MINUTE, " +     // 30 分钟的累积
                    "INTERVAL '1' HOUR)) " +       // 周期为 1 小时
            "GROUP BY user, window_start, window_end "
        )
        tableEnv.toDataStream(result).print()
        env.execute()
    }
}复制代码
```

代码执行结果如下：

```
+I[Alice, 1970-01-01T00:30, 2]
+I[Bob, 1970-01-01T00:30, 1]
+I[Alice, 1970-01-01T01:00, 3]
+I[Bob, 1970-01-01T01:00, 1]
+I[Bob, 1970-01-01T01:30, 1]
+I[Cary, 1970-01-01T02:00, 2]
+I[Bob, 1970-01-01T02:00, 1]复制代码
```

结果分析：

输入的前三条数据属于第一个半小时的累积窗口，其中 Alice 的访问数据有两条，Bob 的访问数据有 1 条，所以输出了两条结果[Alice, 1970-01-01T00:30, 2]和[Bob, 1970-01-01T00:30, 1]；而之后又到来的一条 Alice 访问数据属于第二个半小时范围，同时也属于第一个 1 小时的统计周期，所以会在之前两条的基础上进行叠加，输出 [Alice, 1970-01-01T00:30, 3]，而 Bob 没有新的访问数据，因此依然输出[Bob, 1970-01-01T00:30, 1]。从第二个小时起，数据属于新的统计周期，就全部从零开始重新计数了。

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab4_1/image-20210823165604377.png)**注意：窗口聚合查询都属于追加查询！**

> 与分组聚合不同，窗口聚合不会将中间聚合的状态输出，只会最后输出一个结果。所有数据都是以 INSERT 操作追加到结果动态表中的，因此输出每行前面都有`+I` 的前缀。所以窗口聚合查询都属于追加查询，没有更新操作，代码中可以直接用 toDataStream()将结果表转换成流。

##### 2.3 应用实例 — Top N

例如电商行业，实际应用中往往有这样的需求：`统计一段时间内的热门商品`。这就需要先开窗口，在窗口中统计每个商品的点击量；然后将统计数据收集起来，按窗口进行分组，并按点击量大小降序排序，选取前 N 个作为结果返回。

具体实现思路如下：

- 先做一个窗口聚合，将窗口信息 window_start、window_end 连同每个商品的点击量一并返回，这样就得到了聚合的结果表，包含了窗口信息、商品和统计的点击量。
- 接下来定义 OVER 窗口，按窗口分组，按点击量排序，用ROW_NUMBER()统计行号并筛选前 N 行就可以得到结果。

提示：由于用户访问事件 Event 中没有商品相关信息，因此我们统计的是每小时内有最多访问行为的用户，取前两名，相当于是一个每小时活跃用户的查询。

```
package com.atstudy.chapter04
import com.atstudy.chapter03.Event
import org.apache.flink.streaming.api.scala._
import org.apache.flink.table.api.Expressions.$
import org.apache.flink.table.api.bridge.scala.StreamTableEnvironment

object WindowTopNExample {
    def main(args: Array[String]): Unit = {
        val env = StreamExecutionEnvironment.getExecutionEnvironment
        env.setParallelism(1)
        // 读取数据源，并分配时间戳、生成水位线
        val eventStream = env.fromElements(
            Event("Alice", "./home", 1000L),
            Event("Bob", "./cart", 1000L),
            Event("Alice", "./prod?id=1", 25 * 60 * 1000L),
            Event("Alice", "./prod?id=4", 55 * 60 * 1000L),
            Event("Bob", "./prod?id=5", 3600 * 1000L + 60 * 1000L),
            Event("Cary", "./home", 3600 * 1000L + 30 * 60 * 1000L),
            Event("Cary", "./prod?id=7", 3600 * 1000L + 59 * 60 * 1000L)
        ).assignAscendingTimestamps(_.timestamp)
        
        // 创建表环境
        val tableEnv = StreamTableEnvironment.create(env)
        
        // 将数据流转换成表，并指定时间属性
        val eventTable = tableEnv.fromDataStream(
            eventStream,
            $("user"),
            $("url"),
            $("timestamp").rowtime().as("ts")
            // 将 timestamp 指定为事件时间，并命名为 ts
        )
        
        // 为方便在 SQL 中引用，在环境中注册表 EventTable
        tableEnv.createTemporaryView("EventTable", eventTable)
        
        // 定义子查询，进行窗口聚合，得到包含窗口信息、用户以及访问次数的结果表
        val subQuery =
            "SELECT window_start, window_end, user, COUNT(url) as cnt " +
                "FROM TABLE ( " +
            		"TUMBLE( TABLE EventTable, DESCRIPTOR(ts), INTERVAL '1' HOUR )) " +
            "GROUP BY window_start, window_end, user "
        
        // 定义 Top N 的外层查询
        val topNQuery ="SELECT * " +
        "FROM (" +
            "SELECT *, " +
            "ROW_NUMBER() OVER ( " +
            "PARTITION BY window_start, window_end " +
                	"ORDER BY cnt desc " +
        ") AS row_num " +
        "FROM (" + subQuery + ")) " +
        "WHERE row_num <= 2"
        
        // 执行 SQL 得到结果表
        val result = tableEnv.sqlQuery(topNQuery)
        tableEnv.toDataStream(result).print()
        env.execute()
    }
}复制代码
```

（1）首先基于 ts 时间字段定义 1 小时滚动窗口，统计 EventTable 中每个用户的访问次数，重命名为 cnt；为了方便后面做排序，我们将窗口信息 window_start 和 window_end 也提取出来，与 user 和 cnt 一起作为聚合结果表中的字段。

（2）然后套用 Top N 模板，对窗口聚合的结果表中每一行数据进行 OVER 聚合统计行号。这里以窗口信息进行分组，按访问次数 cnt 进行排序，并筛选行号小于等于 2 的数据，就可以得到每个窗口内访问次数最多的前两个用户了。

运行结果如下：

```
+I[1970-01-01T00:00, 1970-01-01T01:00, Alice, 3, 1]
+I[1970-01-01T00:00, 1970-01-01T01:00, Bob, 1, 2]
+I[1970-01-01T01:00, 1970-01-01T02:00, Cary, 2, 1]
+I[1970-01-01T01:00, 1970-01-01T02:00, Bob, 1, 2]复制代码
```

过程分析：可以看到，第一个 1 小时窗口中，Alice 有 3 次访问排名第一，Bob 有 1 次访问排名第二；而第二小时内，Cary 以 2 次访问占据活跃榜首，Bob 仍以 1 次访问排名第二。由于窗口的统计结果只会最终输出一次，所以排名也是确定的，这里结果表中只有插入（INSERT）操作。也就是说，窗口 Top N 是追加查询，可以直接用 toDataStream()将结果表转换成流打印输出。
