# hive-3.1.2分布式搭建文档

## 1、上传解压配置环境变量

```shell
# 1、解压
tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /usr/local/soft/

# 2、重名名
mv apache-hive-3.1.2-bin hive-3.1.2

# 3、配置环境变量
vim /etc/profile

# 4、在最后增加配置
export HIVE_HOME=/usr/local/soft/hive-3.1.2
export PATH=$PATH:$HIVE_HOME/bin

# 5、使环境变量剩下
source /etc/profile
```

## 2、修改配置文件

### 1、进入hive配置文件所在目录

```shell
cd /usr/local/soft/hive-3.1.2/conf
```

### 2、创建hive-site.xml配置文件

vim hive-site.xml

```xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.cj.jdbc.Driver</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://master:3306/hive?useSSL=false&amp;createDatabaseIfNotExist=true&amp;characterEncoding=utf8&amp;useUnicode=true</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>root</value>
  </property>
  <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>123456</value>
  </property>
  <property>
    <name>hive.metastore.uris</name>
    <value>thrift://master:9083</value>
  </property>
  <property>
	<name>hive.server2.enable.doAs</name>
	<value>false</value>
  </property>
    
<property>
<name>hive.querylog.location</name>
<value/>
</property>

 

（同上）
<property>
<name>hive.exec.local.scratchdir</name>
<value/>
</property>

 

（同上）
<property>
<name>hive.downloaded.resources.dir</name>
<value/>
</property>
</configuration>
```

### 3、创建log4j.properties配置文件

```properties
# 将日志级别改成WARN，避免执行sql出现很多日志
log4j.rootLogger=WARN,CA
log4j.appender.CA=org.apache.log4j.ConsoleAppender
log4j.appender.CA.layout=org.apache.log4j.PatternLayout
log4j.appender.CA.layout.ConversionPattern=%-4r [%t] %-5p %c %x - %m%n%
```

## 3、上传mysql驱动

```shell
# 将nysql驱动包上传到hive的lib目录下
mysql-connector-java-8.0.29.jar
```

### 1.3.6	将hadoop的jline-0.9.94.jar的jar替换成hive的版本。

```
cp /usr/local/soft/hive-3.1.2/lib/jline-2.12.jar /usr/local/soft/hadoop-3.1.1/share/hadoop/yarn/lib/
```

## 4、初始化hive元数据库

```shell
# 2、初始化hive的元数据(表结构)到mysql中
schematool -dbType mysql -initSchema
```

## 5、启动hive元数据服务

```shell
# 后台启动元数据服务
nohup hive --service metastore &
```

## 6、进入hive命令行

```shell
# 进入hive
hive

# 测试
# 1、创建表
CREATE EXTERNAL TABLE IF NOT EXISTS student(
    id string ,
    `name` string ,
    age string  ,
    gender string  ,
    clazz string 
) 
ROW FORMAT DELIMITED 
    FIELDS TERMINATED BY ',' 
STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' 
    OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'  
location '/data/student'; 

# 2、执行sql查询数据
select clazz,count(1) as num from student group by clazz;
```

## 解决 log4j 兼容性问题

> 警告信息，不影响使用
> LF4J: Class path contains multiple SLF4J bindings.

````shell
cd /usr/local/soft/hive-3.1.2/lib

# hive 与 Hadoop 在运行时会出现 log4j 兼容性问题，这是因为 hive 的 log4j 版本与 Hadoop 的产生了冲突，我们这里将 hive 的 log4j 设置为备份。
mv log4j-slf4j-impl-2.10.0.jar log4j-slf4j-impl-2.10.0.jar.bak

hadoop的etc/hadoop/core-site.xml
<property>
        <name>hadoop.proxyuser.root.hosts</name>
        <value>*</value>
</property>
<property>
        <name>hadoop.proxyuser.root.groups</name>
        <value>*</value>
</property>
````

## hive常见问题

### 1、hadoop未启动，就开始执行hive命令

![image-20240408112306610](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20240408112306610.png)

> 解决方案：启动hadoop

### 2、hadoop处于安全模式

![](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20240408112306610.png)

> 解决方案：
>
> 1、等着，等hadoop日志和快照数据恢复完毕，自动离开安全模式
>
> 2、使用命令强制离开

### 3、元数据服务没开，操作不了sql语句

![image-20240408112344560](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20240408112344560.png)

> 解决方案：nohup hive --service metastore &





# Hive3.1.2概述与基本操作

## 1、Hive基本概念

### 1.1	Hive简介

![image-20220531201458239](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220531201458239.png)

Hive本质是将SQL转换为MapReduce的任务进行运算，底层由HDFS来提供数据存储，说白了hive可以理解为一个将SQL转换为MapReduce的任务的工具，甚至更近一步说hive就是一个MapReduce客户端。

思考：计算文件user.txt中张三出现几次，使用mapreduce怎么写，然后再比照下图的hive实现过程（**画图带同学理解**）

```
面试题：什么是hive?
1、hive是数据仓库建模的工具之一。
2、可以向hive传入一条交互式的sql,在海量数据中查询分析得到结果的平台。
```

> 带同学画图

**为什么使用Hive?**

> 如果直接使用hadoop的话，人员学习成本太高，项目要求周期太短，MapReduce实现复杂查询逻辑开发难度太大。如果使用hive的话，可以操作接口采用类SQL语法，提高开发能力，免去了写MapReduce，减少开发人员学习成本，功能扩展很方便（比如：开窗函数）。

**Hive的特点：**

> 1、可扩展性
>
> ​	Hive可以自由的扩展集群的规模，一般情况下不需要重启服务
>
> 2、延申性
>
> ​	Hive支持自定义函数，用户可以根据自己的需求来实现自己的函数
>
> 3、容错
>
> ​	即使节点出现错误，SQL仍然可以完成执行

**Hive的优缺点：**

> **优点：**
>
> ​	1、操作接口采用类sql语法，提供快速开发的能力（简单、容易上手）
>
> ​	2、避免了去写MapReduce,减少开发人员的学习成本
>
> ​	3、Hive的延迟性比较高，因此Hive常用于数据分析，适用于对实时性要求不高的场合
>
> ​	4、Hive 优势在于处理大数据，对于处理小数据没有优势，因为 Hive 的执行延迟比较高。（不断地开关JVM虚拟机）
>
> ​	5、Hive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。
>
> ​	6、集群可自由扩展并且具有良好的容错性，节点出现问题SQL仍可以完成执行
>
> **缺点：**
>
> ​	1、Hive的HQL表达能力有限
>
> ​			（1）迭代式算法无法表达    （反复调用，mr之间独立，只有一个map一个reduce，反复开关）                
>
> ​			（2）数据挖掘方面不擅长
>
> ​	2、Hive 的效率比较低
>
> ​				（1）Hive 自动生成的 MapReduce 作业，通常情况下不够智能化
>
> ​				（2）Hive 调优比较困难，粒度较粗   （hql根据模板转成mapreduce，不能像自己编写mapreduce一样精细，无法控制在map处理数据还是在reduce处理数据）

**Hive和传统数据库对比**

> hive和mysql什么区别？

![image-20220531213145918](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220531213145918.png)

**Hive应用场景**

> 日志分析：大部分互联网公司使用hive进行日志分析，如百度、淘宝等。
>
> ​	统计一个网站一个时间段内的**pv,uv，SKU,SPU,SKC**
>
> ​	多维度数据分析（**数据仓库**）
>
> 海量结构化数据离线分析
>
> **构建数据仓库**

```
PV（Page View）访问量, 即页面浏览量或点击量，衡量网站用户访问的网页数量；在一定统计周期内用户每打开或刷新一个页面就记录1次，多次打开或刷新同一页面则浏览量累计。

UV（Unique Visitor）独立访客，统计1天内访问某站点的用户数(以cookie为依据);访问网站的一台电脑客户端为一个访客。可以理解成访问某网站的电脑的数量。网站判断来访电脑的身份是通过来访电脑的cookies实现的。如果更换了IP后但不清除cookies，再访问相同网站，该网站的统计中UV数是不变的。如果用户不保存cookies访问、清除了cookies或者更换设备访问，计数会加1。00:00-24:00内相同的客户端多次访问只计为1个访客。
```

### 1.2	Hive架构

![image-20220531214038409](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220531214038409.png)

####  1.2.1	Client

> Hive允许client连接的方式有三个CLI（hive shell）、JDBC/ODBC(java访问hive)、WEBUI（浏览器访问 hive）。JDBC访问时中间件Thrift软件框架，跨语言服务开发。DDL DQL DML,整体仿写一套SQL语句。
>
> ​		1）client–需要下载安装包
>
> ​		2）JDBC/ODBC 也可以连接到Hive
> ​				现在主流都在倡导第二种 HiveServer2/beeline
> ​				做基于用户名和密码安全的一个校验
>
> ​		3）Web Gui
> ​				hive给我们提供了一套简单的web页面
> ​				我们可以通过这套web页面访问hive 做的太简陋了

#### 1.2.2	Metastore

> **元数据**包括表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是 外部表）、表的数据所在目录等。
>
> ​		一般需要借助于其他的数据载体（数据库）
>
> ​		主要用于存放数据库的建表语句等信息
>
> ​		推荐使用Mysql数据库存放数据
>
> ​		连接数据库需要提供：uri username password driver

#### 1.2.3	Driver（面试题：sql语句是如何转化成MR任务的？）

> 元数据存储在数据库中，默认存在自带的derby数据库（单用户局限性）中，推荐使用Mysql进行存储。
>
> ​			1） 解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST（从3.x版本之后，转换成一些的stage），这一步一般都用第三方工具库完 成，比如ANTLR；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。
>
> ​			2） 编译器（Physical Plan）：将AST编译（从3.x版本之后，转换成一些的stage）生成逻辑执行计划。
>
> ​			3） 优化器（Query Optimizer）：对逻辑执行计划进行优化。
>
> ​			4） 执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是 MR/Spark/flink。

#### 1.2.4	数据处理

> Hive的数据存储在HDFS中，计算由MapReduce完成。HDFS和MapReduce是源码级别上的整合，两者结合最佳。解释器、编译器、优化器完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。

### 1.3	Hive的安装

> 在之前博客中我有记录安装JDK和Hadoop和Mysql的过程，如果还没有安装，请先进行安装配置好，对应的随笔我也提供了百度云下载连接。
>
> 安装JDK:  https://www.cnblogs.com/wyh-study/p/12014368.html
>
> 安装Hadoop  https://www.cnblogs.com/wyh-study/p/12043948.html
>
> 安装Mysql   https://www.cnblogs.com/wyh-study/p/12044652.html
>
> （注意：安装mysql的时候一定要确保已经执行 ：

#### 1.3.1	上传压缩包并解压

```shell
tar -zxvf apache-hive-1.2.1-bin.tar.gz
```

#### 1.3.2	修改目录名称

```shell
mv apache-hive-1.2.1-bin hive-1.2.1
```

#### 1.3.3	备份配置文件

```shell
cp hive-env.sh.template hive-env.sh

cp hive-default.xml.template hive-site.xml
```

#### 1.3.4	**修改配置hive的配置文件（在conf目录下）**

> **修改hive-env,sh**
>
> 加入三行内容（大家根据自己的情况来添加,每个人安装路径可能有所不同）

```shell
HADOOP_HOME=/usr/local/soft/hadoop-2.7.6
JAVA_HOME=/usr/local/soft/jdk1.8.0_171
HIVE_HOME=/usr/local/soft/hive-1.2.1
```

> **修改hive-site.xml (找到对应的键对值进行修改，注意！！！是修改，而不是全部直接复制粘贴)**

```xml
<！--数据存储位置就是我们在HDFS上看的目录-->
<property>
    <name>hive.metastore.warehouse.dir</name>
    <value>/user/hive/warehouse</value>
</property>

(注意：修改自己安装mysql的主机地址）
<property>
<name>javax.jdo.option.ConnectionURL</name>
<value>jdbc:mysql://192.168.40.110:3306/hive?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false</value>
</property>

(固定写法，mysql驱动类的位置)
<property>
<name>javax.jdo.option.ConnectionDriverName</name>
<value>com.mysql.jdbc.Driver</value>
</property>

（mysql的用户名）
<property>
<name>javax.jdo.option.ConnectionUserName</name>
<value>root</value>
</property>


（mysql的用户密码）
<property>
<name>javax.jdo.option.ConnectionPassword</name>
<value>123456</value>
</property>
    
（你的hive安装目录的tmp目录）

<property>

<name>hive.querylog.location</name>

<value>/usr/local/soft/hive-1.2.1/tmp</value>

</property>

 

（同上）

<property>

<name>hive.exec.local.scratchdir</name>

<value>/usr/local/soft/hive-1.2.1/tmp</value>

</property>

 

（同上）

<property>

<name>hive.downloaded.resources.dir</name>

<value>/usr/local/soft/hive-1.2.1/tmp</value>

</property>


<!--指定这个的时候，为了启动metastore服务的时候不用指定端口-->
<!--hive --service metastore -p 9083 & | hive --service metastore-->
<property>
    <name>hive.metastore.uris</name>
    <value>thrift://master:9083</value>
    <description>thrift://master:9083</description>
</property>
```

> 修改core-site.xml   直接改，改完重启就行，为后面beeline连接做准备
>
> **注意：三个节点上的都要改。**

```xml
<!--该参数表示可以通过httpfs接口hdfs的ip地址限制-->
<property>
 <name>hadoop.proxyuser.root.hosts</name>
 <value>*</value>
</property>
<!--通过httpfs接口访问的用户获得的群组身份-->
<property>
 <name>hadoop.proxyuser.root.groups</name>
 <value>*</value>
</property>
```

#### 1.3.5	拷贝mysql驱动到$HIVE_HOME/lib目录下

```shell
cp /usr/local/soft/mysql-connector-java-5.1.49.jar ../lib/
```

#### 1.3.6	将hadoop的jline-0.9.94.jar的jar替换成hive的版本。

```shell
cp /usr/local/soft/hive-3.1.2/lib/jline-2.12.jar /usr/local/soft/hadoop-3.1.1/share/hadoop/yarn/lib/
```

#### 1.3.7	**将hive的bin目录配置到环境变量中去**

```shell
export HIVE_HOME=/usr/local/soft/hive-1.2.1
export PATH=.:$HIVE_HOME/bin
```

![img](https://img2018.cnblogs.com/i-beta/1479352/201912/1479352-20191220201924356-507914917.png)

#### 1.3.8	source命令让环境变量生效

![img](https://img2018.cnblogs.com/i-beta/1479352/201912/1479352-20191220201951825-389882979.png)

#### 1.3.9	拷贝到其他两个节点中去，因为可能我们会在其他的节点上当作客户端访问hive，注意，也需要配置环境变量，增加驱动jar包，将hadoop的jline-0.9.94.jar的jar替换成hive的版本

#### 1.3.10	启动

> **启动hadoop**
>
> start-all.sh
>
> **启动hive**
>
> ​	hive --service metastore
>
> ​	nohup hive --service metastore  &
>
> ​	hive
>
> **启动HiveServer2**
>
> ​	hiveserver2
>
> ​	nohup hiveserver2 >/dev/null &
>
> ​	beeline -u jdbc:hive2://master:10000 -n root

### 1.4	Hive的三种交互方式

**1）第一种交互方式**

> shell交互Hive，用命令hive启动一个hive的shell命令行，在命令行中输入sql或者命令来和Hive交互。

```
服务端启动metastore服务（后台启动）：nohup hive --service metastore >/dev/null &
进入命令:hive
退出命令行：quit;
```

**2）第二种交互方式**

> **Hive启动为一个服务器，对外提供服务**，其他机器可以通过客户端通过协议连接到服务器，来完成访问操作，这是生产环境用法最多的

```shell
服务端启动hiveserver2服务：
nohup hive --service metastore >/dev/null &
nohup hiveserver2 &

需要稍等一下，启动服务需要时间：
进入命令:1)先执行： beeline ，再执行： !connect jdbc:hive2://master:10000 
        2)或者直接执行：  beeline -u jdbc:hive2://master:10000 -n root
退出命令行：！exit
```

**3）第三种交互方式**

> 使用 –e 参数来直接执行hql的语句

```shell
bin/hive -e "show databases;"
```

> 使用 –f 参数通过指定文本文件来执行hql的语句
>
> 特点：执行完sql后，回到linux命令行。

```shell
vim hive.sql

create database bigdata29;
use bigdata29;

create table test1
(
    id bigint,
    name string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
show tables;
```

```shell
hive -f hive.sql
```

4）hive cli和beeline cli的区别

![image-20220531230402802](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220531230402802.png)

### 1.5	Hive元数据

**Hive元数据库中一些重要的表结构及用途**，方便Impala、SparkSQL、Hive等组件访问元数据库的理解。

1、存储Hive版本的**元数据表(VERSION)**，该表比较简单，但很重要,如果这个表出现问题，根本进不来Hive-Cli。比如该表不存在，当启动Hive-Cli的时候，就会报错“Table 'hive.version' doesn't exist”

2、Hive数据库相关的元数据表(DBS、DATABASE_PARAMS)

​		**DBS**：该表存储Hive中所有数据库的基本信息。

​		DATABASE_PARAMS：该表存储数据库的相关参数。

3、Hive表和视图相关的元数据表

​		主要有TBLS、TABLE_PARAMS、TBL_PRIVS，这三张表通过TBL_ID关联。
​		**TBLS**:该表中存储Hive表，视图，索引表的基本信息。
​		TABLE_PARAMS:该表存储表/视图的属性信息。
​		TBL_PRIVS：该表存储表/视图的授权信息。
4、Hive文件存储信息相关的元数据表

​		主要涉及SDS、SD_PARAMS、SERDES、SERDE_PARAMS，由于HDFS支持的文件格式很多，而建Hive表时候也可以指定各种文件格式，Hive在将HQL解析成MapReduce时候，需要知道去哪里，使用哪种格式去读写HDFS文件，而这些信息就保存在这几张表中。
​		**SDS**：该表保存文件存储的基本信息，如INPUT_FORMAT、OUTPUT_FORMAT、是否压缩等。TBLS表中的SD_ID与该表关联，可以获取Hive表的存储信息。
​		SD_PARAMS: 该表存储Hive存储的属性信息。
​		SERDES:该表存储序列化使用的类信息。
​		**SERDE_PARAMS**:该表存储序列化的一些属性、格式信息，比如:行、列分隔符。
5、Hive表字段相关的元数据表

​		主要涉及COLUMNS_V2：该表存储表对应的字段信息。

## 2、Hive的基本操作

### 2.1	Hive库操作

#### 2.1.1	创建数据库

> 1）创建一个数据库，数据库在**HDFS上的默认存储路径是/hive/warehouse/\*.db**。

```sql
create database testdb;
```

> 2）避免要创建的数据库已经存在错误，增加if not exists判断。**（标准写法）**

```sql
create database [if not exists] testdb; 

create database if not exists testdb; 
```

#### 2.2.2	创建数据库和位置

```sql
create database if not exists bigdata29_test1 location '/bigdata29/shangpingdb';
```

#### 2.2.3	修改数据库

> **数据库的其他元数据信息都是不可更改的**，包括数据库名和数据库所在的目录位置。（**重点关注哪些不能改，以及为什么！！**）

```sql
alter database dept set dbproperties('createtime'='20220531');
```

#### 2.2.4	数据库详细信息

> 1）显示数据库（show）

```sql
show databases;
```

> 2）可以通过like进行过滤

```sql
show databases like 't*';
```

> 3）查看详情（desc）

```sql
desc database testdb;
```

> 4）切换数据库（use）

```sql
use testdb;
```

#### 2.2.5	删除数据库（将删除的目录移动到回收站中）

> 1）最简写法

```sql
drop database testdb;
```

> 2）如果删除的数据库不存在，最好使用if exists判断数据库是否存在。否则会报错：FAILED: SemanticException [Error 10072]: Database does not exist: db_hive

```sql
drop database if exists testdb;
```

> 3)如果数据库不为空，使用cascade命令进行强制删除。报错信息如下FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database db_hive is not empty. One or more tables exist.)

```sql
drop database if exists testdb cascade;
```

### 2.2	Hive数据类型

#### 2.2.1	基础数据类型

| 类型                         | Java数据类型 | 描述                                                         |
| ---------------------------- | ------------ | ------------------------------------------------------------ |
| TINYINT                      | byte         | 8位有符号整型。取值范围：-128~127。                          |
| SMALLINT                     | short        | 16位有符号整型。取值范围：-32768~32767。                     |
| INT                          | int          | 32位有符号整型。取值范围：-2 31 ~2 31 -1。                   |
| **BIGINT**                   | long         | 64位有符号整型。取值范围：-2 63 +1~2 63 -1。                 |
| BINARY                       |              | 二进制数据类型，目前长度限制为8MB。                          |
| FLOAT                        | float        | 32位二进制浮点型。                                           |
| DOUBLE                       | double       | 64位二进制浮点型。                                           |
| **DECIMAL(precision,scale)** |              | 10进制精确数字类型。precision：表示最多可以表示多少位的数字。取值范围：1 <= precision <= 38。scale：表示小数部分的位数。取值范围： 0 <= scale <= 38。如果不指定以上两个参数，则默认为decimal(10,0)。 |
| VARCHAR(n)                   |              | 变长字符类型，n为长度。取值范围：1~65535。                   |
| CHAR(n)                      |              | 固定长度字符类型，n为长度。最大取值255。长度不足则会填充空格，但空格不参与比较。 |
| **STRING**                   | string       | 字符串类型，目前长度限制为8MB。                              |
| DATE                         |              | 日期类型，格式为`yyyy-mm-dd`。取值范围：0000-01-01~9999-12-31。 |
| DATETIME                     |              | 日期时间类型。取值范围：0000-01-01 00:00:00.000~9999-12-31 23.59:59.999，精确到毫秒。 |
| **TIMESTAMP**                |              | 与时区无关的时间戳类型。取值范围：0000-01-01 00:00:00.000000000~9999-12-31 23.59:59.999999999，精确到纳秒。说明 对于部分时区相关的函数，例如cast(<a timestamp> as string)，要求TIMESTAMP按照与当前时区相符的方式来展现。 |
| **BOOLEAN**                  | boolean      | BOOLEAN类型。取值：True、False。                             |

#### 2.2.2	复杂的数据类型

| 类型   | 定义方法                                            | 构造方法                                                     |
| ------ | --------------------------------------------------- | ------------------------------------------------------------ |
| ARRAY  | `array<int>``array<struct<a:int, b:string>>`        | `array(1, 2, 3)``array(array(1, 2), array(3, 4))`            |
| MAP    | `map<string, string>``map<smallint, array<string>>` | `map(“k1”, “v1”, “k2”, “v2”)``map(1S, array(‘a’, ‘b’), 2S, array(‘x’, ‘y’))` |
| STRUCT |                                                     | struct<x:int, y:int>``struct<field1:bigint, field2:array<int>, field3:map<int, int>>	named_struct(‘x’, 1, ‘y’, 2)``named_struct(‘field1’, 100L, ‘field2’, array(1, 2), ‘field3’, map(1, 100, 2, 200)) |

> Hive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。还有一个uniontype< 所有类型，所有类型… > 。
>
> ​		数组：array< 所有类型 >；
> ​		Map < 基本数据类型，所有数据类型 >；
> ​		struct < 名：所有类型[注释] >;
> ​		uniontype< 所有类型，所有类型… >

### 2.3	Hive表操作

> Hive的存储格式:
>
> Hive没有专门的数据文件格式,常见的有以下几种:
>
> ​		**TEXTFILE**
> ​		SEQUENCEFILE
> ​		AVRO
> ​		**RCFILE**
> ​		**ORCFILE**
> ​		**PARQUET**

```
TextFile:
       TEXTFILE 即正常的文本格式，是Hive默认文件存储格式，因为大多数情况下源数据文件都是以text文件格式保存（便于查看验数和防止乱码）。此种格式的表文件在HDFS上是明文，可用hadoop fs -cat命令查看，从HDFS上get下来后也可以直接读取。
        TEXTFILE 存储文件默认每一行就是一条记录，可以指定任意的分隔符进行字段间的分割。但这个格式无压缩，需要的存储空间很大。虽然可结合Gzip、Bzip2、Snappy等使用，使用这种方式，Hive不会对数据进行切分，从而无法对数据进行并行操作。
一般只有与其他系统由数据交互的接口表采用TEXTFILE 格式，其他事实表和维度表都不建议使用。

RCFile:
Record Columnar的缩写。是Hadoop中第一个列文件格式。能够很好的压缩和快速的查询性能。通常写操作比较慢，比非列形式的文件格式需要更多的内存空间和计算量。 RCFile是一种行列存储相结合的存储方式。首先，其将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。其次，块数据`列式存储`，有利于数据压缩和快速的列存取。

ORCFile:
Hive从0.11版本开始提供了ORC的文件格式，ORC文件不仅仅是一种列式文件存储格式，最重要的是有着很高的压缩比，并且对于MapReduce来说是可切分（Split）的。因此，在Hive中使用ORC作为表的文件存储格式，不仅可以很大程度的节省HDFS存储资源，而且对数据的查询和处理性能有着非常大的提升，因为ORC较其他文件格式压缩比高，查询任务的输入数据量减少，使用的Task也就减少了。ORC能很大程度的节省存储和计算资源，但它在读写时候需要消耗额外的CPU资源来压缩和解压缩，当然这部分的CPU消耗是非常少的。

Parquet:
通常我们使用关系数据库存储结构化数据，而关系数据库中使用数据模型都是扁平式的，遇到诸如数组、Map和自定义Struct的时候就需要用户在应用层解析。但是在大数据环境下，通常数据的来源是服务端的埋点数据，很可能需要把程序中的某些对象内容作为输出的一部分，而每一个对象都可能是嵌套的，所以如果能够原生的支持这种数据，这样在查询的时候就不需要额外的解析便能获得想要的结果。Parquet的灵感来自于2010年Google发表的Dremel论文，文中介绍了一种支持嵌套结构的存储格式，并且使用了列式存储的方式提升查询性能。Parquet仅仅是一种存储格式，它是语言、平台无关的，并且不需要和任何一种数据处理框架绑定。这也是parquet相较于orc的仅有优势：支持嵌套结构。Parquet 没有太多其他可圈可点的地方,比如他不支持update操作(数据写成后不可修改),不支持ACID等.

SEQUENCEFILE:
SequenceFile是Hadoop API 提供的一种二进制文件，它将数据以<key,value>的形式序列化到文件中。这种二进制文件内部使用Hadoop 的标准的Writable 接口实现序列化和反序列化。它与Hadoop API中的MapFile 是互相兼容的。Hive 中的SequenceFile 继承自Hadoop API 的SequenceFile，不过它的key为空，使用value 存放实际的值， 这样是为了避免MR 在运行map 阶段的排序过程。SequenceFile支持三种压缩选择：NONE, RECORD, BLOCK。 Record压缩率低，一般建议使用BLOCK压缩。 SequenceFile最重要的优点就是Hadoop原生支持较好，有API，但除此之外平平无奇，实际生产中不会使用。

AVRO:
Avro是一种用于支持数据密集型的二进制文件格式。它的文件格式更为紧凑，若要读取大量数据时，Avro能够提供更好的序列化和反序列化性能。并且Avro数据文件天生是带Schema定义的，所以它不需要开发者在API 级别实现自己的Writable对象。Avro提供的机制使动态语言可以方便地处理Avro数据。最近多个Hadoop 子项目都支持Avro 数据格式，如Pig 、Hive、Flume、Sqoop和Hcatalog。
```

>Hive的四大常用存储格式存储效率及执行速度对比

![image-20220531234505119](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220531234505119.png)

![image-20220531234553385](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220531234553385.png)

> 结论：ORCFILE存储文件读操作效率最高
>
> 耗时比较：ORC<Parquet<RC<Text

![image-20220531234659264](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220531234659264.png)

> 结论：ORCFILE存储文件占用空间少，压缩效率高
>
> 占用空间：ORC<Parquet<RC<Text

#### 2.3.1	创建表

```sql
CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name 
[(col_name data_type [COMMENT col_comment], ...)] 
[COMMENT table_comment] 
[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] 
[CLUSTERED BY (col_name, col_name, ...) 
[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] 
[ROW FORMAT row_format] 
[STORED AS file_format] 
[LOCATION hdfs_path]


字段解释说明:
- CREATE TABLE 
	创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。

- EXTERNAL
	关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION）
	创建内部表时，会将数据移动到数据仓库指向的路径（默认位置）；
	创建外部表时，仅记录数据所在的路径，不对数据的位置做任何改变。在
	删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。

- COMMENT：
	为表和列添加注释。

- PARTITIONED BY
	创建分区表

- CLUSTERED BY
	创建分桶表

- SORTED BY
	不常用

- ROW FORMAT 
  DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]
	用户在建表的时候可以自定义SerDe或者使用自带的SerDe。
	如果没有指定ROW FORMAT 或者ROW FORMAT DELIMITED，将会使用自带的SerDe。
	在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。
	SerDe是Serialize/Deserilize的简称，目的是用于序列化和反序列化。

- STORED AS指定存储文件类型
	常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）
	如果文件数据是纯文本，可以使用STORED AS TEXTFILE。
	如果数据需要压缩，使用 STORED AS SEQUENCEFILE。

- LOCATION ：
	指定表在HDFS上的存储位置。

- LIKE
	允许用户复制现有的表结构，但是不复制数据。
```

##### 建表1：全部使用默认建表方式

```
create table IF NOT EXISTS students
(
    id bigint,
    name string,
    age int,
    gender string,
    clazz string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; // 必选，指定列分隔符 
```

##### 建表2：指定location （这种方式也比较常用）

```
create table IF NOT EXISTS students2
(
    id bigint,
    name string,
    age int,
    gender string,
    clazz string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/bigdata27/input1'; // 指定Hive表的数据的存储位置，一般在数据已经上传到HDFS，想要直接使用，会指定Location，通常Locaion会跟外部表一起使用，内部表一般使用默认的location


create table IF NOT EXISTS person_avg_counts
(
	name string,
	avg_count bigint
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
LOCATION '/bigdata29/out4'; 



```

##### 建表3：指定存储格式

```
create table IF NOT EXISTS test_orc_tb
(
    id bigint,
    name string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
STORED AS ORC
LOCATION '/bigdata29/out6'; // 指定储存格式为orcfile，inputFormat:RCFileInputFormat,outputFormat:RCFileOutputFormat，如果不指定，默认为textfile，注意：除textfile以外，其他的存储格式的数据都不能直接加载，需要使用从表加载的方式。
```

##### 建表4：create table xxxx as select_statement(SQL语句) (这种方式比较常用)

```
create table IF NOT EXISTS bigdata29.students(id bigint,name string,age int,gender string,clazz string)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

create table IF NOT EXISTS bigdata29.xuqiu2(clazz string,number bigint)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

insert into students3_1 values(1002,'吴小康',19,'男','27期特训营');

create table students4 as select * from students2;
```

##### 建表5：create table xxxx like table_name  只想建表，不需要加载数据

```
create table students5 like students;
```

> **简单用户信息表创建：**
>
> ```sql
> create table t_user(
> id int,
> uname string,
> pwd string,
> gender string,
> age int
> )
> row format delimited fields terminated by ','
> lines terminated by '\n';
> ```
>
> ```
> 1,admin,123456,男,18
> 2,zhangsan,abc123,男,23
> 3,lisi,654321,女,16
> ```
>
> 
>
> 复杂人员信息表创建：
>
> ```sql
> create table IF NOT EXISTS t_person(
> name string,
> friends array<string>,
> children map<string,int>,
> address struct<street:string ,city:string>
> )
> row format delimited fields terminated by ',' -- 列与列之间的分隔符
> collection items terminated by '_' -- 元素与元素之间分隔符
> map keys terminated by ':' -- Map数据类型键与值之间的分隔符
> lines terminated by '\n';  -- 行与行之间的换行符
> ```
>
> ```sql
> songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,beng bu_anhui
> yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,he fei_anhui
> ```

#### 2.3.2	显示表

```sql
show tables;
show tables like 'u*';
desc t_person;
desc formatted students; // 更加详细
```

#### 2.3.3	加载数据

##### 1、使用```hdfs dfs -put '本地数据' 'hive表对应的HDFS目录下'```

##### 2、使用 load data

> 下列命令需要在hive shell里执行

```
create table IF NOT EXISTS students4
(
    id bigint,
    name string,
    age int,
    gender string,
    clazz string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

// 将HDFS上的/input1目录下面的数据 移动至 students表对应的HDFS目录下，注意是 移动、移动、移动
load data inpath '/input1/students.txt' into table students;
```

```
// 清空表
truncate table students;
// 加上 local 关键字 可以将Linux本地目录下的文件 上传到 hive表对应HDFS 目录下 原文件不会被删除
load data local inpath '/usr/local/soft/data/students.txt' into table students;
// overwrite 覆盖加载
load data local inpath '/usr/local/soft/data/students.txt' overwrite into table students;
```

##### 3、create table xxx as SQL语句

##### 4、insert into table xxxx SQL语句 （没有as）

```
// 将 students表的数据插入到students2 这是复制 不是移动 students表中的表中的数据不会丢失
insert into table students2 select * from students;

// 覆盖插入 把into 换成 overwrite
insert overwrite table students2 select * from students;
```

#### 2.3.4	修改列

> 查询表结构

```sql
desc students2;
```

> 添加列

```sql
alter table students2 add columns (education string);
```

> 查询表结构

```sql
desc students2;
```

> 更新列

```sql
alter table stduents2 change education educationnew string;
```

#### 2.3.5	删除表

```sql
drop table students2;
```

### 2.4	Hive内外部表

> **面试题：内部表和外部表的区别？如何创建外部表？工作中使用外部表**

#### 2.4.1	hive内部表

> 当**创建好表的时候，HDFS会在当前表所属的库中创建一个文件夹**
>
> 当设置表路径的时候，如果直接指向一个已有的路径,可以直接去使用文件夹中的数据
>
> **当load数据的时候，就会将数据文件存放到表对应的文件夹中**
>
> 而且**数据一旦被load，就不能被修改**
>
> 我们查询数据也是查询文件中的文件,这些数据最终都会存放到HDFS
>
> 当我们**删除表的时候，表对应的文件夹会被删除，同时数据也会被删除**
>
> **默认建表的类型就是内部表**

```sql
// 内部表
create table students_internal
(
    id bigint,
    name string,
    age int,
    gender string,
    clazz string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/input2';

hive> dfs -put /usr/local/soft/data/students.txt /input2/;
```

#### 2.4.1	Hive外部表

> 外部表说明
>
> ​	**外部表因为是指定其他的hdfs路径的数据加载到表中来，所以hive会认为自己不完全独占这份数据**
>
> ​	**删除hive表的时候，数据仍然保存在hdfs中，不会删除。**

```sql
// 外部表
create external table students_external
(
    id bigint,
    name string,
    age int,
    gender string,
    clazz string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/input3';

hive> dfs -put /usr/local/soft/data/students.txt /input3/;
```

> 删除表测试一下：

```sql
hive> drop table students_internal;
Moved: 'hdfs://master:9000/input2' to trash at: hdfs://master:9000/user/root/.Trash/Current
OK
Time taken: 0.474 seconds
hive> drop table students_external;
OK
Time taken: 0.09 seconds
hive> 
```

> 一般在公司中，使用外部表多一点，因为数据可以需要被多个程序使用，避免误删，通常外部表会结合location一起使用
>
> 外部表还可以将其他数据源中的数据 映射到 hive中，比如说：hbase，ElasticSearch......
>
> 设计外部表的初衷就是 让 表的元数据 与 数据 解耦

- 操作案例:  分别创建dept，emp，salgrade。并加载数据。

> 创建数据文件存放的目录

```sql
hdfs dfs -mkdir -p /shujia/bigdata17/dept
hdfs dfs -mkdir -p /shujia/bigdata17/emp
hdfs dfs -mkdir -p /shujia/bigdata17/salgrade
```

- 创建dept表

```sql
CREATE EXTERNAL TABLE IF NOT EXISTS dept (
  DEPTNO int,
  DNAME varchar(255),
  LOC varchar(255)
) row format delimited fields terminated by ','
location '/shujia/bigdata17/dept';

10,ACCOUNTING,NEW YORK
20,RESEARCH,DALLAS
30,SALES,CHICAGO
40,OPERATIONS,BOSTON
```

- 创建emp表

```sql
CREATE EXTERNAL TABLE IF NOT EXISTS emp (
   EMPNO int,
   ENAME varchar(255),
   JOB varchar(255),
   MGR int,
   HIREDATE date,
   SAL decimal(10,0),
   COMM decimal(10,0),
   DEPTNO int
 ) row format delimited fields terminated by ','
 location '/shujia/bigdata17/emp';
 
7369,SMITH,CLERK,7902,1980-12-17,800,null,20
7499,ALLEN,SALESMAN,7698,1981-02-20,1600,300,30
7521,WARD,SALESMAN,7698,1981-02-22,1250,500,30
7566,JONES,MANAGER,7839,1981-04-02,2975,null,20
7654,MARTIN,SALESMAN,7698,1981-09-28,1250,1400,30
7698,BLAKE,MANAGER,7839,1981-05-01,2850,null,30
7782,CLARK,MANAGER,7839,1981-06-09,2450,null,10
7788,SCOTT,ANALYST,7566,1987-07-13,3000,null,20
7839,KING,PRESIDENT,null,1981-11-17,5000,null,10
7844,TURNER,SALESMAN,7698,1981-09-08,1500,0,30
7876,ADAMS,CLERK,7788,1987-07-13,1100,null,20
7900,JAMES,CLERK,7698,1981-12-03,950,null,30
7902,FORD,ANALYST,7566,1981-12-03,3000,null,20
7934,MILLER,CLERK,7782,1982-01-23,1300,null,10
```

- 创建salgrade表

```sql
CREATE EXTERNAL TABLE IF NOT EXISTS salgrade (
  GRADE int,
  LOSAL int,
  HISAL int
) row format delimited fields terminated by ','
location '/shujia/bigdata17/salgrade';

1,700,1200
2,1201,1400
3,1401,2000
4,2001,3000
5,3001,9999
```

### 2.5	Hive导出数据

> 将表中的数据备份

- 将查询结果存放到本地

```sql
//创建存放数据的目录
mkdir -p /usr/local/soft/shujia

//导出查询结果的数据(导出到Node01上)
insert overwrite local directory '/usr/local/soft/shujia/person_data' select * from t_person;
```

- 按照指定的方式将数据输出到本地

```sql
-- 创建存放数据的目录
mkdir -p /usr/local/soft/shujia

-- 导出查询结果的数据
insert overwrite local directory '/usr/local/soft/shujia/person' 
ROW FORMAT DELIMITED fields terminated by ',' 
collection items terminated by '-' 
map keys terminated by ':' 
lines terminated by '\n' 
select * from t_person;
```

- 将查询结果输出到HDFS

```sql
-- 创建存放数据的目录
hdfs dfs -mkdir -p /shujia/bigdata17/copy

-- 导出查询结果的数据
insert overwrite local directory '/usr/local/soft/shujia/students_data2' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' select * from students
```

- 直接使用HDFS命令保存表对应的文件夹

```shell
// 创建存放数据的目录
hdfs dfs -mkdir -p /shujia/bigdata17/person

// 使用HDFS命令拷贝文件到其他目录
hdfs dfs -cp /hive/warehouse/t_person/*  /shujia/bigdata17/person
```

- 将表结构和数据同时备份

  ​		将数据导出到HDFS

  ```sql
  //创建存放数据的目录
  hdfs dfs -mkdir -p /shujia/bigdata17/copy
  
  //导出查询结果的数据
  export table t_person to '/shujia/bigdata17/copy';
  ```

  ​		删除表结构

  ```sql
  drop table t_person;
  ```

  ​		恢复表结构和数据

  ```sql
  import from '/shujia/bigdata17';
  ```

  > 注意：时间不同步，会导致导入导出失败







# Hive1.2.1分区与排序（内置函数）                                               

## 1、Hive分区(十分重要！！)

> 在大数据中，最常见的一种思想就是**分治**，我们可以**把大的文件切割划分成一个个的小的文件**，这样每次操作一个个小的文件就会很容易了，同样的道理，在hive当中也是支持这种思想的，就是我们可以把大的数据，按照每天或者每小时切分成一个个小的文件，这样去操作小的文件就会容易很多了。
>
> 
>
> 假如现在我们公司一天产生3亿的数据量，那么为了方便管理和查询，就做以下的事情。
>
> ​		1）建立分区（可按照日期，部门等等具体业务分区）
>
> ​		2）分门别类的管理



### 1.2	静态分区（SP）

> 静态分区（SP）static partition–partition by (字段 类型)
>
> ​		**借助于物理的文件夹分区，实现快速检索的目的。**
>
> ​		**一般对于查询比较频繁的列设置为分区列。**
>
> ​		**分区查询的时候直接把对应分区中所有数据放到对应的文件夹中**。

> 创建单分区表语法：

```sql
CREATE TABLE IF NOT EXISTS t_student (
sno int,
sname string
) partitioned by(grade int)
row format delimited fields terminated by ',';
--  分区的字段不要和表的字段相同。相同会报错error10035


1,xiaohu01,1
2,xiaohu02,1
3,xiaohu03,1
4,xiaohu04,1
5,xiaohu05,1

6,xiaohu06,2
7,xiaohu07,2
8,xiaohu08,2

9,xiaohu09,3
10,xiaohu10,3
11,xiaohu11,3
12,xiaohu12,3
13,xiaohu13,3
14,xiaohu14,3
15,xiaohu15,3

16,xiaohu16,4
17,xiaohu17,4
18,xiaohu18,4
19,xiaohu19,4
20,xiaohu20,4
21,xiaohu21,4
-- 载入数据
-- 将相应年级一次导入
load data local inpath '/usr/local/soft/bigdata29/grade2.txt' into table t_student partition(grade=2);

-- 演示多拷贝一行上传，分区的列的值是分区的值，不是原来的值
```

> 静态多分区表语法：

```sql
CREATE TABLE IF NOT EXISTS t_teacher (
tno int,
tname string
) partitioned by(grade int,clazz int)
row format delimited fields terminated by ',';

--注意：前后两个分区的关系为父子关系，也就是grade文件夹下面有多个clazz子文件夹。
1,xiaoge01,1,1
2,xiaoge02,1,1

3,xiaoge03,1,2
4,xiaoge04,1,2

5,xiaoge05,1,3
6,xiaoge06,1,3

7,xiaoge07,2,1
8,xiaoge08,2,1

9,xiaoge09,2,2

--载入数据
load data local inpath '/usr/local/soft/bigdata19/hivedata/teacher_1.txt' into table t_teacher partition(grade=1,clazz=1);
```

> 分区表查询

```sql
select * from t_student where grade = 1;

// 全表扫描，不推荐，效率低
select count(*) from students_pt1;

// 使用where条件进行分区裁剪，避免了全表扫描，效率高
select count(*) from students_pt1 where grade = 1;

// 也可以在where条件中使用非等值判断
select count(*) from students_pt1 where grade<3 and grade>=1;
```

> 查看分区

```sql
show partitions t_teacher;
```

> 添加分区

```sql
alter table t_student add partition (grade=6);

alter table t_teacher add partition (grade=3,clazz=1) location '/user/hive/warehouse/bigdata29.db/t_teacher/grade=3/clazz=1';
```

> 删除分区

```sql
alter table t_student drop partition (grade=5);
```

### 1.3	动态分区（DP）

- 动态分区（DP）dynamic partition
- 静态分区与动态分区的**主要区别在于静态分区是手动指定，而动态分区是通过数据来进行判断。**
- 详细来说，静态分区的列是在编译时期通过用户传递来决定的；**动态分区只有在SQL执行时才能决定**。

> 开启动态分区首先要在hive会话中设置如下的参数

```sql
# 表示开启动态分区
hive> set hive.exec.dynamic.partition=true;

# 表示动态分区模式：strict（需要配合静态分区一起使用）、nostrict
# strict： insert into table students_pt partition(dt='anhui',pt) select ......,pt from students;
hive> set hive.exec.dynamic.partition.mode=nonstrict;

===================以下是可选参数======================

# 表示支持的最大的分区数量为1000，可以根据业务自己调整
hive> set hive.exec.max.dynamic.partitions.pernode=1000;
```

> 其余的参数详细配置如下

```sql
设置为true表示开启动态分区的功能（默认为false）
--hive.exec.dynamic.partition=true;

设置为nonstrict，表示允许所有分区都是动态的（默认为strict）
-- hive.exec.dynamic.partition.mode=nonstrict; 
-- hive.exec.dynamic.partition.mode=strict; 

每个mapper或reducer可以创建的最大动态分区个数(默认为100) 
比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错
--hive.exec.max.dynamic.partition.pernode=100; 

一个动态分区创建可以创建的最大动态分区个数（默认值1000）
--hive.exec.max.dynamic.partitions=1000;

全局可以创建的最大文件个数（默认值100000）
--hive.exec.max.created.files=100000; 

当有空分区产生时，是否抛出异常（默认false） 
-- hive.error.on.empty.partition=false;  
```

- 案例1： 动态插入学生年级班级信息

```sql
--创建分区表
CREATE TABLE IF NOT EXISTS t_student_d (
sno int,
sname string
) partitioned by (grade int,clazz int)
row format delimited fields terminated by ',';

--创建外部表
CREATE EXTERNAL TABLE IF NOT EXISTS t_student_e (
sno int,
sname string,
grade int,
clazz int
) 
row format delimited fields terminated by ','
location "/bigdata29/teachers";
```

```
数据：

1,xiaohu01,1,1
2,xiaohu02,1,1
3,xiaohu03,1,1
4,xiaohu04,1,2
5,xiaohu05,1,2
6,xiaohu06,2,3
7,xiaohu07,2,3
8,xiaohu08,2,3
9,xiaohu09,3,3
10,xiaohu10,3,3
11,xiaohu11,3,3
12,xiaohu12,3,4
13,xiaohu13,3,4
14,xiaohu14,3,4
15,xiaohu15,3,4
16,xiaohu16,4,4
17,xiaohu17,4,4
18,xiaohu18,4,5
19,xiaohu19,4,5
20,xiaohu20,4,5
21,xiaohu21,4,5
```

> 如果静态分区的话，我们插入数据必须指定分区的值。
>
> 如果想要插入多个班级的数据，我要写很多SQL并且执行24次很麻烦。
>
> 而且静态分区有可能会产生数据错误问题

```sql
-- 会报错 
insert overwrite table t_student_d partition (grade=1,clazz=1) select * from t_student_e where grade=1;
```

> 如果使用动态分区，动态分区会根据select的结果自动判断数据应该load到哪儿分区去。

```sql
insert overwrite table t_student_d partition (grade,clazz) select * from t_student_e;
```

> 优点：不用手动指定了，自动会对数据进行分区
>
> 缺点：可能会出现数据倾斜

## 2、Hive分桶

### 2.1	业务场景

> 数据分桶的适用场景：
> 		分区提供了一个隔离数据和优化查询的便利方式，不过并非所有的数据都可形成合理的分区，尤其是需要确定合适大小的分区划分方式
> 		不合理的数据分区划分方式可能导致有的分区数据过多，而某些分区没有什么数据的尴尬情况
> 		分桶是将数据集分解为更容易管理的若干部分的另一种技术。
> 		分桶就是将数据按照字段进行划分，可以将数据按照字段划分到多个文件当中去。(都各不相同)

### 2.2	数据分桶原理

- Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。
  - bucket num = hash_function(bucketing_column) mod num_buckets
  - 列的值做哈希取余 决定数据应该存储到哪个桶

### 2.3	数据分桶优势

> **方便抽样**
>
> ​		使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便
>
> **提高join查询效率**
>
> ​		获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 Map 端连接 （Map-side join）高效的实现。比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行JOIN操作就可以，可以大大较少JOIN的数据量。

### 2.4	分桶实战

> ​	首先，分区和分桶是两个不同的概念，很多资料上说需要先分区在分桶，其实不然，分区是对数据进行划分，而分桶是对文件进行划分。
>
> ​	当我们的分区之后，最后的文件还是很大怎么办，就引入了分桶的概念。
>
> 将这个比较大的文件再分成若干个小文件进行存储，我们再去查询的时候，在这个小范围的文件中查询就会快很多。
>
> ​		对于hive中的每一张表、分区都可以进一步的进行分桶。
>
> ​		当然，分桶不是说将文件随机进行切分存储，而是有规律的进行存储。在看完下面的例子后进行解释，现在干巴巴的解释也不太好理解。它是由列的哈希值除以桶的个数来决定每条数据划分在哪个桶中。
>
> 创建顺序和分区一样，创建的方式不一样。

```markdown
# 分区和分桶的区别
1、在HDFS上的效果区别，分区产生的是一个一个子文件夹，分桶产生的是一个一个文件

2、无论是分区还是分桶，在建表的时候都要指定字段，分区使用partitioned by指定分区字段，分桶使用clustered by指定分桶字段

3、partitioned by指定分区字段的时候，字段后面需要加上类型，而且不能在建表小括号中出现。clustered by指定分桶字段的时候，字段已经出现定义过了，只需要指定字段的名字即可

4、分区字段最好选择固定类别的，分桶字段最好选择值各不相同的。

5、分桶不是必须要建立在分区之上，可以不进行分区直接分桶
```



> **首先我们需要开启分桶的支持**

```sql
（依然十分重要，不然无法进行分桶操作！！！！）
set hive.enforce.bucketing=true; 
```

> **数据准备（id,name,age）**

```
1,tom,11
2,cat,22
3,dog,33
4,hive,44
5,hbase,55
6,mr,66
7,alice,77
8,scala,88
```

> **创建一个普通的表**

```sql
create table person
(
id int,
name string,
age int
)
row format delimited
fields terminated by ',';
```

> **将数据load到这张表中**

```sql
load data local inpath '文件在Linux上的绝对路径' into table psn31;
```

> **创建分桶表**

```sql
create table psn_bucket
(
id int,
name string,
age int
)
clustered by(age) into 4 buckets
row format delimited fields terminated by ',';
```

> **将数据insert到表psn_bucket中**
>
> **(注意：这里和分区表插入数据有所区别，分区表需要select 和指定分区，而分桶则不需要)**

```sql
insert into psn_bucket select * from person;
```

> **在HDFS上查看数据**

![image-20220601223434297](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220601223434297.png)

> **查询数据**
>
> **我们在linux中使用Hadoop的命令查看一下（与我们猜想的顺序一致）**

```
hadoop fs -cat /user/hive/warehouse/bigdata17.db/psn_bucket/*
```

> 这里设置的桶的个数是4 数据按照 年龄%4 进行放桶(文件)
> 11%4 == 3 -----> 000003_0
> 22%4 == 2 -----> 000002_0
> 33%4 == 1 -----> 000001_0
> 44%4 == 0 -----> 000000_0
> ...以此类推

![](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/%E5%88%86%E6%A1%B6%E9%80%BB%E8%BE%91%E5%8F%91%E9%80%BB%E8%BE%91.png)

> 面试题：分桶和分区的区别？



## 3、Hive JDBC

##### 启动hiveserver2

```shell
hive --service hiveserver2 &
或者
hiveserver2 &
```

##### 新建maven项目并添加两个依赖

```
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>2.7.6</version>
    </dependency>
    <!-- https://mvnrepository.com/artifact/org.apache.hive/hive-jdbc -->
    <dependency>
        <groupId>org.apache.hive</groupId>
        <artifactId>hive-jdbc</artifactId>
        <version>1.2.1</version>
    </dependency>
```

##### 编写JDBC代码

```java
import java.sql.*;

public class HiveJDBC {
    public static void main(String[] args) throws ClassNotFoundException, SQLException {
        Class.forName("org.apache.hive.jdbc.HiveDriver");
        Connection conn = DriverManager.getConnection("jdbc:hive2://master:10000/bigdata29");
        Statement stat = conn.createStatement();
        ResultSet rs = stat.executeQuery("select * from students limit 10");
        while (rs.next()) {
            int id = rs.getInt(1);
            String name = rs.getString(2);
            int age = rs.getInt(3);
            String gender = rs.getString(4);
            String clazz = rs.getString(5);
            System.out.println(id + "," + name + "," + age + "," + gender + "," + clazz);
        }
        rs.close();
        stat.close();
        conn.close();
    }
}
```

## 4、Hive查询语法(DQL)

```sql
SELECT [ALL | DISTINCT] select_expr, select_expr, ...
FROM table_reference
[WHERE where_condition]
[GROUP BY col_list]
[ORDER BY col_list]
[CLUSTER BY col_list
| [DISTRIBUTE BY col_list] [SORT BY col_list]
]
[LIMIT [offset,] rows]
```

### 4.1	全局排序

- **order by 会对输入做全局排序，因此只有一个reducer**，会导致当输入规模较大时，需要较长的计算时间
- 使用 order by子句排序 :ASC（ascend）升序（默认）| DESC（descend）降序
- **order by放在select语句的结尾**

```sql
select * from 表名 order by 字段名1[，别名2...];
```

### 4.2	局部排序(对reduce内部做排序)

- **sort by 不是全局排序,其在数据进入reducer前完成排序**。
- 如果用sort by进行排序，并且设置mapred.reduce.tasks>1,则sort by 只保证每个reducer的输出有序，**不保证全局有序**。asc,desc
- 设置reduce个数

```sql
set mapreduce.job.reduces=3;
```

- 查看reduce个数

```sql
set mapreduce.job.reduces;
```

- 排序

```sql
select * from 表名 sort by 字段名[,字段名...];
```

### 4.3	分区排序(本身没有排序)

> **distribute by（字段）根据指定的字段将数据**分到不同的reducer，且分发算法是hash散列。
>
> **类似MR中partition,进行分区，结合sort by使用。**（注意：distribute by 要在sort by之前）
>
> 对于distrbute by 进行测试，一定要多分配reduce进行处理，否则无法看到distribute by的效果。
>
> 设置reduce个数

```sql
set mapreduce.job.reduce=7;
```

- 排序

```sql
select * from 表名 distribute by 字段名[,字段名...] sort by 字段;
```

### 4.3	分区并排序

- cluster by（字段）除了具有Distribute by的功能外，还会对该字段进行排序 asc desc
- cluster by = distribute by + sort by 只能默认升序，不能使用倒序

```sql
select * from 表名 cluster by 字段名[,字段名...];
select * from 表名 distribute by 字段名[,字段名...] sort by 字段名[,字段名...];
```



## 5、Hive内置函数

```sql
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF
```

```sql
-- 1.查看系统自带函数
show functions;
-- 2.显示自带的函数的用法
desc function xxxx;
-- 3.详细显示自带的函数的用法
desc function extended upper;
```



### 5.1	内置函数分类

```sql
关系操作符：包括 = 、 <> 、 <= 、>=等

算数操作符：包括 + 、 - 、 *、／等

逻辑操作符：包括AND 、 && 、 OR 、 || 等

复杂类型构造函数：包括map、struct、create_union等

复杂类型操作符：包括A[n]、Map[key]、S.x

数学操作符：包括ln(double a)、sqrt(double a)等

集合操作符：包括size(Array)、sort_array(Array)等

类型转换函数： binary(string|binary)、cast(expr as )

日期函数：包括from_unixtime(bigint unixtime[, string format])、unix_timestamp()等

条件函数：包括if(boolean testCondition, T valueTrue, T valueFalseOrNull)等

字符串函数：包括acat(string|binary A, string|binary B…)等

其他：xpath、get_json_objectscii(string str)、con
```

### 5.2	UDTF hive中特殊的一个功能（进一出多）

```sql
-- UDF 进一出一


-- UDAF 进多出一
-- collect_set()和collect_list()都是对多列转成一行，区别就是list里面可重复而set里面是去重的
-- concat_ws(':',collect_set(type))   ':' 表示你合并后用什么分隔，collect_set(stage)表示要合并表中的那一列数据
select 字段名,concat_ws(':',collect_set(列名)) as 别名 from 表名 group by id;

-- UDTF 进一出多
-- explode  可以将一组数组的数据变成一列表
select  explode(split(列名,"数据的分隔符")) from 表名;
-- lateral view 表生成函数，可以将explode的数据生成一个列表
select id,name,列名 from 表1,lateral view explode(split(表1.列名,"数据的分隔符"))新列名 as 别列名;
```

```sql
-- 创建数据库表
create table t_movie1(
id int,
name string,
types string
)
row format delimited fields terminated by ','
lines terminated by '\n';

-- 电影数据  movie1.txt
-- 加载数据到数据库 load data inpath '/shujia/movie1.txt' into table t_movie1;
1,这个杀手不太冷,剧情-动作-犯罪
2,七武士,动作-冒险-剧情
3,勇敢的心,动作-传记-剧情-历史-战争
4,东邪西毒,剧情-动作-爱情-武侠-古装
5,霍比特人,动作-奇幻-冒险

-- explode  可以将一组数组的数据变成一列表
select  explode(split(types,"-")) from t_movie1;

-- lateral view 表生成函数，可以将explode的数据生成一个列表
select id,name,type from t_movie1 lateral view explode(split(types,"-")) typetable as type;
```

```sql
-- 创建数据库表
create table t_movie2(
id int,
name string,
type string
)
row format delimited fields terminated by ','
lines terminated by '\n';

-- 电影数据 movie2.txt
-- 加载数据到数据库 load data inpath '/shujia/movie2.txt' into table t_movie2;
1,这个杀手不太冷,剧情
1,这个杀手不太冷,动作
1,这个杀手不太冷,犯罪
2,七武士,动作
2,七武士,冒险
2,七武士,剧情
3,勇敢的心,动作
3,勇敢的心,传记
3,勇敢的心,剧情
3,勇敢的心,历史
3,勇敢的心,战争
4,东邪西毒,剧情
4,东邪西毒,动作
4,东邪西毒,爱情
4,东邪西毒,武侠
4,东邪西毒,古装
5,霍比特人,动作
5,霍比特人,奇幻
5,霍比特人,冒险

-- collect_set()和collect_list()都是对列转成行，区别就是list里面可重复而set里面是去重的
-- concat_ws(':',collect_set(type))   ':' 表示你合并后用什么分隔，collect_set(stage)表示要合并表中的那一列数据
select id,concat_ws(':',collect_set(type)) as types from t_movie2 group by id;
```

### 5.3	WordCount案例

> 数据准备

```
hello,world
hello,bigdata
like,life
bigdata,good
```

> 建表

```sql
create table wc2
(
line string
)
row format delimited fields terminated by '\n'
```

> 导入数据

```sql
load data local inpath '/usr/local/soft/data/wc1.txt' into table wc;
```

> **步骤1：先对一行数据进行切分**

```sql
select split(line,',') from wc;
```

> **步骤2：将行转列**

```sql
select explode(split(line,',')) from wc; 
```

> **步骤3：将相同的进行分组统计**

```sql
select w.word,count(*) from (select explode(split(line,',')) as word from wc) w group by w.word;
```





# Hive函数学习

[toc]

#### SQL练习

1、count(*)、count(1) 、count('字段名') 区别

> **从执行结果来看**
>
> - count(*)包括了所有的列，相当于行数，在统计结果的时候，不会忽略列值为NULL              最慢的
> - count(1)包括了忽略所有列，用1代表代码行，在统计结果的时候，不会忽略列值为NULL      最快的
> - count(列名)只包括列名那一列，在统计结果的时候，会忽略列值为空（这里的空不是只空字符串或者0，而是表示null）的计数，即某个字段值为NULL时，不统计  仅次于count(1)
>
> **从执行效率来看**
>
> - 如果列为主键，count(列名)效率优于count(1)
> - 如果列不为主键，count(1)效率优于count(列名)
> - 如果表中存在主键，count(主键列名)效率最优
> - **如果表中只有一列，则count(*)效率最优**
> - 如果表有多列，且不存在主键，则count(1)效率优于count(*)
>
> **在工作中如果没有特殊的要求，就使用count(1)来进行计数。**
>
> **hive语句的执行顺序**
> 1.from 
>
> 2.join on 或 lateral view explode(需炸裂的列)  tbl   as  炸裂后的列名
>
> 3.where
>
> 4.group by  
>
> 5.聚合函数 如Sum()  avg()  count(1)等
>
> 6.having  在此开始可以使用select中的别名
>
> 7.select  若包含over（）开窗函数，此时select中的内容作为窗口函数的输入，窗口中所选的数据范围也是在group by，having之后，并不是针对where后的数据进行开窗，这点要注意。需要注意开窗函数的执行顺序及时间点。
>
> 8.distinct
>
> 9.order by 
>
> 10.limit（建议：今后在大数据环境中，一张表的数据量肯定十分庞大的，养成加limit的习惯）

3、where 条件里不支持不等式子查询，实际上是支持 in、not in、exists、not exists( hive3.x版本是支持的 )

```sql
-- 列出与“SCOTT”从事相同工作的所有员工。
select  t1.EMPNO
        ,t1.ENAME
        ,t1.JOB
from emp t1
where t1.ENAME != "SCOTT" and t1.job in(
    select  job
    from emp
    where ENAME = "SCOTT");
    
7900,JAMES,CLERK,7698,1981-12-03,950,null,30
7902,FORD,ANALYST,7566,1981-12-03,3000,null,20

select  t1.EMPNO
        ,t1.ENAME
        ,t1.JOB
from emp t1
where t1.ENAME != "SCOTT" and exists(
    select  job
    from emp t2
    where ENAME = "SCOTT"
    and t1.job = t2.job
);
```

4、hive中大小写不敏感（**列名无所谓大小写**）

5、在hive中，数据中如果有null字符串，加载到表中的时候会变成 null （不是字符串）

> 如果需要判断 null，使用 某个字段名 is null 这样的方式来判断
>
> 或者使用 nvl() 函数，不能 直接 某个字段名 == null

6、使用explain查看SQL执行计划

> 面试题：hive中一条sql语句如何解析成MapReduce作业执行的？

```sql
explain select  t1.EMPNO
        ,t1.ENAME
        ,t1.JOB
from emp t1
where t1.ENAME != "SCOTT" and t1.job in(
    select  job
    from emp
    where ENAME = "SCOTT");
    
# 查看更加详细的执行计划，加上extended
explain extended select  t1.EMPNO
        ,t1.ENAME
        ,t1.JOB
from emp t1
where t1.ENAME != "SCOTT" and t1.job in(
    select  job
    from emp
    where ENAME = "SCOTT");
```

#### Hive 常用函数

##### 关系运算

```
// 等值比较 = == < = >
// 不等值比较 != <>
// 区间比较： select * from default.students where id between 1500100001 and 1500100010;
// 空值/非空值判断：isnull、isnotnull、nvl()、isnull()
// like、rlike、regexp用法
```

##### 数值计算

```sql
取整函数(四舍五入)：round
向上取整：ceil
向下取整：floor
```

##### 条件函数(主要使用场景是数据清洗的过程中使用，有些构建表的过程也是需要的)

* if： if(表达式,如果表达式成立的返回值,如果表达式不成立的返回值) （**重点**）
* 条件表达式?表达式1:表达式2;

```sql
create table sc(
sno string,
cno string,
score bigint
)row format delimited fields terminated by '\n';


select if(1>0,1,0); 
select if(1>0,if(-1>0,-1,1),0);
select score,if(score>120,'优秀',if(score>100,'良好',if(score>90,'及格','不及格'))) as pingfen from sc;
```

* COALESCE

```sql
select COALESCE(null,'1','2'); // 1 从左往右 依次匹配 直到非空为止
select COALESCE('1',null,'2'); // 1
```

* case when（**重点**）

```sql
select  score
        ,case when score>90 then '优秀'
              when score>80 then '良好'
              when score>=60 then '及格'
        else '不及格'
        end as pingfen
from sc;

select  name
        ,case name when "施笑槐" then "槐ge"
                  when "吕金鹏" then "鹏ge"
                  when "单乐蕊" then "蕊jie"
        else "算了不叫了"
        end as nickname
from students limit 10;
```

> 注意条件的顺序

##### 日期函数重点！！！

````sql
select from_unixtime(1712815703,'YYYY年MM月dd日 HH时mm分ss秒');

select from_unixtime(unix_timestamp(),'YYYY/MM/dd HH:mm:ss');


// '2021年01月14日' -> '2021-01-14'
select from_unixtime(unix_timestamp('2022年06月06日','yyyy年MM月dd日'),'yyyy-MM-dd');
// "04牛2021数加16强" -> "2021/04/16"
select from_unixtime(unix_timestamp("04牛2024数加11强","MM牛yyyy数加dd强"),"yyyy年MM月dd日");
````

##### 字符串函数

```sql
concat('123','456'); // 123456
concat('123','456',null); // NULL

select concat_ws('#','a','b','c'); // a#b#c
select concat_ws('#','a','b','c',NULL); // a#b#c 可以指定分隔符，并且会自动忽略NULL
select concat_ws("|",cast(id as string),name,cast(age as string),gender,clazz) from students limit 10;

select substring("abcdefg",1); // abcdefg HQL中涉及到位置的时候 是从1开始计数
// '2021/01/14' -> '2021-01-14'
select concat_ws("-",substring('2021/01/14',1,4),substring('2021/01/14',6,2),substring('2021/01/14',9,2));
// 建议使用日期函数去做日期
select from_unixtime(unix_timestamp('2021/01/14','yyyy/MM/dd'),'yyyy-MM-dd');

select split("abcde,fgh",","); // ["abcde","fgh"]
select split("a,b,c,d,e,f",",")[2]; // c 数组的下标依旧是从0开始

select explode(split("abcde,fgh",",")); // abcde
										//  fgh

// 解析json格式的数据
select get_json_object('{"name":"zhangsan","age":18,"score":[{"course_name":"math","score":100},{"course_name":"english","score":60}]}',"$.score[0].score"); // 60

{
	"name": "zhangsan",
	"age": 18,
	"score": [{
		"course_name": "math",
		"score": 100
	}, {
		"course_name": "english",
		"score": 60
	}]
}
$.score[0].score
```

#### Hive 中的wordCount

```sql
create table words(
    words string
)row format delimited fields terminated by '\n';

// 数据
hello,java,hello,java,scala,python
hbase,hadoop,hadoop,hdfs,hive,hive
hbase,hadoop,hadoop,hdfs,hive,hive

select word,count(*) from (select explode(split(words,',')) word from words) a group by a.word;

// 结果
hadoop	4
hbase	2
hdfs	2
hello	2
hive	4
java	2
python	1
scala	1
```

### 1.1	Hive窗口函数

> 普通的聚合函数每组(Group by)只返回一个值，而开窗函数则可为窗口中的每行都返回一个值。
> 简单理解，就是对查询的结果多出一列，这一列可以是聚合值，也可以是排序值。
> 开窗函数一般就是说的是over()函数，其窗口是由一个 OVER 子句 定义的多行记录
> 开窗函数一般分为两类,聚合开窗函数和排序开窗函数。	

```sql
-- 聚合格式
select sum(字段名) over([partition by 字段名] [ order by 字段名]) as 别名,
	max(字段名) over() as 别名 
from 表名;

-- 排序窗口格式
select rank() over([partition by 字段名] [ order by 字段名]) as 别名 from 表名;
```

**注意点：**

- over()函数中的分区、排序、指定窗口范围可组合使用也可以不指定，根据不同的业务需求结合使用
- over()函数中如果不指定分区，窗口大小是针对查询产生的所有数据，如果指定了分区，窗口大小是针对每个分区的数据

> 测试数据

```sql
-- 创建表
create table t_fraction(
name string,
subject string, 
score int) 
row format delimited fields terminated by ","
lines terminated by '\n';

-- 测试数据 fraction.txt
孙悟空,语文,10
孙悟空,数学,73
孙悟空,英语,15
猪八戒,语文,10
猪八戒,数学,73
猪八戒,英语,11
沙悟净,语文,22
沙悟净,数学,70
沙悟净,英语,31
唐玄奘,语文,21
唐玄奘,数学,81
唐玄奘,英语,23

-- 上传数据
load data local inpath '/usr/local/soft/bigdata17/xiaohu/data/fraction.txt' into table t_fraction;
```

#### 1.1.1	聚合开窗函数

> sum(求和)
>
> min(最小)
>
> max(最大)
>
> avg(平均值)
>
> count(计数)
>
> lag(获取当前行上一行的数据）

```sql
-- 
select name,subject,score,sum(score) over() as sumover from t_fraction;
+-------+----------+--------+----------+
| name  | subject  | score  | sumover  |
+-------+----------+--------+----------+
| 唐玄奘   | 英语       | 23     | 321      |
| 唐玄奘   | 数学       | 81     | 321      |
| 唐玄奘   | 语文       | 21     | 321      |
| 沙悟净   | 英语       | 31     | 321      |
| 沙悟净   | 数学       | 12     | 321      |
| 沙悟净   | 语文       | 22     | 321      |
| 猪八戒   | 英语       | 11     | 321      |
| 猪八戒   | 数学       | 73     | 321      |
| 猪八戒   | 语文       | 10     | 321      |
| 孙悟空   | 英语       | 15     | 321      |
| 孙悟空   | 数学       | 12     | 321      |
| 孙悟空   | 语文       | 10     | 321      |
+-------+----------+--------+----------+

select name,subject,score,
sum(score) over() as sum1,
sum(score) over(partition by subject) as sum2,
sum(score) over(partition by subject order by score) as sum3, 

-- 由起点到当前行的窗口聚合，和sum3一样
sum(score) over(partition by subject order by score rows between unbounded preceding and current row) as sum4, 

-- 当前行和前面一行的窗口聚合
sum(score) over(partition by subject order by score rows between 1 preceding and current row) as sum5,

-- 当前行的前面一行到后面一行的窗口聚合  前一行+当前行+后一行
sum(score) over(partition by subject order by score rows between 1 preceding and 1 following) as sum6,

-- 当前行与后一行之和
sum(score) over(partition by subject order by score rows between current row and 1 following) as sum6,

-- 当前和后面所有的行
sum(score) over(partition by subject order by score rows between current row and unbounded following) as sum7
from t_fraction;

rows：行
unbounded preceding：起点
unbounded following：终点
n preceding：前 n 行
n following：后 n 行
current row：当前行


+-------+----------+--------+-------+-------+-------+-------+-------+-------+-------+
| name  | subject  | score  | sum1  | sum2  | sum3  | sum4  | sum5  | sum6  | sum7  |
+-------+----------+--------+-------+-------+-------+-------+-------+-------+-------+
| 孙悟空   | 数学       | 12     | 359   | 185   | 12    | 12    | 12    | 31    | 185   |
| 沙悟净   | 数学       | 19     | 359   | 185   | 31    | 31    | 31    | 104   | 173   |
| 猪八戒   | 数学       | 73     | 359   | 185   | 104   | 104   | 92    | 173   | 154   |
| 唐玄奘   | 数学       | 81     | 359   | 185   | 185   | 185   | 154   | 154   | 81    |
| 猪八戒   | 英语       | 11     | 359   | 80    | 11    | 11    | 11    | 26    | 80    |
| 孙悟空   | 英语       | 15     | 359   | 80    | 26    | 26    | 26    | 49    | 69    |
| 唐玄奘   | 英语       | 23     | 359   | 80    | 49    | 49    | 38    | 69    | 54    |
| 沙悟净   | 英语       | 31     | 359   | 80    | 80    | 80    | 54    | 54    | 31    |
| 孙悟空   | 语文       | 10     | 359   | 94    | 10    | 10    | 10    | 31    | 94    |
| 唐玄奘   | 语文       | 21     | 359   | 94    | 31    | 31    | 31    | 53    | 84    |
| 沙悟净   | 语文       | 22     | 359   | 94    | 53    | 53    | 43    | 84    | 63    |
| 猪八戒   | 语文       | 41     | 359   | 94    | 94    | 94    | 63    | 63    | 41    |
+-------+----------+--------+-------+-------+-------+-------+-------+-------+-------+
```

**rows必须跟在Order by 子句之后，对排序的结果进行限制，使用固定的行数来限制分区中的数据行数量。**

> **OVER()：**指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化。
>
> **CURRENT ROW：**当前行
>
> **n PRECEDING：**往前n行数据
>
> **n FOLLOWING：**往后n行数据
>
> **UNBOUNDED：**起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点
>
> **LAG(col,n,default_val)：**往前第n行数据，col是列名，n是往上的行数，当第n行为null的时候取default_val
>
> **LEAD(col,n, default_val)：**往后第n行数据，col是列名，n是往下的行数，当第n行为null的时候取default_val
>
> **NTILE(n)：**把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。
>
> **cume_dist（）**，计算某个窗口或分区中某个值的累积分布。假定升序排序，则使用以下公式确定累积分布：
>
> ​		小于等于当前值x的行数 / 窗口或partition分区内的总行数。其中，x 等于 order by 子句中指定的列的当前行中的值。

##### 聚合开窗函数实战：

##### 实战1：Hive用户购买明细数据分析

> 创建表和加载数据

```sql
name，orderdate，cost
jack,2017-01-01,10
tony,2017-01-02,15
jack,2017-02-03,23
tony,2017-01-04,29
jack,2017-01-05,46
jack,2017-04-06,42
tony,2017-01-07,50
jack,2017-01-08,55
mart,2017-04-08,62
mart,2017-04-09,68
neil,2017-05-10,12
mart,2017-04-11,75
neil,2017-06-12,80
mart,2017-04-13,94


建表加载数据
vim business.txt

create table business
(
name string, 
orderdate string,
cost int
)ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

load data local inpath "/shujia/bigdata17/xiaohu/data/business.txt" into table business;
```

##### 实战1需求：

> 需求1：查询在2017年4月份购买过的顾客及总人数

```sql
# 分析：按照日期过滤、分组count求总人数
select t1.name,t1.orderdate,count(1) over() as counts_04 from (select name,orderdate from business where month(orderdate)='04') t1;
```

> 需求2：查询顾客的购买明细及月购买总额

```sql
# 分析：按照顾客分组、sum购买金额
select name,orderdate,cost,sum(cost) over(partition by name,month(orderdate))  from business;
```

> 需求3：上述的场景,要将cost按照日期进行累加

```sql
# 分析：按照顾客分组、日期升序排序、组内每条数据将之前的金额累加
select name,orderdate,cost,sum(cost) over(partition by name order by orderdate rows between unbounded preceding and current row)  from business;
```

> 需求4：查询顾客上次的购买时间

```sql
·# 分析：查询出明细数据同时获取上一条数据的购买时间(肯定需要按照顾客分组、时间升序排序)
select name,orderdate,cost,lag(orderdate,1) over(partition by name order by orderdate) as last_time from business;
```

> 需求5：查询前20%时间的订单信息

```sql
分析：按照日期升序排序、取前20%的数据
select t1.name,t1.orderdate,t1.cost from (select name,orderdate,cost,ntile(5) over(order by orderdate) as n from business) t1 where t1.n=1;
```

#### 1.1.2	排序开窗函数(重点)

- RANK() 排序相同时会重复，总数不会变
- DENSE_RANK() 排序相同时会重复，总数会减少
- ROW_NUMBER() 会根据顺序计算
- PERCENT_RANK()计算给定行的百分比排名。可以用来计算超过了百分之多少的人(当前行的rank值-1)/(分组内的总行数-1)

```sql
select name,subject,
score,
rank() over(partition by subject order by score desc) rp,
dense_rank() over(partition by subject order by score desc) drp,
row_number() over(partition by subject order by score desc) rnp,
percent_rank() over(partition by subject order by score) as percent_rank 
from t_fraction;
```

```sql
select name,subject,score,
rank() over(order by score) as row_number,
percent_rank() over(partition by subject order by score) as percent_rank
from t_fraction;
```



##### 实战2：Hive分析学生成绩信息

> 创建表语加载数据

```sql
name	subject	score
李毅	语文	87
李毅	数学	95
李毅	英语	68
黄仙	语文	94
黄仙	数学	56
黄仙	英语	84
小虎	语文	64
小虎	数学	86
小虎	英语	84
许文客	语文	65
许文客	数学	85
许文客	英语	78

建表加载数据
vim score.txt

create table score2
(
name string,
subject string, 
score int
) row format delimited fields terminated by "\t";

load data local inpath '/shujia/bigdata17/xiaohu/data/score.txt' into table score;
```

> 需求1：每门学科学生成绩排名(是否并列排名、空位排名三种实现)

```sql
分析：学科分组、成绩降序排序、按照成绩排名

select name,subject,score,
rank() over(partition by subject order by score desc) rp,
dense_rank() over(partition by subject order by score desc) drp,
row_number() over(partition by subject order by score desc) rmp
from 
score;
```

> 需求2：每门学科成绩排名top 2的学生

```sql
select t1.name,t1.subject,t1.score from (select name,subject,score,row_number() over(partition by subject order by score desc) as rn from score2) t1 where t1.rn<3;
```



#### Hive 行转列

lateral view explode

```sql
create table testArray2(
    name string,
    weight array<string>
)row format delimited 
fields terminated by '\t'
COLLECTION ITEMS terminated by ',';

小虎	"150","170","180"
火火	"150","180","190"



select name,col1  from testarray2 lateral view explode(weight) t1 as col1;

小虎	150
小虎	170
小虎	180
火火	150
火火	180
火火	190

select key from (select explode(map('key1',1,'key2',2,'key3',3)) as (key,value)) t;

key1
key2
key3

select name,col1,col2  from testarray2 lateral view explode(map('key1',1,'key2',2,'key3',3)) t1 as col1,col2;
小虎	key1	1
小虎	key2	2
小虎	key3	3
火火	key1	1
火火	key2	2
火火	key3	3


select name,pos,col1  from testarray2 lateral view posexplode(weight) t1 as pos,col1;

小虎	0	150
小虎	1	170
小虎	2	180
火火	0	150
火火	1	180
火火	2	190
```

#### Hive 列转行

```sql
// testLieToLine
name col1
小虎	150
小虎	170
小虎	180
火火	150
火火	180
火火	190

create table testLieToLine(
    name string,
    col1 int
)row format delimited 
fields terminated by '\t';


select name,collect_list(col1) from testLieToLine group by name;

// 结果
小虎	["150","180","190"]
火火	["150","170","180"]

select  t1.name
        ,collect_list(t1.col1) 
from (
    select  name
            ,col1 
    from testarray2 
    lateral view explode(weight) t1 as col1
) t1 group by t1.name;
```

#### Hive自定义函数UserDefineFunction

##### UDF：一进一出

> 定义UDF函数要注意下面几点:
>
> 1. 继承`org.apache.hadoop.hive.ql.exec.UDF`
> 2. 重写`evaluate`()，这个方法不是由接口定义的,因为它可接受的参数的个数,数据类型都是不确定的。Hive会检查UDF,看能否找到和函数调用相匹配的evaluate()方法

* 创建maven项目，并加入依赖

```
        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-exec</artifactId>
            <version>1.2.1</version>
        </dependency>

```

> 打包的时候可能会出现错误
>
> ### Could not transfer artifact org.pentaho:pentaho-aggdesigner-algorithm:pom:5.1.5-jhyde

> 解决方案：
> `在pom文件中修改hive-exec的配置`

```xml
        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-exec</artifactId>
            <exclusions>
                <!--排除pentaho-aggdesigner-algorithm依赖，不将它引入-->
                <exclusion>
                    <groupId>org.pentaho</groupId>
                    <artifactId>pentaho-aggdesigner-algorithm</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
```



* 编写代码，继承org.apache.hadoop.hive.ql.exec.UDF，实现evaluate方法，在evaluate方法中实现自己的逻辑

```java

```

* 打成jar包并上传至Linux虚拟机
* 在hive shell中，使用 ```add jar 路径```将jar包作为资源添加到hive环境中

```
add jar /usr/local/soft/bigdata19/hive-bigdata19-1.0-SNAPSHOT.jar;
```

* 使用jar包资源注册一个临时函数，fxxx1是你的函数名，'MyUDF'是主类名

```
create temporary function fxxx1 as 'MyUDF';
```

* 使用函数名处理数据

```
select fxx1(name) as fxx_name from students limit 10;

#施笑槐$
#吕金鹏$
#单乐蕊$
#葛德曜$
#宣谷芹$
#边昂雄$
#尚孤风$
#符半双$
#沈德昌$
#羿彦昌$
```

**案例2：转大写**

```java

```

##### 函数加载方式

> 命令加载
>
> 这种加载只对本session有效

```shell
# 1、将项目打包上传服务器：将打好的jar包传到linux系统中。(不要打依赖)
# 进入到hive客户端,执行下面命令
hive> add jar /usr/local/soft/bigdata17/data/xiaohu/hadoop-mapreduce-1.0-SNAPSHOT.jar
# 2、创建一个临时函数名,要跟上面hive在同一个session里面：
hive> create temporary function toUP as 'com.shujia.testHiveFun.udf.FirstUDF';

3、检查函数是否创建成功
show functions;

4. 测试功能
select toUp('abcdef');

5. 删除函数 
drop temporary function if exists toUp;
```

> **创建永久函数**
>
> 将jar上传HDFS：

```shell
hadoop fs -put hadoop-mapreduce-1.0-SNAPSHOT.jar /jar/
```

> 在hive命令行中创建永久函数：

```sql
create function myUp as 'com.shujia.testHiveFun.udf.FirstUDF' using jar 'hdfs:/jar/hadoop-mapreduce-1.0-SNAPSHOT.jar';

create function bfy_fun as 'com.shujia.udfdemo.HiveTest' using jar 'hdfs:/shujia/bigdata19/jar/hive-udf.jar';
```

>  退出hive，再进入，执行测试：

![image-20220606011312030](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220606011312030.png)

删除永久函数，并检查：

![image-20220606011343387](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220606011343387.png)

##### UDTF：一进多出

> UDTF是一对多的输入输出,实现UDTF需要完成下面步骤
>
> M1001#xiaohu#S324231212,lkd#M1002#S2543412432,S21312312412#M1003#bfy
>
> 
>
> 1001 xiaohu 324231212
>
> 1002 lkd 2543412432
>
> 1003 bfy 21312312412
>
> 继承org.apache.hadoop.hive.ql.udf.generic.GenericUDTF，
> 重写initlizer（）、process（）、close()。
> 执行流程如下:
>
> UDTF首先会调用initialize方法，此方法返回UDTF的返回行的信息（返回个数，类型）。
>
> 初始化完成后，会调用process方法,真正的处理过程在process函数中，在process中，每一次forward()调用产生一行；**如果产生多列可以将多个列的值放在一个数组中**，然后将该数组传入到forward()函数。
>
> 最后close()方法调用，对需要清理的方法进行清理。

> "key1:value1,key2:value2,key3:value3"
>
> key1 value1
>
> key2 value2
>
> key3 value3

###### 方法一：使用 explode+split

```

```

###### 方法二：自定UDTF

* 代码

```sql

```

* SQL

```sql
create temporary function my_udtf as 'com.shujia.testHiveFun.udtf.HiveUDTF';

select my_udtf("key1:value1,key2:value2,key3:value3");
```

> 字段：id,col1,col2,col3,col4,col5,col6,col7,col8,col9,col10,col11,col12 共13列
>
> 数据： 
>
> a,1,2,3,4,5,6,7,8,9,10,11,12
>
> b,11,12,13,14,15,16,17,18,19,20,21,22
>
> c,21,22,23,24,25,26,27,28,29,30,31,32
>
> 转成3列：id,hours,value
>
> 例如：
>
> a,1,2,3,4,5,6,7,8,9,10,11,12
>
> a,0时,1
>
> a,2时,2
>
> a,4时,3
>
> a,6时,4
>
> ......

````sql
create table udtfData(
    id string
    ,col1 string
    ,col2 string
    ,col3 string
    ,col4 string
    ,col5 string
    ,col6 string
    ,col7 string
    ,col8 string
    ,col9 string
    ,col10 string
    ,col11 string
    ,col12 string
)row format delimited fields terminated by ',';
````

代码：

```java

```

添加jar资源:

```
add jar /usr/local/soft/HiveUDF2-1.0.jar;
```

注册udtf函数：

```
create temporary function my_udtf as 'MyUDTF';
```

SQL:

```
select id,hours,value from udtfData lateral view my_udtf(col1,col2,col3,col4,col5,col6,col7,col8,col9,col10,col11,col12) t as hours,value ;
```

##### UDAF：多进一出

#### Hive Shell

##### 第一种：

```
hive -e "select * from test1.students limit 10"
```

##### 第二种：

```
hive -f hql文件路径
```

> 将HQL写在一个文件里，再使用 -f 参数指定该文件

#### 连续登陆问题

> 在电商、物流和银行可能经常会遇到这样的需求：统计用户连续交易的总额、连续登陆天数、连续登陆开始和结束时间、间隔天数等

##### 数据：

> 注意：每个用户每天可能会有多条记录

```
id	datestr	  amount
1,2019-02-08,6214.23 
1,2019-02-08,6247.32 
1,2019-02-09,85.63 
1,2019-02-09,967.36 
1,2019-02-10,85.69 
1,2019-02-12,769.85 
1,2019-02-13,943.86 
1,2019-02-14,538.42
1,2019-02-15,369.76
1,2019-02-16,369.76
1,2019-02-18,795.15
1,2019-02-19,715.65
1,2019-02-21,537.71
2,2019-02-08,6214.23 
2,2019-02-08,6247.32 
2,2019-02-09,85.63 
2,2019-02-09,967.36 
2,2019-02-10,85.69 
2,2019-02-12,769.85 
2,2019-02-13,943.86 
2,2019-02-14,943.18
2,2019-02-15,369.76
2,2019-02-18,795.15
2,2019-02-19,715.65
2,2019-02-21,537.71
3,2019-02-08,6214.23 
3,2019-02-08,6247.32 
3,2019-02-09,85.63 
3,2019-02-09,967.36 
3,2019-02-10,85.69 
3,2019-02-12,769.85 
3,2019-02-13,943.86 
3,2019-02-14,276.81
3,2019-02-15,369.76
3,2019-02-16,369.76
3,2019-02-18,795.15
3,2019-02-19,715.65
3,2019-02-21,537.71
```

##### 建表语句

```sql
create table deal_tb(
    id string
    ,datestr string
    ,amount string
)row format delimited fields terminated by ',';
```

##### 计算逻辑

* 先按用户和日期分组求和，使每个用户每天只有一条数据

```sql

```



* 根据用户ID分组按日期排序，将日期和分组序号相减得到连续登陆的开始日期，如果开始日期相同说明连续登陆

* *datediff(string end_date,string start_date);* 等于0说明连续登录
* 统计用户连续交易的总额、连续登陆天数、连续登陆开始和结束时间、间隔天数

```

```



* 结果

```
1	2019-02-07	13600.23	3	2019-02-08	2019-02-10 NULL
1	2019-02-08	2991.650	5	2019-02-12	2019-02-16	1
1	2019-02-09	1510.8		2	2019-02-18	2019-02-19	1
1	2019-02-10	537.71		1	2019-02-21	2019-02-21	1
2	2019-02-07	13600.23	3	2019-02-08	2019-02-10 NULL
2	2019-02-08	3026.649	4	2019-02-12	2019-02-15	1
2	2019-02-10	1510.8		2	2019-02-18	2019-02-19	2
2	2019-02-11	537.71		1	2019-02-21	2019-02-21	1
3	2019-02-07	13600.23	3	2019-02-08	2019-02-10 NULL
3	2019-02-08	2730.04		5	2019-02-12	2019-02-16	1
3	2019-02-09	1510.8		2	2019-02-18	2019-02-19	1
3	2019-02-10	537.71		1	2019-02-21	2019-02-21	1
```





# Hive优化（面试宝典）

## 1.1	**hive的随机抓取策略**

> 理论上来说，Hive中的所有sql都需要进行mapreduce，但是hive的抓取策略帮我们
> 省略掉了这个过程，把切片split的过程提前帮我们做了。
> set hive.fetch.task.conversion=none;
> (一旦进行这么设置，select字段名也是需要进行mapreduce的过程，默认是more) 

```sql
Fetch抓取的模式
可以通过 set hive.fetch.task.conversion查看，有以下3种模式：

none：所有涉及hdfs的读取查询都走mapreduce任务；
mininal：在进行简单的select *，简单的过滤或涉及分区字段的过滤时走mr；
more:在mininal模式的基础上，增加了针对查询语句字段进行一些别名的计算操作。
以下HQL，mininal模式与more模式下都不会走mr任务:
SELECT
	sale_ord_id,
	store_id
FROM
	test_table
where 
	dt = '2021-01-01'
 limit 10;
 
以下HQL,mininal模式会走mr任务，more模式不会：
SELECT
	sale_ord_id,
	store_id,
	if(store_id > 20,1,0) as store_id_new
FROM
	test_table
where 
	dt = '2021-01-01'
 limit 10;
```

> 查看怎么将一个sql转化成一个MR任务的
> explain sql语句
> 例如：
> explain select count(*) from stu_dy1_1;
> 更加详细的查看，例如：
> **explain extended select count(*) from students2;**
> 当你输入一个sql语句的时候，hive会将对其关键字进行截串，截完串之后，变成
> 都是一些TOK开头的一些东西，然后经过这样的抽象语法树，再转成具体的查询块，
> 最后变成逻辑查询计划

## 1.2	本地运行模式

```
大多数的 Hadoop Job 是需要 Hadoop 提供的完整的可扩展性来处理大数据集的。不过，
有时 Hive 的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能
会比实际 job 的执行时间要多的多。对于大多数这种情况， Hive 可以通过本地模式在单台机
器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。
用户可以通过设置 hive.exec.mode.local.auto 的值为 true ，来让 Hive 在适当的时候自动
启动这个优化。

本地模式运行比集群模式块很多，33秒的任务降到2秒
更改为本地模式：
hive> set hive.exec.mode.local.auto=true
注意：
hive> set hive.exec.mode.local.auto.inputbytes.max=134217728     ---> 128M
（默认值就是128）
表示加载文件的最大值，若大于该配置仍然会以集群的方式去运行。
97万行数据，50MB
当我们开发或者测试阶段，可以去使用本地模式进行运行，默认是集群模式
但是，这里有个问题，当我们去更改为本地模式的时候，在8088的页面上就看不到
任务的执行情况了。

测试：select count(*) from emp group by deptno;
```

## 1.3	**并行计算**

```
通过设置以下参数开启并行模式（默认是false）
set hive.exec.parallel=true;

注意：hive.exec.parallel.thread.number
(一次SQl计算中允许并行执行的job个数最大值，默认是8个)

举例：
select t1.n1,t2.n2 from (select count(ename) as n1 from emp) t1,(select count(dname) as n2 from dept) t2;
注意，有时候开启并行计算运行时间并没有不开启的快，那是因为，资源的问题。
需要两套资源，资源申请会浪费点时间，最多可以并行8个，默认是8个。
所以，并行的越多，不一定是越快，因为它涉及到一个资源申请的策略。
```

## 1.4	**严格模式(理解为增加一些限制)**

​	**1.什么是Hive的严格模式**
​		hive中的一种模式,在该模式下禁止一些不好SQL的执行。

​	**2.Hive的严格模式不允许哪些SQL执行**
​		**2.1 禁止分区表全表扫描**
   			分区表往往数据量大,如果不加分区查询会带来巨大的资源消耗 。例如以下分区表
   			SELECT DISTINCT(planner_id) FROM fracture_ins WHERE planner_id=5;

​				报错如下:
​       		FAILED: Error in semantic analysis: No Partition Predicate Found for Alias “fracture_ins” Table "fracture_ins

​       		解决如下:
​              SELECT DISTINCT(planner_id) FROM fracture_ins WHERE planner_id=5 AND hit_date=20120101;

​      **2.2 禁止排序不加limit**
​        排序最终是要都进到一个Reduce中操作,防止reducer额外执行很长一段时间
​        SELECT * FROM fracture_ins WHERE hit_date>2012 ORDER BY planner_id;
​        出现如下错误
​               FAILED: Error in semantic analysis: line 1:56 In strict mode,limit must be specified if ORDER BY is present planner_id
​        解决方案就是增加一个limit关键字：
​               hive> SELECT * FROM fracture_ins WHERE hit_date>2012 ORDER BY planner_id LIMIT 100000;

​      **2.3 禁止笛卡尔积**
​          笛卡尔积是什么: A={a,b}, B={0,1,2}，则 A×B={(a, 0), (a, 1), (a, 2), (b, 0), (b, 1), (b, 2)}

​          SELECT * FROM fracture_act JOIN fracture_ads;
​		解决方法
​		SELECT * FROM fracture_act JOIN fracture_ads WHERE fracture_act.planner_id = fracture_ads.planner_id;

**3.Hive的严格模式怎样开启**

```sql
// 查看当前严格模式的状态
set hive.mapred.mode;
// 设置为严格模式
set hive.mapred.mode=strict;
// 设置为非严格模式
set hive.mapred.mode=nonstrict;
```

```
注意，这里的严格模式和动态分区的那个严格模式半毛钱关系没有）
通过设置以下参数开启严格模式：
set hive.mapred.mode=strict;
(默认为：nonstrict非严格模式)

查询限制：
1、对于分区表，必须添加where对于分区字段的条件过滤
2、order by 语句必须包含limit输出限制
3、限制执行笛卡尔积的查询
这些限制是帮助我们提高查询效率的。
```

## 1.5	**Hive排序(掌握distribute by和sort by)  回顾**

```sql
order by 对于查询结果做全排序，只允许有一个reduce处理
（注意：它会把我们所有的字段或者查询结果全部放在一个reduce里进行处理
当数据量较大时候，有可能reduce执行不完，所以，我们以后把这个给弃用掉）



**   sort by 对于单个reduce进行排序 但是我们将每个reduce里面进行排序，没有考虑到
每个reduce之间的排序。所以我们引出下一个
**   distribute by 分区排序，通常结合sort by一起使用
（distribute by column sort by column asc|desc）

cluster by 相当于distribute by + sort by  (注意，虽然是两个结合，但是我们也不去用它
原因很简单，cluster by不能通过asc desc的方式指定排序方式规则)
```

## 1.6	Hive join数据倾斜(相当重要，记住这块，面试到hive数据倾斜稳过)

1、小表join小表 不管他

2、小表join大表   map-join

3、大表join大表  map-side

考虑会不会发生reduce,并且考虑reduce压力是否大（是否会出现某个reduce数据量庞大的情况）

```sql
join计算的时候，将小表（驱动表）放在join的左边
Map join：在Map端完成join
两种实现方式：
1、sql方式，在sql语句中添加Mapjoin标记（mapjoin hint）
语法糖
select /*+MAPJOIN(A)*/ * from A join B on (A.key=B.key);
>>语法：
select /*+MAPJOIN(smallTable)*/ smallTable.key bigTable.value from smallTable join bigTable on smallTable.key=bigTable.key;
2、自动开启mapjoin
通过修改以下配置启用自动的mapjoin：
set hive.auto.convert.join=true;
(注意：该参数为true的时候，Hive自动对左边的表统计量，如果
是小表，就加入到内存，即对小表使用Mapjoin)

相关配置参数
　　hive.mapjoin.smalltable.filesize;(默认25M,大表小表判断的阈值，如果表的大小小于该值则会被加载到内存中运行。)
　　hive.ignore,mapjoin.hint;(默认值：true;是否忽略mapjoin hint的标记)
　　hive.auto.convert.join.noconditionaltask;(默认值：true；将普通的join转换为mapjoin时，是否将多个mapjoin转化为一个mapjoin)
　　hive.auto.convert.join.noconditionaltask.size;(将多个mapjoin转化为一个mapjoin时，这个表的最大值)
3、尽可能使用相同的连接键，如果不同，多一个join就会多开启一个mapreduce，执行速度变得慢。
4、大表join大表（当两个都是大表的时候，只能发生reduce了，但是这里有两个优化策略）（面试的时候说，加分）
　　a: 空key过滤:
　　　　有时join超时是因为某些key对应的数据太多,而相同key对应的数据都会发送到相同的 reducer上,从而导致内存不够。
　　　　此时我们应该仔细分析这些异常的key,很多情况下,这些key对应的数据是异常数据,我们需要在SQL语句中进行过滤。
　　　　但是这个的前提条件是异常数据，但是我们一般拿到的数据都是经过ETL数据清洗过后的，一般影响不大，面试的时候可以说。
　　b: 空key转换:
　　　　有时虽然某个key为空对应的数据很多,但是相应的数据不是异常数据,必须要包含在join的结果中,
　　　　此时我们可以表a中key为空的字段赋随机的值,使得数据随机均匀地分不到不同的 reducer上。(加盐)
　　　　但是我们一般拿到的数据都是经过ETL数据清洗过后的，规则数据，一般影响不大，面试的时候可以说。
5、Map-Side聚合
通过设置以下参数开启在Map端的聚合
set hive.map.aggr=true;（一定要进行开启，虽然进行了两个mapreduce，但是当数据倾斜发生的时候，很多时候会根本跑不出结果，卡死在99%或者100%，慢总比出不来结果要好）！！！！！！！
相关配置参数
　　hive. groupby mapaggr. checkinterval;
　　map端 igroup by执行聚合时处理的多少行数据(默认:10000
　　hive.map.aggr.hash.min.reduction;比例(若聚合之后的数据100大该0.5,map端聚合使用的内存的最大值
　　hive.mapaggr.hashforce.flush.memory.threshold;map端做聚合操作是has表的最大可用内容,大于该值则会触发fush
　　hive.groupby.skewindata-是否对 GroupBy产生的数据倾斜做优化,默认为false(十分重要！！！)
6、数据倾斜，尽可能地让我们的数据散列到不同的reduce里面去,负载均衡（Hbase中热点数据）
```

## 1.7	**合并小文件**

```sql
1、hadoop不适合存储小文件
2、MR不适合处理小文件
3、Hive不适合处理小文件

Hive优化
合并小文件
文件数目小,容易在文件存储端造成压力,给hdfs造成压力,影响效率
设置合并属性
　　是否合并map输出文件: hive.merge.mapfiles=true
　　是否合并reduce输出文件: hive.merge.mapredfiles=true
　　合并文件的大小: hive.merge.size.per.task=256*1000*1000
去重统计
数据量小的时候无所谓,数据量大的情况下,由于 COUNT DISTINCT操作需要用一个 Reduce Task来完成,
这一个 Reduce需要处理的数据量太大,就会导致整个JOb很难完成,一般 COUNT DISTINCT使用先 GROUP BY再COUNT的方式替换
```

## 1.8	**控制map和reduce的数量(一般情况下我们不去动它)**

```sql
控制Hive中Map以及 Reduce的数量
Map数量相关的参数
mapred.max.split.size;一个split的最大值,即每个map处理文件的最大值
mapred.min.split.size.per.node个节点上split的最小值
mapred.min.split.size.per.rack一个机架上spit的最小值
Reduce数量相关的参数
mapred.reduce.tasks;强制指定reduce任务的数量
hive.exec.reducers.bytes.per.reducer每个reduce任务处理的数据量
hive.exec.reducers.max每个任务最大的reduce数
```

## 1.9	**JVM重用**

```sql
当我们的小文件个数过多，task个数过多，需要申请的资源过多的时候，我们可以先申请一部分资源，全部执行完毕后再释放，
比我们申请一个释放一个要快。
通过 set mapred.job.reuse.jvm.num.tasks=n;来设置
（n为task插槽个数）
缺点：
设置开启后，task插槽会一直占用资源，无论是否有task进行，直到所有的task,
即整个job全部执行完毕后，才会释放所有的task插槽，所以我们要合理地设置这个n
(比如，我们设置申请了10个，但是现在来了6个，剩下4个插槽会在job全部执行完毕之前一直占用资源)
```

**mapreduce叫懒加载，当执行任务需要资源的时候再去申请资源**





# 实验1-1：实战Hive大数据分析环境搭建

## 实验概述

本次实验主要介绍了HIve大数据分析开发环境的整体搭建过程，会涉及：JDK的安装配置、Hadoop的安装配置、Hive的安装配置等。

![image.png](https://cdn.atstudy.com/lab/manual/1631503786225247.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631518770346537.png?fileid=3701925924192383572)

视频-1、实验概述

## 实验环境

- AtStudy项目实训平台
- MySQL
- JDK 1.8
- Hadoop 3.2.1
- Hive 3.1.2

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16315105052419911.png)

## 实验目标

完成本实验后，您将能够

- 掌握Linux系统下JDK安装与配置
- 掌握Linux系统下Hadoop的安装与使用
- 掌握Linux系统下Hadoop基本使用
- 熟悉Hadoop常用的端口
- 掌握Linux系统下Hive的安装与配置

## 实验任务

### 任务一、JDK安装与配置

**【任务目标】**

本任务主要目标为完Linux系统上JDK的安装与环境变量配置

**【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16315038847302978.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315190534917461.png?fileid=3701925924192717429)

视频-2、JDK安装与配置

**1、打开Terminal终端**

进入项目实训平台后，如下图所示

![image.png](https://cdn.atstudy.com/lab/manual/16315105441087836.png)

在桌面空白位置点击鼠标右键，选择`在此打开终端`如下图所示

![image.png](https://cdn.atstudy.com/lab/manual/16315105602754258.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315105735482121.png)

输入以下命令，进入到`/opt/data` 目录

```
cd /opt/data复制代码
```

查看`/opt/data` 目录下所有的文件

```
ls -l复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315105876554943.png)

如上图所示，主要关注在`/opt/data` 目录中三个tar压缩包，它们分别为：

- hadoop.tar.gz：Hadoop 3.2.1 安装文件压缩包
- java-8-openjdk-amd64.tar.gz：JDK 8 安装文件压缩包
- apache-hive-3.1.2-bin.tar.gz：hive 3.1.2 安装文件压缩包

**2、解压JDK安装包**

新建JDK安装目录

```
mkdir /usr/lib/jvm复制代码
```

接下来对JDK安装包进行解压操作

```
tar -zxvf java-8-openjdk-amd64.tar.gz -C /usr/lib/jvm/复制代码
```

解压完成后如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16315106107291733.png)

**3、配置JDK环境变量**

执行以下命令，使用`vim`编辑器修改系统环境配置文件

```
vim /etc/profile复制代码
```

打开后，按键盘 `i` 键将文档处于 `插入` 状态，将光标移动到文件末尾，添加如下内容：

```
#java
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export JRE_HOME=${JAVA_HOME}/jre    
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib    
export PATH=${JAVA_HOME}/bin:$PATH复制代码
```

添加完成后如下图所示![image.png](https://cdn.atstudy.com/lab/manual/16315106261249259.png)

按一下键盘 `esc` 键，然后再输入 ` :wq` 保存并退出！

**4、测试JDK**

刷新环境变量

```
source /etc/profile复制代码
```

输入以下命令，查看JDK版本

```
java -version复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315106413603396.png)

若如上图所示结果，则说明JDK安装成功

### 任务二、Hadoop安装与配置

**【任务目标】**

本任务主要目标为完Linux系统上Hadoop的安装与配置，主要包含一下步骤

（1）解压安装Hadoop

（2）修改Hadoop配置文件

（3）配置Hadoop环境变量

**【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16315106732058406.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631519275578907.png?fileid=3701925924198818413)

视频-3、Hadoop安装与配置概述

**1、解压Hadoop**

打开一个新的`Terminal` 输入一下命令，进入`/opt/data`

```
cd /opt/data复制代码
```

查看`/opt/data` 目录下所有文件

```
ls -l复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315106878452230.png)

执行一下命令，将`hadoop.tar.gz` 解压至 `/usr/local` 目录下

```
tar -zxvf hadoop.tar.gz -C /usr/local/复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315107007829971.png)

**2、修改Hadoop配置文件**

解压完成后，执行以下命令，进入到Hadoop的配置文件目录 `/usr/local/hadoop/etc/hadoop`

```
cd /usr/local/hadoop/etc/hadoop复制代码
```

查看所有配置文件

```
ls复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/1631510720184370.png)

**2.1、core-site.xml**

![image.png](https://cdn.atstudy.com/lab/manual/16315107342155518.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315193300163730.png?fileid=3701925924199773434)

视频-4、Hadoop安装与配置01：core-site.xml

**文件作用：**

core-site.xml 是 Hadoop中的核心配置文件，主要用于配置以下内容：

1）指定namenode的位置 2）hadoop.tmp.dir 是hadoop文件系统依赖的基础配置，很多路径都依赖它，配置它的路径

**具体配置：**

执行以下命令，修改`core-site.xml` 文件

```
vim core-site.xml复制代码
```

添加如下内容（注意：若打开后已经修改好了，则无需再修改）

```
<configuration>
        <property>
                <name>fs.default.name</name>
                <value>hdfs://localhost:9000</value>
        </property>
        <property>
                <name>hadoop.tmp.dir</name>
                <value>/home/hadoop3/hadoop/tmp</value>
        </property>
        <property>
                 <name>hadoop.proxyuser.root.hosts</name>
                 <value>*</value>
        </property>
        <property>
                 <name>hadoop.proxyuser.root.groups</name>
                 <value>*</value>
        </property>
</configuration>复制代码
```

按一下键盘 `esc` 键，然后再输入 ` :wq` 保存并退出！

修改完成后如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16315107551943483.png)

**2.2、hadoop-env.sh**

![image.png](https://cdn.atstudy.com/lab/manual/16315107712965860.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315193990185983.png?fileid=3701925924200532654)

视频-5、Hadoop安装与配置02：hadoop-env.sh

**文件作用：**由于Hadoop是java进程，所以需要在Hadoop中添加jdk的环境变量配置。

**具体配置：**

执行以下命令，修改`hadoop-env.sh` 文件

```
vim hadoop-env.sh复制代码
```

在该文件的末尾，添加如下内容（注意：若打开后已经修改好了，则无需再修改）

```
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root复制代码
```

按一下键盘 `esc` 键，然后再输入 ` :wq` 保存并退出！

如下图所示

![image.png](https://cdn.atstudy.com/lab/manual/16315109355915496.png)

**2.3、hdfs-site.xml**

![image.png](https://cdn.atstudy.com/lab/manual/16315110836091604.png) ![image.png](https://cdn.atstudy.com/lab/manual/16315194467134027.png?fileid=3701925924200565736)

视频-6、Hadoop安装与配置03：hdfs-site.xml

**文件作用：**是 Hadoop中的核心配置文件，主要用于配置以下内容：

1）配置namenode和datanode存放文件的具体路径 2）配置副本的数量，最小值为3，否则会影响到数据的可靠性

**具体配置：**

执行以下命令，修改`hdfs-site.xml` 文件

```
vim hdfs-site.xml复制代码
```

添加如下内容（注意：若打开后已经修改好了，则无需再修改）

```
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/home/hadoop3/hadoop/hdfs/name</value>
    </property>
    <property>
        <name>dfs.namenode.data.dir</name>
        <value>/home/hadoop3/hadoop/hdfs/data</value>
    </property>
</configuration>复制代码
```

按一下键盘 `esc` 键，然后再输入 ` :wq` 保存并退出！

如下图所示

![image.png](https://cdn.atstudy.com/lab/manual/16315112412617074.png)

**2.4、yarn-site.xml**

![image.png](https://cdn.atstudy.com/lab/manual/16315112557796181.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315195156519912.png?fileid=3701925924200682533)

视频-7、Hadoop安装与配置04：yarn-site.xml

**文件作用：** 主要用于配置以下内容：

1）Yarn.resourcemanager.hostname: 资源管理器所在节点的主机名 2）Yarn.nodemanager.aux-services: 一个逗号分隔的辅助服务列表，这些服务由节点管理器执行。该属性默认为空。

**具体配置：**

执行以下命令，修改`yarn-site.xml` 文件

```
vim yarn-site.xml复制代码
```

添加如下内容（注意：若打开后已经修改好了，则无需再修改）

```
<configuration>
        <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>localhost</value>
        </property>
         <property>
                 <name>yarn.nodemanager.aux-services</name>
                 <value>mapreduce_shuffle</value>
         </property>
</configuration>复制代码
```

按一下键盘 `esc` 键，然后再输入 ` :wq` 保存并退出！

如下图所示

![image.png](https://cdn.atstudy.com/lab/manual/16315112790499230.png)

**3、配置Hadoop环境变量**

![image.png](https://cdn.atstudy.com/lab/manual/16315112959543992.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631519570973847.png?fileid=3701925924200714100)

视频-8、Hadoop安装与配置05：Hadoop环境变量

执行以下命令，使用`vim`编辑器修改系统环境配置文件

```
vim /etc/profile复制代码
```

打开后，将光标移动到文件末尾，添加如下内容

```
# Hadoop
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export HADOOP_COMMON_HOME=$HADOOP_HOME 
export HADOOP_HDFS_HOME=$HADOOP_HOME 
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME 
export HADOOP_INSTALL=$HADOOP_HOME 
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native 
export HADOOP_LIBEXEC_DIR=$HADOOP_HOME/libexec 
export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY_PATH
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HDFS_DATANODE_USER=root
export HDFS_DATANODE_SECURE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export HDFS_NAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root复制代码
```

按一下键盘 `esc` 键，然后再输入 ` :wq` 保存并退出！

添加完成后如下图所示

![image.png](https://cdn.atstudy.com/lab/manual/16315113145747544.png)

执行以下命令刷新环境变量

```
source /etc/profile复制代码
```

执行以下命令，查看`hadoop` 版本

```
hadoop version复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315113430792308.png)

**4、Hadoop初始化**

![image.png](https://cdn.atstudy.com/lab/manual/16315113550294282.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315196145712090.png?fileid=3701925924200931156)

视频-9、Hadoop安装与配置06：Hadoop初始化

执行以下命令，对Hadoop hdfs分布式文件系统进行初始化

```
hadoop namenode -format复制代码
```

格式化完成后如下图输出所示

![image.png](https://cdn.atstudy.com/lab/manual/1631511376904269.png)

**5、配置Hadoop免密登录**

![image.png](https://cdn.atstudy.com/lab/manual/16315113911863251.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315196623332051.png?fileid=3701925924200880780)

视频-10、Hadoop安装与配置07：SSH免密登录

**特别说明：**为了省去在启动Hadoop服务时手动输入密码的繁琐过程，先配置一下本机的免密登录，但在进行Hadoop免费登录配置之前，需要先重置一下系统root用户的密码：

在Linux终端中输入`passwd` 命令，将root用户的密码更改为：123456，具体操作截图如下：

![image.png](https://cdn.atstudy.com/lab/manual/16315114059464293.png)

接下来，就可以进行Hadoop的免费登录配置了。

首先创建ssh-key

```
ssh-keygen -t rsa复制代码
```

注意，在创建ssh-key的过程中，若某些步骤需要用户手动输入，直接回车即可，如下图所示

![image.png](https://cdn.atstudy.com/lab/manual/16315114369008227.png)

创建 `/home/.ssh` 目录

```
mkdir /home/.ssh复制代码
```

将ssh-key公钥复制到本机中

```
ssh-copy-id -i /root/.ssh/id_rsa.pub localhost复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315114536052506.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315114641916042.png)

> 注意：此时系统root用户配置的默认密码为：**123456**

密码输入完成后，若如下图所示，则说明本机免密登录配置成功

![image.png](https://cdn.atstudy.com/lab/manual/16315114753637841.png)

**6、启动Haoop服务**

![image.png](https://cdn.atstudy.com/lab/manual/163151172365055.png) ![image.png](https://cdn.atstudy.com/lab/manual/1631517381724531.png?fileid=3701925924201062936)

视频-11、启动、验证及关闭Hadoop服务

执行以下命令，启动`Hadoop`相关服务

```
start-all.sh复制代码
```

如下图所示

![image.png](https://cdn.atstudy.com/lab/manual/16315118816521774.png)

查看服务运行状态

```
jps复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315118966097923.png)

**7、查看Hadoop Web UI 端口**

回到系统桌面，打开桌面的浏览器， 如下图：

![image.png](https://cdn.atstudy.com/lab/manual/16315119076536188.png)

在打开的浏览器地址栏中输入 `http://localhost:9870/` 如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16315119238051294.png)

输入完成后，按回车进入到`Hadoop Web UI` 页面，如下图所示

![image.png](https://cdn.atstudy.com/lab/manual/1631511940614785.png)

至此，说明Hadoop的已经安装完成且能够正常使用，可以使用如下命令关闭Hadoop相关服务

**8、停止Hadoop服务**

可以在Linux终端中输入如下命令，可以停止hadoop服务：

```
stop-all.sh复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315119529082925.png)

可以再次输入 jps 命令查看Hadoop的几个服务 ，发现已经不存在了，说明 Hadoop服务已经停止。

```
jps复制代码
```

### 任务三、Hive安装与配置

**【任务目标】**

本任务主要目标为完Linux系统上Hadoop的安装与配置，主要包含一下步骤

（1）解压安装Hive

（2）修改Hive配置文件

（3）配置Hive环境变量

**【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16315119656997274.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315176234735839.png?fileid=3701925924201134019)

视频-12、Hive的安装及环境变量配置

**1、解压Hive安装包**

打开一个新的`Terminal` 输入一下命令，进入`/opt/data`

```
cd /opt/data复制代码
```

查看`/opt/data` 目录下所有文件

```
ls -l复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/1631511983460293.png)

执行如下命令，将hive安装包解压到 `/usr/local` 目录下

```
tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /usr/local/复制代码
```

解压完成后如下图所示

![image.png](https://cdn.atstudy.com/lab/manual/16315120013084518.png)

**2、配置Hive环境变量**

执行以下命令，使用`vim`编辑器修改系统环境配置文件

```
vim /etc/profile复制代码
```

打开后，将光标移动到文件末尾，添加如下内容

```
# hive
export HIVE_HOME=/usr/local/apache-hive-3.1.2-bin
export PATH=$PATH:$HIVE_HOME/bin复制代码
```

按一下键盘 `esc` 键，然后再输入 ` :wq` 保存并退出！

修改完成后，刷新环境变量

```
source /etc/profile复制代码
```

**3、Hive核心配置文件**

![image.png](https://cdn.atstudy.com/lab/manual/1631512017404627.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315177964807965.png?fileid=3701925924201200107)

视频-13、Hive核心配置-1:hive-site.xml

首先，进入 `/usr/local/apache-hive-3.1.2-bin/conf/` 目录下，查看配置文件：

```
cd /usr/local/apache-hive-3.1.2-bin/conf/
ls复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315120345719117.png)

然后，将上述黄框中的 `hive-default.xml.template` 文件复制一份复本并更名为 `hive-site.xml` ：

```
cp hive-default.xml.template hive-site.xml复制代码
```

最后，执行以下命令，修改 `hive-site.xml` 配置文件

```
vim hive-site.xml复制代码
```

**1）第一步，配置一些hive所需的用户变量：java临时变量，用户名等**

打开`hive-site.xml` 文件后，首先将光标定位到`<configuration>`的后面`<property>`的前面，如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16315120487746757.png)

按 `i` 键将文档处于插入状态，然后如下配置插入到该位置处：

```
<property>
    <name>system:java.io.tmpdir</name>
    <value>/tmp/hive/java</value>
</property>
<property>
    <name>system:user.name</name>
    <value>${user.name}</value>
</property>复制代码
```

效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16315120638319043.png)

**2）第二步，通过搜索关键词配置修改数据库url、驱动、用户名，url等配置项**

在Vim浏览模式下通过输入 ` /javax.jdo.option.ConnectionUserName`

、`/javax.jdo.option.ConnectionPassword`、`/javax.jdo.option.ConnectionURL`

、`/javax.jdo.option.ConnectionDriverName` 、 `/hive.metastore.schema.verification`

搜索对应name,然后下述方式修改它们的 value 值：

> 注意：
>
> 1）上述搜索方式一定是要在 vim的浏览模式下，即刚打开时的状态，如果当前处于 【插入】状态，则请按【esc】键，回到浏览模式。
>
> 2）建议，在 vim 浏览模式下，先按 `gg` 键 ，让光标到达文档的开始，再按 `/`输入name键值进行搜索。
>
> 3）如果 `/` 方式搜不到对应的name值，则可以键盘按 `G ` 键 ，将光标定位到文件末尾，然后再通过在vim浏览状态下输入键盘的 `? ` 号的方式进行搜索。
>
> 4）在vim中 `/` 代表从前往后搜索，`?` 号代表从后往前搜索。

```
    ......
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>
    ......
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>123123</value>
    </property>
    ......
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false</value>
    </property>
    ......
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>
    .....
    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
    <property>
    ......复制代码
```

修改完成后，按一下键盘 `esc` 键，然后再输入 ` :wq` 保存并退出！

**4、Hive与Hadoop的相关兼容性配置**

![image.png](https://cdn.atstudy.com/lab/manual/16315120818022735.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315182689233707.png?fileid=3701925924201465806)

视频-14、Hive核心配置-2:Hive与Hadoop相关兼容性配置

因为当前系统所使用的Hive及Hadoop均为最新版本，存在着一定的兼容性问题 ，需要额外再做一些配置，具体配置如下：

1）将`/opt/data` 下的mysql-connector-java-5.1.49.jar包上传到服务器，复制到hive的lib目录下

定位到 `/opt/data` 目录下

```
cd /opt/data复制代码
```

将mysql-connector-java-5.1.49.jar包复制到hive的lib目录下

```
cp mysql-connector-java-5.1.49.jar /usr/local/apache-hive-3.1.2-bin/lib/复制代码
```

2）log4j-slf4j-impl-2.10.0.jar 这个包hadoop及hive两边只能有一个，此处删掉hive中的此jar包

```
rm -rf /usr/local/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar复制代码
```

3）guava-27.0-jre.jar 这个包hadoop及hive两边只删掉版本低的那个，把版本高的复制过去

删除掉hive中的低版本guava-27.0-jre.jar 包

```
rm -rf /usr/local/apache-hive-3.1.2-bin/lib/guava-19.0.jar复制代码
```

将hadoop中高版本的guava-27.0-jre.jar包复制到hive中

```
cp /usr/local/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar /usr/local/apache-hive-3.1.2-bin/lib复制代码
```

**5、初始化元数据库**

![image.png](https://cdn.atstudy.com/lab/manual/16315120999998371.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315183599447923.png?fileid=3701925924201555752)

视频-15、初始化Hive元数据库

首先，再次重新加载一下环境变量：

```
source /etc/profile复制代码
```

然后，执行以下命令，进行Hive元数据库的初始化操作

```
schematool -initSchema -dbType mysql复制代码
```

完成后如下图所示

![image.png](https://cdn.atstudy.com/lab/manual/16315121147615405.png)

特别说明：如果上述步骤出错，请按照如下提示操作：

![image.png](https://cdn.atstudy.com/lab/manual/16315121267546768.png)

```
cd /usr/local/apache-hive-3.1.2-bin/conf 
vim hive-site.xml复制代码
```

1）在打开发vim文档中，在浏览状态下输入 `:set number` 调出行号

2）通过 `:行号` 的命令快速定位至上述出错的行处，如：`:3215`

3）发现定位到 hive.txn.xlock.iow 所在的配置块里，将description的内容清空即可，如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16315121393927081.png)

4）`esc` 后再按 `:wq` 保存并退出后，最好再输入以下命令重新刷新一次环境变量：

```
source /etc/profile复制代码
```

5）最后再执行一次Hive元数据库的初始化操作，应该就不会报错了

```
schematool -initSchema -dbType mysql复制代码
```

### 任务四、Hive大数据分析环境整体测试

**【任务目标】**

本任务主要目标为完成Hive大数据分析环境的整体功能测试，主要包含一下步骤

（1）启动Hadoop相关服务

（2）普通方式启动 hive客户端

（3）启动 `hiveserver2` 服务

（4）Beeline连接hive数据仓库

**【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16315121538031011.png) ![image.png](https://cdn.atstudy.com/lab/manual/16315185304928892.png?fileid=3701925924201712238)

视频-16、Hive大数据分析环境整体测试

**1、打开Terminal终端**

进入项目实训平台后，如下图所示

![image.png](https://cdn.atstudy.com/lab/manual/16315121666167744.png)

在桌面空白位置点击鼠标右键，选择`在此打开终端`如下图所示

![image.png](https://cdn.atstudy.com/lab/manual/16315121778125883.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315121940893517.png)

**2、启动hadoop**

首先，为确保没有问题，建议再次重新加载刷新一下系统环境变量：

```
source /etc/profile复制代码
```

然后在命令终端输入以下命令，启动hadoop相关服务

```
start-all.sh复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315122060699005.png)

**3、查看服务状态**

```
jps复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315122161243325.png)

**4、普通方式启动Hive**

```
# 启动hive客户端
hive
# 显示数据库复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315122266171917.png)

**5、退出hive客户端**

```
exit;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315122390479077.png)

**6、JDBC方式启动HIve**

**（1）启动 Hiveserver2**

```
nohup hiveserver2 >/dev/null 2>/dev/null &复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/1631512254081417.png)

注意：该命令执行完后由于是在后台启动Hiveserver2服务，因此执行完该命令需要约1分钟左右Hiveserver2服务才能启动

**7、通过beeline连接并查询**

查看Hiverserver2服务是否启动成功

```
netstat -ntulp |grep 10000复制代码
```

如若启动成功，应看到如下图所示输出(注意：若未出现如下图所示结果，则说明还正在启动，稍等一会儿后再重新查看)

![image.png](https://cdn.atstudy.com/lab/manual/16315122677975691.png)

启动beeline连接查询工具

```
beeline --showDbInPrompt=true复制代码
```

如下图所示

![image.png](https://cdn.atstudy.com/lab/manual/16315122791897031.png)

通过以下命令正式连接到hive数据仓库默认`default`数据库

```
# 通过jdbc方式连结default数据库
!connect jdbc:hive2://localhost:10000/default复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315122946426792.png)

> mysql数据库帐号是root，密码是 123123

**8、退出hive仓库**

使用 `!q ` 命令退出beeline方式连结的Hive客户端，效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16315123048642993.png)

## 实验总结

Hive的安装要经历三大任务，12个步骤，分别是：

1）JDK的安装与配置

- JDK的解压安装
  - tar -zxvf java-8-openjdk-amd64.tar.gz -C /usr/lib/jvm/
- JDK环境变量的配置
  - vim /etc/profile

2）Hadoop安装与配置

- 解压安装Hadoop
  - tar -zxvf hadoop.tar.gz -C /usr/local/
- 修改Hadoop配置文件
  - core-site.xml
  - hadoop-env.sh
  - hdfs-site.xml
  - yarn-site.xml
- 配置Hadoop环境变量
  - vim /etc/profile

3）Hive安装与配置

- 解压安装Hive
  - tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /usr/local/
- 修改Hive配置文件
  - hive-site.xml
  - schematool -initSchema -dbType mysql
- 配置Hive环境变量
  - vim /etc/profile





# 实验1-2：Hive原理、架构与启动方式

## 实验概述

本次实验主要讲解了什么是大数据，HIve的基本概念，优缺点，适用场景，原理与架构，并演示了Hive的两种启动方式 ，为后继更加深入的学习HIve大数据分析打下基础。

## 实验环境

- AtStudy 实训平台
- Linux 操作系统
- Hadoop 3.x
- Hive 3.x
- JDK 1.8
- MySQL 8.x

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16315203908751450.png)

## 实验目标

学习完成本实验后，您将能够

- 了解 什么是大数据以及大数据的特点
- 了解 Hive 的基本概念与优缺点
- 了解 Hive 的原理与系统架构
- 了解MapReduce的实现原理与技术架构

## 实验任务

### 任务一、大数据扫盲

#### **【任务目标】**

 本次任务主要是对大数据进行扫盲的，了解什么是大数据，大数据的基本特性及大数据架构和涉及的技术框 架等。

#### **【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16315204049267449.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315882395701768.png?fileid=3701925924294580180)

视频-1、大数据的定义与特性

##### 1.1、大数据的定义

![image.png](https://cdn.atstudy.com/lab/manual/16315204553592232.png)

最早提出“大数据”一词的是全球知名咨询公司麦肯锡。麦肯锡给出的大数据定义：一种规模大到在获取、存储、管理、分析方面大大超出了传统数据库软件工具能力范围的数据集合，具有5V特点。5V特点：Volume（数据量大）、Velocity（处理速度快）、Variety（类型多样）、Value（价值密度）、Veracity（真实性）。

大数据最大的特征，自然就是数据量巨大，大到传统的数据处理软件如Excel、Mysql等都无法很好的支持分析。这也意味着大数据阶段，无论是数据的存储还是加工计算等等过程，用到的处理技术也会完全不同，例如Hadoop、HIve 、Spark等等。

##### 1.2、大数据的特点

- **数据量大（Volume）**

大数据第一个特征就是“大”。起初，一个MP3可能只要256M就能满足听歌需求，随着储存技术的发展，数据存储慢慢的从MB级别到了GB、TB级别，大数据时代，数据储存起始计量至少是1个PB（1024TB），甚至达到EB（1024PB）、ZB（1024EB）级别。

- **类型多样（Variety）**

大数据另一个特征就是数据来源广泛，所有的数据都会被记录储存，平台得到的不再是随机样本，而是全体数据。这些数据会以日志数据形式被处理分析。比如现在的淘宝、今日头条、抖音等就是通过分析用户数据去智能推荐用户喜爱的内容。

![image.png](https://cdn.atstudy.com/lab/manual/16315204713668951.png)

- **处理速度快（Velocity）**

海量的数据如果处理的速度太慢将毫无意义。因此，大数据对处理速度有着严格要求，可以实现对海量数据实时分析，真正做到随时产生数据，随时进行处理。面对日益增加的数据量，平台的处理速度将面临新的挑战。未来谁的处理速度更快，谁将更有优势。

- **价值密度（Value）**

因为在收集数据方面是来者不拒，大数据另一个特征是，除了收集有效价值之外，更多的是收集无意义的数据。平台通过对这些数据分析，迅速挖掘有效价值，从而提高管理效率、实现新的价值。

- **真实性（Veracity）**

大数据最后一项特征是真实。通过互联网上用户真实的操作轨迹，可以得到真实的数据，从而对现实进行反映和预测。但随着研究发现，网络到处充斥着虚假数据。比如刷单、刷粉的行为，虽然都是真实的操作轨迹，但反映的并不是真实的情况。因此近年很多研究者都把这一大数据特征删掉。

![image.png](https://cdn.atstudy.com/lab/manual/16315205316136875.png)

在大数据智能时代，大数据即带来了机遇也带来了挑战。在未来几年，大数据应用将从膨胀性发展转为理性发展，前景依然非常乐观。围绕大数据，大量的企业将实现数字化转型转型，从而改变各行各业的业态。

##### 1.3、大数据的架构

![image.png](https://cdn.atstudy.com/lab/manual/16315205417014618.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315889893807092.png?fileid=3701925924295122115)

视频-2、大数据的架构

在企业内部，数据从生产、存储，到分析、应用，会经历各个处理流程。它们相互关联，形成了整体的大数据架构。

![image.png](https://cdn.atstudy.com/lab/manual/16315205709273480.png)

通常来说，在我们最终查看数据报表，或者使用数据进行算法预测之前，数据都会经历以下这么几个处理环节：

1. 数据采集：是指将应用程序产生的数据和日志等同步到大数据系统中。
2. 数据存储：海量的数据，需要存储在系统中，方便下次使用时进行查询。
3. 数据处理：原始数据需要经过层层过滤、拼接、转换才能最终应用，数据处理就是这些过程的统称。一般来说，有两种类型的数据处理，一种是离线的批量处理，另一种是实时在线分析。
4. 数据应用：经过处理的数据可以对外提供服务，比如生成可视化的报表、作为互动式分析的素材、提供给推荐系统训练模型等等。

我们现在常用的大数据技术，其实都是基于Hadoop生态的。Hadoop是一个分布式系统基础架构，换言之，它的数据存储和加工过程都是分布式的，由多个机器共同完成。通过这样的并行处理，提高安全性和数据处理规模。

Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算。我们可以把HDFS（Hadoop Distributed File System）理解为一套分布式的文件系统，大数据架构里的海量数据就是存储在这些文件里，我们每次分析，也都是从这些文件里取数。

而MapReduce则是一种分布式计算过程，它包括Map（映射）和Reduce（归约）。当你向MapReduce框架提交一个计算作业时，它会首先把计算作业拆分成若干个Map任务，然后分配到不同的节点上去执行，每一个Map任务处理输入数据中的一部分，当Map任务完成后，Reduce会把前面若干个Map的输出汇总到一起并输出。相当于利用了分布式的机器，完成了大规模的计算任务。

### 任务二、初识 Hive

#### 【任务目标】

 本次任务主要了解：什么是Hive，HIve的由来，Hive要解决什么问题 以及Hive的优缺点等。

#### 【任务步骤】

![image.png](https://cdn.atstudy.com/lab/manual/16315205908759300.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315893793988089.png?fileid=3701925924295237909)

视频-3、Hive简介与Hive特点

##### 2.1、Hive 简介

Hive官网：https://hive.apache.org/

![image.png](https://cdn.atstudy.com/lab/manual/16315206087722598.png)

 **Hive是一个数据仓库基础工具，在Hadoop中用来处理结构化数据**。它架构在Hadoop之上，总归为大数据，并使得查询和分析方便。并**提供简单的sql查询**功能，可以将sql语句转换为MapReduce任务进行运行。

 最初，Hive是由Facebook开发，后来由Apache软件基金会开发，并作为进一步将它作为名义下Apache Hive为一个开源项目。Hive 没有专门的数据格式。 **Hive不适用于在线事务处理。 它最适用于传统的数据仓库任务**。

 Hive 构建在基于静态批处理的Hadoop 之上，Hadoop 通常都有较高的延迟并且在作业提交和调度的时候需要大量的开销。因此，**Hive 并不能够在大规模数据集上实现低延迟快速的查询**，例如，Hive 在几百MB 的数据集上执行查询一般有分钟级的时间延迟。因此，**Hive 并不适合那些需要低延迟的应用**，例如，联机事务处理（OLTP）。

 Hive 查询操作过程严格遵守Hadoop MapReduce 的作业执行模型，Hive 将用户的HiveQL 语句通过解释器转换为MapReduce 作业提交到Hadoop 集群上，Hadoop 监控作业执行过程，然后返回作业执行结果给用户。Hive 并非为联机事务处理而设计，**Hive 并不提供实时的查询和基于行级的数据更新操作**。Hive 的**最佳使用场合是大数据集的批处理作业**，例如，网络日志分析。

**一句话介绍Hive：**

 Hive是由 Facebook 开源用于解决海量结构化日志的数据统计工具。 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类 SQL 查询功能。

##### 2.2、Hive 特点

1. 通过SQL轻松访问数据的工具，从而实现数据仓库任务（如提取/转换/加载（ETL），报告和数据分析）。
2. 一种对各种数据格式施加结构的机制
3. 访问直接存储在Apache HDFS或其他数据存储系统（如Apache HBase）中的文件
4. 通过Apache Tez，Apache Spark或MapReduce执行查询
5. 程序语言与HPL-SQL
6. 通过Hive LLAP，Apache YARN和Apache Slider进行亚秒级查询检索。

> 特别说明：
>
> ```
> - Hive提供了标准的SQL功能，其中包括许多后来用于分析的SQL：2003和SQL：2011功能。
> - Hive的SQL还可以通过用户定义的函数（UDF），用户定义的聚合（UDAF）和用户定义的表函数（UDTF）用用户代码进行扩展。
> - 没有一个数据必须存储的“Hive格式”。 Hive带有用于逗号和制表符分隔值（CSV）文本文件，Apache Parquet，Apache ORC和其他格式的内置连接器。用户可以使用其他格式的连接器来扩展Hive。
> - Hive不适用于联机事务处理（OLTP）工作负载。它最适用于传统的数据仓库任务。
> - Hive旨在最大限度地提高可伸缩性（在Hadoop集群中动态添加更多机器的规模），性能，可扩展性，容错性以及与输入格式的松散耦合。复制代码
> ```

##### 2.3、Hive 本质

![image.png](https://cdn.atstudy.com/lab/manual/16315206252034669.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315895556338329.png?fileid=3701925924295299307)

视频-4、Hive本质与优缺点

**Hive 本质：**将 HQL 转化成 MapReduce 程序 Hive SQL = HQL

![image.png](https://cdn.atstudy.com/lab/manual/16315206389308854.png)

##### 2.4、Hive 优缺点

**1）优点**

（1）操作接口采用类 SQL 语法，提供快速开发的能力（简单、容易上手）。

（2）避免了去写 MapReduce，减少开发人员的学习成本。

（3）Hive 的执行延迟比较高，因此 Hive 常用于数据分析，对实时性要求不高的场合。

（4）Hive 优势在于处理大数据，对于处理小数据没有优势，因为 Hive 的执行延迟比较高。

（5）Hive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。

**2）缺点**

（1）Hive 的 的 HQL 表达能力有限

- 迭代式算法无法表达
- 数据挖掘方面不擅长，由于 MapReduce 数据处理流程的限制，效率更高的算法无法实现。

（2）Hive 的效率比较低

- Hive 自动生成的 MapReduce 作业，通常情况下不够智能化
- Hive 调优比较困难，粒度较粗

### 任务三、Hive 的原理与架构

#### 【任务目标】

 本次任务主要了解HIve的原理与架构设计，了解HIve与传统数据库的区别与适用场景。

#### 【任务步骤】

![image.png](https://cdn.atstudy.com/lab/manual/1631520780813604.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315897540516247.png?fileid=3701925924295407636)

视频-5、Hive的原理与架构

##### 3.1、Hive 架构原理

![image.png](https://cdn.atstudy.com/lab/manual/16315208318656769.png)

**1) 用户接口：Client**

CLI（command-line interface）、JDBC/ODBC( jdbc 访问 hive )、WEBUI（ 浏览器访问 hive ）

**2) 元数据：Metastore**

元数据包括：表名、表所属的数据库（默认是 default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；

默认存储在自带的 derby 数据库中，推荐使用 MySQL 存储 Metastore

**3) Hadoop**

使用 HDFS 进行存储，使用 MapReduce 进行计算。

**4) 驱动器：Driver**

（1）解析器（SQL Parser）：将 SQL 字符串转换成抽象语法树 AST，这一步一般都用第三方工具库完成，比如 antlr；对 AST 进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。

（2）编译器（Physical Plan）：将 AST 编译生成逻辑执行计划。

（3）优化器（Query Optimizer）：对逻辑执行计划进行优化。

（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于 Hive 来说，就是 MR/Spark。

##### 3.2、HIve的运行机制

![image.png](https://cdn.atstudy.com/lab/manual/16315209075048710.png)

Hive 通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的 Driver，结合元数据(MetaStore)，将这些指令翻译成 MapReduce，提交到 Hadoop 中执行，最后，将执行返回的结果输出到用户交互接口。

##### 3.3、Hive与传统数据库的比较

![image.png](https://cdn.atstudy.com/lab/manual/16315209237322243.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315898348954322.png?fileid=3701925924295410706)

视频-6、Hive与传统数据库的比较

由于 Hive 采用了类似 SQL 的查询语言 HQL(Hive Query Language)，因此很容易将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无类似之处。我们将从多个方面来阐述 Hive 和数据库的差异。数据库可以用在 Online 的应用中，但是Hive 是为数据仓库而设计的，清楚这一点，有助于从应用角度理解 Hive 的特性。从九个方面做一个比较：

1. **查询语言。**由于 SQL 被广泛的应用在数据仓库中，因此专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发。
2. **数据存储位置。**Hive是建立在Hadoop之上的，所有Hive的数据都是存储在HDFS中的。而数据库则可以将数据保存在块设备或者本地文件系统中。
3. **数据格式。**Hive中没有定义专门的数据格式，数据格式可以由用户指定，用户定义数据格式需要指定三个属性：列分隔符（通常为空格、”\t”、”\x001″）、行分隔符（”\n”）以及读取文件数据的方法（Hive中默认有三个文件格式TextFile，SequenceFile以及RCFile）。由于在加载数据的过程中，不需要从用户数据格式到Hive定义的数据格式的转换，因此，Hive在加载的过程中不会对数据本身进行任何修改，而只是将数据内容复制或者移动到相应的HDFS目录中。而在数据库中，不同的数据库有不同的存储引擎，定义了自己的数据格式。所有数据都会按照一定的组织存储，因此，数据库加载数据的过程会比较耗时。
4. **数据更新。**由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive中不支持对数据的改写和添加，所有的数据都是在加载的时候中确定好的。而数据库中的数据通常是需要经常进行修改的，因此可以使用INSERT INTO ... VALUES添加数据，使用UPDATE ... SET修改数据。
5. **索引。**之前已经说过，Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。Hive要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。由于MapReduce的引入， Hive可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了Hive不适合在线数据查询。
6. **执行。**Hive中大多数查询的执行是通过Hadoop提供的MapReduce来实现的（类似select * from tbl的查询不需要MapReduce）。而数据库通常有自己的执行引擎。
7. **执行延迟。**之前提到，Hive在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致Hive执行延迟高的因素是MapReduce框架。由于MapReduce本身具有较高的延迟，因此在利用MapReduce执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势。
8. **可扩展性。**由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的。而数据库由于ACID语义的严格限制，扩展行非常有限。
9. **数据规模。**由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。

### 任务四、Hive运行环境启动与测试

#### 【任务目标】

 本次任务主要测试HIve的平台运行环境，掌握如何连结HIve以及Hive的一些最基本的命令。主要包括如下:

- 了解AtStudy项目实训平台基本使用方法
- 了解AtStudy项目实训平台如何启动hadoop服务
- 了解AtStudy项目实训平台如何启动Hivesserver2服务
- 了解AtStudy项目实训平台如何通过beeline连接hive数据仓库

#### 【任务步骤】

![image.png](https://cdn.atstudy.com/lab/manual/16315209531259759.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315899283247585.png?fileid=3701925924295490917)

视频-7、Hive运行环境启动与测试

##### 4.1、打开Terminal终端

进入项目实训平台后，如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16315209640536897.png)

在桌面空白位置点击鼠标右键，选择 `在此打开终端 `如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16315209753853868.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315209861248141.png)

##### 4.2、启动hadoop

在命令终端输入以下命令，启动hadoop相关服务

```
start-all.sh复制代码
```

启动效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16315210020573350.png)

##### 4.3、查看服务状态

可以通过如下命令查看Hadoop服务的启动状态：

```
jps复制代码
```

执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16315210126049473.png)

如果出现如上服务，则恭喜你的Hadoop启动成功。

##### 4.4、启动Hive

**1）方式一：直接启动Hive客户端**

可以通过如下命令启动Hive客户端：

```
hive复制代码
```

执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16315210226031793.png)

出现如上效果，就说明Hive客户端已经成功启动。接下来，可以做一些简单测试：

- 创建一张测试表

```
create table stuInfo(
stuid int,
name string,
age int);复制代码
```

- 查看有哪些表

```
show tables;复制代码
```

- 插入一条记录

```
insert into stuInfo(stuid,name,age)values(1,'zhangsan',18);复制代码
```

- 查看数据表

```
select * from stuInfo;复制代码
```

上述代码执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16315210777006775.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315210861211117.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315210946206690.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315211028083666.png)

最后，退出Hive客户端的命令是：exit; 或 quit; 或 `ctrl+c`

```
exit;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315211135685160.png)

**2）方式二、JDBC方式启动Hive**

![image.png](https://cdn.atstudy.com/lab/manual/16315211245436809.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315900541454126.png?fileid=3701925924295644669)

视频-8、JDBC方式启动Hive

**（1）前端方式启动 hiveserver2 服务**

**第一步，**首先要先启动hiveserver2这个服务，此种方式为前端启动（即该窗口不能关闭或退出服务）

```
hive --service hiveserver2复制代码
```

注意：该命令执行完后由于是在后台启动Hiveserver2服务，因此执行完该命令需要约1分钟左右Hiveserver2服务才能启动。

动行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16315211426118449.png)

为确保此服务已正常启动，建议另开一个窗口，查看10000进程是否正常启动：

![image.png](https://cdn.atstudy.com/lab/manual/16315211538506907.png)

再次打开一个新的窗口，在新的窗口中输入如下命令查看hiveserver2进程是否已启动：

```
netstat -ntulp |grep 10000复制代码
```

运行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16315211680838585.png)

> 特别说明：若未出现上图所示结果，则说明还正在启动，稍等一会儿后再重新查看！

**第二步，**通过beeline连接hiveserver2

启动beeline连接查询工具：

```
beeline复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315211851995734.png)

通过以下命令正式连接到hive数据仓库默认`default`数据库：

```
!connect jdbc:hive2://localhost:10000/default复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315873980547917.png)

**第三步，**测试连结及数据库表操作

可以在以上命令行中输入如下命令来进行数据的相关查询操作：

```
# 查看有哪些表
show tables;
# 查看数据表
select * from stuInfo;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315874152962952.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631587422873164.png)

如果想退出beeline客户端，可以输入`!q` 退出或直接按 `ctrl+c` 强行退出！

**（2）后端运行方式启动 hiveserver2 服务**

也可以使用如下命令后台启动 hiveserver2 这个服务（即该窗口关闭服务仍在后台运行）：

```
nohup hive --service hiveserver2 2>&1 &复制代码
```

> 特别说明：
>
> 1、nohup: 放在命令开头，表示不挂起,也就是关闭终端进程也继续保持运行状态 2、/dev/null：是 Linux 文件系统中的一个文件，被称为黑洞，所有写入改文件的内容 都会被自动丢弃 3、2>&1 : 表示将错误重定向到标准输出上 4、&: 放在命令结尾,表示后台运行

![image.png](https://cdn.atstudy.com/lab/manual/16315874447992293.png)

后继操作步骤同 `1）前端方式启动 hiveserver2 服务` ，可以尝试使用如下命令测试：

```
beeline
!connect jdbc:hive2://localhost:10000/default
create database testdb;
show databases;
show tables;
insert into stuinfo values(2,'lishi',20);
select * from stuinfo;复制代码
```

具体运行效果，略！

> 特别说明：使用beeline连接Hive仓库会有一个 `use database ` 后，库名称不会切换的问题
>
> 解决方案：
>
> 启动 beeline 时，加一个 `showDbInPrompt` 参数即可，完整命令如下：
>
> ```
>  beeline --showDbInPrompt=true
> ```

- 演示代码

```
# 首先结束 hiveserver2服务
jps  # 查看所有服务
# 强行结束hiveserver2服务，特别说明7997为hiveserver2运行的进程：RunJar  ID，
kill -9 7997  # RunJar ID系统随机分配，每次可能都不太一样
# 再次后台启动hiveserver2服务
nohup hive --service hiveserver2 2>&1 &
# 然后再重新启动beeline客户端
beeline --showDbInPrompt=true
# 连结到默认的default数据库
!connect jdbc:hive2://localhost:10000/default
# 查看deault数据库下有哪些表
show tables;
# 查看hive仓库有哪些数据库
show databases;
# 切换到testdb数据库
use testdb;
# 再次查看testdb数据库有哪些表
show tables;复制代码
```

- 运行效果截图

![image.png](https://cdn.atstudy.com/lab/manual/163158746049456.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315874726299233.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315874821656767.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631587494756542.png)

##### 4.5、查看产生的HDFS文件

可以通过9870端口查看Hive上述操作产生的HDFS文件，在实训平台内部打开浏览器，输入地址如下：

```
http://localhost:9870复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315875184353551.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315875298244197.png)

点击 `stuinfo ` 链结进去，可以看到，默认情况下，hdfs中会将每一条记录做为一个文件存储的。

![image.png](https://cdn.atstudy.com/lab/manual/16315875632003933.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631587574311912.png)

##### 4.6、查看MySQL元数据

Hive 的元数据信息在企业由通常是配置存储在关系型数据库：hive 中，常用MySQL数据库作为元数据库管理。

Hive的元数据信息在MySQL数据中有57张表，如下图：

![image.png](https://cdn.atstudy.com/lab/manual/16315876082989794.png)

> 特别说明：
>
> 关注其中的两张表：DBS 与 TBLS ,想了解更多表结构信息见：https://www.cnblogs.com/frankdeng/p/9126088.html

**1）DBS表：** 存储Hive中所有数据库的基本信息，表结构说明如下：

![image.png](https://cdn.atstudy.com/lab/manual/16315876205451265.png)

**2）TBLS表：** 该表中存储Hive表、视图、索引表的基本信息，表结构说明如下：

![image.png](https://cdn.atstudy.com/lab/manual/16315876303206828.png)

- 演示代码

```
# 登录mysql,平台沙箱帐号：root,密码 123123
mysql -uroot -p123123
# 查看数据库
show databases;
# 进入 hive 库
use hive;
# 查看数据表
show tables;
# 查看DBS表数据
select * from DBS;
# 查看TBLS表数据
select * from TBLS;复制代码
```

- 部分运行效果截图

![image.png](https://cdn.atstudy.com/lab/manual/16315876439893672.png)

> 注意：上述程序访问hdfs路径为本机9000端口，如果浏览器访问则为9870端口：[http://localhost:9870](http://localhost:9870/)

![image.png](https://cdn.atstudy.com/lab/manual/16315876561669411.png)

### 知识拓展：MapReduce原理解析

#### 【任务目标】

 本次任务主要补充介绍了什么是MapReduce，其要解决的问题，实现原理，内部实现架构，与具体实现过程。通过本次任务讲解，能够对Hive所依赖的Hadoop层框架MapReduce有一个基本的理解。为后继更好的学习Hive大数据分析打下基础。

#### 【任务步骤】

##### 5.1 、什么是MapReduce

![image.png](https://cdn.atstudy.com/lab/manual/16315876682875879.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315901570617991.png?fileid=3701925924295600443)

视频-9、什么是 MapReduce

MapReduce是一种编程模型，其理论来自Google公司发表的三篇论文（MapReduce，BigTable，GFS）之一，

主要应用于海量数据的并行计算。

可以通过如下几个生活案例更进一步的理解什么是MapReduce？

![image.png](https://cdn.atstudy.com/lab/manual/16315876837814575.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315876949061131.png)

上面这张图是什么意思呢？我们来分别解释一下步骤：

**1.Map**：以各个省为单位，多个线程并行读取不同省的人口数据，每一条记录生成一个Key-Value键值对。图中仅仅是简化了的数据。

**2.Shuffle:** Shuffle这个概念在前文并未提及，它的中文意思是“洗牌”。Shuffle的过程是对数据映射的排序、分组、拷贝。

**3.Reduce:** 执行之前分组的结果，并进行汇总和输出

需要注意的是，这里描述的Shuffle只是抽象的概念，在实际执行过程中Shuffle被分成了两部分，一部分在Map任务中完成，一部分在Reduce任务中完成。

##### 5.2、MapReduce 模型简述

![image.png](https://cdn.atstudy.com/lab/manual/16315877087957328.png)

##### 5.3、从打牌到 map-reduce 工作原理

![image.png](https://cdn.atstudy.com/lab/manual/1631587718290299.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315902487683182.png?fileid=3701925924295772754)

视频-10、从一个生活案例理解 MapReduce

这里采用了生活中最常见的一个案例：打牌，利用生活化娱乐化，漫话形式寓教于乐的教学方式，采用通俗易懂的语言对Hadoop的Map-Reduce原理进行深入解析，并进行代码实现，并且对Map-Reduce进行了部分的优化处理操作。

**（1）任务背景简介**

![image.png](https://cdn.atstudy.com/lab/manual/16315877330289094.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315877430513228.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315877508794209.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315877608755180.png)

**（2）Map-Reduce 原理解析**

![image.png](https://cdn.atstudy.com/lab/manual/16315877805551371.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315877982781607.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631587835806159.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315878457181687.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315878547877091.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315878684009076.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631587876799748.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315878859181112.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315878955034671.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315879042931255.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315879137068240.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315879229439923.png)

## 实验总结

- 大数据的五大特点（5V）：数据量大（Volume）、类型多样（Variety）、处理速度快（Velocity）、价值密度（Value）、真实性（Veracity）
- Hive是由 Facebook 开源用于解决海量结构化日志的数据统计工具。 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类 SQL 查询功能。
- Hive的主要的优缺点及适用场景：
  - 优点
    - 操作接口采用类 SQL 语法，提供快速开发的能力（简单、容易上手）
    - 避免了去写 MapReduce，减少开发人员的学习成本。
    - Hive 支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。
  - 缺点
    - Hive 的 的 HQL 表达能力有限
    - Hive 的效率比较低
  - 适用场景
    - Hive 的执行延迟比较高，因此 Hive 常用于数据分析，对实时性要求不高的场合。
    - Hive 优势在于处理大数据，对于处理小数据没有优势，因为 Hive 的执行延迟比较高。
- HIve架构原理

![image.png](https://cdn.atstudy.com/lab/manual/16315880336588617.png)

- Hive启动两种方式

  - 方式一：直接Hive启动

    ```
    hive复制代码
    ```

  - 方式二：JDBC方式启动

    （1）前端方式启动 hiveserver2 服务

    ```
    hive --service hiveserver2
    beeline
    !connect jdbc:hive2://localhost:10000/default复制代码
    ```

    （2）后端方式启动 hiveserver2 服务

    ```
    nohup hive --service hiveserver2 2>&1 &
    beeline
    !connect jdbc:hive2://localhost:10000/default复制代码
    ```

- 可以通过9870端口查看Hive上述操作产生的HDFS文件

  ```
  http://localhost:9870复制代码
  ```

- Hive 的元数据信息在企业由通常是配置存储在关系型数据库：hive 中





# 实验2-1：Hive中的数据类型及实际应用

## 实验概述

本次实验主要讲解了Hive的基础数据类型、集合数据类型。并借助案例演示了具体数据类型的不同使用技巧及其不同数据类型间的相互转换 .

## 实验环境

- Linux 操作系统
- Hadoop 3.x
- Hive 3.x
- JDK 1.8
- MySQL 8.x

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16315973046669273.png)

## 实验目标

学习完成本实验后，您将能够

- 了解 Hive的数据类型
- 能够熟练为不同的数据定义不同的数据类型
- 熟练使用集合数据类型
- 了解如何在Hive中进行不同类型之间的转换

## 实验任务

### 任务一、实验环境准备

#### **【任务目标】**

 本次任务主要准备好Hive运行的初始化实验环境，为后继实验任务做好准备。

#### 【任务步骤】

![image.png](https://cdn.atstudy.com/lab/manual/16315973188978756.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315989753229454.png?fileid=3701925924300517157)

视频-1、Hive运行环境启动与测试

##### 1.1、打开Terminal终端

进入项目实训平台后，如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16315973350774335.png)

在桌面空白位置点击鼠标右键，选择 `在此打开终端 `如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16315973452962590.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315973555731691.png)

##### 1.2、启动hadoop

在命令终端输入以下命令，启动hadoop相关服务

```
start-all.sh复制代码
```

启动效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16315973673178685.png)

##### 1.3、查看服务状态

可以通过如下命令查看Hadoop服务的启动状态：

```
jps复制代码
```

执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16315974166815891.png)

如果出现如上服务，则恭喜你的Hadoop启动成功。

##### 1.4、启动Hive

可以通过如下命令启动Hive客户端：

```
hive复制代码
```

执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16315974288221231.png)

##### 1.5、创建testdb数据库

```
create database testdb;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315974414521096.png)

### 任务二、Hive中的数据类型

#### 【任务目标】

 本次任务主要讲解了Hive中有哪些基础的数据类型以及复杂的数据类型：集合数据类型。

#### 【任务步骤】

##### 2.1、基本数据类型

![image.png](https://cdn.atstudy.com/lab/manual/16315974590434367.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315997764073591.png?fileid=3701925924301083659)

视频-2、Hive中常见的数据类型

![image.png](https://cdn.atstudy.com/lab/manual/16315974752708020.png)

> 说明：
>
> 对于 Hive 的 String 类型相当于数据库的 varchar 类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储 2GB 的字符数。

##### 2.2、集合数据类型

![image.png](https://cdn.atstudy.com/lab/manual/16315974848019661.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631599869694778.png?fileid=3701925924301075579)

视频-3、Hive中的集合数据类型

![image.png](https://cdn.atstudy.com/lab/manual/16315974996177497.png)

> 说明：
>
> Hive 有三种复杂数据类型 ARRAY、MAP 和 STRUCT。ARRAY 和 MAP 与 Java 中的 Array和 Map 类似，而 STRUCT 与 C 语言中的 Struct 类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。

### 任务三、Hive集合类型演练

#### 【任务目标】

 本次任务结合具体案例对Hive中的复杂集合数据类型进行演练。

#### 【任务步骤】

![image.png](https://cdn.atstudy.com/lab/manual/16315975158654966.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316000958194345.png?fileid=3701925924301422760)

视频-4、Hive中的集合类型实操演示

##### 3.1、需求描术

将从某交友信息网站中爬虫的JSON格式的数据保存至Hive仓库中，假设从某网站API接口获取二条JSON格式的数

据，在 Hive 下访问的格式为：

```
{
    "name": "xiao ming",
    "friends": ["jiyou01" , "jiyou02"] , //列表 Array,
    "children": { //键值 Map,
    "zhang san": 20 ,
    "li shi": 19
    }
    "address": { //结构 Struct,  
    "street": "nan jing road",
    "city": "shanghai"
    }
},
{
    "name": "rose",
    "friends": ["haojiyou01" , "haojiyou02"] , //列表 Array,
    "children": { //键值 Map,
    "tom": 18 ,
    "jack": 19
    }
    "address": { //结构 Struct,  
    "street": "zhong shan bei road", 
    "city": "shanghai"
    }
}复制代码
```

##### 3.2、具体实现

（1）基于上述数据结构，将数据清洗为Hive方便读取的格式，保存在本地测试文件中：

在 `home/atstudy` 目录下新建 data 目录，在其中创建 test.txt ,内容如下：

```
xiao ming,jiyou01_jiyou02,zhang san:20_li shi:19,nan jing road_shanghai
rose,haojiyou01_haojiyou01,tom:18_jack:19,zhong shan bei road_shanghai复制代码
```

> 注意：MAP，STRUCT 和 ARRAY 里的元素间关系都可以用同一个字符表示，这里用“_”。

![image.png](https://cdn.atstudy.com/lab/manual/16315975335289379.png)

（2）Hive 上创建测试表 test

- 再次后台启动hiveserver2服务

```
nohup hive --service hiveserver2 2>&1 &复制代码
```

- 然后再重新启动beeline客户端

```
beeline --showDbInPrompt=true复制代码
```

- 连结到testdb数据库

```
!connect jdbc:hive2://localhost:10000/testdb复制代码
```

> 注：过程中可能需要输入连结MySQL的帐号与密码，请分别输入：root/123123

- 创建表

```
create table test(
name string,
friends array<string>,
children map<string, int>,
address struct<street:string, city:string>
)
row format delimited fields terminated by ','
collection items terminated by '_'
map keys terminated by ':'
lines terminated by '\n';复制代码
```

> 字段解释： row format delimited fields terminated by ',' -- 列分隔符 collection items terminated by '_' --MAP STRUCT 和 ARRAY 的分隔符(数据分割符号) map keys terminated by ':' -- MAP 中的 key 与 value 的分隔符 lines terminated by '\n'; -- 行分隔符

![image.png](https://cdn.atstudy.com/lab/manual/16315975490849441.png)

![image.png](https://cdn.atstudy.com/lab/manual/16315975604142688.png)

（3）导入文本数据到Hive测试表

```
load data local inpath '/home/atstudy/data/test.txt' into table test;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315975727851223.png)

（4）访问三种集合列里的数据，以下分别是 ARRAY，MAP，STRUCT 的访问方式

```
select * from test;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315975826565528.png)

```
select friends[1],children['tom'],address.city from test where name="rose";复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315975986698517.png)

### 任务四、Hive 类型转换

#### 【任务目标】

 本次任务主要讲解如何在Hive中进行不同类型之间的数据转换。

#### 【任务步骤】

![image.png](https://cdn.atstudy.com/lab/manual/16315976107581408.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316010282506231.png?fileid=3701925924301773615)

视频-5、Hive中的类型转换

##### 4.1、类型转换

Hive 的原子数据类型是可以进行隐式转换的，类似于 Java 的类型转换，例如某表达式使用 INT 类型，TINYINT 会自动转换为 INT 类型，但是 Hive 不会进行反向转化，例如，某表达式使用 TINYINT 类型，INT 不会自动转换为 TINYINT 类型，它会返回错误，除非使用 CAST操作。

（1）隐式类型转换规则如下

- 任何整数类型都可以隐式地转换为一个范围更广的类型，如 TINYINT 可以转换成 INT，INT 可以转换成 BIGINT。
- 所有整数类型、FLOAT 和 STRING 类型都可以隐式地转换成 DOUBLE。
- TINYINT、SMALLINT、INT 都可以转换为 FLOAT。
- BOOLEAN 类型不可以转换为任何其它的类型。

（2）可以使用 CAST 操作显示进行数据类型转换

- 例如 CAST('1' AS INT) 将把字符串'1' 转换成整数 1；如果强制类型转换失败，如执行CAST('X' AS INT)，表达式返回空值 NULL。

##### 4.2、案例演示

示例：

```
select '1'+1, cast('1'as int) + 1;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16315976251486399.png)

### 任务五、能力拓展训练

#### 【任务目标】

 本次任务主要对上述实验的内容再次进行代码演练，以达到熟练掌握的目的。

#### 【任务步骤】

需求：

（1）启动 Hadoop 服务

```
start-all.sh复制代码
```

（2）查看Hadoop是否正常启动，查看后台运行的进程

```
jps复制代码
```

（3）普通方式启动 hive

3.1、普通方式启动hive客户端：

```
hive复制代码
```

3.2、创建一个newdb数据库

```
create database newdb;复制代码
```

3.3、查看hive仓库中数据库有哪些

```
show databases;复制代码
```

3.4、退出Hive客户端

```
exit;复制代码
```

（4）JDBC 方式启动 hive

4.1、启动 hiveserver2 服务（后台启动）

```
nohup hive --service hiveserver2 2>&1 &复制代码
```

4.2、启动 beeline 客户端

```
beeline --showDbInPrompt=true复制代码
```

4.3、连结到 newdb 数据库

```
!connect jdbc:hive2://localhost:10000/default;
use newdb;复制代码
```

4.4、查看该数据库中有没有表，有哪些表

```
show tables;复制代码
```

4.5、创建一个学生表：stu_info , 三个字段：id int , name string , age int

```
create table  stu_info(id int,name string,age int);
或
create table if not exists stu_info(id int,name string,age int);复制代码
```

4.6、向stu_info表中插入一条数据：1，zhangsan,20

```
insert into stu_info(id,name,age)values(1,'zhangsan',20);复制代码
```

4.7、查看该表 stu_info 数据是否插入成功

```
select * from stu_info;复制代码
```

4.8、通过浏览器查看上述Hive操作产生的HDFS数据

```
http://localhost:9870
下面可以配一张截图复制代码
```

4.9、查看MYSQL中的HIve元数据：DBS 与 TBLS，查看两个表中相关对应的库表数据

```
mysql -uroot -p123123
use hive;
select * from DBS;
select * form TBLS;复制代码
```

> mysql帐号密码分别是：root/123123

## 实验总结

- Hive运行环境与常规命令

```
# 1,后台启动hiveserver2服务
nohup hive --service hiveserver2 2>&1 &
# 2,启动beeline客户端
beeline --showDbInPrompt=true
# 3,连结到testdb数据库
beeline> !connect jdbc:hive2://localhost:10000/testdb复制代码
```

- Hive 有三种复杂数据类型 ARRAY、MAP 和 STRUCT。ARRAY 和 MAP 与 Java 中的 Array和 Map 类似，而 STRUCT 与 C 语言中的 Struct 类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。
- 可以使用 CAST 操作显示进行数据类型转换





# 实验2-2：Hive中常用交互命令

## 实验概述

本次实验主要讲解了Hive中的一些常用交互命令，并结合实际案例进行演练。

## 实验环境

- Linux 操作系统
- Hadoop 3.x
- Hive 3.x
- JDK 1.8
- MySQL 8.x

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16316014776603831.png)

## 实验目标

学习完成本实验后，您将能够

- 1、了解Hive中的常用交互命令 2、能够熟练使用Hive交互命令完成Hive常规操作

## 实验任务

### 任务1：初始化实验环境及准备数据

#### **【任务目标】**

 本次任务主要初始化Hive的实验环境以及准备好一些实验数据，为后继的任务打下基础。

#### **【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16316015340484114.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316060804536899.png?fileid=3701925924303917570)

视频-1：Hive开发环境及数据准备

##### 1.1、打开Terminal终端

进入项目实训平台后，如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16316015533749475.png)

在桌面空白位置点击鼠标右键，选择 `在此打开终端 `如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16316015652716597.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316015780907935.png)

##### 1.2、启动hadoop

在命令终端输入以下命令，启动hadoop相关服务

```
start-all.sh复制代码
```

启动效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16316015908485932.png)

##### 1.3、查看服务状态

可以通过如下命令查看Hadoop服务的启动状态：

```
jps复制代码
```

执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16316016081439053.png)

如果出现如上服务，则恭喜你的Hadoop启动成功。

##### 1.4、启动Hive

```
# 启动hive 
hive
# 查看有没有表
show tables;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316016254364380.png)

##### 1.5、准备实验数据

- 创建数据库

```
create database if not exists testdb;复制代码
```

- 选择数据库

```
use testdb;复制代码
```

- 创建表

```
create table if not exists student(id int,name string,age int) ;复制代码
```

- 插入二条记录

```
insert into student(id,name,age)values(1,'zhangsan',18);
insert into student(id,name,age)values(2,'lishi',19);复制代码
```

- 查看表

```
select * from student;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316016405228841.png)

##### 1.6、退出hive客户端

最后记得，通过 `exit;` 命令退出hive客户端。

### 任务2：Hive常用交互命令

#### **【任务目标】**

 本次任务主要是了解常用的Hive交互命令：包括但不限于：使用帮助，退出，执行SQL及hive cli方式查看hdfs等常见命令。

#### **【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16316016525905620.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316061497656706.png?fileid=3701925924304082379)

视频-2：Hive常用交互命令

##### 2.1、help命令

在Linux终端中，可以使用 ` hive -help` 命令查看相关帮助，效果如下图所示：

```
hive -help复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316016647119726.png)

##### 2.2、 “-e ”不进入 hive 的交互窗口执行 sql

可能通过 “-e ”在不进入 hive 的交互窗口的情况下执行 sql 语句：

```
hive -e "select * from student;"复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316048649121382.png)

> 说明：
>
> hive -e "select * from 库名.表名" ， 如果库名没有指定，则默认为 default 库

##### 2.3、 “-f ” 执行脚本中 sql

（1）在/home/atstudy下创建 datas 目录并在 datas 目录下创建 hivef.sql 文件

```
cd /home/atstudy
mkdir datas
cd datas
touch hivef.sql
vim hivef.sql复制代码
```

（2）文件中写入正确的 sql 语句

```
select * from student;复制代码
```

（3）执行文件中的 sql 语句

```
hive -f /home/atstudy/datas/hivef.sql复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316048822928396.png)

（4）执行文件中的 sql 语句并将结果写入文件中

```
hive -f /home/atstudy/datas/hivef.sql >/home/atstudy/datas/student.txt复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316048945848779.png)

##### 2.4、HIve其他命令

（1）退出 hive 窗口：

```
hive> exit;
hive> quit;复制代码
```

（2））在 hive cli 命令窗口中如何查看 hdfs 文件系统

```
dfs -ls /;
dfs -ls /user/hive/warehouse/student;
dfs -cat /user/hive/warehouse/student/000000_0;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316049072907175.png)

### 任务3：Hive常用命令拓展训练

#### **【任务目标】**

 本次任务主要将Hive中常用的交互命令结合具体案例进行演练。

#### **【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16316049427659044.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316061950458302.png?fileid=3701925924304283621)

视频-3：Hive常用命令能力拓展训练

##### 3.1、 “-e” 不进入hive的交互窗口执行sql语句

1）查找学生表中的内容(如果不指定，自动默认访问)

```
hive -e "select * from student;"复制代码
```

2）指定访问

```
hive -e "select * from default.student;"复制代码
```

3）向学生表中插入信息

```
hive -e "insert into student values(3,"tom",18)复制代码
hive -e "insert into student values(4,"jack",20)复制代码
```

4）把结果保存为文件

```
hive -e "select * from student;" > 1.txt复制代码
```

5）查看结果

```
cat 1.txt 复制代码
```

6）删除学生表

```
hive -e "drop table student;"复制代码
```

##### 3.2、 “-f” 执行sql脚本

1）在`/home/atstudy` 目录下创建 test.sql 文件

```
cd /home/atstudy
vim test.sql复制代码
```

test.sql 中写入：`select * from student limit 2`

2）执行test.sql

```
hive -f test.sql 复制代码
```

3）执行文件中的sql语句并将结果写入文件中

```
hive -f test.sql > hive_result.txt复制代码
```

4）查看 hive_result.txt 文件

```
cat hive_result.txt复制代码
```

### 拓展知识：HDFS原理与架构

#### **【任务目标】**

本次任务主要补充讲解了Hadoop架构解析以及HDFS的原理与架构，为后继更好的学习HIve大数据分析打下扎实的理论基础。

#### **【任务步骤】**

##### 4.1、HDFS 简介、原理、架构

![image.png](https://cdn.atstudy.com/lab/manual/16316049706234630.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316062559166090.png?fileid=3701925924304493231)

视频-4：HDFS简介、原理、架构

**（1）Hadoop 架构解析**

![image.png](https://cdn.atstudy.com/lab/manual/16316049936197252.png)

Hadoop框架中最核心的设计就是：**MapReduce** 和 **HDFS**

- MapReduce 的思想是由 Google 的一篇论文所提及而被广为流传的，简单的一句话解释 MapReduce 就是“任务的分解与结果的汇总”
- HDFS 是 Hadoop 分布式文件系统（Hadoop Distributed File System）的缩写，为分布式计算存储提供了底层支持

**（2）HDFS简介**

HDFS（Hadoop Distributed File System ）Hadoop分布式文件系统。是根据google发表的论文翻版的。论文为GFS（Google File System）Google 文件系统（中文，英文）。HDFS （Hadoop Distributed File System）是 Hadoop 下的分布式文件系统，具有高容错、高吞吐量等特性，可以部署在低成本的硬件上。

**（3）HDFS 架构**

HDFS 遵循主/从架构，由单个 NameNode(NN) 和多个 DataNode(DN) 组成：

- NameNode : 负责执行有关 文件系统命名空间 的操作，例如打开，关闭、重命名文件和目录等。它同时还负责集群元数据的存储，记录着文件中各个数据块的位置信息。
- DataNode：负责提供来自文件系统客户端的读写请求，执行块的创建，删除等操作。

**（4）HDFS 架构的稳定性**

- 心跳机制和重新复制
- 数据的完整性
- 元数据的磁盘故障
- 支持快照

**（5）HDFS 的特点**

- 高容错

  由于 HDFS 采用数据的多副本方案，所以部分硬件的损坏不会导致全部数据的丢失。

- 高吞吐量

  HDFS 设计的重点是支持高吞吐量的数据访问，而不是低延迟的数据访问。

- 大文件支持

  HDFS 适合于大文件的存储，文档的大小应该是是 GB 到 TB 级别的。

- 简单一致性模型

  HDFS 更适合于一次写入多次读取 (write-once-read-many) 的访问模型。支持将内容追加到文件末尾，但不支持数据的随机访问，不能从文件任意位置新增数据。

- 跨平台移植性

  HDFS 具有良好的跨平台移植性，这使得其他大数据计算框架都将其作为数据持久化存储的首选方案。

**（6）HDFS 架构原理**

![image.png](https://cdn.atstudy.com/lab/manual/16316050111609244.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316050210746281.png)

**（7）HDFS 的一些关键元素**

- **Block：**将文件分块，通常为64M。
- **NameNode：**是Master节点，是大领导。管理数据块映射；处理客户端的读写请求；配置副本策略；管理HDFS的名称空间。保存整个文件系统的目录信息、文件信息及分块信息，由唯一一台主机专门保存。
- **SecondaryNameNode：**是一个小弟，分担大哥 NameNode 的工作量；是NameNode的 冷备份；合并 fsimage 和 editslog 然后再发给 NameNode。（热备份：b是a的热备份，如果a坏掉。那么b马上运行代替a的工作。冷备份：b是a的冷备份，如果a坏掉。那么b不能马上代替a工作。但是b上存储a的一些信息，减少a坏掉之后的损失。）
- **DataNode：**是Slave节点，奴隶，干活的。负责存储Client发来的数据块block；执行数据块的读写操作。
- **fsimage：**元数据镜像文件（文件系统的目录树）
- **editslog：**元数据的操作日志（针对文件系统做的修改操作记录）

##### 4.2、漫话 HDFS 底层实现原理

![image.png](https://cdn.atstudy.com/lab/manual/16316050431393212.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316065641011367.png?fileid=3701925924304654523)

视频-5：漫话HDFS的底层原理架构

![image.png](https://cdn.atstudy.com/lab/manual/16316050584843732.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316050721913875.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316050820295944.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316050916448292.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316051033691338.png) ![image.png](https://cdn.atstudy.com/lab/manual/16316051146378912.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316051242153125.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316051343596269.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316051429032108.png)

## 实验总结

- 查看 hive 命令：hive -help
- hive -e 不进入 hive 的交互窗口执行 sql
- hive -f 执行脚本中 sql
- exit 或 quit 退出 hive
  - exit:先隐性提交数据，再退出；
  - quit:不提交数据，退出；
- 在 hive cli 命令窗口中如何查看 hdfs 文件系统
  - dfs -ls /;
  - dfs -ls /user/hive/warehouse/student;
  - dfs -cat /user/hive/warehouse/student/000000_0;
- 在hive client命令窗口中如何查看本地文件系统
  - hive> ! ls /opt/module/datas;



# 实验2-3：Hive中的DDL操作

## 实验概述

本次实验主要讲解了Hive中的数据定义语言（DDL）语法及其注意事项，并结合具体案例进行实际代码演练。

## 实验环境

- Linux 操作系统
- Hadoop 3.x
- Hive 3.x
- JDK 1.8
- MySQL 8.x

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16316072105294278.png)

## 实验目标

学习完成本实验后，您将能够

- 了解Hive中的数据定义语言基础语法
- 能够熟练使用Hive的DDL语言完成数据库表操作

## 实验任务

### 任务1：初始化实验环境及准备数据

#### **【任务目标】**

 本次任务主要初始化Hive的实验环境以及准备好一些实验数据，为后继的任务打下基础。

#### **【任务步骤】**

##### 1.1、打开Terminal终端

![image.png](https://cdn.atstudy.com/lab/manual/16316072385802368.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316081872849756.png?fileid=3701925924305512257)

视频-01：Hive开发环境及数据准备

进入项目实训平台后，如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16316073146808347.png)

在桌面空白位置点击鼠标右键，选择 `在此打开终端 `如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16316073286495613.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316073400644613.png)

##### 1.2、启动hadoop

在命令终端输入以下命令，启动hadoop相关服务

```
start-all.sh复制代码
```

启动效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/163160735232355.png)

##### 1.3、查看服务状态

可以通过如下命令查看Hadoop服务的启动状态：

```
jps复制代码
```

执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16316073729024470.png)

如果出现如上服务，则恭喜你的Hadoop启动成功。

##### 1.4、启动Hive

```
# 启动hive 
hive
# 查看有没有表
show tables;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316073893379765.png)

##### 1.5、准备实验数据

- 创建数据库

```
create database if not exists testdb;复制代码
```

- 选择数据库

```
use testdb;复制代码
```

- 创建表

```
create table if not exists student(id int,name string,age int) ;复制代码
```

- 插入二条记录

```
insert into student(id,name,age)values(1,'zhangsan',18);
insert into student(id,name,age)values(2,'lishi',19);复制代码
```

- 查看表

```
select * from student;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316074035997023.png)

##### 1.6、退出hive客户端

最后记得，通过 `exit;` 命令退出hive客户端。

### 任务2：实战Hive库管理

#### 【任务目标】

 本次任务主要学习如何在HIve进行数据定义语言的DDL操作中的关于库的管理，包括但不限于：创建数据库，删除数据库等。

#### 【任务步骤】

![image.png](https://cdn.atstudy.com/lab/manual/16316074184945964.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316083219078298.png?fileid=3701925924305488640)

视频-02：Hive中的库管理操作

##### 2.1、创建数据库

```
CREATE DATABASE [IF NOT EXISTS] database_name
[COMMENT database_comment]
[LOCATION hdfs_path]
[WITH DBPROPERTIES (property_name=property_value, ...)];复制代码
```

1）创建一个数据库，数据库在 HDFS 上的默认存储路径是 `/user/hive/warehouse/*.db ` .

```
create database mydb;复制代码
```

2）避免要创建的数据库已经存在错误，增加 if not exists 判断。（标准写法）

```
create database if not exists mydb;复制代码
```

3）创建一个数据库，指定数据库在 HDFS

```
create database mydb2 location '/mydb2.db';复制代码
```

##### 2.2、查询数据库

1）显示数据库

```
show databases;复制代码
```

2）过滤数据库

```
show databases like 'my*';复制代码
```

##### 2.3、查看数据库详情

1）显示数据库信息

```
desc database mydb;复制代码
```

2）显示数据库详细信息 extended

```
desc database extended mydb;复制代码
```

##### 2.4、切换当前数据库

```
use db_hive;复制代码
```

##### 2.5、删除数据库

1）删除空数据库

```
drop database db_name;复制代码
```

2）如果删除的数据库不存在 ， 最好采用 if exists

```
drop database if exists db_name;复制代码
```

3）如果数据库不为空，可以采用 cascade 命令，强制删除

```
drop database db_name cascade;复制代码
```

### 任务3：实战Hive表管理

#### 【任务目标】

 本次任务主要学习如何在HIve进行数据定义语言的DDL操作中的关于表的管理，包括但不限于：表的创建，内部表，外总表，内外部间的转换及区别，删除表等。

#### 【任务步骤】

##### 3.1、创建表语法

![image.png](https://cdn.atstudy.com/lab/manual/16316074412249875.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316098733911839.png?fileid=3701925924306287195)

视频-03：Hive表管理：创建表语法

1）建表语法

```
CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name
[(col_name data_type [COMMENT col_comment], ...)]
[COMMENT table_comment]
[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
[CLUSTERED BY (col_name, col_name, ...)
[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
[ROW FORMAT row_format]
[STORED AS file_format]
[LOCATION hdfs_path]
[TBLPROPERTIES (property_name=property_value, ...)]
[AS select_statement]复制代码
```

解释说明：

- CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。
- EXTERNAL 关键字可以让用户创建一个外部表，在建表的同时可以指定一个指向实际数据的路径（LOCATION），在删除表的时候内部表的元数据和数据会被一起删除，而外部表只删除元数据不删除数据。
- COMMENT：为表和列添加注释。
- PARTITIONED BY 创建分区表
- CLUSTERED BY 创建分桶表
- SORTED BY 不常用，对桶中的一个或多个列另外排序
- ROW FORMAT DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value,property_name=property_value, ...)] 用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需 要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive 通过 SerDe 确定表 的具体的列的数据。SerDe 是 Serialize/Deserilize 的简称， hive 使用 Serde 进行行对象的序列与反序列化。
- STORED AS 指定存储文件类型，常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。
- LOCATION ：指定表在 HDFS 上的存储位置。
- AS：后跟查询语句，根据查询结果创建表。
- LIKE 允许用户复制现有的表结构，但是不复制数据

##### 3.2、内部表

![image.png](https://cdn.atstudy.com/lab/manual/16316074854499057.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316101605178088.png?fileid=3701925924306407152)

视频-04：Hive表管理：内部表管理

默认创建的表都是所谓的内部表，有时也被称为管理表。因为这种表，Hive 会（或多或少地）控制着数据的生命周期。Hive 默认情况下会将这些表的数据存储在由配置项 hive.metastore.warehouse.dir ( 例如：/user/hive /warehouse)所定义的目录的子目录下。当我们删除一个内部表时，Hive 也会删除这个表中数据。内部表不适合和其他工具共享数据。

> 查看HDFS：[http://localhost:9870](http://localhost:9870/)

**示例代码：**

1）创建内部表

```
create table if not exists employee(
id int, name string
)
row format delimited fields terminated by '\t'
stored as textfile
location '/user/hive/warehouse/employee';复制代码
```

2）插入测试数据

```
insert into employee(id,name) values (1001,'user01'),(1002,'user02'),(1003,'user03');复制代码
```

查看结果：

![image.png](https://cdn.atstudy.com/lab/manual/16316075013309475.png)

3）根据查询结果创建新表

```
create table if not exists employee2 as select id, name from employee;复制代码
```

查看结果：

![image.png](https://cdn.atstudy.com/lab/manual/1631607513158123.png)

4）根据已经存在的表结构创建表

```
create table if not exists employee3 like employee;复制代码
```

查看结果：

![image.png](https://cdn.atstudy.com/lab/manual/16316075290938262.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316075395576084.png)

5）查看创建表的类型

```
desc formatted employee;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316075497858958.png)

可以查看一下MySQL中TBLS表，了解相关表的类型：

![image.png](https://cdn.atstudy.com/lab/manual/16316075609825147.png)

##### 3.3、外部表

![image.png](https://cdn.atstudy.com/lab/manual/163160757206643.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316102958007285.png?fileid=3701925924306583557)

视频-05：外部表默认存储示例

可以使用 `create external table ` 关健字创建外部表。因为表是外部表，所以 Hive 并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，不过描述表的元数据信息会被删除掉。

**示例代码-1 ：外部表默认存储**

1）准备测试数据

- 导出 testdb.employee 表中的数据到 `/home/atstudy/datas/employee.txt` 文件中

```
cd /home/atstudy/datas复制代码
vim employee.sql 复制代码
```

内容为：`select * from testdb.employee;`

```
hive -f /home/atstudy/datas/employee.sql >/home/atstudy/datas/employee.txt复制代码
```

2）在hive中，创建一个名叫 extdb的数据库，其中用来存储外部表

```
create database extdb;复制代码
```

3）在 extdb 数据库中，创建 employee4 外部表

进入HIve命令行，执行如下命令：

- 进入extdb数据库

```
use extdb;复制代码
```

- 创建外部表 employee4

```
create external table if not exists employee4(
deptno int,
dname string
) row format delimited fields terminated by '\t';复制代码
```

4）将 employee.txt 文件手工导入至 HDFS extdb 默认的 employee4 的存储目录下：

```
dfs -put /home/atstudy/datas/employee.txt /user/hive/warehouse/extdb.db/employee4复制代码
```

5）查询数据

```
select * from employee4复制代码
```

6）再做一个实验，向employee4目录中再添加一个文件，再次验证数据，发现两个文件的内容自动合并

- 退出hive ,进入本地环境的/home/atstudy/datas/目录下

```
cd /home/atstudy/datas复制代码
```

- 产生一个复本文件

```
cp employee.txt 0000.txt复制代码
```

- 进入hive中，再次将此文件上传至hdfs目录

```
dfs -put /home/atstudy/datas/0000.txt /user/hive/warehouse/extdb.db/employee4复制代码
```

- 再次查询数据，发现0000.txt 与原来的 employee.txt 中的数据自动累加

```
selet * from employee4;复制代码
```

**示例代码-2：外部表指定存储**

![image.png](https://cdn.atstudy.com/lab/manual/16316075910375317.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316105443855925.png?fileid=3701925924306600177)

视频-06：外部表指定存储示例

1）准备数据

- dept

```
10	ACCOUNTING	1700
20	RESEARCH	1800
30	SALES	1900
40	OPERATIONS	1700复制代码
```

> 特别注意：以上的每个元素之间的间隔用tab键，在拷贝到Linux环境中时，最好每个数据之间的tab键再手工输入一次。

- emp

```
7369,SMITH,CLERK,7902,1980-12-17,800.00,100.00,20
7499,ALLEN,SALESMAN,7698,1981-2-20,1600.00,300.00,30
7521,WARD,SALESMAN,7698,1981-2-22,1250.00,500.00,30
7566,JONES,MANAGER,7839,1981-4-2,2975.00,300.00,20
7654,MARTIN,SALESMAN,7698,1981-9-28,1250.00,1400.00,30
7698,BLAKE,MANAGER,7839,1981-5-1,2850.00,1000.00,30
7782,CLARK,MANAGER,7839,1981-6-9,2450.00,1200.00,10
7788,SCOTT,ANALYST,7566,1987-4-19,3000.00,1000.00,20
7839,KING,PRESIDENT,7860,1981-11-17,5000.00,800.00,10
7844,TURNER,SALESMAN,7698,1981-9-8,1500.00,0.00,30
7876,ADAMS,CLERK,7788,1987-5-23,1100.00,300.00,20
7900,JAMES,CLERK,7698,1981-12-3,950.00,200.00,30
7902,FORD,ANALYST,7566,1981-12-3,3000.00,200.00,20
7934,MILLER,CLERK,7782,1982-1-23,1300.00,100.00,10复制代码
```

脚本代码 ：

```
cd /home/atstudy/
mkdir datas
touch dept
vim dept
touch emp
vim emp复制代码
```

2）上传数据到 HDFS

进入HIve环境，输入命令如下：

```
dfs -mkdir -p /extdb/dept;复制代码
dfs -put /home/atstudy/datas/dept /extdb/dept;复制代码
dfs -mkdir -p /extdb/emp;复制代码
dfs -put /home/atstudy/datas/emp /extdb/emp;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316076053714265.png)

3）创建外部表

- 创建部门表

```
create external table if not exists dept(
deptno int,
dname string,
loc int
) row format delimited fields terminated by '\t'
location '/extdb/dept';复制代码
```

- 创建员工表

```
create external table if not exists emp(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int)
row format delimited fields terminated by ','
location '/extdb/emp';复制代码
```

4）查看创建的表

```
show tables;复制代码
```

5）查看格式化的表数据

```
desc formatted dept;复制代码
```

6）在MySQL中查看外部表元数据：

进入mysql,查询 TBLS 表：

```
use hive;复制代码
select * from TBLS;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316076200907554.png)

7）删除外部表

```
drop table dept;复制代码
```

外部表删除后，发现 hdfs 中的数据还在，但是 metadata 中 dept 的元数据已被删除 !

##### 3.4、内部表与外部表的相互转换

![image.png](https://cdn.atstudy.com/lab/manual/16316076320985457.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316108012673864.png?fileid=3701925924306738884)

视频-07：内部表与外部表的相互转换

1）内部表转外部表

```
use testdb;
show tables;
select * from student;
# 查看表详细
desc formatted student;
# 修改内部表为外部
alter table student set tblproperties('EXTERNAL'='TRUE');
# 修改location
alter table student set location '/extdb/student';
# 查看表详情
desc formatted student;复制代码
```

2）外部表转内部表

```
# 修改外部表为内部表
alter table student set tblproperties('EXTERNAL'='FALSE');
# 修改location
alter table student set location '/user/hive/warehouse/testdb.db/student';
# 查看表详情
desc formatted student;复制代码
```

适用场景:

```
现象：如果创建的外部表在默认的路径下，不会删除文件，即使更改表名，该路径下的文件的名字也不会改变，此时如果创建再创建一个和原来名字相同的外部表，会造成两个表的数据路径是一样的。如果第二次创建的表和第一次创建的表结构不一致，在查询第二个表时，会报错。
场景：这种情景通常用在备份表的时候出现：
当表结构出现变化时，要将历史表：xm_test表备份为xm_test_back，再创建一个新具有的表结构的xm_test，但是由于上述的原因，会出现错误。
解决方案：
将xm_text，转换成内部表，将其改名为：xm_test_back，再创建一个具有新的表结构的外部表xm_test。复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316076535271856.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316110739971586.png?fileid=3701925924306877755)

视频-08：内外部表的区别及Hive中表的删除

##### 3.5、内部表与外部表的区别

**1）内部表与外部表的异同**

- 建表时带有external关键字为外部表，否则为内部表
- 内部表和外部表建表时都可以自己指定location
- 删除表时，外部表不会删除对应的数据，只会删除元数据信息，内部表则会删除
- 其他用法是一样的

**2）内部表和外部表使用场景**

- 外部表：每天将收集到的网站日志定期流入 HDFS 文本文件。
- 内部表：在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储

##### 3.6、删除表

- truncate table

可以使用truncate table来清空表：

```
truncate table复制代码
```

> 注意：Truncate 只能删除管理表（内部表），不能删除外部表中数据 ！

- drop table

也可以使用drop table 来删除表：

```
drop table employee;
drop table id exists employee;复制代码
```

- delete from table

默认情况下，hive不支持delete方式删除表数据

## 实验总结

- 创建库的基础语法

  - create database if not exists mydb;
  - 创建一个数据库，指定数据库在 HDFS ：create database mydb2 location '/mydb2.db';

- 过滤数据库：show databases like 'my*';

- 显示数据库详情：desc database extended mydb;

- 创建内部表的典型语法

  ```
  复制代码
  ```

create table if not exists employee( id int, name string ) row format delimited fields terminated by '\t' stored as textfile location '/user/hive/warehouse/employee';

```
- 创建外部表的典型语法

```mysql
create external table if not exists employee4(
deptno int,
dname string
) row format delimited fields terminated by '\t';复制代码
```

- 内部表与外部表的区别
  - 建表时带有external关键字为外部表，否则为内部表
  - 内部表和外部表建表时都可以自己指定location
  - 删除表时，外部表不会删除对应的数据，只会删除元数据信息，内部表则会删除
  - 其他用法是一样的
- 内外部表的使用场景
  - 外部表：每天将收集到的网站日志定期流入 HDFS 文本文件。
  - 内部表：在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储







# 实验2-4：Hive中的DML操作

## 实验概述

本次实验主要讲解了Hive中的数据管理语言（DML）语法及其注意事项，并结合具体案例进行实际代码演练。

## 实验环境

- Linux 操作系统
- Hadoop 3.x
- Hive 3.x
- JDK 1.8
- MySQL 8.x

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16316688079674100.png)

## 实验目标

学习完成本实验后，您将能够

- 熟练掌握如何向Hive中插入数据
- 熟练掌握如何把Hive中的数据导出

## 实验任务

### 任务1：初始化实验环境及准备数据

#### **【任务目标】**

 本次任务主要初始化Hive的实验环境以及准备好一些实验数据，为后继的任务打下基础。

#### **【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16316688232539275.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316721899999176.png?fileid=3701925924339457684)

视频-0：Hive开发环境及数据准备

##### 1.1、打开Terminal终端

进入项目实训平台后，如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16316688397312300.png)

在桌面空白位置点击鼠标右键，选择 `在此打开终端 `如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16316688523979150.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316688666195286.png)

##### 1.2、启动hadoop

在命令终端输入以下命令，启动hadoop相关服务

```
start-all.sh复制代码
```

启动效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/1631668880750285.png)

##### 1.3、查看服务状态

可以通过如下命令查看Hadoop服务的启动状态：

```
jps复制代码
```

执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16316688934272810.png)

如果出现如上服务，则恭喜你的Hadoop启动成功。

##### 1.4、启动Hive

```
# 启动hive 
hive
# 查看有没有表
show tables;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316689087454710.png)

##### 1.5、准备实验数据

- 创建数据库

```
create database if not exists testdb;复制代码
```

- 选择数据库

```
use testdb;复制代码
```

- 创建表

```
create table if not exists student(id int,name string,age int) ;复制代码
```

- 插入二条记录

```
insert into student(id,name,age)values(1,'zhangsan',18);
insert into student(id,name,age)values(2,'lishi',19);复制代码
```

- 查看表

```
select * from student;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/1631668921722889.png)

##### 1.6、退出hive客户端

最后记得，通过 `exit;` 命令退出hive客户端。

### 任务2：向Hive仓库中添加数据

#### 【任务目标】

 本次任务主要学习如何向Hive数据仓库中添加数据，包括但不限于：向表中装载数据、通过查询语句向表中插入数据，创建表时指定加载数据路径、查询语句中创建表并加数据、import方式导入数据等。

#### 【任务步骤】

![image.png](https://cdn.atstudy.com/lab/manual/16316689345131077.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316724387606956.png?fileid=3701925924339623194)

视频-01：从HDFS上面去加载数据

##### 2.1、向表中装载数据（Load）

1）语法

```
hive> load data [local] inpath '数据的 path' [overwrite] into table 数据表 [partition (partcol1=val1,…)];复制代码
```

**说明：** （1）load data: 表示加载数据 （2）local: 表示从本地加载数据到 hive 表；否则从 HDFS 加载数据到 hive 表 （3）inpath: 表示加载数据的路径 （4）overwrite: 表示覆盖表中已有数据，否则表示追加 （5）into table: 表示加载到哪张表 （6）student: 表示具体的表 （7）partition: 表示上传到指定分区

2）**示例代码-1：** 从本地加载数据

（1）准备数据

- users.txt

```
1,user01,1988-10-1
2,user02,1989-10-1
3,user03,1990-10-1复制代码
```

在Linux终端中，输入代码如下：

```
cd /home/atstudy/datas
vim users.txt复制代码
```

（2）创建 t_user 表

进入Hive客户端，输入如下代码：

```
create table t_user(id int, name string,birthday string) row format delimited fields terminated by ',';复制代码
```

（3）加载本地文件到 hive

进入HIve客户端，输入如下代码：

```
 load data local inpath
'/home/atstudy/datas/users.txt' into table default.t_user;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316689517157026.png)

3）**示例代码-2：** 加载 HDFS 文件到 hive 中

（1）准备数据

- users01.txt

```
4,user04,1980-10-1
5,user05,1990-10-1
6,user06,2000-10-1复制代码
```

在Linux终端中，输入代码如下：

```
cd /home/atstudy/datas
vim users01.txt复制代码
```

（2）上传文件到 HDFS

进入 hive 客户端，输入代码 如下：

```
# hdfs上创建目录
dfs -mkdir -p /hive_data/user;
# 上传数据到HDFS
dfs -put /home/atstudy/datas/users01.txt /hive_data/user;复制代码
```

（3）加载 HDFS 上数据

```
load data inpath '/hive_data/user/users01.txt' into table default.t_user;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316689636235241.png)

4）**示例代码-3：** 加载 HDFS数据覆盖表中已有的数据

![image.png](https://cdn.atstudy.com/lab/manual/16316689751506228.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316725994736362.png?fileid=3701925924339628969)

视频-02： 加载 HDFS数据覆盖表中已有的数据数据

（1）上传文件到 HDFS

- users02.txt

```
7,user07,1981-10-1
8,user08,1991-10-1复制代码
```

在Linux终端中，输入代码如下：

```
cd /home/atstudy/data
vim users2.txt复制代码
```

进入hive ，输入代码如下：

```
dfs -put /home/atstudy/datas/users02.txt /hive_data/user;复制代码
```

（2）加载数据覆盖表中已有的数据

在Hive客户端中输入代码如下：

```
load data inpath '/hive_data/user/users02.txt' overwrite into table  default.t_user;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/1631669199419843.png)

##### 2.2、通过查询语句向表中插入数据（Insert ）

![image.png](https://cdn.atstudy.com/lab/manual/16316692085774933.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316727112309017.png?fileid=3701925924339824938)

视频-03：通过查询语句向表中插入数据

1）创建一张表

在Hive客户端中，输入代码如下：

```
create table t_user_02(id int, name string,birthday string) row format delimited fields terminated by ',';复制代码
```

2）批量插入数据

```
insert into table t_user_02 values(1,'zhangsan','1998-10-1'),(2,'lishi','1999-10-1');复制代码
```

3） 根据单张表查询结果插入

```
insert overwrite table t_user_02
select id, name,birthday from t_user where name='user07';复制代码
```

> 注意：
>
> 1、t_user_02表必须事先存在
>
> 2、insert into：以追加数据的方式插入到表或分区，原有数据不会删除
>
> 3、insert overwrite：会覆盖表中已存在的数据

4 ）提取数据插入到多张表中

```
# 根据t_user数据结构创建一个空表users_01
create table if not exists users_01 like t_user;
# 根据t_user数据结构创建一个空表users_02
create table if not exists users_02 like t_user;
# 从t_user表中提取数据插入到多张表中
from t_user
insert into table users_01
select id,name,birthday where name='user07'
insert into table users_02
select id,name,birthday where name='user08';复制代码
```

##### 2.3、创建表时通过 Location 指定加载数据路径

![image.png](https://cdn.atstudy.com/lab/manual/1631669237793793.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316729368007667.png?fileid=3701925924339840328)

视频-04：创建表时通过location指定加载数据路径

1）上传数据到 hdfs 上

- 准备数据：student.txt

```
1	zhangsan	20
2	lishi	22
3	wangwu	18复制代码
```

在Linux终端中，输入代码如下：

```
cd /home/atstudy/datas/
vim student.txt复制代码
```

- 上传到HDFS中

进入到Hive客户端中，输入代码如下：

```
dfs -mkdir /student;复制代码
dfs -put /home/atstudy/datas/student.txt /student;复制代码
```

2）创建表，并指定在 hdfs 上的位置

```
create  table if not exists my_student(
id int, name string
)
row format delimited fields terminated by '\t'
location '/student';复制代码
```

3）查询数据

```
select * from my_student;复制代码
```

4）再创建一张表

```
create table if not exists my_student2 like my_student;
# 查看数据
select * from my_student2;复制代码
```

5）更改此表指定加载路径

```
alter table my_student2 set location '/student';
# 查看数据
select * from my_student2;复制代码
```

6）尝试删除my_student表

```
drop table my_student;复制代码
```

7）查看数据

```
# 查看数据
select * from my_student2;复制代码
# 查看HDFS
dfs -ls /;复制代码
```

> 说明：
>
> 从以上实验可以看出，非外部表，即内部表在进行drop删除时，会一并把hdfs上关联的数据同时删除！

##### 2.4、查询语句中创建表并加载数据（As Select）

![image.png](https://cdn.atstudy.com/lab/manual/16316692539192569.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631673033715561.png?fileid=3701925924339982633)

视频-05：查询语句中创建表并加载数据数据

根据查询结果创建表（查询的结果会添加到新创建的表中）

```
create table if not exists student3
as select id, name from student;复制代码
```

> 此种方式，student3表不需要重新创建

##### 2.5、Import 数据到指定 Hive

说明：以下实验可以等于 export 方式实现数据导出后，再进行Import导入,现在可以先不进行此步操作。

```
import table default.employee from '/extdb/employee/employee05';复制代码
```

> 注意：
>
> 1、先用 export 导出后，再将数据导入。
>
> 2、employee表不需要事先创建

### 任务3：从Hive仓库中导出数据

#### 【任务目标】

 本次任务主要学习如何从Hive数据仓库中导出数据，包括但不限于：Insert命令方式导出、dfs hadoop命令方式导出、Hive Shell 命令导出、Export命令导出等。

#### 【任务步骤】

##### 3.1、Insert 命令方式导出

![image.png](https://cdn.atstudy.com/lab/manual/16316692700138323.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316741939322986.png?fileid=3701925924340577984)

视频-06：Insert命令方式导出数据

1 ） 将查询的结果导出到本地

```
insert overwrite local directory
'/home/atstudy/datas/employee02'
select * from employee;复制代码
```

> 注意：这个 /home/atstudy/datas/employee02 导出来的是个路径

可以查看数据，代码如下：

```
cat /home/atstudy/datas/employee02/000*复制代码
```

2） 将查询的结果格式化导出到本地

```
insert overwrite local directory
'/home/atstudy/datas/employee03'
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
select * from employee;复制代码
```

> 注意：这个 /home/atstudy/datas/employee03 导出来的是个路径

可以查看数据，代码如下：

```
cat /home/atstudy/datas/employee03/000*复制代码
```

3） 将查询的结果导出到 HDFS 上( 没有 local)

```
# hdfs上创建目录
dfs -mkdir -p /extdb/employee
# 将查询的结果导出到 HDFS 上
insert overwrite directory '/extdb/employee/employee04'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
select * from employee;复制代码
```

##### 3.2、dfs hadoop命令方式导出到本地

```
dfs -get '/extdb/employee/employee04' /home/atstudy/datas/employee04;复制代码
```

> 注意：上述导出来的数据为目录

查看数据代码如下：

```
cat /home/atstudy/datas/employee04/employee04/00*复制代码
```

##### 3.3、Hive Shell 命令导出

基本语法：（hive -f/-e 执行语句或者脚本 > file）

```
hive -e 'select * from employee;' >/home/atstudy/datas/employee05.txt;复制代码
```

##### 3.4、Export 导出到 HDFS 上

![image.png](https://cdn.atstudy.com/lab/manual/16316692910019997.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316751089669636.png?fileid=3701925924341196032)

视频-07：export导出与import导入

```
export table testdb.employee2 to '/extdb/employee/employee05';复制代码
```

> export 和 import 主要用于两个 Hadoop 平台集群之间 Hive 表迁移。

### 任务4、能力拓展训练

#### 【任务目标】

 本次任务主要通过完成具体需求训练Hive仓库的数据导入导出命令。

#### 【任务步骤】

![image.png](https://cdn.atstudy.com/lab/manual/16316694586691146.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316753747011560.png?fileid=3701925924341606938)

视频-08：能力拓展训练

需求：

（1）启动 Hadoop 服务

```
start-all.sh复制代码
```

（2）查看Hadoop是否正常启动，查看后台运行的进程

```
jps复制代码
```

（3）启动 hive

```
hive复制代码
```

（4）创建一个mydb数据库

```
create database mydb;复制代码
```

（5）进入mydb数据库

```
use mydb;复制代码
```

（6）创建 一个sales 内部表

三字段，分别为：

sales_employee：string ，表示员工名称

fiscal_year：int , 表示年份

sale ：double，表示销售额

> 说明，因为要加载的数格式为 Alice,2016,150.00 形式，所以注意 row format delimited 要设置为 `,`

```
create table if not exists sales (
sales_employee string, fiscal_year int,sale double
)
row format delimited fields terminated by ','复制代码
```

（7）在 `/home/atstudy/datas` 目录下，新建：sales.txt 文件，内容如下：

```
Alice,2016,150.00
Alice,2017,100.00
Alice,2018,200.00
Bob,2016,100.00
Bob,2017,150.00
Bob,2018,200.00
John,2016,200.00
John,2017,150.00
John,2018,250.00复制代码
cd /home/atstudy/datas
vim sales.txt复制代码
```

（8）通过 `load data` 方式 将数据加载至 sales 表中

```
 load data local inpath '/home/atstudy/datas/sales.txt' into table sales;复制代码
```

（9）查看sales表中的数据

```
select * from sales;复制代码
```

（10）将此表中的数据再次导出至本地 `/home/atstudy/datas/sales02.txt` 文件中

```
dfs -get '`/user/hive/warehouse/mydb.db/sales/0*' /home/atstudy/datas/sales02.txt;复制代码
```

或：

```
hive -e 'select * from mydb.sales;' >/home/atstudy/datas/sales02.txt;复制代码
```

（11）在HDFS上新建一个 `/mydata/sales02` 目录，然后将该文件上传至该目录下：

```
dfs -mkdir -p /mydata/sales02;复制代码
dfs -put /home/atstudy/datas/sales02.txt /mydata/sales02;复制代码
```

（12）再次新建一个 sales02外部表，数据结构同 sales 表，数据指向hdfs上的 /mydata/sales02 目录下

```
create external table if not exists sales02 like sales;复制代码
```

修改location

```
alter table sales02 set location '/mydata/sales02';复制代码
```

（13）查看该表数据

```
select * from sales02;复制代码
```

（14）退出Hive客户端

```
exit;复制代码
```

## 课程总结

- load方式向HIve仓库中插入数据

  ```
  复制代码
  ```

load data local inpath '/home/atstudy/datas/users.txt' into table default.t_user;

```
- 创建表时通过 Location 指定加载数据路径

```mysql
create  table if not exists my_student(
id int, name string
) row format delimited fields terminated by '\t' location '/student';复制代码
```

- 查询语句中创建表并加载数据（As Select）

  ```
  create table if not exists student3 as select id, name from student;复制代码
  ```

- Import 数据到指定 Hive

  ```
  复制代码
  ```

import table default.employee from '/extdb/employee/employee05';

```
- 从Hive中导出方式的方式有

- Insert 命令方式导出

```mysql
  insert overwrite local directory '/home/atstudy/datas/employee02'
  select * from employee;复制代码
```

- dfs hadoop命令方式导出到本地

```
dfs -get '/extdb/employee/employee04' /home/atstudy/datas/employee04;复制代码
```

- Hive Shell 命令导出

```
hive -e 'select * from employee;' >/home/atstudy/datas/employee05.txt;复制代码
```

- Export 导出到 HDFS 上

```
export table testdb.employee2 to '/extdb/employee/employee05';复制代码
```







# 实验3-1：实战Hive大数据分析之基本数据查询

## 实验概述

本次实验主要讲解了Hive中的基本数据查询操作，包括但不限于：全表查询、选择特定列查询、列别名、列的计算、统计函数、Limit语句、where 语句、Like 和 和 Rlike、逻辑运算符（And/Or/Not）等，并结合具体案例进行实操演练。

## 实验环境

- Linux 操作系统
- Hadoop 3.x
- Hive 3.x
- JDK 1.8
- MySQL 8.x

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16316764609956098.png)

## 实验目标

学习完成本实验后，您将能够

- 熟练掌握如何在Hive中利用HQL语句进行基本的数据查询
- 熟练掌握如何在Hive中利用HQL语句进行基础的数据统计与分析

## 实验任务

### 任务1：Hive开发环境及数据准备

#### **【任务目标】**

 本次任务主要是准备好基础的Hive开发环境，启动Hadoop，启动Hive服务及客户端以及部分实验数据准备，为后继实验打下基础。

#### **【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16316764794149472.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316847504156409.png?fileid=3701925924346790332)

视频-01：Hive开发环境及数据准备

\##### 1.1、打开Terminal终端

进入项目实训平台后，如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16316764935241862.png)

在桌面空白位置点击鼠标右键，选择 `在此打开终端 `如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16316765036709084.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316765165455783.png)

##### 1.2、启动hadoop

在命令终端输入以下命令，启动hadoop相关服务

```
start-all.sh复制代码
```

启动效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16316765328947040.png)

##### 1.3、查看服务状态

可以通过如下命令查看Hadoop服务的启动状态：

```
jps复制代码
```

执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16316765431588624.png)

如果出现如上服务，则恭喜你的Hadoop启动成功。

##### 1.4、启动 hiveserver2 服务

```
# 后台方式启动hiveserver2服务
nohup hive --service hiveserver2 2>&1 &复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316765559782403.png)

通过如下命令确认 hiveserver2 服务是否已经启动：

```
netstat -ntulp | grep 10000复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/1631676566061105.png)

##### 1.5、JDBC方式连结Hive客户端

```
# 然后再重新启动beeline客户端
beeline --showDbInPrompt=true
# 连结到默认的default数据库
!connect jdbc:hive2://localhost:10000/default复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316765998017331.png)

##### 1.6、准备实验数据

1）准备数据

- dept.txt

```
10	ACCOUNTING	1700
20	RESEARCH	1800
30	SALES	1900
40	OPERATIONS	1700复制代码
```

> 特别注意：以上的每个元素之间的间隔用tab键，在拷贝到Linux环境中时，最好每个数据之间的tab键再手工输入一次。

- emp.txt

```
7369,SMITH,CLERK,7902,1980-12-17,800.00,100.00,20
7499,ALLEN,SALESMAN,7698,1981-2-20,1600.00,300.00,30
7521,WARD,SALESMAN,7698,1981-2-22,1250.00,500.00,30
7566,JONES,MANAGER,7839,1981-4-2,2975.00,300.00,20
7654,MARTIN,SALESMAN,7698,1981-9-28,1250.00,1400.00,30
7698,BLAKE,MANAGER,7839,1981-5-1,2850.00,1000.00,30
7782,CLARK,MANAGER,7839,1981-6-9,2450.00,1200.00,10
7788,SCOTT,ANALYST,7566,1987-4-19,3000.00,1000.00,20
7839,KING,PRESIDENT,7860,1981-11-17,5000.00,800.00,10
7844,TURNER,SALESMAN,7698,1981-9-8,1500.00,0.00,30
7876,ADAMS,CLERK,7788,1987-5-23,1100.00,300.00,20
7900,JAMES,CLERK,7698,1981-12-3,950.00,200.00,30
7902,FORD,ANALYST,7566,1981-12-3,3000.00,200.00,20
7934,MILLER,CLERK,7782,1982-1-23,1300.00,100.00,10复制代码
```

- loc.txt

```
1700,Beijing
1800,London
1900,Tokyo复制代码
```

脚本代码 ：

```
cd /home/atstudy/
mkdir datas
touch dept.txt
vim dept.txt
touch emp.txt
vim emp.txt
touch loc.txt
vim loc.txt复制代码
```

2）创建表

- 创建部门表

```
create  table if not exists dept(
deptno int,
dname string,
loc int
) row format delimited fields terminated by '\t';复制代码
```

- 创建员工表

```
create  table if not exists emp(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int)
row format delimited fields terminated by ',';复制代码
```

- 创建地址表

```
create table if not exists location(
loc int,
loc_name string
)
row format delimited fields terminated by ',';复制代码
```

3）导入数据

- 导入dept数据

```
load data local inpath '/home/atstudy/datas/dept.txt' into table dept;复制代码
```

- 导入emp数据

```
load data local inpath '/home/atstudy/datas/emp.txt' into table emp;复制代码
```

- 导入loc数据

```
load data local inpath '/home/atstudy/datas/loc.txt' into table location;复制代码
```

4）查看数据

```
use default;
select * from dept;
select * from emp;
select * from location;复制代码
```

### 任务2：Hive中简单查询

#### **【任务目标】**

 本次任务主要介绍Hive查询的基础语法及通过示例演示Hive的基本查询操作。

#### **【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16316766225613022.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316885982341638.png?fileid=3701925924349263350)

视频-02：Hive中的简单查询

**查询的基本语法 ：**

```
SELECT [ALL | DISTINCT] select_expr, select_expr, ...
FROM table_reference
[WHERE where_condition]
[GROUP BY col_list]
[ORDER BY col_list]
[CLUSTER BY col_list
| [DISTRIBUTE BY col_list] [SORT BY col_list]
] [LIMIT number]复制代码
```

**注意：**

（1）SQL 语言大小写不敏感。 （2）SQL 可以写在一行或者多行 （3）关键字不能被缩写也不能分行 （4）各子句一般要分行写。 （5）使用缩进提高语句的可读性。

##### 2.1、全表查询

```
select * from emp;复制代码
select empno,ename,job,mgr,hiredate,sal,comm,deptno from emp ;复制代码
```

##### 2.2、选择特定列查询

```
select empno, ename,hiredate,sal from emp;复制代码
```

##### **2.3、列别名**

（1）重命名一个列 （2） 便于计算 （3）紧跟列名，也可以在列名和别名之间加入关键字‘AS ’

示例：

```
select ename AS employee_name, deptno depart_no from emp;复制代码
```

##### 2.4、列的计算

**算法运算符：**

![image.png](https://cdn.atstudy.com/lab/manual/16316766379103001.png)

**示例：**

**需求：**将所有员工的薪水+1000元显示

**HQL:**

```
select empno,ename,sal+1000 as sal from emp;复制代码
```

### 任务3：利用Hive函数进行数据统计

#### **【任务目标】**

 本次任务主要介绍了Hive中常见的数据统计函数：count\max\min\sum\avg 等。

#### 【任务步骤】

![image.png](https://cdn.atstudy.com/lab/manual/16316766496037450.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316887749299526.png?fileid=3701925924349266326)

视频-03：Hive中常见函数

##### 3.1、常见数据统计函数：count函数

- 求总行数（count ）

```
select count(*) cnt from emp;复制代码
```

##### 3.2、常见数据统计函数：max函数

- 求工资的最大值（max ）

```
select max(sal) max_sal from emp;复制代码
```

##### 3.3、常见数据统计函数：min函数

- 求工资的最小值（min ）

```
select min(sal) min_sal from emp;复制代码
```

##### 3.4、常见数据统计函数：sum函数

- 求工资的总和（sum ）

```
select sum(sal) sum_sal from emp;复制代码
```

##### 3.5、常见数据统计函数：avg函数

- 求工资的平均值（avg ）

```
select avg(sal) avg_sal from emp;复制代码
```

### 任务4：Hive条件查询及运算符的使用

#### **【任务目标】**

 本次任务主要介绍Hive中的条件精确查询、模糊查询、逻辑运算符及Limit限制查询语句，并结合具体代码进行演练。

#### 【任务步骤】

![image.png](https://cdn.atstudy.com/lab/manual/16316766647886690.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316888481534447.png?fileid=3701925924349465137)

视频-04：Hive条件查询及运算符的使用

##### 4.1、Limit 语句

典型的查询会返回多行数据。LIMIT 子句用于限制返回的行数。

```
select * from emp limit 5;复制代码
```

##### 4.2、where 语句

**说明：**

（1）使用 WHERE 子句，将不满足条件的行过滤掉 （2）WHERE 子句紧随 FROM

**示例：**

**需求：**查询出薪水大于 1000 的所有员工

```
select * from emp where sal >1000;复制代码
```

> 注意：where 子句中不能使用字段别名。

**比较运算符：**

1）下面表中描述了谓词操作符 ， 这些操作符同样可以用于 JOIN…ON 和 和 HAVING 语句中

![image.png](https://cdn.atstudy.com/lab/manual/16316766810041984.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316766892973971.png)

**示例：**

（1）查询出薪水等于 5000 的所有员工

```
select * from emp where sal =5000;复制代码
```

（2）查询工资在 500 到 1000 的员工信息

```
select * from emp where sal between 500 and 1000;复制代码
```

（3）查询 comm 为空的所有员工信息

```
select * from emp where comm is null;复制代码
```

（4）查询工资是 1500 或 5000 的员工信息

```
select * from emp where sal IN (1500, 5000);复制代码
```

##### 4.3、Like 和 和 RLike

**说明：**

1 ）使用 LIKE 运算选择类似的值

2 ）选择条件可以包含字符或数字:

- % 代表零个或多个字符(任意个字符)。
- _ 代表一个字符。

3 ）RLIKE 子句

RLIKE 子句是 Hive 中这个功能的一个扩展，其可以通过 Java 的正则表达式这个更强大的语言来指定匹配条件。

**示例：**

（1）查找名字以 A 开头的员工信息

```
select * from emp where ename LIKE 'A%';复制代码
```

（2）查找名字中第二个字母为 A 的员工信息

```
select * from emp where ename LIKE '_A%';复制代码
```

（3）查找名字中带有 A 的员工信息

```
select * from emp where ename RLIKE '[A]';复制代码
```

**关于RLIKE的注意事项：**

**1、语法规则:**

- A RLIKE B ，表示B是否在A里面即可。而A LIKE B,则表示B是否是A.
- B中的表达式可以使用JAVA中全部正则表达式，具体正则规则参考java，或者其他标准正则语法。

**2、操作类型:** strings

**3、使用描述:** 如果字符串A或者字符串B为NULL，则返回NULL；如果字符串A符合JAVA正则表达式B的正则语

法，则为TRUE；否则为FALSE。

**4、使用示例：**

```
select 'foobar' rlike 'foo';  --注意同样表达式，用正则匹配成功复制代码
select 'foobar' like 'foo'; --注意同样表达式，用like匹配失败复制代码
select '123456' rlike '^\\d+$'; --使用rlike匹配数字复制代码
```

##### 4.4、逻辑运算符（And/Or/Not）

![image.png](https://cdn.atstudy.com/lab/manual/16316767090288552.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316890940378722.png?fileid=3701925924349542930)

视频-05：Hive中的逻辑运算符

![image.png](https://cdn.atstudy.com/lab/manual/16316767193577606.png)

**示例：**

（1）查询薪水大于 1000，部门编号是 30的员工

```
select * from emp where sal>1000 and deptno=30;复制代码
```

（2）查询薪水大于 1000，或者部门是 30

```
select * from emp where sal>1000 or deptno=30;复制代码
```

（3）查询除了 20 部门和 30 部门以外的员工信息

```
select * from emp where deptno not IN(30, 20);复制代码
```

## 课程总结

- Hive查询的基本语法

```
SELECT [ALL | DISTINCT] select_expr, select_expr, ...
FROM table_reference
[WHERE where_condition]
[GROUP BY col_list]
[ORDER BY col_list]
[CLUSTER BY col_list
| [DISTRIBUTE BY col_list] [SORT BY col_list]
] [LIMIT number]复制代码
```





# 实验3-2：实战Hive大数据分析之复杂数据查询

## 实验概述

本次实验主要讲解了Hive中的复杂数据查询操作，包括但不限于：分组统计、JOIN语句，排序、分区、Cluster By等，并结具体案例进行实操演练。

## 实验环境

- Linux 操作系统
- Hadoop 3.x
- Hive 3.x
- JDK 1.8
- MySQL 8.x

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16316893594888713.png)

## 实验目标

学习完成本实验后，您将能够

- 熟练掌握如何在Hive中利用HQL语句进行复杂的数据查询
- 熟练掌握如何在Hive中利用HQL语句进行复杂的数据统计与分析

## 实验任务

### 任务1：Hive开发环境及数据准备

#### **【任务目标】**

 本次任务主要是准备好基础的Hive开发环境，启动Hadoop，启动Hive服务及客户端以及部分实验数据准备，为后继实验打下基础。

#### **【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16316893740758294.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316903846494795.png?fileid=3701925924350208981)

视频-01：Hive开发环境及数据准备

##### 1.1、打开Terminal终端

进入项目实训平台后，如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16316893936369202.png)

在桌面空白位置点击鼠标右键，选择 `在此打开终端 `如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/1631689504904335.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316895172767021.png)

##### 1.2、启动hadoop

在命令终端输入以下命令，启动hadoop相关服务

```
start-all.sh复制代码
```

启动效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16316901196959723.png)

##### 1.3、查看服务状态

可以通过如下命令查看Hadoop服务的启动状态：

```
jps复制代码
```

执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16316901347071458.png)

如果出现如上服务，则恭喜你的Hadoop启动成功。

##### 1.4、启动 hiveserver2 服务

```
# 后台方式启动hiveserver2服务
nohup hive --service hiveserver2 2>&1 &复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/1631690148325288.png)

通过如下命令确认 hiveserver2 服务是否已经启动：

```
netstat -ntulp | grep 10000复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316901788949319.png)

##### 1.5、JDBC方式连结Hive客户端

```
# 然后再重新启动beeline客户端
beeline --showDbInPrompt=true
# 连结到默认的default数据库
!connect jdbc:hive2://localhost:10000/default复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316901911276455.png)

##### 1.6、准备实验数据

1）准备数据

- dept.txt

```
10	ACCOUNTING	1700
20	RESEARCH	1800
30	SALES	1900
40	OPERATIONS	1700复制代码
```

> 特别注意：以上的每个元素之间的间隔用tab键，在拷贝到Linux环境中时，最好每个数据之间的tab键再手工输入一次。

- emp.txt

```
7369,SMITH,CLERK,7902,1980-12-17,800.00,100.00,20
7499,ALLEN,SALESMAN,7698,1981-2-20,1600.00,300.00,30
7521,WARD,SALESMAN,7698,1981-2-22,1250.00,500.00,30
7566,JONES,MANAGER,7839,1981-4-2,2975.00,300.00,20
7654,MARTIN,SALESMAN,7698,1981-9-28,1250.00,1400.00,30
7698,BLAKE,MANAGER,7839,1981-5-1,2850.00,1000.00,30
7782,CLARK,MANAGER,7839,1981-6-9,2450.00,1200.00,10
7788,SCOTT,ANALYST,7566,1987-4-19,3000.00,1000.00,20
7839,KING,PRESIDENT,7860,1981-11-17,5000.00,800.00,10
7844,TURNER,SALESMAN,7698,1981-9-8,1500.00,0.00,30
7876,ADAMS,CLERK,7788,1987-5-23,1100.00,300.00,20
7900,JAMES,CLERK,7698,1981-12-3,950.00,200.00,30
7902,FORD,ANALYST,7566,1981-12-3,3000.00,200.00,20
7934,MILLER,CLERK,7782,1982-1-23,1300.00,100.00,10复制代码
```

- loc.txt

```
1700,Beijing
1800,London
1900,Tokyo复制代码
```

脚本代码 ：

```
cd /home/atstudy/
mkdir datas
touch dept.txt
vim dept.txt
touch emp.txt
vim emp.txt
touch loc.txt
vim loc.txt复制代码
```

2）创建表

- 创建部门表

```
create  table if not exists dept(
deptno int,
dname string,
loc int
) row format delimited fields terminated by '\t';复制代码
```

- 创建员工表

```
create  table if not exists emp(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int)
row format delimited fields terminated by ',';复制代码
```

- 创建地址表

```
create table if not exists location(
loc int,
loc_name string
)
row format delimited fields terminated by ',';复制代码
```

3）导入数据

- 导入dept数据

```
load data local inpath '/home/atstudy/datas/dept.txt' into table dept;复制代码
```

- 导入emp数据

```
load data local inpath '/home/atstudy/datas/emp.txt' into table emp;复制代码
```

- 导入loc数据

```
load data local inpath '/home/atstudy/datas/loc.txt' into table location;复制代码
```

4）查看数据

```
use default;
select * from dept;
select * from emp;
select * from location;复制代码
```

### 任务2：Hive中分组查询与统计

#### **【任务目标】**

 本次任务主要介绍Hive中的分组查询与统计及过滤，并结合具体代码进行实际演练。

#### **【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16316902118024245.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316908576668247.png?fileid=3701925924350615679)

视频-02：Hive中的分组查询与统计

**查询的基本语法 ：**

```
SELECT [ALL | DISTINCT] select_expr, select_expr, ...
FROM table_reference
[WHERE where_condition]
[GROUP BY col_list]
[ORDER BY col_list]
[CLUSTER BY col_list
| [DISTRIBUTE BY col_list] [SORT BY col_list]
] [LIMIT number]复制代码
```

##### 2.1、Group By 语句

GROUP BY 语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作。

**示例：**

（1）计算 emp 表每个部门的平均工资

```
select t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno;复制代码
```

（2）计算 emp 每个部门中每个岗位的最高薪水

```
select t.deptno, t.job, max(t.sal) max_sal from emp t group by t.deptno, t.job;复制代码
```

##### 2.2、Having 子句

1）having 与 与 where 不同点

- where 后面不能写分组函数，而 having 后面可以使用分组函数。
- having 只用于 group by 分组统计语句。

**示例：**

求每个部门的平均薪水大于 2000 的部门

- step1：求每个部门的平均工资

  ```
  select deptno, avg(sal) as avg_sal from emp group by deptno;复制代码
  ```

- step2：求每个部门的平均薪水大于 2000 的部门

  ```
  select deptno, avg(sal) as avg_sal from emp group by deptno having avg_sal > 2000;复制代码
  ```

### 任务3：Hive中的 JOIN 语句

#### **【任务目标】**

 本次任务主要介绍了Hive中SQL JOIN语句，包括但不限于：内连接、外连接、多表关联、笛卡尔积等。

#### 【任务步骤】

![image.png](https://cdn.atstudy.com/lab/manual/16316902383109686.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316911504567843.png?fileid=3701925924350679656)

视频-03：Hive中的JOIN语句

##### 3.1、SQL JOIN 语句

**示例：**

（1）根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门名称；

```
select empno,ename,dname from emp join dept on emp.deptno = dept.deptno;复制代码
```

**表的别名**

**作用：** （1）使用别名可以简化查询。 （2）使用表名前缀可以提高执行效率。

**示例：**

合并员工表和部门表：

```
select e.empno, e.ename, d.deptno,d.dname from emp e join dept d on e.deptno = d.deptno;复制代码
```

##### 3.2、内连接

内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来。

```
select e.empno, e.ename, d.deptno from emp e  join dept d on e.deptno = d.deptno;复制代码
```

##### 3.3、外连结

**1）左外连结**

左外连接：JOIN 操作符左边表中的记录全部返回，右表只有符合 WHERE 子句的记录才会被返回。

```
select d.deptno ,e.empno, e.ename from dept d left join emp e on e.deptno = d.deptno;复制代码
```

**2）右外连结**

右外连接：JOIN 操作符右边表中的记录全部返回，左表只有符合 WHERE 子句的记录才会被返回。

```
select d.deptno,e.empno, e.ename from emp e right join dept d on e.deptno = d.deptno;复制代码
```

##### 3.4、多表关联

**注意：**连接 n 个表，至少需要 n-1 个连接条件。例如：连接三个表，至少需要两个连接条件。

**示例：**

```
SELECT e.ename, d.dname, l.loc_name
FROM emp e
JOIN dept d
ON d.deptno = e.deptno
JOIN location l
ON d.loc = l.loc;复制代码
```

**特别说明：**

- 大多数情况下，Hive 会对每对 JOIN 连接对象启动一个 MapReduce 任务。本例中会首先启动一MapReduce

  job对表 e 和表 d 进行连接操作，然后会再启动一个 MapReduce job 将第一个 MapReduce job 的输出和表 l进行连接操作。

- 注意：为什么不是表 d 和表 l 先进行连接操作呢？这是因为 Hive 总是按照从左到右的顺序执行的。

- 优化：当对 3 个或者更多表进行 join 连接时，如果每个 on 子句都使用相同的连接键的话，那么只会产生一个 MapReduce job。

##### 3.5、笛卡尔积

笛卡尔集会在下面条件下产生： （1）省略连接条件 （2）连接条件无效 （3）所有表中的所有行互相连接

示例：

```
select empno, dname from emp, dept;复制代码
```

### 任务4：Hive中的排序与分区

#### **【任务目标】**

 本次任务主要介绍Hive中的排序，Reduce的内部排序，分区、Cluster By等操作，且讲解了几咱排序的区别及适用场景，同时结合具体代码进行演练。

#### 【任务步骤】

##### 4.1、全局排序

![image.png](https://cdn.atstudy.com/lab/manual/16316902599819759.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316914096116508.png?fileid=3701925924350882091)

视频-04：Hive中的全局排序（Order By）

**Order By：**全局排序，只有一个 Reducer

使用 ORDER BY 子句排序

- ASC（ascend）: 升序（默认）
- DESC（descend）: 降序

**注意：ORDER BY 子句在 SELECT 语句的结尾 ！**

1）基础用法

（1）查询员工信息按工资升序排列

```
select * from emp order by sal;复制代码
```

（2）查询员工信息按工资降序排列

```
select * from emp order by sal desc;复制代码
```

2）按照别名排序

（1）按照员工薪水的 2 倍排序

```
select ename, sal*2 twosal from emp order by twosal;复制代码
```

3）多个列排列

（1）按照部门和工资降序排列

```
select ename, deptno, sal from emp order by deptno, sal desc;复制代码
```

##### 4.2、每个 Reduce 内部排序（Sort By ）

![image.png](https://cdn.atstudy.com/lab/manual/16316902751443360.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316915722831414.png?fileid=3701925924351061542)

视频-05：Sort By的用法与实现原理

**Sort By：**对于大规模的数据集 order by 的效率非常低。在很多情况下，并不需要全局排序，此时可以使用 sort by。Sort by 为每个 reducer 产生一个排序文件。每个 Reducer 内部进行排序，对全局结果集来说不是排序。

1 ） 设置 reduce 个数

```
set mapreduce.job.reduces=3;复制代码
```

2 ） 查看设置 reduce 个数

```
set mapreduce.job.reduces;复制代码
```

3 ） 根据部门编号降序查看员工信息

```
select * from emp sort by deptno desc;复制代码
```

4 ） 将查询结果导入到文件中（按照部门编号降序排序）

```
insert overwrite local directory '/home/atstudy/datas/sortby-result'
select * from emp sort by deptno desc;复制代码
```

查看文件内容：

```
cd /home/atstudy/datas
cd sortby-result
ls
cat *0_0
cat *1_0
cat *2_0复制代码
```

##### 4.3、分区（Distribute By）

![image.png](https://cdn.atstudy.com/lab/manual/1631690291269668.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316922645206587.png?fileid=3701925924351209927)

视频-06：分区（Distribute By）的用法与原理

**Distribute By：** 在有些情况下，我们需要控制某个特定行应该到哪个 reducer，通常是为了进行后续的聚集操作。distribute by 子句可以做这件事。distribute by 类似 MR 中 partition（自定义分区），进行分区，结合 sort by 使用。对于 distribute by 进行测试，一定要分配多 reduce 进行处理，否则无法看到 distribute by 的效果。

**示例：**

1）先按照部门编号分区，再按照员工编号降序排序。

```
set mapreduce.job.reduces=3;复制代码
insert overwrite local directory '/home/atstudy/datas/distribute-result' 
select * from emp distribute by deptno sort by empno desc;复制代码
```

查看文件内容：

```
cd /home/atstudy/datas
cd distribute-result
ls
cat *0_0
cat *1_0
cat *2_0复制代码
```

**注意：**

- distribute by 的分区规则是根据分区字段的 hash 码与 reduce 的个数进行模除后，余数相同的分到一个区。
- Hive 要求 DISTRIBUTE BY 语句要写在 SORT BY 语句之前。

##### 4.4、Cluster By

![image.png](https://cdn.atstudy.com/lab/manual/16316903070102836.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316926337782605.png?fileid=3701925924351476107)

视频-07：Cluster By的用法与原理

当 distribute by 和 sorts by 字段相同时，可以使用 cluster by 方式。cluster by 除了具有 distribute by 的功能外还兼具 sort by 的功能。但是排序只能是升序排序，不能指定排序规则为 ASC 或者 DESC。

**以下两种写法等价**

```
select * from emp cluster by deptno;
select * from emp distribute by deptno sort by deptno;复制代码
```

**注意：**按照部门编号分区，不一定就是固定死的数值，可以是 20 号和 30 号部门分到一个分区里面去。

```
insert overwrite local directory '/home/atstudy/datas/cluster-result' 
select * from emp cluster by deptno;复制代码
```

##### 4.5、几种排序的区别与适用场景

![image.png](https://cdn.atstudy.com/lab/manual/1631690323273623.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316927473804853.png?fileid=3701925924351592264)

视频-08：Hive中几种排序的区别与适用场景

**Hive中order by，sort by，distribute by，cluster by的区别：**

**1）order by**

order by会对输入做全局排序，因此只有一个Reducer(多个Reducer无法保证全局有序)，然而只有一个Reducer，会导致当输入规模较大时，消耗较长的计算时间。

**2）sort by**

sort by不是全局排序，其在数据进入reducer前完成排序，因此，如果用sort by进行排序，并且设置mapred.reduce.tasks>1，则sort by只会保证每个reducer的输出有序，并不保证全局有序。sort by不同于order by，它不受hive.mapred.mode属性的影响，sort by的数据只能保证在同一个reduce中的数据可以按指定字段排序。使用sort by你可以指定执行的reduce个数(通过set mapred.reduce.tasks=n来指定)，对输出的数据再执行归并排序，即可得到全部结果。

**3）distribute by**

distribute by是控制在map端如何拆分数据给reduce端的。hive会根据distribute by后面列，对应reduce的个数进行分发，默认是采用hash算法。sort by为每个reduce产生一个排序文件。在有些情况下，你需要控制某个特定行应该到哪个reducer，这通常是为了进行后续的聚集操作。distribute by刚好可以做这件事。因此，distribute by经常和sort by配合使用。

**注：Distribute by和sort by的使用场景**

1、Map输出的文件大小不均。

2、Reduce输出文件大小不均。

3、小文件过多。

4、文件超大。

**4）cluster by**

cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是倒叙排序，不能指定排序规则为ASC或者DESC。

## 课程总结

1）Hive查询的基本语法

```
SELECT [ALL | DISTINCT] select_expr, select_expr, ...
FROM table_reference
[WHERE where_condition]
[GROUP BY col_list]
[ORDER BY col_list]
[CLUSTER BY col_list
| [DISTRIBUTE BY col_list] [SORT BY col_list]
] [LIMIT number]复制代码
```

2）注意 Hive中order by，sort by，distribute by，cluster by的区别

3）Distribute by 和 Sort by 的使用场景：

- Map输出的文件大小不均。
- Reduce输出文件大小不均。
- 小文件过多。
- 文件超大。





# 实验3-3：实战Hive大数据分析之高级数据查询

## 实验概述

本次实验主要讲解了Hive中的高级数据查询操作，包括但不限于：分区表、二级分区动态分区、分桶表等，并结具体案例进行实操演练。

## 实验环境

- Linux 操作系统
- Hadoop 3.x
- Hive 3.x
- JDK 1.8
- MySQL 8.x

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16316930240286936.png)

## 实验目标

学习完成本实验后，您将能够

- 理解Hive分区表的概述、原理及适用场景
- 熟练掌握如何在Hive中创建分区表并进行数据的导入、导出及其查询
- 理解Hive分桶表的概述、原理及适用场景
- 熟练掌握如何在Hive中进行分桶表的创建与查询

## 实验任务

### 任务1：Hive开发环境及数据准备

#### **【任务目标】**

 本次任务主要是准备好基础的Hive开发环境，启动Hadoop，启动Hive服务及客户端以及部分实验数据准备，为后继实验打下基础。

#### **【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16316930413535365.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316936584768074.png?fileid=3701925924352051934)

视频-01：Hive开发环境及数据准备

##### 1.1、打开Terminal终端

进入项目实训平台后，如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16316930561012808.png)

在桌面空白位置点击鼠标右键，选择 `在此打开终端 `如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16316930673691507.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316930797394725.png)

##### 1.2、启动hadoop

在命令终端输入以下命令，启动hadoop相关服务

```
start-all.sh复制代码
```

启动效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16316930940111945.png)

##### 1.3、查看服务状态

可以通过如下命令查看Hadoop服务的启动状态：

```
jps复制代码
```

执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16316931058121931.png)

如果出现如上服务，则恭喜你的Hadoop启动成功。

##### 1.4、启动 hiveserver2 服务

```
# 后台方式启动hiveserver2服务
nohup hive --service hiveserver2 2>&1 &复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316931158867354.png)

通过如下命令确认 hiveserver2 服务是否已经启动：

```
netstat -ntulp | grep 10000复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316931260999635.png)

##### 1.5、JDBC方式连结Hive客户端

```
# 然后再重新启动beeline客户端
beeline --showDbInPrompt=true
# 连结到默认的default数据库
!connect jdbc:hive2://localhost:10000/default复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16316931378643139.png)

##### 1.6、准备实验数据

1）准备数据

- dept.txt

```
10	ACCOUNTING	1700
20	RESEARCH	1800
30	SALES	1900
40	OPERATIONS	1700复制代码
```

> 特别注意：以上的每个元素之间的间隔用tab键，在拷贝到Linux环境中时，最好每个数据之间的tab键再手工输入一次。

- emp.txt

```
7369,SMITH,CLERK,7902,1980-12-17,800.00,100.00,20
7499,ALLEN,SALESMAN,7698,1981-2-20,1600.00,300.00,30
7521,WARD,SALESMAN,7698,1981-2-22,1250.00,500.00,30
7566,JONES,MANAGER,7839,1981-4-2,2975.00,300.00,20
7654,MARTIN,SALESMAN,7698,1981-9-28,1250.00,1400.00,30
7698,BLAKE,MANAGER,7839,1981-5-1,2850.00,1000.00,30
7782,CLARK,MANAGER,7839,1981-6-9,2450.00,1200.00,10
7788,SCOTT,ANALYST,7566,1987-4-19,3000.00,1000.00,20
7839,KING,PRESIDENT,7860,1981-11-17,5000.00,800.00,10
7844,TURNER,SALESMAN,7698,1981-9-8,1500.00,0.00,30
7876,ADAMS,CLERK,7788,1987-5-23,1100.00,300.00,20
7900,JAMES,CLERK,7698,1981-12-3,950.00,200.00,30
7902,FORD,ANALYST,7566,1981-12-3,3000.00,200.00,20
7934,MILLER,CLERK,7782,1982-1-23,1300.00,100.00,10复制代码
```

- loc.txt

```
1700,Beijing
1800,London
1900,Tokyo复制代码
```

脚本代码 ：

```
cd /home/atstudy/
mkdir datas
touch dept.txt
vim dept.txt
touch emp.txt
vim emp.txt
touch loc.txt
vim loc.txt复制代码
```

2）创建表

- 创建部门表

```
create  table if not exists dept(
deptno int,
dname string,
loc int
) row format delimited fields terminated by '\t';复制代码
```

- 创建员工表

```
create  table if not exists emp(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int)
row format delimited fields terminated by ',';复制代码
```

- 创建地址表

```
create table if not exists location(
loc int,
loc_name string
)
row format delimited fields terminated by ',';复制代码
```

3）导入数据

- 导入dept数据

```
load data local inpath '/home/atstudy/datas/dept.txt' into table dept;复制代码
```

- 导入emp数据

```
load data local inpath '/home/atstudy/datas/emp.txt' into table emp;复制代码
```

- 导入loc数据

```
load data local inpath '/home/atstudy/datas/loc.txt' into table location;复制代码
```

4）查看数据

```
use default;
select * from dept;
select * from emp;
select * from location;复制代码
```

### 任务2：Hive 分区表

#### **【任务目标】**

 本次任务主要介绍Hive中的分区表：分区表概念、分区表创建、加载数据到分区表、分区表管理（增加分区表、修改分区表、删除分区表）

#### **【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16316931843651739.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316942894619970.png?fileid=3701925924352260652)

视频-02：Hive中的分区表简介与使用

##### 2.1、分区表简介

分区表实际上就是对应一个 HDFS 文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive 中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过 WHERE 子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。

##### 2.2、分区表的基本使用

1）引入分区表（需要根据日期对日志进行管理, 通过部门信息模拟 ）

```
dept_20200401.log
dept_20200402.log
dept_20200403.log复制代码
```

2）数据准备

- dept_20200401.log

```
10,ACCOUNTING,1700
20,RESEARCH,1800复制代码
```

- dept_20200402.log

```
30,SALES,1900
40,OPERATIONS,1700复制代码
```

- dept_20200403.log

```
50,TEST,2000
60,DEV,1900复制代码
```

在LInux终端中，输入shell命令如下：

```
cd /home/atstudy/datas
vim dept_20200401.log
vim dept_20200402.log
vim dept_20200403.log复制代码
```

3）创建分区表语法

进入Hive，在HIve命令行中输入如下SQL：

```
create table dept_partition(
deptno int, dname string, loc string
)
partitioned by (day string)
row format delimited fields terminated by ',';复制代码
```

**注意：**分区字段不能是表中已经存在的数据，可以将分区字段看作表的伪列。

4）加载数据到分区表中

```
load data local inpath
'/home/atstudy/datas/dept_20200401.log' into table dept_partition
partition(day='20200401');复制代码
load  data  local  inpath
'/home/atstudy/datas/dept_20200402.log'  into  table  dept_partition
partition(day='20200402');复制代码
load data local inpath
'/home/atstudy/datas/dept_20200403.log' into table dept_partition
partition(day='20200403');复制代码
```

**注意：**分区表加载数据时，必须指定分区

5）查看HDFS分区表存储

[http://localhost:9870](http://localhost:9870/)

![image.png](https://cdn.atstudy.com/lab/manual/16316932089065269.png)

6）查询分区表中的数据

- 单分区查询

```
select * from dept_partition where day='20200401';复制代码
```

- 多分区联合查询

```
select * from dept_partition where day='20200401' or
day='20200402' or day='20200403';复制代码
select * from dept_partition where day='20200401'  and deptno=20
union
select * from dept_partition where day='20200402'  and  deptno=40
union
select * from dept_partition where day='20200403'  and  deptno=60;复制代码
```

7）增加分区

- 创建单个分区

```
alter table dept_partition add partition(day='20200404');复制代码
```

- 同时创建多个分区

```
alter table dept_partition add partition(day='20200405') partition(day='20200406');复制代码
```

8）删除分区 删除单个分区

```
alter table dept_partition drop partition(day='20200406');复制代码
```

同时删除多个分区

```
alter table dept_partition drop partition(day='20200404'), partition(day='20200405');复制代码
```

9）查看分区表有多少分区

```
show partitions dept_partition;复制代码
```

10） 查看分区表结构

```
desc formatted dept_partition;复制代码
```

### 任务3：Hive 二级分区表

#### **【任务目标】**

 本次任务主要介绍了Hive中二级分区表的原理、用法及其二级分区表的管理并结合实际代码进行演练。

#### 【任务步骤】

![image.png](https://cdn.atstudy.com/lab/manual/16316932446262976.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316956879586956.png?fileid=3701925924353094101)

视频-03：Hive中的二级分区表的创建与使用

**思考:** 如何一天的日志数据量也很大，如何再将数据拆分?

##### 3.1、准备数据

1）准备数据

```
dept_20200401_12.log
dept_20200401_13.log
dept_20200401_14.log复制代码
```

dept_20200401_12.log

```
10,ACCOUNTING,1700
20,RESEARCH,1800复制代码
```

dept_20200401_13.log

```
30,SALES,1900
40,OPERATIONS,1700复制代码
```

dept_20200401_14.log

```
50,TEST,2000
60,DEV,1900复制代码
```

dept_20200401_15.log

```
70,DA,2100
80,BIGDATA,2200复制代码
```

2）将数据创建在 `/home/atstudy/datas ` 目录下：

在Linux终端中，输入命令如下：

```
cd /home/atstudy/datas
cp dept_20200401.log dept_20200401_12.log
cp dept_20200402.log dept_20200401_13.log
cp dept_20200403.log dept_20200401_14.log
vim dept_20200401_15.log复制代码
```

##### 3.2、创建二级分区表

进入Hive客户端，输入SQL如下：

```
create table dept_partition2(
   deptno int, dname string, loc string
)
partitioned by (day string, hour string)
row format delimited fields terminated by ',';复制代码
```

##### 3.3、插入数据到二级分区表

插入数据的二种方式：

**方式一：正常加载数据**

- 加载数据到二级分区表中

```
load data local inpath
'/home/atstudy/datas/dept_20200401_12.log' into table
dept_partition2 partition(day='20200401', hour='12');复制代码
```

- 查询分区数据

```
select * from dept_partition2 where day='20200401' and hour='12';复制代码
```

**方式二：非正常加载数据**

把数据直接上传到分区目录上 ， 让分区表和数据产生关联的三种方法：

- **方法01、上传数据后修复**

上传数据

```
dfs -mkdir -p /user/hive/warehouse/dept_partition2/day=20200401/hour=13;复制代码
dfs -put /home/atstudy/datas/dept_20200401_13.log /user/hive/warehouse/dept_partition2/day=20200401/hour=13;复制代码
```

查询数据（查询不到刚上传的数据）

```
select * from dept_partition2 where day='20200401' and hour='13';复制代码
```

执行修复命令

```
msck repair table dept_partition2;复制代码
```

再次查询数据

```
select * from dept_partition2 where day='20200401' and hour='13';复制代码
```

- **方法02、上传数据后添加分区**

上传数据

```
dfs -mkdir -p /user/hive/warehouse/dept_partition2/day=20200401/hour=14;复制代码
dfs -put /home/atstudy/datas/dept_20200401_14.log /user/hive/warehouse/dept_partition2/day=20200401/hour=14;复制代码
```

执行添加分区

```
alter table dept_partition2 add partition(day='20200401',hour='14');复制代码
```

查询数据

```
select * from dept_partition2 where day='20200401' and hour='14';复制代码
```

- **方法03、创建文件夹后 load 数据到分区**

创建目录

```
dfs -mkdir -p /user/hive/warehouse/dept_partition2/day=20200401/hour=15;复制代码
```

上传数据

```
load data local inpath
'/home/atstudy/datas/dept_20200401_15.log' into table
dept_partition2 partition(day='20200401',hour='15');复制代码
```

查询数据

```
select * from dept_partition2 where day='20200401' and hour='15';复制代码
```

### 任务4：Hive 动态分区

#### **【任务目标】**

 本次任务主要介绍Hive动态分区的概念、原理及实现语法，同时结合具体代码进行演练。

#### 【任务步骤】

![image.png](https://cdn.atstudy.com/lab/manual/16316932722887934.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316959664068735.png?fileid=3701925924353232894)

视频-04：Hive动态分区

##### 4.1、动态分区概念

**说明：**关系型数据库中，对分区表 Insert 数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive 中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用 Hive 的动态分区，需要进行相应的配置。

##### 4.2、动态分区参数配置

（1）开启动态分区功能（默认 true，开启）

```
hive.exec.dynamic.partition=true复制代码
```

> 备注：一般此项不需额外设置，使用默认值即可！

（2）设置为非严格模式（动态分区的模式，默认 strict，表示必须指定至少一个分区为静态分区，nonstrict 模式表示允许所有的分区字段都可以使用动态分区。）

```
hive.exec.dynamic.partition.mode=nonstrict复制代码
```

> 备注：此项需要额外设置，因为其默认值是strict !

（3）在所有执行 MR 的节点上，最大一共可以创建多少个动态分区。默认 1000

```
hive.exec.max.dynamic.partitions=1000复制代码
```

> 备注：此项一般不需额外设置，使用默认值即可！

（4）在每个执行 MR 的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即 day 字段有 365 个值，那么该参数就需要设置成大于 365，如果使用默认值 100，则会报错。

```
hive.exec.max.dynamic.partitions.pernode=100复制代码
```

> 备注：此项一般根据实际业务情况进行设置！

（5）整个 MR Job 中，最大可以创建多少个 HDFS 文件。默认 100000

```
hive.exec.max.created.files=100000复制代码
```

> 备注：此项一般不需额外设置，使用默认值即可！

（6）当有空分区生成时，是否抛出异常。一般不需要设置。默认 false

```
hive.error.on.empty.partition=false复制代码
```

> 备注：此项一般不需额外设置，使用默认值即可！

##### 4.3、动态分区示例

**需求：**将 dept 表中的数据按照地区（loc 字段），插入到目标表 dept_partition 的相应分区中。

（1）创建目标分区表

```
create table dept_partition_dy(id int, name string)
partitioned by (loc int) row format delimited fields terminated by ',';复制代码
```

（2）设置动态分区

```
set hive.exec.dynamic.partition.mode = nonstrict;复制代码
insert into table dept_partition_dy partition(loc) select
deptno, dname, loc from dept;复制代码
```

（3）查看目标分区表的分区情况

```
show partitions dept_partition_dy;复制代码
```

（4）查看HDFS分区表存储

[http://localhost:9870](http://localhost:9870/)

![image.png](https://cdn.atstudy.com/lab/manual/16316933553653428.png)

### 任务5：Hive 分桶表

#### **【任务目标】**

 本次任务主要介绍Hive分桶表的概念、原理及实现语法，同时结合具体代码进行演练。

#### 【任务步骤】

![image.png](https://cdn.atstudy.com/lab/manual/16316933685284702.png)

![image.png](https://cdn.atstudy.com/lab/manual/16316975170847703.png?fileid=3701925924354052680)

视频-05：Hive中的分桶表

##### 5.1、分桶表概述

**说明：**分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。分桶是将数据集分解成更容易管理的若干部分的另一个技术。

分区针对的是数据的存储路径；分桶针对的是数据文件。

##### 5.2、分桶表基本操作

1）先创建分桶表

（1）数据准备

- stu.txt

```
1001,ss1
1002,ss2
1003,ss3
1004,ss4
1005,ss5
1006,ss6
1007,ss7
1008,ss8
1009,ss9
1010,ss10
1011,ss11
1012,ss12
1013,ss13
1014,ss14
1015,ss15
1016,ss16复制代码
```

在Linux命令行终端中，输入命令如下：

```
cd /home/atstudy/datas;
vim stu.txt复制代码
```

（2）创建分桶表

进入HIve客户端,首先将stu.txt上传至HDFS

```
dfs -mkdir -p /student/student_buck;
dfs -put /home/atstudy/datas/stu.txt /student/student_buck;复制代码
```

在Hive命令行中输入SQL如下：

```
create table student_buck(id int, name string)
clustered by(id)
into 5 buckets
row format delimited fields terminated by ',';复制代码
```

（3）查看表结构

```
desc formatted student_buck;复制代码
```

> Num Buckets: 5

（4）导入数据到分桶表中，load 的方式

```
load data  inpath '/student/student_buck/stu.txt' into table student_buck;复制代码
```

（5）查看创建的分桶表中是否分成 5 个桶

![image.png](https://cdn.atstudy.com/lab/manual/16316933939561083.png)

（6）查询分桶的数据

```
select * from student_buck;复制代码
```

##### 5.3、分桶注意事项

（1）分桶规则

 根据结果可知：Hive 的分桶采用对分桶字段的值进行哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中.

> 扩展补充：关于HASH算法的通俗理解
>
> 1）假设现在有1000个人的档案资料需要存放进档案柜子里。要求是能够快速查询到某人档案是否已经存档，如果已经存档则能快速调出档案。如果是你，你会怎么做？最普通的做法就是把每个人的档案依次放到柜子里，然后柜子外面贴上人名，需要查询某个人的档案的时候就根据这个人的姓名来确定是否已经存档。但是1000个人最坏的情况下我们查找一个人的姓名就要对比1000次！ 优化：
>
> 假设以上每个人的姓名笔划数都是不重复的，那么我们通过一个函数把要存档的人姓名笔划数转换到1000以内，然后把这个人的资料就放在转换后的数字指定的柜子里，这个函数就叫做哈希函数，按照这种方式存放的这1000个柜子就叫哈系表(散列表)，人名笔画数就是哈系表的元素，转换后的数就是人名笔划数的哈希值(也就是柜子的序号)。当要查询某个人是否已经存档的时候，我们就通过哈希函数把他的姓名笔划数转化成哈希值，如果哈希值在1000以内，那么恭喜你这个人已经存档，可以到哈希值指定的柜子里去调出他的档案，否则这个人就是黑户，没有存档！

（2）关于分桶表的注意事项

- reduce 的个数设置为-1,让 Job 自行决定需要用多少个 reduce 或者将 reduce 的个数设置为大于等于分桶表的桶数
- 从 hdfs 中 load 数据到分桶表中，避免本地文件找不到问题
- 不要使用本地模式

（3）insert 方式将数据导入分桶表

```
insert into table student_buck select * from student;复制代码
```

### 任务六、能力拓展训练

#### **【任务目标】**

 本次任务主要对分区表、分桶表结合具体案例进行代码演练，以达到熟练掌握的目的。

#### 【任务步骤】

![image.png](https://cdn.atstudy.com/lab/manual/16316934097337757.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631698132368905.png?fileid=3701925924354220926)

视频-06：能力拓展训练

（1）启动 Hadoop 服务

```
start-all.sh复制代码
```

（2）查看Hadoop是否正常启动，查看后台运行的进程

```
jps复制代码
```

（3）启动 hive

```
# 第一种方式
hive
# 使用第二种方式 
nohup hive --service hiveserver2 2>&1 &
# 然后再重新启动beeline客户端
beeline --showDbInPrompt=true
# 连结到默认的default数据库
!connect jdbc:hive2://localhost:10000/default复制代码
```

（4）创建一个scoredb数据库

```
create database scoredb;复制代码
```

（5）进入scoredb数据库

```
use scoredb;复制代码
```

（6）创建 一个score内部表

三个 字段，分别为：

name：string ，学员名称

subject：string , 表示考试科目

score ：double，表示考试成绩

> 说明，因为要加载的数格式为 ly,english,80.00 形式，所以注意 row format delimited 要设置为 `,`

```
create  table if not exists score(
name string,
subject string,
score  double
) row format delimited fields terminated by ',';复制代码
```

（7）在 `/home/atstudy/datas` 目录下，新建：score.txt 文件，内容如下：

```
ly,language,90
ly,math,70
ly,english,80
lc,language,60
lc,math,77
lc,english,96
zl,language,66
zl,math,88
zl,english,99复制代码
```

（8）通过 `load data` 方式 将数据加载至 score 表中

```
load data local inpath '/home/atstudy/datas/score.txt' into table score;复制代码
```

（9）查看score 表中的数据

```
select * from score;复制代码
```

（10）统计该班学员各科成绩的总和、平均分，最高分和最低分

```
select t.subject,sum(t.score) as sum_score,avg(t.score) as avg_socre,max(t.score) as max_score,min(t.score) as min_score from score t group by t.subject;复制代码
```

##### 6.2、拓展训练-2：HQL分区表练习

（1）数据准备如下(三个班级数据考试成绩数据)：

class01.txt

```
ly,language,91
ww,math,70
zl,english,80复制代码
```

class02.txt

```
aa,language,90
bb,language,60
cc,language,66复制代码
```

class03.txt

```
dd,math,70
ee,math,77
ff,math,88复制代码
```

（2）创建分区表 score2 ,分区字段名称为：class

```
create table score2(
name string,
subject string,
score  double
)
partitioned by (class string)
row format delimited fields terminated by ',';复制代码
```

(3）分别加载本地数据到分区表中（三个班级：class01,class02,class03）

```
load data local inpath
'/home/atstudy/datas/class01.txt' into table score2
partition(class='class01');

load data local inpath
'/home/atstudy/datas/class02.txt' into table score2
partition(class='class02');

load data local inpath
'/home/atstudy/datas/class03.txt' into table score2
partition(class='class03');复制代码
```

（4）查看有几个分区

```
show partitions score2;复制代码
```

（5）查看分区表详细信息

```
select * from score2;复制代码
```

（6）查看HDFS上各个分区数据

HDFS分区截图

![image.png](https://cdn.atstudy.com/lab/manual/16316934381359961.png)

（7）SQL查询各个分区数据

```
select * from score2 where class='class01';
select * from score2 where class='class02';
select * from score2 where class='class03';复制代码
```

（8）将各个分区中分数高于80分的成绩的学员查询出来，并联合显示出来。

```
select * from score2 where class='class01' and score>80
union
select * from score2 where class='class02' and score>80
union
select * from score2 where class='class03' and score>80;复制代码
```

## 课程总结

1）分区表实际上就是对应一个 HDFS 文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。Hive 中的分区就是分目录。

2）分区表的基础语法：

```
create table dept_partition(
deptno int, dname string, loc string
)
partitioned by (day string)
row format delimited fields terminated by ',';复制代码
```

3）二级分区表的基础语法

```
create table dept_partition2( 
   deptno int, dname string, loc string
)
partitioned by (day string, hour string)
row format delimited fields terminated by ',';复制代码
```

4）动态分区：Hive 中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用 Hive 的动态分区，需要进行相应的配置。

5）分桶是将数据集分解成更容易管理的若干部分的另一个技术，分区针对的是数据的存储路径；分桶针对的是数据文件。







# 实验4-1：Hive中的内置函数及其实战应用

## 实验概述

 本次实验主要讲解了Hive中的常用系统内部函数、行转列函数、列转行数等，并通过实际案例进行代码演练。

## 实验环境

- Linux 操作系统
- Hadoop 3.x
- Hive 3.x
- JDK 1.8
- MySQL 8.x

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16317545558678849.png)

## 实验目标

学习完成本实验后，您将能够

- 掌握 HIve 常见系统内置函数及其应用
- 掌握 HIve 中选择函数、行转列及列转行等高级函数语法及具体应用

## 实验任务

### 任务1：Hive开发环境准备

#### **【任务目标】**

 本次任务主要是准备好基础的Hive开发环境，启动Hadoop，启动Hive服务及客户端以及部分实验数据准备，为后继实验打下基础。

#### **【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16317545723008104.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317548376564573.png?fileid=3701925924385402789)

视频-01：Hive开发环境准备

##### 1.1、打开Terminal终端

进入项目实训平台后，如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317545874873453.png)

在桌面空白位置点击鼠标右键，选择 `在此打开终端 `如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/1631754599588946.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317546137407252.png)

##### 1.2、启动hadoop

在命令终端输入以下命令，启动hadoop相关服务

```
start-all.sh复制代码
```

启动效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317546379608394.png)

##### 1.3、查看服务状态

可以通过如下命令查看Hadoop服务的启动状态：

```
jps复制代码
```

执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317546489922737.png)

如果出现如上服务，则恭喜你的Hadoop启动成功。

##### 1.4、启动 hiveserver2 服务

后台方式启动hiveserver2服务

```
nohup hive --service hiveserver2 2>&1 &复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317546629862832.png)

通过如下命令确认 hiveserver2 服务是否已经启动：

```
netstat -ntulp | grep 10000复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317546750678205.png)

##### 1.5、JDBC方式连结Hive客户端

然后再重新启动beeline客户端

```
beeline --showDbInPrompt=true复制代码
```

连结到默认的default数据库

```
!connect jdbc:hive2://localhost:10000/default复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317546885048444.png)

##### 1.6、准备实验数据

![image.png](https://cdn.atstudy.com/lab/manual/16317546991192851.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317549781656974.png?fileid=3701925924385406034)

视频-02：Hive实验数据准备

1）准备数据

- dept.txt

```
10	ACCOUNTING	1700
20	RESEARCH	1800
30	SALES	1900
40	OPERATIONS	1700复制代码
```

> 特别注意：以上的每个元素之间的间隔用tab键，在拷贝到Linux环境中时，最好每个数据之间的tab键再手工输入一次。

- emp.txt

```
7369,SMITH,CLERK,7902,1980-12-17,800.00,100.00,20
7499,ALLEN,SALESMAN,7698,1981-2-20,1600.00,300.00,30
7521,WARD,SALESMAN,7698,1981-2-22,1250.00,500.00,30
7566,JONES,MANAGER,7839,1981-4-2,2975.00,300.00,20
7654,MARTIN,SALESMAN,7698,1981-9-28,1250.00,1400.00,30
7698,BLAKE,MANAGER,7839,1981-5-1,2850.00,,30
7782,CLARK,MANAGER,7839,1981-6-9,2450.00,1200.00,10
7788,SCOTT,ANALYST,7566,1987-4-19,3000.00,1000.00,20
7839,KING,PRESIDENT,7860,1981-11-17,5000.00,800.00,10
7844,TURNER,SALESMAN,7698,1981-9-8,1500.00,,30
7876,ADAMS,CLERK,7788,1987-5-23,1100.00,300.00,20
7900,JAMES,CLERK,7698,1981-12-3,950.00,200.00,30
7902,FORD,ANALYST,7566,1981-12-3,3000.00,200.00,20
7934,MILLER,CLERK,7782,1982-1-23,1300.00,100.00,10复制代码
```

- loc.txt

```
1700,Beijing
1800,London
1900,Tokyo复制代码
```

脚本代码 ：

```
cd /home/atstudy/
mkdir datas
touch dept.txt
vim dept.txt
touch emp.txt
vim emp.txt
touch loc.txt
vim loc.txt复制代码
```

2）创建表

- 创建部门表

```
create  table if not exists dept(
deptno int,
dname string,
loc int
) row format delimited fields terminated by '\t';复制代码
```

- 创建员工表

```
create  table if not exists emp(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int)
row format delimited fields terminated by ',';复制代码
```

- 创建地址表

```
create table if not exists location(
loc int,
loc_name string
)
row format delimited fields terminated by ',';复制代码
```

3）导入数据

- 导入dept数据

```
load data local inpath '/home/atstudy/datas/dept.txt' into table dept;复制代码
```

- 导入emp数据

```
load data local inpath '/home/atstudy/datas/emp.txt' into table emp;复制代码
```

- 导入loc数据

```
load data local inpath '/home/atstudy/datas/loc.txt' into table location;复制代码
```

4）查看数据

```
use default;
select * from dept;
select * from emp;
select * from location;复制代码
```

### 任务2：Hive中的系统内置函数

#### **【任务目标】**

 本次任务主要介绍Hive中常见的系统内置函数及其使用和注意事项。

#### **【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16317547301771270.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317551636689343.png?fileid=3701925924385505489)

视频-03：Hive中的系统内置函数

##### 2.1 、查看系统内置函数

1）查看系统自带的函数

```
show functions;复制代码
```

2）显示自带的函数的用法

```
desc function upper;复制代码
```

3）详细显示自带的函数的用法

```
desc function extended upper;复制代码
```

##### 2.2、空字段赋值函数

1）函数说明

**NVL：**给值为 NULL 的数据赋值，它的格式是 NVL( value，default_value)。它的功能是如果 value 为 NULL，则 NVL 函数返回 default_value 的值，否则返回 value 的值，如果两个参数都为 NULL ，则返回 NULL。

2）数据准备：采用员工表

3）查询：如果员工的 comm 为 为 NULL ，则用-1 代替

```
select comm,nvl(comm, -1) from emp;复制代码
```

4）查询：如果员工的 comm 为 为 NULL ，则用领导 id 代替

```
select comm, nvl(comm,mgr) from emp;复制代码
```

### 任务3：Hive中的选择函数及应用

#### **【任务目标】**

 本次任务主要介绍Hive中CASE WHEN选择函数及实际应用。

#### **【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16317547491628548.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317552865238525.png?fileid=3701925924385615755)

视频-04：Hive中的选择函数及应用

##### 3.1、示例：数据准备

- emp_by_sex.txt

```
tom,it,boy
jack,it,boy
mary,net,boy
smith,it,girl
rose,net,girl
jason,net,girl复制代码
```

> 以上表达的是三列：name,dept_id,sex

##### 3.2、示例：需求

求出不同部门男女各多少人。结果如下：

```
dept_Id boy girl
it       2   1
net      1   2复制代码
```

创建本地 emp_by_sex.txt ，导入数据。在Linux终端中，输入如下命令：

```
cd /home/atstudy/datas
vim emp_by_sex.txt复制代码
```

创建 hive 表并导入数据，进入Hive客户端，输入如下HQL：

```
create table emp_by_sex(
name string,
dept_id string,
sex string)
row format delimited fields terminated by ",";复制代码
```

导入数据：

```
load data local inpath '/home/atstudy/datas/emp_by_sex.txt' into table emp_by_sex;复制代码
```

按需求查询数据：

```
select
dept_id,
sum(case sex when 'boy' then 1 else 0 end) boy,
sum(case sex when 'girl' then 1 else 0 end) girl
from emp_by_sex
group by dept_id;复制代码
```

### 任务4：Hive中的行转列函数及其应用

#### **【任务目标】**

 本次任务主要介绍Hive中行转列CONCAT、CONCAT_WS 函数的语法注意事项及其实际应用。

#### **【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16317547667271588.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317556108108454.png?fileid=3701925924385648422)

视频-05：Hive中的行转列函数及其应用

##### 4.1、函数说明

**CONCAT(string A/col, string B/col…)：**返回输入字符串连接后的结果，支持任意个输入字符串; **CONCAT_WS(separator, str1, str2,...)：**它是一个特殊形式的 CONCAT()。第一个参数是剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;

> 注意: CONCAT_WS must be "string or array COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生 Array 类型字段。

##### 4.2、函数示例

1）数据准备

```
tom,白羊座,A
jack,射手座,A
mary,白羊座,B
smith,白羊座,A
rose,射手座,A
jason,白羊座,B复制代码
```

> 数据结构：name ：姓名，constellation ：星座，blood_type ：血型

2）需求

把星座和血型一样的人归类到一起。结果如下：

```
射手座,A 	jack|rose
白羊座,A 	tom|smith
白羊座,B 	mary|jason复制代码
```

3）创建本地 constellation.txt

在Linux终端中，输入如下命令：

```
cd /home/atstudy/datas
vim constellation.txt复制代码
```

4） 创建 hive 表并导入数据

进入Hive客户端，输入 HQL：

```
create table person_info(
name string,
constellation string,
blood_type string)
row format delimited fields terminated by ",";复制代码
load data local inpath "/home/atstudy/datas/constellation.txt" into table
person_info;复制代码
```

5） 按需求查询数据

```
SELECT
t1.c_b,
CONCAT_WS("|",collect_set(t1.name))
FROM (
    SELECT
    NAME,
    CONCAT_WS(',',constellation,blood_type) c_b
    FROM person_info
) t1
GROUP BY t1.c_b;复制代码
```

### 任务5：Hive中的列转行函数及其应用

#### **【任务目标】**

 本次任务主要介绍Hive中列转行 EXPLODE、LATERAL VIEW 函数的语法注意事项及其实际应用。

#### **【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16317547914384772.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317558382024806.png?fileid=3701925924385834250)

视频-06：Hive中的列转行函数及其应用

##### 5.1、函数说明

**EXPLODE(col)：**将 hive 一列中复杂的 Array 或者 Map 结构拆分成多行。

**LATERAL VIEW**

```
用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias复制代码
```

> 解释：用于和 split, explode 等 UDTF 一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。

##### 5.2、函数示例

1）数据准备

```
《疑犯追踪》	悬疑,动作,科幻,剧情
《Lie to me》	悬疑,警匪,动作,心理,剧情
《战狼2》	战争,动作,灾难复制代码
```

> 数据结构：movie：电影名称， category：电影类别

2）需求

将电影分类中的数组数据展开。结果如下：

```
《疑犯追踪》 悬疑
《疑犯追踪》 动作
《疑犯追踪》 科幻
《疑犯追踪》 剧情
《Lie to me》 悬疑
《Lie to me》 警匪
《Lie to me》 动作
《Lie to me》 心理
《Lie to me》 剧情
《战狼 2》 战争
《战狼 2》 动作
《战狼 2》 灾难复制代码
```

3）创建本地 movie.txt ，导入数据

在Linux终端中，输入如下命令：

```
cd /home/atstudy/datas
vim movie.txt复制代码
```

4）创建 hive 表并导入数据

进入Hive客户端，输入 HQL：

```
create table movie_info(
movie string,
category string)
row format delimited fields terminated by "\t";复制代码
load data local inpath "/home/atstudy/datas/movie.txt" into table movie_info;复制代码
```

5）按需求查询数据

```
SELECT
movie,
category_name
FROM
movie_info
lateral VIEW
explode(split(category,",")) movie_info_tmp AS category_name;复制代码
```

## 实验总结

1）查看Hive的内置函数

- show functions;
- desc function upper;
- desc function extended upper;

2）空字段赋值函数,它的格式是 NVL( value，default_value)

```
select comm,nvl(comm, -1) from emp;复制代码
```

3）选择函数CASE WHEN：

```
select
dept_id,
sum(case sex when 'boy' then 1 else 0 end) boy,
sum(case sex when 'girl' then 1 else 0 end) girl
from emp_by_sex
group by dept_id;复制代码
```

4）利用 CONCAT_WS 可以实现Hive中的行转列

5）利用 EXPLODE 函数结合 LATERAL VIEW 实现 Hive 中的列转行

```
用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias
```





# 实验4-2：实战Hive中的开窗函数

## 实验概述

 本次实验主要讲解了Hive中开窗函数及其在实际工作中的具体应用，并通过实际案例进行代码演练。

## 实验环境

- Linux 操作系统
- Hadoop 3.x
- Hive 3.x
- JDK 1.8
- MySQL 8.x

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16317580762021017.png)

## 实验目标

学习完成本实验后，您将能够

- 理解 HIve 开窗函数的作用与适用场景
- 掌握 HIve 中的开窗函数及其实际应用

## 实验任务

### 任务1：Hive开发环境准备

#### **【任务目标】**

 本次任务主要是准备好基础的Hive开发环境，启动Hadoop，启动Hive服务及客户端以及部分实验数据准备，为后继实验打下基础。

#### **【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16317581258357977.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317586489827917.png?fileid=3701925924387199421)

视频-01：Hive开发环境准备

##### 1.1、打开Terminal终端

进入项目实训平台后，如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/1631758140629366.png)

在桌面空白位置点击鼠标右键，选择 `在此打开终端 `如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/163175815367941.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317581655621646.png)

##### 1.2、启动hadoop

在命令终端输入以下命令，启动hadoop相关服务

```
start-all.sh复制代码
```

启动效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/1631758178589750.png)

##### 1.3、查看服务状态

可以通过如下命令查看Hadoop服务的启动状态：

```
jps复制代码
```

执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317581882825736.png)

如果出现如上服务，则恭喜你的Hadoop启动成功。

##### 1.4、启动 hiveserver2 服务

后台方式启动hiveserver2服务

```
nohup hive --service hiveserver2 2>&1 &复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317582317085102.png)

通过如下命令确认 hiveserver2 服务是否已经启动：

```
netstat -ntulp | grep 10000复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317582433219477.png)

##### 1.5、JDBC方式连结Hive客户端

然后再重新启动beeline客户端

```
beeline --showDbInPrompt=true复制代码
```

连结到默认的default数据库

```
!connect jdbc:hive2://localhost:10000/default复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317582551833322.png)

##### 1.6、准备实验数据

![image.png](https://cdn.atstudy.com/lab/manual/16317582660865743.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317611593451555.png?fileid=3701925924388633362)

视频-02：Hive实验数据准备

1）准备数据

- dept.txt

```
10	ACCOUNTING	1700
20	RESEARCH	1800
30	SALES	1900
40	OPERATIONS	1700复制代码
```

> 特别注意：以上的每个元素之间的间隔用tab键，在拷贝到Linux环境中时，最好每个数据之间的tab键再手工输入一次。

- emp.txt

```
7369,SMITH,CLERK,7902,1980-12-17,800.00,100.00,20
7499,ALLEN,SALESMAN,7698,1981-2-20,1600.00,300.00,30
7521,WARD,SALESMAN,7698,1981-2-22,1250.00,500.00,30
7566,JONES,MANAGER,7839,1981-4-2,2975.00,300.00,20
7654,MARTIN,SALESMAN,7698,1981-9-28,1250.00,1400.00,30
7698,BLAKE,MANAGER,7839,1981-5-1,2850.00,,30
7782,CLARK,MANAGER,7839,1981-6-9,2450.00,1200.00,10
7788,SCOTT,ANALYST,7566,1987-4-19,3000.00,1000.00,20
7839,KING,PRESIDENT,7860,1981-11-17,5000.00,800.00,10
7844,TURNER,SALESMAN,7698,1981-9-8,1500.00,,30
7876,ADAMS,CLERK,7788,1987-5-23,1100.00,300.00,20
7900,JAMES,CLERK,7698,1981-12-3,950.00,200.00,30
7902,FORD,ANALYST,7566,1981-12-3,3000.00,200.00,20
7934,MILLER,CLERK,7782,1982-1-23,1300.00,100.00,10复制代码
```

- loc.txt

```
1700,Beijing
1800,London
1900,Tokyo复制代码
```

脚本代码 ：

```
cd /home/atstudy/
mkdir datas
touch dept.txt
vim dept.txt
touch emp.txt
vim emp.txt
touch loc.txt
vim loc.txt复制代码
```

2）创建表

- 创建部门表

```
create  table if not exists dept(
deptno int,
dname string,
loc int
) row format delimited fields terminated by '\t';复制代码
```

- 创建员工表

```
create  table if not exists emp(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int)
row format delimited fields terminated by ',';复制代码
```

- 创建地址表

```
create table if not exists location(
loc int,
loc_name string
)
row format delimited fields terminated by ',';复制代码
```

3）导入数据

- 导入dept数据

```
load data local inpath '/home/atstudy/datas/dept.txt' into table dept;复制代码
```

- 导入emp数据

```
load data local inpath '/home/atstudy/datas/emp.txt' into table emp;复制代码
```

- 导入loc数据

```
load data local inpath '/home/atstudy/datas/loc.txt' into table location;复制代码
```

4）查看数据

```
use default;
select * from dept;
select * from emp;
select * from location;复制代码
```

### 任务2：Hive 窗口函数说明

#### **【任务目标】**

 本次任务主要介绍什么是窗口函数，常见的窗口函数有哪些，窗口函数的适用场景等。

#### **【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16317582908276445.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317613283396853.png?fileid=3701925924388765648)

视频-03：MySQL8用Hive中的窗口函数简介

##### 2.1、什么是窗口函数

 窗口函数是 SQL 中一类特别的函数。和聚合函数相似，窗口函数的输入也是多行记录。不同的是，聚合函数的作用于由 GROUP BY 子句聚合的组，而窗口函数则作用于一个窗口， 这里，窗口是由一个 OVER子句 定义的多行记录。聚合函数对其所作用的每一组记录输 出一条结果，而窗口函数对其所作用的窗口中的每一行记录输出一条结果。一些聚合函 数，如 sum, max, min, avg,count 等也可以当作窗口函数使用。

 与聚集函数一样，窗口函数也针对定义的行集（组）执行聚集，但它不像聚集函数那样每组之返回一个值，窗口函数可以为每组返回多个值。实际上，DB2中称这种函数为联机分析处理OLAP函数，而Oracle把它们称为解析函数，但ISO SQL标准把它们称为窗口函数。窗口函数一般在OLAP分析、制作报表过程中会使用到。MySQL从8.0开始支持窗口函数，这个功能在大多数据库中早已支持，有的也叫分析函数。

##### 2.2、窗口的概念

那么什么是窗口呢？ 窗口的概念非常重要，它可以理解为记录集合，窗口函数也就是在满足某种条件的记录集合上执行 的特殊函数。对于每条记录都要在此窗口内执行函数，有的函数随着记录不同，窗口大小都是固定 的，这种属于静态窗口；有的函数则相反，不同的记录对应着不同的窗口，这种动态变化的窗口叫 滑动窗口。简单的说窗口函数就是对于查询的每一行，都使用与该行相关的行进行计算。

##### 2.3、窗口函数和聚合函数的区别

窗口函数和普通聚合函数很容易混淆，二者区别如下：

- 聚合函数是将多条记录聚合为一条；而窗口函数是每条记录都会执行，有几条记录执行完还是几 条。
- 聚合函数也可以用于窗口函数中

##### 2.4、常见窗口函数

- OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化。
- CURRENT ROW：当前行
- n PRECEDING：往前 n 行数据
- n FOLLOWING：往后 n 行数据
- UNBOUNDED：起点，
  - UNBOUNDED PRECEDING 表示从前面的起点，
  - UNBOUNDED FOLLOWING 表示到后面的终点
- LAG(col,n,default_val)：往前第 n 行数据
- LEAD(col,n, default_val)：往后第 n 行数据
- NTILE(n)：把有序窗口的行分发到指定数据的组中，各个组有编号，编号从 1 开始，对于每一行，NTILE 返回此行所属的组的编号。注意：n 必须为 int 类型。

### 任务3：Hive 窗口函数示例

#### **【任务目标】**

 本次任务主要介绍了Hive窗口函数的具体使用示例。

#### **【任务步骤】**

##### 3.1、数据准备

- business.txt

```
jack,2017-01-01,10
tony,2017-01-02,15
jack,2017-02-03,23
tony,2017-01-04,29
jack,2017-01-05,46
jack,2017-04-06,42
tony,2017-01-07,50
jack,2017-01-08,55
mart,2017-04-08,62
mart,2017-04-09,68
neil,2017-05-10,12
mart,2017-04-11,75
neil,2017-06-12,80
mart,2017-04-13,94复制代码
```

> 数据结构说明：name: 客户，orderdate:下单日期，cost：下单金额

##### 3.2、函数示例

1）需求

（1）查询在 2017 年 4 月份购买过的顾客及总人数

（2）查询顾客的购买明细及月购买总额

（3）上述的场景, 将每个顾客的 cost 按照日期进行累加

（4）查询每个顾客上次的购买时间

（5）查询前 20%时间的订单信息

2）创建本地 business.txt ，导入数据

```
cd /home/atstudy/datas
vim business.txt复制代码
```

3）创建 hive

新建business表

```
create table business(
name string,
orderdate string,
cost int
) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';复制代码
```

导入数据：

```
load data local inpath "/home/atstudy/datas/business.txt" into table business;复制代码
```

4）按需求查询数据

- 查询在 2017 年 4 月份购买过的顾客及总人数

```
select name,count(*) over ()
from business
where substring(orderdate,1,7) = '2017-04'
group by name;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317583180232527.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317614167033436.png?fileid=3701925924388822909)

视频-04：Hive开窗函数示例-1 查询购买过的顾客及总人数

- 查询顾客的购买明细及月购买总额

```
select name,orderdate,cost,sum(cost) over(partition by month(orderdate)) from business;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317583318167648.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317617934353915.png?fileid=3701925924389082411)

视频-05：Hive开窗函数示例-2 查询顾客的购买明细及月购买总额

- 将每个顾客的 cost 按照日期进行累加

```
select name,orderdate,cost,
sum(cost) over() as sample1,-- 所有行相加
sum(cost) over(partition by name) as sample2,-- 按 name 分组，组内数据相加
sum(cost) over(partition by name order by orderdate) as sample3,-- 按 name分组，组内数据累加
sum(cost) over(partition by name order by orderdate rows between
UNBOUNDED PRECEDING and current row ) as sample4  ,-- 和 sample3 一样,由起点到当前行的聚合
sum(cost) over(partition by name order by orderdate rows between 1
PRECEDING and current row) as sample5, -- 当前行和前面一行做聚合
sum(cost) over(partition by name order by orderdate rows between 1
PRECEDING AND 1 FOLLOWING ) as sample6,-- 当前行和前边一行及后面一行
sum(cost) over(partition by name order by orderdate rows between current
row and UNBOUNDED FOLLOWING ) as sample7 -- 当前行及后面所有行
from business;复制代码
```

> 说明：rows 必须跟在 order by 子句之后，对排序的结果进行限制，使用固定的行数来限制分区中的数据行数量

![image.png](https://cdn.atstudy.com/lab/manual/16317583437282238.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317627797016131.png?fileid=3701925924389312890)

视频-06：Hive开窗函数示例-3 将每个顾客的 cost 按照日期进行累加

- 查看顾客上次的购买时间

```
select name,orderdate,cost,
lag(orderdate,1,'1900-01-01') over(partition by name order by orderdate )
as time1, lag(orderdate,2) over (partition by name order by orderdate) as time2
from business;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317583558621473.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631762821667567.png?fileid=3701925924389671718)

视频-07：Hive开窗函数示例-4 查看顾客上次的购买时间

- 查询前 20%时间的订单信息

```
select * from (
select name,orderdate,cost, ntile(5) over(order by orderdate) sorted
from business
) t
where sorted = 1;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/1631758370657517.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631762905771481.png?fileid=3701925924389622941)

视频-08：Hive开窗函数示例-5 查询前20%时间的订单信息

### 任务4：Rank函数及其具体使用

#### **【任务目标】**

 本次任务主要介绍Hive中的Rank函数语法及其具体应用，并结合具体案例进行代码演练。

#### **【任务步骤】**

![image.png](https://cdn.atstudy.com/lab/manual/16317583829421169.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631762988006823.png?fileid=3701925924389625591)

视频-09：Rank函数及其具体使用

##### 4.1、函数说明

RANK() 排序相同时会重复，总数不会变

DENSE_RANK() 排序相同时会重复，总数会减少

ROW_NUMBER() 会根据顺序计算

##### 4.2、函数应用

1）数据准备

- score.txt

```
AA,chinese,87
AA,math,95
AA,english,68
BB,chinese,94
BB,math,56
BB,english,84
CC,chinese,64
CC,math,86
CC,english,84
DD ,chinese,65
DD,math,85
DD,english,78复制代码
```

> 数据结构：name：姓名 ，subject：科目， score：成绩

2）需求

计算每门学科成绩排名。

3）创建本地 score.txt ，导入数据

```
cd /home/atstusy/datas
vim score.txt复制代码
```

4）创建 hive 表并导入数据

```
create table score(
name string,
subject string,
score int)
row format delimited fields terminated by ",";
load data local inpath '/home/atstudy/datas/score.txt' into table score;复制代码
```

5）按需求查询数据

```
select name,
subject,
score,
rank() over(partition by subject order by score desc) rp,
dense_rank() over(partition by subject order by score desc) drp,
row_number() over(partition by subject order by score desc) rmp
from score;复制代码
```

扩展：求出每门学科前三名的学生？

## 实验总结

1）常见窗口函数

- OVER()：指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化。
- LAG(col,n,default_val)：往前第 n 行数据
- LEAD(col,n, default_val)：往后第 n 行数据
- NTILE(n)：把有序窗口的行分发到指定数据的组中，各个组有编号，编号从 1 开始，对于每一行，NTILE 返回此行所属的组的编号。注意：n 必须为 int 类型。

2）利用Rank窗口函数实现排序功能

- RANK() 排序相同时会重复，总数不会变
- DENSE_RANK() 排序相同时会重复，总数会减少
- ROW_NUMBER() 会根据顺序计算







# 实验6-1：Hive调优实战-1_执行计划、Fetch抓取与本地模式

## 实验概述

 本系列实训任务主要讲解了如何对Hive进行数据处理及数据查询统计的性能调优的方法、策略及背后的调优原理等。此为本系列课程的第一个实验，主要讲解如何查看HIve的查询执行计划配置Fetch抓取模式及本地模式配置 。本次课是调优的前提和基础。

## 实验环境

- Linux 操作系统
- Hadoop2.7.7
- Hive-1.2.2
- MySQL5.7

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16317805909131631.png)

## 实验目标

完成本实验后，您将能够

- 了解如何查看HIve的查询执行计划
- 掌握如何配置Fetch抓取模式及本地模式配置

## 实验任务

### 任务1：Hive开发环境及数据准备

#### **【任务目标】**

 本次任务主要是准备好基础的Hive开发环境，启动Hadoop，启动Hive服务及客户端以及部分实验数据准备，为后继实验打下基础。

#### **【任务步骤】**

##### 1.1、打开Terminal终端

进入项目实训平台后，如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317806021782995.png)

在桌面空白位置点击鼠标右键，选择 `在此打开终端 `如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317806113977006.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317806209993307.png)

##### 1.2、启动hadoop

在命令终端输入以下命令，启动hadoop相关服务

```
start-all.sh复制代码
```

启动效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317806311503189.png)

##### 1.3、查看服务状态

可以通过如下命令查看Hadoop服务的启动状态：

```
jps复制代码
```

执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/1631780641384571.png)

如果出现如上服务，则恭喜你的Hadoop启动成功。

##### 1.4、启动 hiveserver2 服务

```
# 后台方式启动hiveserver2服务
nohup hive --service hiveserver2 2>&1 &复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317806530909794.png)

通过如下命令确认 hiveserver2 服务是否已经启动：

```
netstat -ntulp | grep 10000复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317806651561394.png)

##### 1.5、JDBC方式连结Hive客户端

```
# 然后再重新启动beeline客户端
beeline --showDbInPrompt=true
# 连结到默认的default数据库
!connect jdbc:hive2://localhost:10000/default复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317806760263227.png)

##### 1.6、准备实验数据

1）准备数据

- dept.txt

```
10	ACCOUNTING	1700
20	RESEARCH	1800
30	SALES	1900
40	OPERATIONS	1700复制代码
```

> 特别注意：以上的每个元素之间的间隔用tab键，在拷贝到Linux环境中时，最好每个数据之间的tab键再手工输入一次。

- emp.txt

```
7369,SMITH,CLERK,7902,1980-12-17,800.00,100.00,20
7499,ALLEN,SALESMAN,7698,1981-2-20,1600.00,300.00,30
7521,WARD,SALESMAN,7698,1981-2-22,1250.00,500.00,30
7566,JONES,MANAGER,7839,1981-4-2,2975.00,300.00,20
7654,MARTIN,SALESMAN,7698,1981-9-28,1250.00,1400.00,30
7698,BLAKE,MANAGER,7839,1981-5-1,2850.00,1000.00,30
7782,CLARK,MANAGER,7839,1981-6-9,2450.00,1200.00,10
7788,SCOTT,ANALYST,7566,1987-4-19,3000.00,1000.00,20
7839,KING,PRESIDENT,7860,1981-11-17,5000.00,800.00,10
7844,TURNER,SALESMAN,7698,1981-9-8,1500.00,0.00,30
7876,ADAMS,CLERK,7788,1987-5-23,1100.00,300.00,20
7900,JAMES,CLERK,7698,1981-12-3,950.00,200.00,30
7902,FORD,ANALYST,7566,1981-12-3,3000.00,200.00,20
7934,MILLER,CLERK,7782,1982-1-23,1300.00,100.00,10复制代码
```

- loc.txt

```
1700,Beijing
1800,London
1900,Tokyo复制代码
```

脚本代码 ：

```
cd /home/atstudy/
mkdir datas
touch dept.txt
vim dept.txt
touch emp.txt
vim emp.txt
touch loc.txt
vim loc.txt复制代码
```

2）创建表

- 创建部门表

```
create  table if not exists dept(
deptno int,
dname string,
loc int
) row format delimited fields terminated by '\t';复制代码
```

- 创建员工表

```
create  table if not exists emp(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int)
row format delimited fields terminated by ',';复制代码
```

- 创建地址表

```
create table if not exists location(
loc int,
loc_name string
)
row format delimited fields terminated by ',';复制代码
```

3）导入数据

- 导入dept数据

```
load data local inpath '/home/atstudy/datas/dept.txt' into table dept;复制代码
```

- 导入emp数据

```
load data local inpath '/home/atstudy/datas/emp.txt' into table emp;复制代码
```

- 导入loc数据

```
load data local inpath '/home/atstudy/datas/loc.txt' into table location;复制代码
```

4）查看数据

```
use default;
select * from dept;
select * from emp;
select * from location;复制代码
```

### 任务2：查看Hive的执行计划

#### 【任务目标】

 本次任务主要是了解如何查看Hive中的执行计划以及如何阅读Hive中的执行计划中的关键节点，为后继Hive大数据分析的优化打下基础。

#### 【任务步骤】

##### 2.1、Hive架构设计

![image.png](https://cdn.atstudy.com/lab/manual/16317820049509305.png)

##### 2.2、基本语法

```
EXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query复制代码
```

##### 2.3、实例演示

（1）查看下面这条语句的执行计划

首先演示一个没有MR任务的：

```
explain select * from emp;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317808494082714.png)

接下来再演示一个有MR任务的：

```
explain select deptno, avg(sal) avg_sal from emp group by deptno;复制代码
```

输出

```
+----------------------------------------------------+
|                      Explain                       |
+----------------------------------------------------+
| STAGE DEPENDENCIES:                                |
|   Stage-1 is a root stage                          |
|   Stage-0 depends on stages: Stage-1               |
|                                                    |
| STAGE PLANS:                                       |
|   Stage: Stage-1                                   |
|     Map Reduce                                     |
|       Map Operator Tree:                       |
|           TableScan                                |
|             alias: emp                             |
|             Statistics: Num rows: 1 Data size: 7240 Basic stats: COMPLETE Column stats: NONE |
|             Select Operator                        |
|               expressions: sal (type: double), deptno (type: int) |
|               outputColumnNames: sal, deptno       |
|               Statistics: Num rows: 1 Data size: 7240 Basic stats: COMPLETE Column stats: NONE |
|               Group By Operator                    |
|                 aggregations: sum(sal), count(sal) |
|                 keys: deptno (type: int)           |
|                 mode: hash                         |
|                 outputColumnNames: _col0, _col1, _col2 |
|                 Statistics: Num rows: 1 Data size: 7240 Basic stats: COMPLETE Column stats: NONE |
|                 Reduce Output Operator             |
|                   key expressions: _col0 (type: int) |
|                   sort order: +                    |
|                   Map-reduce partition columns: _col0 (type: int) |
|                   Statistics: Num rows: 1 Data size: 7240 Basic stats: COMPLETE Column stats: NONE |
|                   value expressions: _col1 (type: double), _col2 (type: bigint) |
|       Execution mode: vectorized                   |
|       Reduce Operator Tree:                        |
|         Group By Operator                          |
|           aggregations: sum(VALUE._col0), count(VALUE._col1) |
|           keys: KEY._col0 (type: int)              |
|           mode: mergepartial                       |
|           outputColumnNames: _col0, _col1, _col2   |
|           Statistics: Num rows: 1 Data size: 7240 Basic stats: COMPLETE Column stats: NONE |
|           Select Operator                          |
|             expressions: _col0 (type: int), (_col1 / _col2) (type: double) |
|             outputColumnNames: _col0, _col1        |
|             Statistics: Num rows: 1 Data size: 7240 Basic stats: COMPLETE Column stats: NONE |
|             File Output Operator                   |
|               compressed: false                    |
|               Statistics: Num rows: 1 Data size: 7240 Basic stats: COMPLETE Column stats: NONE |
|               table:                               |
|                   input format: org.apache.hadoop.mapred.SequenceFileInputFormat |
|                   output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat |
|                   serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe |
|                                                    |
|   Stage: Stage-0                                   |
|     Fetch Operator                                 |
|       limit: -1                                    |
|       Processor Tree:                              |
|         ListSink                                   |
|                                                    |
+----------------------------------------------------+
53 rows selected (0.574 seconds)
复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/1631780881010798.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317808905497319.png)

（2）查看详细执行计划

```
explain extended select * from emp;
explain extended select deptno, avg(sal) avg_sal from emp group by deptno;复制代码
```

### 任务3：Hive中的Fetch抓取模式配置

#### 【任务目标】

 Fetch 抓取是指，Hive 中对某些情况的查询可以不必使用 MapReduce 计算。可以通过设置Fetch抓取减少不必要的MR操作从而达到提升查询效率的效果。

#### 【任务步骤】

##### 3.1、什么是Fetch抓取

Fetch 抓取是指，Hive 中对某些情况的查询可以不必使用 MapReduce 计算。例如：

`select * from emp ` 在这种情况下，Hive 可以简单地读取 employee 对应的存储目录下的文件，然后输

出查询结果到控制台。

可以去做一下测试， 可以在hive控制台中输入如下HQL：

```
select * from emp;复制代码
```

执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317809029459309.png)

##### 3.2、Fetch 抓取配置

在 `hive-default.xml.template ` 文件中 `hive.fetch.task.conversion` 默认是 `more` ，老版本 hive 默认是 minimal，该属性修改为 more 以后，在全局查找、字段查找、limit 查找等都不走 mapreduce。

在系统沙箱中，定位到 `hive-site.xml` 文件中,查看 `hive.fetch.task.conversion` 选项：

```
cd $HIVE_HOME/conf复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317809150819023.png)

```
vim hive-site.xml复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/163178093163546.png)

注意 `hive.fetch.task.conversion` 的三个选项

![image.png](https://cdn.atstudy.com/lab/manual/16317809418678373.png)

##### 3.3、实例演示

（1）把 hive.fetch.task.conversion 设置为none，再次尝试 select * from emp 操作

```
set hive.fetch.task.conversion = none;复制代码
```

> 注：以上是设置 仅此次hive执行有效，如果想hive重启后仍生效需修改site.xml配置！

再次执行如下命令，来查看是否执行MR操作：

```
select * from emp;复制代码
```

执行效果：

![image.png](https://cdn.atstudy.com/lab/manual/16317809628956339.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631780973442585.png)

> 注意：从以上效果可以看出，再次执行 select * 操作，系统就直接走MR了

（2）把 hive.fetch.task.conversion 设置成 more，然后执行查询语句，如下查询方式都不会执行 mapreduce 程

序。

```
set hive.fetch.task.conversion = more;复制代码
select * from emp;复制代码
select empno,ename from emp;复制代码
select empno,ename from emp limit 3;复制代码
```

### 任务4、本地模式

#### 【任务目标】

Hive 可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。本次任务即带领我们完成Hive本地模式的配置。

#### 【任务步骤】

##### 4.1、何谓本地模式

Hadoop Job的执行模式常见的有两种：local（本地模式） 及 yarn（集群模式） , 默认是 yarn (集群模式)。大多数的 Hadoop Job 是需要 Hadoop 提供的完整的可扩展性来处理大数据集的。不过，有时 Hive 的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际 job 的执行时间要多的多。对于大多数这种情况，Hive 可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。

##### 4.2、本地模式配置

用户可以通过设置 `hive.exec.mode.local.auto` 的值为 true，来让 Hive 在适当的时候自动启动这个优化

首先，可以设置 `set hive.exec.mode.local.auto=true` 开启本地 mr, 设置 local mr 的最大输入数据量，当输

入数据量小于这个值时采用 local mr 的方式，默认为 134217728，即 128M：

```
set hive.exec.mode.local.auto.inputbytes.max = 50000000;复制代码
```

然后，设置 local mr 的最大输入文件个数，当输入文件个数小于这个值时采用 local mr 的方式，默认为 4：

```
set hive.exec.mode.local.auto.input.files.max = 10;复制代码
```

> 注：也可以在 hive-site.xml 中设置上述选项，以让上述配置永久生效。

##### 4.3、实例演示

1）关闭本地模式，并执行查询语句：

通过如下代码关闭本地模式（默认是关闭的）：

```
set hive.exec.mode.local.auto = false;复制代码
```

执行如下HQL语句：

```
select deptno,count(*) as num from emp group by deptno;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317810057439098.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317810171881238.png)

2）开启本地模式，并执行查询语句

通过如下代码开启本地模式：

```
set hive.exec.mode.local.auto = true;复制代码
```

再一次执行如下HQL语句：

```
select deptno,count(*) as num from emp group by deptno;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317810343177026.png)

> 特别说明：
>
> 如果执行上述代码出错，请重启HIve，具体步骤如下：
>
> 1. 首先，通过 !q 命令退出 jdbc hive客户端
>
> 2）jps 查看运行的 RunJar 的进程ID (即为beeline客户端进程ID)
>
> 3）通过执行 kill -9 RunJar的进程ID ，强行退出 beeline 客户端
>
> 4）再重新启动一次hive ,再重新执行一次本地模式运行，就发现一切正常了

**再次强调：**

1）对于 local 的本地模式，只适合于少量数据的hive分析，如果数据量比较大的话，执行 local 模式会报错，还是应采用 yarn 模式的集群客户端方式运行。

2）local模式，相当于单机执行。对于小数据通过 local 方式 运行可以加快执行速度 ，但如果数据量很大的话，还是yarn模式运行的速度明显会快很多。

### 实验总结

1）如何在Hive中查看一条HQL语句的执行计划

```
explain select * from emp;复制代码
```

2）查看详细的Hive执行计划

```
explain extended select * from emp;复制代码
```

3）Fetch 抓取是指，Hive 中对某些情况的查询可以不必使用 MapReduce 计算

4）Fetch 抓取配置

```
在  `hive-default.xml.template ` 文件中 ` hive.fetch.task.conversion ` 默认是 `more` ，老版本 hive默认是 minimal，该属性修改为 more 以后，在全局查找、字段查找、limit 查找等都不走 mapreduce。复制代码
```

5）Hive 可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。

6）本地模式的注意事项

- 对于 local 的本地模式，只适合于少量数据的hive分析，如果数据量比较大的话，执行 local 模式会报错，还是应采用 yarn 模式的集群客户端方式运行。
- local模式，相当于单机执行。对于小数据通过 local 方式 运行可以加快执行速度 ，但如果数据量很大的话，还是yarn模式运行的速度明显会快很多。





# 实验6-2：Hive调优实战-2_大小表优化查询

## 实验概述

 本系列实训任务主要讲解了如何对Hive进行数据处理及数据查询统计的性能调优的方法、策略及背后的调优原理等。此为本系列课程的第二个实验，本次实验主要讲解了HIve的中表的优化及大小表关联查询优化问题。

## 实验环境

- Linux 操作系统
- Hadoop2.7.7
- Hive-1.2.2
- MySQL5.7

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16317831707685156.png)

## 实验目标

完成本实验后，您将能够

- 理解大小表关联查询的原理
- 掌握小表JOIN大表的查询优化
- 掌握大表JOIN小表的查询优化

## 实验任务

### 任务1：Hive开发环境及数据准备

#### **【任务目标】**

 本次任务主要是准备好基础的Hive开发环境，启动Hadoop，启动Hive服务及客户端以及部分实验数据准备，为后继实验打下基础。

#### **【任务步骤】**

##### 1.1、打开Terminal终端

进入项目实训平台后，如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317831839842448.png)

在桌面空白位置点击鼠标右键，选择 `在此打开终端 `如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317832125966401.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317832233086354.png)

##### 1.2、启动hadoop

在命令终端输入以下命令，启动hadoop相关服务

```
start-all.sh复制代码
```

启动效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317832376426631.png)

##### 1.3、查看服务状态

可以通过如下命令查看Hadoop服务的启动状态：

```
jps复制代码
```

执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/1631783247657752.png)

如果出现如上服务，则恭喜你的Hadoop启动成功。

##### 1.4、启动 hiveserver2 服务

```
# 后台方式启动hiveserver2服务
nohup hive --service hiveserver2 2>&1 &复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317832592316457.png)

通过如下命令确认 hiveserver2 服务是否已经启动：

```
netstat -ntulp | grep 10000复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317832683957094.png)

##### 1.5、JDBC方式连结Hive客户端

```
# 然后再重新启动beeline客户端
beeline --showDbInPrompt=true
# 连结到默认的default数据库
!connect jdbc:hive2://localhost:10000/default复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317832792518294.png)

##### 1.6、准备实验数据

1）准备数据

- dept.txt

```
10	ACCOUNTING	1700
20	RESEARCH	1800
30	SALES	1900
40	OPERATIONS	1700复制代码
```

> 特别注意：以上的每个元素之间的间隔用tab键，在拷贝到Linux环境中时，最好每个数据之间的tab键再手工输入一次。

- emp.txt

```
7369,SMITH,CLERK,7902,1980-12-17,800.00,100.00,20
7499,ALLEN,SALESMAN,7698,1981-2-20,1600.00,300.00,30
7521,WARD,SALESMAN,7698,1981-2-22,1250.00,500.00,30
7566,JONES,MANAGER,7839,1981-4-2,2975.00,300.00,20
7654,MARTIN,SALESMAN,7698,1981-9-28,1250.00,1400.00,30
7698,BLAKE,MANAGER,7839,1981-5-1,2850.00,1000.00,30
7782,CLARK,MANAGER,7839,1981-6-9,2450.00,1200.00,10
7788,SCOTT,ANALYST,7566,1987-4-19,3000.00,1000.00,20
7839,KING,PRESIDENT,7860,1981-11-17,5000.00,800.00,10
7844,TURNER,SALESMAN,7698,1981-9-8,1500.00,0.00,30
7876,ADAMS,CLERK,7788,1987-5-23,1100.00,300.00,20
7900,JAMES,CLERK,7698,1981-12-3,950.00,200.00,30
7902,FORD,ANALYST,7566,1981-12-3,3000.00,200.00,20
7934,MILLER,CLERK,7782,1982-1-23,1300.00,100.00,10复制代码
```

- loc.txt

```
1700,Beijing
1800,London
1900,Tokyo复制代码
```

脚本代码 ：

```
cd /home/atstudy/
mkdir datas
touch dept.txt
vim dept.txt
touch emp.txt
vim emp.txt
touch loc.txt
vim loc.txt复制代码
```

2）创建表

- 创建部门表

```
create  table if not exists dept(
deptno int,
dname string,
loc int
) row format delimited fields terminated by '\t';复制代码
```

- 创建员工表

```
create  table if not exists emp(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int)
row format delimited fields terminated by ',';复制代码
```

- 创建地址表

```
create table if not exists location(
loc int,
loc_name string
)
row format delimited fields terminated by ',';复制代码
```

3）导入数据

- 导入dept数据

```
load data local inpath '/home/atstudy/datas/dept.txt' into table dept;复制代码
```

- 导入emp数据

```
load data local inpath '/home/atstudy/datas/emp.txt' into table emp;复制代码
```

- 导入loc数据

```
load data local inpath '/home/atstudy/datas/loc.txt' into table location;复制代码
```

4）查看数据

```
use default;
select * from dept;
select * from emp;
select * from location;复制代码
```

### 任务2：Hive中的大小表优化

#### 【任务目标】

本任务主要是讲解用于多表关联时大表与小表的查询优化及大表与大表之间的优化相关的一些方法及了解背后的原因并掌握优化的步骤与配置。

#### 【任务步骤】

##### 2.1、小表大表 Join （MapJOIN）

1）理论

多表关联时，一个优化原则是：将 key 相对分散，并且数据量小的表放在 join 的左边，可以使用 map join 让小的维度表先进内存。在 map 端完成 join。

> 说明：实际测试发现，新版的 hive 已经对小表 JOIN 大表和大表 JOIN 小表进行了优化。小表放在左边和右边已经没有区别。也就是说 hive 3.x的版本中已经对表默认做过优化

2）实操

需求：测试大表 JOIN 小表和小表 JOIN 大表的效率

实现步骤：

（1） ）开启 MapJoin 的配置

设置自动选择 Mapjoin

```
set hive.auto.convert.join = true; -- 默认为 true复制代码
```

大表小表的阈值设置（默认 25M 以下认为是小表）：

```
set hive.mapjoin.smalltable.filesize = 25000000;  -- 默认为25M大小复制代码
```

可以进入Hive查看以上相关默认配置

![image.png](https://cdn.atstudy.com/lab/manual/1631783302365752.png)

（2）了解Mapjoin背后的工作原理

![image.png](https://cdn.atstudy.com/lab/manual/16317833215961881.png)

（3）创建测试表

接下来建立测试表，将来会导入测试数据，以下是一个电影视频网站日志表，我们将用此大小表来做相关测试。

- 建立测试表：大表

```
create table bigtable(id bigint, t bigint, uid string, keyword string,
url_rank int, click_num int, click_url string) row format delimited
fields terminated by '\t';复制代码
```

- 建立测试表：小表

```
create table smalltable(id bigint, t bigint, uid string, keyword string,
url_rank int, click_num int, click_url string) row format delimited
fields terminated by '\t';复制代码
```

- 创建JOIN后的新表

```
create table jointable(id bigint, t bigint, uid string, keyword string,
url_rank int, click_num int, click_url string) row format delimited
fields terminated by '\t';复制代码
```

执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317833808619003.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317833892865030.png)

（4）导入测试数据

测试数据在实验沙箱中的 `/opt/data` 目录下 ：

![image.png](https://cdn.atstudy.com/lab/manual/1631783409217645.png)

- 向小表中导入数据

```
load data local inpath '/opt/data/smalltable' into table smalltable;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317834879228169.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317834955066278.png)

- 向大表中导入数据

```
load data local inpath '/opt/data/bigtable' into table bigtable;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317835078832128.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317835188281401.png)

（5）性能测试

- 小表 JOIN 大表语句

```
explain select b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url
from smalltable s
join bigtable b
on b.id = s.id;复制代码
```

执行结果：

```
STAGE DEPENDENCIES:
  Stage-4 is a root stage , consists of Stage-5, Stage-1
  Stage-5 has a backup stage: Stage-1
  Stage-3 depends on stages: Stage-5
  Stage-1
  Stage-0 depends on stages: Stage-3, Stage-1

STAGE PLANS:
  Stage: Stage-4
    Conditional Operator

  Stage: Stage-5
    Map Reduce Local Work
      Alias -> Map Local Tables:
        $hdt$_0:s 
          Fetch Operator
            limit: -1
      Alias -> Map Local Operator Tree:
        $hdt$_0:s 
          TableScan
            alias: s
            Statistics: Num rows: 1 Data size: 130150840 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: id is not null (type: boolean)
              Statistics: Num rows: 1 Data size: 130150840 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: id (type: bigint)
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 130150840 Basic stats: COMPLETE Column stats: NONE
                HashTable Sink Operator
                  keys:
                    0 _col0 (type: bigint)
                    1 _col0 (type: bigint)

  Stage: Stage-3
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: b
            Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: id is not null (type: boolean)
              Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: id (type: bigint), t (type: bigint), uid (type: string), keyword (type: string), url_rank (type: int), click_num (type: int), click_url (type: string)
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
                Map Join Operator
                  condition map:
                       Inner Join 0 to 1
                  keys:
                    0 _col0 (type: bigint)
                    1 _col0 (type: bigint)
                  outputColumnNames: _col1, _col2, _col3, _col4, _col5, _col6, _col7
                  Statistics: Num rows: 1 Data size: 143165927 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: _col1 (type: bigint), _col2 (type: bigint), _col3 (type: string), _col4 (type: string), _col5 (type: int), _col6 (type: int), _col7 (type: string)
                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                    Statistics: Num rows: 1 Data size: 143165927 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 1 Data size: 143165927 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
      Execution mode: vectorized
      Local Work:
        Map Reduce Local Work

  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: s
            Statistics: Num rows: 1 Data size: 130150840 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: id is not null (type: boolean)
              Statistics: Num rows: 1 Data size: 130150840 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: id (type: bigint)
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 130150840 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: bigint)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: bigint)
                  Statistics: Num rows: 1 Data size: 130150840 Basic stats: COMPLETE Column stats: NONE
          TableScan
            alias: b
            Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: id is not null (type: boolean)
              Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: id (type: bigint), t (type: bigint), uid (type: string), keyword (type: string), url_rank (type: int), click_num (type: int), click_url (type: string)
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: bigint)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: bigint)
                  Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col1 (type: bigint), _col2 (type: string), _col3 (type: string), _col4 (type: int), _col5 (type: int), _col6 (type: string)
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          keys:
            0 _col0 (type: bigint)
            1 _col0 (type: bigint)
          outputColumnNames: _col1, _col2, _col3, _col4, _col5, _col6, _col7
          Statistics: Num rows: 1 Data size: 143165927 Basic stats: COMPLETE Column stats: NONE
          Select Operator
            expressions: _col1 (type: bigint), _col2 (type: bigint), _col3 (type: string), _col4 (type: string), _col5 (type: int), _col6 (type: int), _col7 (type: string)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
            Statistics: Num rows: 1 Data size: 143165927 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              Statistics: Num rows: 1 Data size: 143165927 Basic stats: COMPLETE Column stats: NONE
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

Time taken: 0.646 seconds, Fetched: 131 row(s)
复制代码
```

执行结果分析：

![image.png](https://cdn.atstudy.com/lab/manual/16317835587209129.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317835700252869.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317835952767350.png)

可以再做一个测试，关闭 MapJoin：

```
set hive.auto.convert.join = false;  -- 关闭 MapJoin复制代码
```

再执行：

```
explain select b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url
from smalltable s
join bigtable b
on b.id = s.id;复制代码
```

执行结果：

```
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: s
            Statistics: Num rows: 1 Data size: 130150840 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: id is not null (type: boolean)
              Statistics: Num rows: 1 Data size: 130150840 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: id (type: bigint)
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 130150840 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: bigint)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: bigint)
                  Statistics: Num rows: 1 Data size: 130150840 Basic stats: COMPLETE Column stats: NONE
          TableScan
            alias: b
            Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: id is not null (type: boolean)
              Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: id (type: bigint), t (type: bigint), uid (type: string), keyword (type: string), url_rank (type: int), click_num (type: int), click_url (type: string)
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: bigint)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: bigint)
                  Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col1 (type: bigint), _col2 (type: string), _col3 (type: string), _col4 (type: int), _col5 (type: int), _col6 (type: string)
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          keys:
            0 _col0 (type: bigint)
            1 _col0 (type: bigint)
          outputColumnNames: _col1, _col2, _col3, _col4, _col5, _col6, _col7
          Statistics: Num rows: 1 Data size: 143165927 Basic stats: COMPLETE Column stats: NONE
          Select Operator
            expressions: _col1 (type: bigint), _col2 (type: bigint), _col3 (type: string), _col4 (type: string), _col5 (type: int), _col6 (type: int), _col7 (type: string)
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
            Statistics: Num rows: 1 Data size: 143165927 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              Statistics: Num rows: 1 Data size: 143165927 Basic stats: COMPLETE Column stats: NONE
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

Time taken: 0.171 seconds, Fetched: 66 row(s)复制代码
```

执行结果分析：

![image.png](https://cdn.atstudy.com/lab/manual/16317836279431843.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317836529505801.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317836744207491.png)

> 以上结果说明不会进行MapJoin，效率将会大大降低！

真正执行插入语句：

```
set hive.auto.convert.join = true;  -- 开启 MapJoin，记得再还原回来复制代码
insert overwrite table jointable
select b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url
from smalltable s
join bigtable b
on b.id = s.id;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/1631783689078329.png)

- 大表JOIN小表语句

首先，查看此种情况下的HQL执行计划：

```
explain select b.id, b.t, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url
from bigtable b
join smalltable s
on s.id = b.id;复制代码
```

运行结果：

```
STAGE DEPENDENCIES:
  Stage-4 is a root stage , consists of Stage-5, Stage-1
  Stage-5 has a backup stage: Stage-1
  Stage-3 depends on stages: Stage-5
  Stage-1
  Stage-0 depends on stages: Stage-3, Stage-1

STAGE PLANS:
  Stage: Stage-4
    Conditional Operator

  Stage: Stage-5
    Map Reduce Local Work
      Alias -> Map Local Tables:
        $hdt$_1:s 
          Fetch Operator
            limit: -1
      Alias -> Map Local Operator Tree:
        $hdt$_1:s 
          TableScan
            alias: s
            Statistics: Num rows: 1 Data size: 130150840 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: id is not null (type: boolean)
              Statistics: Num rows: 1 Data size: 130150840 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: id (type: bigint)
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 130150840 Basic stats: COMPLETE Column stats: NONE
                HashTable Sink Operator
                  keys:
                    0 _col0 (type: bigint)
                    1 _col0 (type: bigint)

  Stage: Stage-3
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: b
            Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: id is not null (type: boolean)
              Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: id (type: bigint), t (type: bigint), uid (type: string), keyword (type: string), url_rank (type: int), click_num (type: int), click_url (type: string)
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
                Map Join Operator
                  condition map:
                       Inner Join 0 to 1
                  keys:
                    0 _col0 (type: bigint)
                    1 _col0 (type: bigint)
                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                  Statistics: Num rows: 1 Data size: 1420730603 Basic stats: COMPLETE Column stats: NONE
                  File Output Operator
                    compressed: false
                    Statistics: Num rows: 1 Data size: 1420730603 Basic stats: COMPLETE Column stats: NONE
                    table:
                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
      Execution mode: vectorized
      Local Work:
        Map Reduce Local Work

  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: b
            Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: id is not null (type: boolean)
              Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: id (type: bigint), t (type: bigint), uid (type: string), keyword (type: string), url_rank (type: int), click_num (type: int), click_url (type: string)
                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
                Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: bigint)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: bigint)
                  Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
                  value expressions: _col1 (type: bigint), _col2 (type: string), _col3 (type: string), _col4 (type: int), _col5 (type: int), _col6 (type: string)
          TableScan
            alias: s
            Statistics: Num rows: 1 Data size: 130150840 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: id is not null (type: boolean)
              Statistics: Num rows: 1 Data size: 130150840 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: id (type: bigint)
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 130150840 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: bigint)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: bigint)
                  Statistics: Num rows: 1 Data size: 130150840 Basic stats: COMPLETE Column stats: NONE
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          keys:
            0 _col0 (type: bigint)
            1 _col0 (type: bigint)
          outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
          Statistics: Num rows: 1 Data size: 1420730603 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            Statistics: Num rows: 1 Data size: 1420730603 Basic stats: COMPLETE Column stats: NONE
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

Time taken: 0.259 seconds, Fetched: 123 row(s)复制代码
```

分析运行结果：

![image.png](https://cdn.atstudy.com/lab/manual/16317837182015659.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317837283351139.png)

**结论：**

在Hive的高版本中（3.x及以上）已经对多表关联有做过优化（即自动的会先扫描小表（低于25M的）再扫描大表的方式）。但在更早期版本中的Hive（如：Hive 1.x）未做过此优化，需要自己在写SQL时手工将小表置前，大表置后，即需要进行手工优化，这一点要尤为注意。

##### 2.2、大表 Join 大表

###### 1）空 Key 过滤

场景：

有时 join 超时是因为某些 key 对应的数据太多，而相同 key 对应的数据都会发送到相同的 reducer 上，从而导致内存不够。此时我们应该仔细分析这些异常的 key，很多情况下，这些 key 对应的数据是异常数据，我们需要在 SQL 语句中进行过滤。

实操：

2）创建原始数据空 id 表

```
create table nullidtable(id bigint, t bigint, uid string, keyword string,
url_rank int, click_num int, click_url string) row format delimited
fields terminated by '\t';复制代码
```

3）加载具有大量null值的数据到 nullidtable中

> 数据事先存在于实验沙箱的 ：/opt/data/nullid 文件中

```
load data local inpath '/opt/data/nullid' into table nullidtable;复制代码
desc nullidtable;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317837401268517.png)

```
select id,t,uid from nullidtable limit 10;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/1631783750139200.png)

```
select count(*) as cunt  from nullidtable where id is null;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317837611398338.png)

4）测试不过滤空 id

```
insert overwrite table jointable select n.* from nullidtable n left join bigtable o on n.id = o.id;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317837713356933.png)

5）测试过滤空 id

即推荐使用此种形式的代码来进行空Key过滤：

```
insert overwrite table jointable select n.* from (select * from nullidtable where id is not null) n left join bigtable o on n.id = o.id;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/1631783782213491.png)

**结论：**

空KEY过滤的使用场景：

（1）非 Inner Join的场合

（2）不需要字段为Null的场合

 - 先JOIN后过滤（不推荐）

 - 先过滤后JOIN（推荐）

###### 2）空 Key 转换

**场景：**

有时虽然某个 key 为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join 的结果中。

**问题：**

一般在生产环境中会跑多个reducer，而我们分组时一般会按着id来进行分组或关联，reducer默认会把null 值作为一组，因此会造成数据倾斜（说白了即某一个reducer被分配的处理数据量较大，而其他的reducer可能闲置）。数据倾斜的最终导致的结果是某一个或几个Reducer跑的比较忙，比较耗时，甚至于没跑成功，导致最终整体任务完成时间较长或失败。

**解决方案：**

此时我们可以表 a 中 key 为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的 reducer 上。

**实操演示：**

1）先测试一个没有做任何优化的场景（不随机分布空 null 值）

（1）配置历史服务器（用于监控MapReduce的任务执行情况）

- 进入Hadoop配置目录

```
cd $HADOOP_HOME/etc/hadoop复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317837946175727.png)

```
vim mapred-site.xml复制代码
```

在最后加入：

```
<property>
<name>mapreduce.jobhistory.address</name>
<value>localhost:10020</value>
</property>
<property>
<name>mapreduce.jobhistory.webapp.address</name>
<value>localhost:19888</value>
</property>复制代码
```

效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317838047711452.png)

保存后，启动历史服务器：

```
cd $HADOOP_HOME复制代码
sbin/mr-jobhistory-daemon.sh start historyserver复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317838162525500.png)

查看 jobhistory：

```
http://localhost:19888/jobhistory复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317839353417946.png)

（2）设置 5 个 reduce 个数

```
set mapreduce.job.reduces = 5;复制代码
```

（3）JOIN 两张表

```
insert overwrite table jointable 
select n.* from nullidtable n left join bigtable b on n.id = b.id;复制代码
```

> 特别说明：如果执行以上代码出错，可能要重启HIve，基至于重启Hadoop，就可以解决报错问题 。

![image.png](https://cdn.atstudy.com/lab/manual/1631783948772274.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317839923215847.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317840052788265.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317840142131630.png)

> 从上面看出，有一个Reducer达到20秒，存在着数据倾斜的现象（目前数据量百万条此种现象还不是很明显，数据量越大，存在数据倾斜的可能性越大）

2）优化（随机分布空 null 值）

（1）设置 5 个 reduce 个数

```
set mapreduce.job.reduces = 5;复制代码
```

（2）JOIN 两张表

```
insert overwrite table jointable
select n.* from nullidtable n left join bigtable o on
nvl(n.id,rand()) = o.id;复制代码
```

> 特别说明：如果执行以上代码出错，可能要重启HIve，基至于重启Hadoop，就可以解决报错问题 。

![image.png](https://cdn.atstudy.com/lab/manual/16317840261459471.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631784437754520.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317844490698096.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317844605702224.png)

**结论：**

1）总体来看，优化之后的查询速度不降反升，主要原因是产生随机数也需要时间，另外，还是由于我们的数据量较小，如果是千万级别以上的数据，性能总体会提升的

2）另外，从JobHistory监控的每个Reducer运行的结果来看，确实有效的解决了 数据倾斜的问题

### 实验总结

1）多表关联时，一个优化原则是：将 key 相对分散，并且数据量小的表放在 join 的左边，可以使用 map join 让小的维度表先进内存。在 map 端完成 join。

2）在Hive的高版本中（3.x及以上）已经对多表关联有做过优化（即自动的会先扫描小表（低于25M的）再扫描大表的方式）。但在更早期版本中的Hive（如：Hive 1.x）未做过此优化，需要自己在写SQL时手工将小表置前，大表置后，即需要进行手工优化，这一点要尤为注意。

3）有时 join 超时是因为某些 key 对应的数据太多，而相同 key 对应的数据都会发送到相同的 reducer 上，从而导致内存不够。此时我们应该仔细分析这些异常的 key，很多情况下，这些 key 对应的数据是异常数据，我们需要在 SQL 语句中进行过滤。

4）空KEY过滤的使用场景：

（1）非 Inner Join的场合

（2）不需要字段为Null的场合

 - 先JOIN后过滤（不推荐）

 - 先过滤后JOIN（推荐）

5）空 Key 转换问题

**场景：**

有时虽然某个 key 为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join 的结果中。

**问题：**

一般在生产环境中会跑多个reducer，而我们分组时一般会按着id来进行分组或关联，reducer默认会把null 值作为一组，因此会造成数据倾斜（说白了即某一个reducer被分配的处理数据量较大，而其他的reducer可能闲置）。数据倾斜的最终导致的结果是某一个或几个Reducer跑的比较忙，比较耗时，甚至于没跑成功，导致最终整体任务完成时间较长或失败。

**解决方案：**

此时我们可以表 a 中 key 为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的 reducer 上。





# 实验6-3：Hive调优实战-3_数据倾斜、去重统计、行列过滤

## 实验概述

 本系列实训任务主要讲解了如何对Hive进行数据处理及数据查询统计的性能调优的方法、策略及背后的调优原理等。此为本系列课程的第三个实验，本次实验主要讲解了Group By数据倾斜、Count去重统计、行列过滤等几方面来进行Hive的性能优化。

## 实验环境

- Linux 操作系统
- Hadoop2.7.7
- Hive-1.2.2
- MySQL5.7

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16318443378219966.png)

## 实验目标

完成本实验后，您将能够

- 理解Hive数据倾斜产生的原因及背后的原理
- 掌握如何解决Group By的数据倾斜问题
- 掌握如何在Hive中进行Count去重统计、行列过滤等优化配置

## 实验任务

### 任务1：Hive开发环境及数据准备

#### **【任务目标】**

 本次任务主要是准备好基础的Hive开发环境，启动Hadoop，启动Hive服务及客户端以及部分实验数据准备，为后继实验打下基础。

#### **【任务步骤】**

##### 1.1、打开Terminal终端

进入项目实训平台后，如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16318443497265381.png)

在桌面空白位置点击鼠标右键，选择 `在此打开终端 `如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16318443610957304.png)

![image.png](https://cdn.atstudy.com/lab/manual/16318443763316900.png)

##### 1.2、启动hadoop

在命令终端输入以下命令，启动hadoop相关服务

```
start-all.sh复制代码
```

启动效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16318443870549115.png)

##### 1.3、查看服务状态

可以通过如下命令查看Hadoop服务的启动状态：

```
jps复制代码
```

执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16318443953473421.png)

如果出现如上服务，则恭喜你的Hadoop启动成功。

##### 1.4、启动 hiveserver2 服务

```
# 后台方式启动hiveserver2服务
nohup hive --service hiveserver2 2>&1 &复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16318444046626479.png)

通过如下命令确认 hiveserver2 服务是否已经启动：

```
netstat -ntulp | grep 10000复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16318444140003843.png)

##### 1.5、JDBC方式连结Hive客户端

```
# 然后再重新启动beeline客户端
beeline --showDbInPrompt=true
# 连结到默认的default数据库
!connect jdbc:hive2://localhost:10000/default复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16318444225249568.png)

##### 1.6、准备实验数据

1）准备数据

- dept.txt

```
10	ACCOUNTING	1700
20	RESEARCH	1800
30	SALES	1900
40	OPERATIONS	1700复制代码
```

> 特别注意：以上的每个元素之间的间隔用tab键，在拷贝到Linux环境中时，最好每个数据之间的tab键再手工输入一次。

- emp.txt

```
7369,SMITH,CLERK,7902,1980-12-17,800.00,100.00,20
7499,ALLEN,SALESMAN,7698,1981-2-20,1600.00,300.00,30
7521,WARD,SALESMAN,7698,1981-2-22,1250.00,500.00,30
7566,JONES,MANAGER,7839,1981-4-2,2975.00,300.00,20
7654,MARTIN,SALESMAN,7698,1981-9-28,1250.00,1400.00,30
7698,BLAKE,MANAGER,7839,1981-5-1,2850.00,1000.00,30
7782,CLARK,MANAGER,7839,1981-6-9,2450.00,1200.00,10
7788,SCOTT,ANALYST,7566,1987-4-19,3000.00,1000.00,20
7839,KING,PRESIDENT,7860,1981-11-17,5000.00,800.00,10
7844,TURNER,SALESMAN,7698,1981-9-8,1500.00,0.00,30
7876,ADAMS,CLERK,7788,1987-5-23,1100.00,300.00,20
7900,JAMES,CLERK,7698,1981-12-3,950.00,200.00,30
7902,FORD,ANALYST,7566,1981-12-3,3000.00,200.00,20
7934,MILLER,CLERK,7782,1982-1-23,1300.00,100.00,10复制代码
```

- loc.txt

```
1700,Beijing
1800,London
1900,Tokyo复制代码
```

脚本代码 ：

```
cd /home/atstudy/
mkdir datas
touch dept.txt
vim dept.txt
touch emp.txt
vim emp.txt
touch loc.txt
vim loc.txt复制代码
```

2）创建表

- 创建部门表

```
create  table if not exists dept(
deptno int,
dname string,
loc int
) row format delimited fields terminated by '\t';复制代码
```

- 创建员工表

```
create  table if not exists emp(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int)
row format delimited fields terminated by ',';复制代码
```

- 创建地址表

```
create table if not exists location(
loc int,
loc_name string
)
row format delimited fields terminated by ',';复制代码
```

3）导入数据

- 导入dept数据

```
load data local inpath '/home/atstudy/datas/dept.txt' into table dept;复制代码
```

- 导入emp数据

```
load data local inpath '/home/atstudy/datas/emp.txt' into table emp;复制代码
```

- 导入loc数据

```
load data local inpath '/home/atstudy/datas/loc.txt' into table location;复制代码
```

4）查看数据

```
use default;
select * from dept;
select * from emp;
select * from location;复制代码
```

### 任务2、优化-5：Group By数据倾斜问题

#### 【任务目标】

本次任务主要用于解决Group By分组查询时的数据倾斜问题。

#### 【任务步骤】

##### 2.1、理论

默认情况下，Map 阶段同一 Key 数据分发给一个 reduce，当一个 key 数据过大时就倾斜了。

![image.png](https://cdn.atstudy.com/lab/manual/16318444443904979.png)

并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进行部分聚合，最后在 Reduce 端得出最终结果。

开启 Map 端聚合参数设置：

（1）是否在 Map 端进行聚合，默认为 True

```
set hive.map.aggr = true复制代码
```

（2）在 Map 端进行聚合操作的条目数目

```
set hive.groupby.mapaggr.checkinterval = 100000复制代码
```

（3）有数据倾斜的时候进行负载均衡（默认是 false）

```
set hive.groupby.skewindata = true复制代码
```

说明：当选项设定为 true，生成的查询计划会有两个 MR Job。第一个 MR Job 中，Map 的输出结果会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 Group By Key 有可能被分发到不同的 Reduce 中，从而达到负载均衡的目的；第二个 MR Job 再根据预处理的数据结果按照 Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。

##### 2.2、实操

**优化前：**

在HIve客户端中执行如下HQL测试：

```
set mapreduce.job.reduces = 5;复制代码
select deptno from emp group by deptno;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16318444684067633.png)

**优化后：**

```
set hive.groupby.skewindata = true;复制代码
select deptno from emp group by deptno;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16318444768263198.png)

![image.png](https://cdn.atstudy.com/lab/manual/16318444877088952.png)

##### 2.3、结论

 可以看到，明显分为两个阶段，第一阶段Map合并，第二阶段再Reduce合并，从结果看优化之后的时间反而更长，原因是数据量较小，体现不出优化的作用。只有数据量相当大，且有很大一部分key相同，有可能出现数据倾斜的情况下使用此方式，才更有效。才是此种优化适用的场景。

### 任务3、优化-6：Count(Distinct) 去重统计

#### 【任务目标】

本次任务主要用于解决数据量较大的数据集的去重统计时间较长，效率较低问题。

#### 【任务步骤】

##### 3.1、理论

数据量小的时候无所谓，数据量大的情况下，由于 COUNT DISTINCT 操作需要用一个Reduce Task 来完成，这一个 Reduce 需要处理的数据量太大，就会导致整个 Job 很难完成，一般 COUNT DISTINCT 使用先 GROUP BY 再 COUNT 的方式替换,但是需要注意 group by 造成的数据倾斜问题.

##### 3.2、实操

（1）优化前执行去重 id 查询

```
select count(distinct id) from bigtable;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16318445027798304.png)

（2）优化：采用 GROUP by 去重 id

```
select count(id) from (select id from bigtable group by id) a;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16318445141202039.png)

##### 3.3、结论

表面上看起来，虽然会多用一个 Job 来完成，在数据量小的情况下，统计时长不减反增。但在数据量大的情况下，这个绝对是值得的。

### 任务4、优化-7：行列过滤

#### 【任务目标】

本次任务主要用于讲解如何通过行列过滤进行大数据查询优化。

#### 【任务步骤】

##### 4.1、理论

**列处理：**在 SELECT 中，只拿需要的列，如果有分区，尽量使用分区过滤，少用 SELECT *。 **行处理：**在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在 Where 后面，那么就会先全表关联，之后再过滤

##### 4.2、实操

（1）测试先关联两张表，再用 where 条件过滤

```
select o.id from bigtable b join bigtable o on o.id = b.id where o.id <= 10;复制代码
```

Time taken: **57.864** seconds, Fetched: **1081 row(s)**

查看执行计划：

```
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: b
            Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: (id <= 10L) (type: boolean)
              Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: id (type: bigint)
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: bigint)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: bigint)
                  Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
          TableScan
            alias: o
            Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: (id <= 10L) (type: boolean)
              Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: id (type: bigint)
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: bigint)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: bigint)
                  Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          keys:
            0 _col0 (type: bigint)
            1 _col0 (type: bigint)
          outputColumnNames: _col1
          Statistics: Num rows: 1 Data size: 1420730603 Basic stats: COMPLETE Column stats: NONE
          Select Operator
            expressions: _col1 (type: bigint)
            outputColumnNames: _col0
            Statistics: Num rows: 1 Data size: 1420730603 Basic stats: COMPLETE Column stats: NONE
            File Output Operator
              compressed: false
              Statistics: Num rows: 1 Data size: 1420730603 Basic stats: COMPLETE Column stats: NONE
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

Time taken: 0.254 seconds, Fetched: 65 row(s)复制代码
```

分析执行计划：

![image.png](https://cdn.atstudy.com/lab/manual/16318445363705974.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631844545677798.png)

（2） 通过子查询后，再关联表

```
select b.id from bigtable b 
join (select id from bigtable where id <= 10) o on b.id = o.id;复制代码
```

Time taken: **44.015** seconds, Fetched: **1081 row(s)**

查看执行计划：

```
explain select b.id from bigtable b 
join (select id from bigtable where id <= 10) o on b.id = o.id;复制代码
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Map Operator Tree:
          TableScan
            alias: b
            Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: (id <= 10L) (type: boolean)
              Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: id (type: bigint)
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: bigint)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: bigint)
                  Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
          TableScan
            alias: bigtable
            Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
            Filter Operator
              predicate: (id <= 10L) (type: boolean)
              Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
              Select Operator
                expressions: id (type: bigint)
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
                Reduce Output Operator
                  key expressions: _col0 (type: bigint)
                  sort order: +
                  Map-reduce partition columns: _col0 (type: bigint)
                  Statistics: Num rows: 1 Data size: 1291573248 Basic stats: COMPLETE Column stats: NONE
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          keys:
            0 _col0 (type: bigint)
            1 _col0 (type: bigint)
          outputColumnNames: _col0
          Statistics: Num rows: 1 Data size: 1420730603 Basic stats: COMPLETE Column stats: NONE
          File Output Operator
            compressed: false
            Statistics: Num rows: 1 Data size: 1420730603 Basic stats: COMPLETE Column stats: NONE
            table:
                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

Time taken: 0.159 seconds, Fetched: 61 row(s)复制代码
```

分析执行计划：

![image.png](https://cdn.atstudy.com/lab/manual/16318445635383448.png)

![image.png](https://cdn.atstudy.com/lab/manual/16318445734625640.png)

##### 4.3、结论

1）默认情况下，Map 阶段同一 Key 数据分发给一个 reduce，当一个 key 数据过大时就倾斜了。。

2）表面上看起来两种方式查询的效率差别不大，原因是背后采用几乎相同的谓词下推机制。前者相当于是系统自动优化为谓词下推形式的查询，后者则相当于手工人工进行谓词下推操作

3）但在较复杂的查询或较长的HQL语句执行时，如果过滤条件放在联合查询的最后，有可能会造成谓语下推失效或失败的情况。

4）所以，综上，为防止复杂查询谓词下推失效，建议优先使用第二种方式来进行联合过滤查询，即强制进行谓词下推操作，以达到最佳的查询性能。

### 实验总结

1）默认情况下，Map 阶段同一 Key 数据分发给一个 reduce，当一个 key 数据过大时就倾斜了。

2）并不是所有的聚合操作都需要在 Reduce 端完成，很多聚合操作都可以先在 Map 端进行部分聚合，最后在 Reduce 端得出最终结果。

3）数据量小的时候无所谓，数据量大的情况下，由于 COUNT DISTINCT 操作需要用一个Reduce Task 来完成，这一个 Reduce 需要处理的数据量太大，就会导致整个 Job 很难完成，一般 COUNT DISTINCT 使用先 GROUP BY 再 COUNT 的方式替换,但是需要注意 group by 造成的数据倾斜问题.

4）较复杂的查询或较长的HQL语句执行时，如果过滤条件放在联合查询的最后，有可能会造成谓语下推失效或失败的情况。



# JVM重用实验6-4：Hive调优实战-4_合理设置Map及Reduce、并行执行、严格模式、JVM重用

## 实验概述

 本系列实训任务主要讲解了如何对Hive进行数据处理及数据查询统计的性能调优的方法、策略及背后的调优原理等。此为本系列课程的第三个实验，本次实验主要讲解了如何从Map及Reduce的合理设置、Hive的并行执行、严格模式、JVM重用等几个方面来进行Hive的性能优化。

## 实验环境

- Linux 操作系统
- Hadoop2.7.7
- Hive-1.2.2
- MySQL5.7

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16318446818819640.png)

## 实验目标

完成本实验后，您将能够

- 掌握如何更加合理的设置 Map 及 Reduce，以更好的优化 Hive 处理数据及查询数据的速度
- 了解如何设置任务并行从而提高 Hive 执行效率
- 掌握如何设置 Hive 的严格模式以防止一些危险操作
- 了解如何设置 JVM 重用以达到优化 HIve 的目的

## 实验任务

### 任务1：Hive开发环境及数据准备

#### **【任务目标】**

 本次任务主要是准备好基础的Hive开发环境，启动Hadoop，启动Hive服务及客户端以及部分实验数据准备，为后继实验打下基础。

#### **【任务步骤】**

##### 1.1、打开Terminal终端

进入项目实训平台后，如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16318446963386610.png)

在桌面空白位置点击鼠标右键，选择 `在此打开终端 `如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16318447071459172.png)

![image.png](https://cdn.atstudy.com/lab/manual/16318447167523517.png)

##### 1.2、启动hadoop

在命令终端输入以下命令，启动hadoop相关服务

```
start-all.sh复制代码
```

启动效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16318447255626536.png)

##### 1.3、查看服务状态

可以通过如下命令查看Hadoop服务的启动状态：

```
jps复制代码
```

执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16318447350806494.png)

如果出现如上服务，则恭喜你的Hadoop启动成功。

##### 1.4、启动 hiveserver2 服务

```
# 后台方式启动hiveserver2服务
nohup hive --service hiveserver2 2>&1 &复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16318447614651698.png)

通过如下命令确认 hiveserver2 服务是否已经启动：

```
netstat -ntulp | grep 10000复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16318447707202412.png)

##### 1.5、JDBC方式连结Hive客户端

```
# 然后再重新启动beeline客户端
beeline --showDbInPrompt=true
# 连结到默认的default数据库
!connect jdbc:hive2://localhost:10000/default复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16318447797481087.png)

##### 1.6、准备实验数据

1）准备数据

- dept.txt

```
10	ACCOUNTING	1700
20	RESEARCH	1800
30	SALES	1900
40	OPERATIONS	1700复制代码
```

> 特别注意：以上的每个元素之间的间隔用tab键，在拷贝到Linux环境中时，最好每个数据之间的tab键再手工输入一次。

- emp.txt

```
7369,SMITH,CLERK,7902,1980-12-17,800.00,100.00,20
7499,ALLEN,SALESMAN,7698,1981-2-20,1600.00,300.00,30
7521,WARD,SALESMAN,7698,1981-2-22,1250.00,500.00,30
7566,JONES,MANAGER,7839,1981-4-2,2975.00,300.00,20
7654,MARTIN,SALESMAN,7698,1981-9-28,1250.00,1400.00,30
7698,BLAKE,MANAGER,7839,1981-5-1,2850.00,1000.00,30
7782,CLARK,MANAGER,7839,1981-6-9,2450.00,1200.00,10
7788,SCOTT,ANALYST,7566,1987-4-19,3000.00,1000.00,20
7839,KING,PRESIDENT,7860,1981-11-17,5000.00,800.00,10
7844,TURNER,SALESMAN,7698,1981-9-8,1500.00,0.00,30
7876,ADAMS,CLERK,7788,1987-5-23,1100.00,300.00,20
7900,JAMES,CLERK,7698,1981-12-3,950.00,200.00,30
7902,FORD,ANALYST,7566,1981-12-3,3000.00,200.00,20
7934,MILLER,CLERK,7782,1982-1-23,1300.00,100.00,10复制代码
```

- loc.txt

```
1700,Beijing
1800,London
1900,Tokyo复制代码
```

脚本代码 ：

```
cd /home/atstudy/
mkdir datas
touch dept.txt
vim dept.txt
touch emp.txt
vim emp.txt
touch loc.txt
vim loc.txt复制代码
```

2）创建表

- 创建部门表

```
create  table if not exists dept(
deptno int,
dname string,
loc int
) row format delimited fields terminated by '\t';复制代码
```

- 创建员工表

```
create  table if not exists emp(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int)
row format delimited fields terminated by ',';复制代码
```

- 创建地址表

```
create table if not exists location(
loc int,
loc_name string
)
row format delimited fields terminated by ',';复制代码
```

3）导入数据

- 导入dept数据

```
load data local inpath '/home/atstudy/datas/dept.txt' into table dept;复制代码
```

- 导入emp数据

```
load data local inpath '/home/atstudy/datas/emp.txt' into table emp;复制代码
```

- 导入loc数据

```
load data local inpath '/home/atstudy/datas/loc.txt' into table location;复制代码
```

4）查看数据

```
use default;
select * from dept;
select * from emp;
select * from location;复制代码
```

### 任务2、优化-8：合理设置Map及Reduce

#### 【任务目标】

本次任务主要用于讲解了如何更加合理的设置Map及Reduce，以更好的优化Hive处理数据及查询数据的速度。

#### 【任务步骤】

（1）通常情况下，作业会通过 input 的目录产生一个或者多个 map 任务。

主要的决定因素有：input 的文件总个数，input 的文件大小，集群设置的文件块大小。

（2）是不是 map 数越多越好？

答案是否定的。如果一个任务有很多小文件（远远小于块大小 128m），则每个小文件也会被当做一个块，用一个 map 任务来完成，而一个 map 任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的 map 数是受限的。

（3）是不是保证每个 map 处理接近 128m 的文件块，就高枕无忧了？

答案也是不一定。比如有一个 127m 的文件，正常会用一个 map 去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果 map 处理的逻辑比较复杂，用一个 map任务去做，肯定也比较耗时。

**针对上面的问题 2 和 3，我们需要采取两种方式来解决：即减少 map 数和增加 map 数；**

##### 2.1、复杂文件增加 Map

当 input 的文件都很大，任务逻辑复杂，map 执行非常慢的时候，可以考虑增加 Map 数，来使得每个 map 处理的数据量减少，从而提高任务的执行效率。

增加 map 的方法为：

```
computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M 复制代码
```

根据以上公式，调整 maxSize 最大值。让 maxSize 最大值低于 blocksize 就可以增加 map 的个数。

实例演示：

```
select count(*) from emp;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16318449440814057.png)

设置最大切片值为 100 个字节

```
set mapreduce.input.fileinputformat.split.maxsize=100;复制代码
select count(*) from emp;复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16318449535788984.png)

##### 2.2、小文件合并

1）在 map 执行前合并小文件的设置：

减少 map 数：CombineHiveInputFormat 具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat 没有对小文件合并功能。

```
set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;复制代码
```

2）在 Map-Reduce 的任务结束时合并小文件的设置：

在 map-only 任务结束时合并小文件，默认 true

```
SET hive.merge.mapfiles = true;复制代码
```

在 map-reduce 任务结束时合并小文件，默认 false

```
SET hive.merge.mapredfiles = true;复制代码
```

合并文件的大小，默认 256M

```
SET hive.merge.size.per.task = 268435456;复制代码
```

当输出文件的平均大小小于该值时，启动一个独立的 map-reduce 任务进行文件 merge

```
SET hive.merge.smallfiles.avgsize = 16777216;复制代码
```

##### 2.3、合理设置 Reduce

1）调整 reduce 个数方法一

（1）每个 Reduce 处理的数据量默认是 256MB

```
hive.exec.reducers.bytes.per.reducer=256000000复制代码
```

（2）每个任务最大的 reduce 数，默认为 1009

```
hive.exec.reducers.max=1009复制代码
```

（3）计算 reducer 数的公式

```
N=min(参数 2，总输入数据量/参数 1)复制代码
```

> 相当于：N = min(hive.exec.reducers.max，总输入数据量/hive.exec.reducers.bytes.per.reducer)

2）调整 reduce 个数方法二

在 hadoop 的 mapred-default.xml 文件中修改，设置每个 job 的 Reduce 个数

```
set mapreduce.job.reduces = 15;复制代码
```

3）reduce 个数并不是越多越好

（1）过多的启动和初始化 reduce 也会消耗时间和资源；

（2）另外，有多少个 reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；

（3）在设置 reduce 个数的时候也需要考虑这两个原则：处理大数据量利用合适的 reduce 数；使单个 reduce 任务处理数据量大小要合适；

### 任务3、优化-9：并行执行

#### 【任务目标】

本次任务主要用于讲解如何设置任务并行从而提高HIve执行效率。

#### 【任务步骤】

Hive 会将一个查询转化成一个或者多个阶段。这样的阶段可以是 MapReduce 阶段、抽样阶段、合并阶段、limit 阶段。或者 Hive 执行过程中可能需要的其他阶段。默认情况下，Hive 一次只会执行一个阶段。不过，某个特定的 job 可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个 job 的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么 job 可能就越快完成。通过设置参数 hive.exec.parallel 值为 true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果 job 中并行阶段增多，那么集群利用率就会增加

```
set hive.exec.parallel = true;  -- 打开任务并行执行
set hive.exec.parallel.thread.number = 16;  -- 同一个 sql 允许最大并行度，默认为8。复制代码
```

当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。

### 任务4、优化-10：严格模式

#### 【任务目标】

本次任务主要用于讲解如何设置Hive 的严格模式以防止一些危险操作。

#### 【任务步骤】

Hive提供了一个严格模式，可以防止用户执行那些可能意想不到的不好的影响的查询。属性hive.mapred.mode值默认值是非严格模式：nonstrict 。

开启严格模式：

```
set hive.mapred.mode=strict;复制代码
```

关闭严格模式：

```
set hive.mapred.mode=undefined;复制代码
```

除了设定严格模式外，也可能通过分别单独设置可以禁止3种类型的查询：

1 ） 分区表不使用分区过滤

```
set hive.strict.checks.no.partition.filter = true;复制代码
```

将 hive.strict.checks.no.partition.filter 设置为 true 时，对于分区表，除非 where 语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。

2）使用 order by 没有 limit 过滤

```
set  hive.strict.checks.orderby.no.limit = true;复制代码
```

将 hive.strict.checks.orderby.no.limit 设置为 true 时，对于使用了 order by 语句的查询，要求必须使用 limit 语句。因为 order by 为了执行排序过程会将所有的结果数据分发到同一个Reducer 中进行处理，强制要求用户增加这个 LIMIT 语句可以防止 Reducer 额外执行很长一段时间。

3）限制笛卡尔积的查询

```
set hive.strict.checks.cartesian.product = true;复制代码
```

将 hive.strict.checks.cartesian.product 设置为 true 时，会限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在 执行 JOIN 查询的时候不使用 ON 语句而是使用 where 语句，这样关系数据库的执行优化器就可以高效地将 WHERE 语句转化成那个 ON 语句。不幸的是，Hive 并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。

### 任务5、优化-11：JVM重用

#### 【任务目标】

本次任务主要用于讲解如何设置JVM重用以达到优化HIve的目的。

#### 【任务步骤】

JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。

Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得 **JVM实例在同一个job中重新使用N次** 。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。

```
mapred.job.reuse.jvm.num.tasks = ?  /**默认为1**/复制代码
```

**这个功能的缺点是，**开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他

Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。

### 实验总结

1）是不是 map 数越多越好？ 答案是否定的。如果一个任务有很多小文件（远远小于块大小 128m），则每个小文件也会被当做一个块，用一个 map 任务来完成，而一个 map 任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的 map 数是受限的。

2）Hive 一次只会执行一个阶段。不过，某个特定的 job 可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个 job 的执行时间缩短。

3）Hive提供了一个严格模式可以防止用户执行那些可能意想不到的不好的影响的查询。属性hive.mapred.mode 值默认值是非严格模式：nonstrict 。

4）JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。





# 实验5-1：基于Hive实现的二手房交易分析（上）

## 实验概述

 本次实训的目的是通过对链家网站的真实交易数据进行分析，以了解企业真实分析案例是如何在Hive中实现的。本实训基于真实的二手房交易数据，通过分析交易数据。从而得出哪个朝向的房子卖的最好，哪个地段的，哪种面积的房子最受欢迎等各种指标。从而为二手房销售提供实际营销策略。此为本项目的第一部分内容。

## 实验环境

- Linux 操作系统
- Hadoop2.7.7
- Hive-1.2.2
- MySQL5.7

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16317730180672982.png)

## 实验目标

完成本实验后，您将能够

- 了解企业网站运营常用指标概念、作用及具体计算方式
- 熟练掌握在企业中如何使用Hive进行大数据分析
- 掌握数据分析结果文件存储及导入导出ETL处理

## 实验任务

### 任务1：数据及环境准备

#### 【任务目标】

 本实验数据集是来源于网络公开的数据集，数据内容为二手房交易数据，首先我们要了解数据集的结构和大概内容 。为后继数据分析与统计打下基础。

#### 【任务步骤】

##### 1.1、数据集结构

本实验数据集是来源于网络公开的数据集，数据内容为二手房交易数据。该数据集格式为csv，数据集文件名称为`2nd_house_price.csv`，其中一条样本数据如下

```
梅园六街坊,2室0厅,47.72,浦东,低区/6层,朝南,500,104777,1992年建复制代码
```

从以上样本数据可知，该数据及共有9个字段，字段解析说明如下

```
village_name string,   //住宅名称
house_type string,    //住宅类型
house_area string, //面积，50以下，50-70 ，70-90，90-110，110-130，130-150，150以上
region string,   //住宅区域
floor_str string,  //楼层
direction string,     //朝向
total_price string,   //总价   
square_price string,  //单价
build_date string  //年份复制代码
```

数据集部分截图如下： ![image.png](https://cdn.atstudy.com/lab/manual/16317730323134818.png)

##### 1.2、查看数据集

首先进入系统云沙箱环璄，界面应如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317730429248758.png)

在上述界面的桌面空白处击右键，在弹出的右键菜单中选择`在此打开终端`

![image.png](https://cdn.atstudy.com/lab/manual/16317730527959226.png)

在弹出的Linux命令行终端中，输入命令如下：

```
cd /home/atstudy/files/data/
ls复制代码
```

敲回车，运行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317730650426205.png)

从上图可以看到在此目录下存在着系统事先为大家准备好的实验数据：`2nd_house_price.csv`

### 任务2：Hive运行环境测试

#### 【任务目标】

 本次任务主要进行Hive的运行环境测试，确保Hive大数据分析所需的环璄已经准备就绪。

#### 【任务步骤】

##### 2.1、启动Hadoop

因为Hive数据分析是基于Hadoop的，所以在启动Hive之前，必须先启动Hadoop分布式文件存储系统及相关服务，在上述的Linux命令行终端中继续输入命令如下：

```
start-all.sh复制代码
```

启动Hadoop相关服务的过程中因要求设置用户的免密登录，在两处弹出的提示处，均输入yes，敲回车继续，最终运行效果应如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317730786603450.png)

上述命令是用于启动Hadoop系统服务的，如何证明Hadoop服务是否启动成功呢？可以通过使用jps命令查看Hadoop相关服务从而验证Hadoop是否成功启动：

```
jps复制代码
```

敲回车，运行效果应如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317730900935217.png)

出现上图所示效果，则说明Hadoop运行所需的服务已经正常启动，接下来就可以运行Hive了。

##### 2.2、启动Hive

继续在上图的 Linux 命令行中输入命令如下，用于启动Hive客户端：

```
hive复制代码
```

敲回车，出现如下界面，即说明 hive 客户端已正常启动：

![image.png](https://cdn.atstudy.com/lab/manual/16317731015965601.png)

### 任务3：建表并导入数据进行数据清洗

#### 【任务目标】

 本任务目标为在Hive中创建存储数据所需要的数据库及数据表，并将二手房交易数据`2nd_house_price.csv`导入到Hive中。

#### 【任务步骤】

##### 3.1、创建数据库及数据表

在Hive中创建实验所需数据库，在上述Linux命令行终端输入命令如下：

```
create database db_lianjia;复制代码
```

敲回车，运行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317731141555604.png)

可以通过以下命令查看刚刚的脚本是否正确执行：

```
show databases;复制代码
```

敲回车，运行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317731253821534.png)

说明上述数据库已经成功创建。接下来继续在Hive中刚刚创建的数据库：db_lianjian中创建实验所需数据表：tb_lj

继续在hive 命令行中输入脚本如下：

> 提示：复制或输入脚本时，不需要将中文注释包含进去！

```
# 选择数据库
use db_lianjia;复制代码
# 创建表
create table tb_lj(
village_name string,
house_type string,
house_area string,
region string,
floor_str string,
direction string,
total_price string,
square_price string,
build_date string
)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile;复制代码
```

敲回车，执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317731399331150.png)

上述执行没有报错，但是否执行成功，还待进一步验证，可以继续在上述hive命令行中输入以下脚本进行验证

```
show tables;复制代码
```

敲回车，执行效果如下： ![image.png](https://cdn.atstudy.com/lab/manual/16317731513283740.png) 说明该表已经成功创建。可以继续在上述命令行中输入以下脚本用来查看该表结构：

```
desc tb_lj;复制代码
```

敲回车，执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317731638881857.png)

如果你的执行效果完全如上图所示，则说明数据表tb_lj已经成功创建。

##### 3.2、数据导入

完成数据表的创建后，接下来就是要将`2nd_house_price.csv`文件中的二手房交易数据加载到Hive的 tb_lj 表中。继续在hive命令行中输入脚本如下：

```
load data local inpath '/home/atstudy/files/data/2nd_house_price.csv' overwrite into table tb_lj;复制代码
```

敲回车，执行效果如下： ![image.png](https://cdn.atstudy.com/lab/manual/16317733595244219.png)

> 说明：以上命令中的local表示从本地文件系统中加载文件数据到Hive表中，你也可以选择从HDFS分布式文件系统中加载数据到Hive表

##### 3.3、查询部分数据

数据有没有成功导入到db_lianjian数据库的tb_lj表中呢。可以通过以下代码来进行验证。

接下来，要做的事情就是从`tb_lj`表中查询部分数据，了解基本的数据内容以及验证数据是否导入成功。

承接步骤2，继续在hive命令行中输入如下脚本：

```
select house_type,build_date from tb_lj limit 5;复制代码
```

敲回车，执行效果如下 ：

![image.png](https://cdn.atstudy.com/lab/manual/16317734221527568.png)

从上述的执行效果中可以看出，数据已经成功导入了！

### 实验总结

本次实训的目的是通过对链家网站的真实交易数据进行分析，以了解企业真实分析案例是如何在Hive中实现的。本实训基于真实的二手房交易数据，通过分析交易数据。从而得出哪个朝向的房子卖的最好，哪个地段的，哪种面积的房子最受欢迎等各种指标。从而为二手房销售提供实际营销策略。本次实验是项目的前半部分，主要介结了项目的需求及数据集结构，同时完成Hadoop+Hive的离线环境搭建，且进行了基础数据的导入，为下一步数据分析与统计做好准备。





# 实验5-2：基于Hive实现的二手房交易分析（下）

## 实验概述

本次实训的目的是通过对链家网站的真实交易数据进行分析，以了解企业真实分析案例是如何在Hive中实现的。本实训基于真实的二手房交易数据，通过分析交易数据。从而得出哪个朝向的房子卖的最好，哪个地段的，哪种面积的房子最受欢迎等各种指标。从而为二手房销售提供实际营销策略。此为第二部分内容。

## 实验环境

- Linux 操作系统
- Hadoop2.7.7
- Hive-1.2.2
- MySQL5.7

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16317748918718045.png)

## 实验目标

完成本实验后，您将能够

- 了解企业网站运营常用指标概念、作用及具体计算方式
- 企业中如何使用Hive进行分析
- 分析结果如何保存为文件

## 实验任务

### 任务1：分析各项指标

#### 【任务目标】

 本任务主要目标为根据现有的二手房交易数据，分析与二手房交易相关的业务指标。

#### 【任务步骤】

##### 1.1、最受欢迎的面积区间

根据二手房交易网站常用的面积划分方法，可以以20平米为一个区间范围来统计，将房屋面积划分为以下7大类

```
50平以下 | 50-60平 | 70-90平 | 90-110平 | 110-130平 | 130-150平 | 150平以上复制代码
```

接下来，编写SQL语句将二手房数据按照以上面积划分，并统计每组下房屋成交数据量。

继续在任务2中的hive命令行中，输入以下SQL查询代码：

> 提示：以下代码中m2 代表平米！

```
select t.area_group,count(*) as total
from 
(select 
 case
  when 0<house_area and house_area<=50 then '<50m2' 
  when 50<house_area and house_area<=70 then '50-70m2' 
  when 70<house_area and house_area<=90 then '70-90m2'
  when 90<house_area and house_area<=110 then '90-110m2'
  when 110<house_area and house_area<=130 then '110-13m2'
  when 130<house_area and house_area<=150 then '130-150m2'
  else '>150m2'
 end as area_group
from db_lianjia.tb_lj) as t 
group by t.area_group order by total desc;复制代码
```

敲回车，运行结果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317749171133403.png)

敲回车，执行结果如下：

```
t.area_group	total
50-70m2	5775
70-90m2	5739
90-110m2	4505
<50m2	4291
110-13m2	2798
>150m2	2775
130-150m2	2318复制代码
```

从上述运行结果可以得出结论：在这一年的时间段内，50-70这个面积的房子最受欢迎（面积又不是很小，总价也不会很贵），相对来说，面积越小越好卖。

##### 1.2、房龄多少的房子最受欢迎

接下来对房龄进行分组，分析得出哪个时间段内的房子比较受欢迎。按照以下方式对所有的房子进行分组

```
5年以内 | 10年以内 | 15年以内 | 20年以内 | 20年以上复制代码
```

这个需求的技术点，是如何过滤或处理数据类型中多余的字符。假设我们这个数据是2019年的数据，房龄：2019-1992(年建)build_date 即可得到房龄数据

继续在上述的hive命令行中输入代码如下：

```
select t.year_group,count(*) as total 
from 
(select 
 case
  when (2019-substring(build_date,0,4)) between 0 and 5 then '0~5 year'
  when (2019-substring(build_date,0,4)) between 6 and 10 then '6~10 year'
  when (2019-substring(build_date,0,4)) between 11 and 15 then '11~15 year'
  when (2019-substring(build_date,0,4)) between 16 and 20 then '16~20 year'
  else '>20 year'
 end as year_group
from db_lianjia.tb_lj) as t 
group by t.year_group order by total desc;复制代码
```

敲回车，运行结果如下：

![image.png](https://cdn.atstudy.com/lab/manual/1631774929970355.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317749399514818.png)

从以上的运行结果中，可以得出的结论：房龄越大越好卖 !

至此为止，任务3 的内容告一段落，可以退出hive ,在上述hive命令行中输入以下命令退出hive :

```
exit;复制代码
```

敲回车，执行结果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317749519903412.png)

### 任务2：分析结果的保存

#### 【任务目标】

本任务主要目标为根据需求完成分析后将相应的结果保存为文件，以方便进行下一阶段的数据分析或可视化展示。

#### 【任务步骤】

##### 2.1、新建 sql 文件

首先回到沙箱系统桌面，如下图所示 ：

![image.png](https://cdn.atstudy.com/lab/manual/16317749642326253.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631775019363522.png) 然后在上述界面的桌面空白处击右键，在弹出的右键菜单中选择`在此打开终端`

![image.png](https://cdn.atstudy.com/lab/manual/16317750294132322.png) 在弹出的Linux命令行终端中，输入命令如下：

```
# 编写`sql`执行文件，例如文件名为`lianjia.sql`
vim /root/lianjia.sql复制代码
```

在打开的文件中添加以下内容

1）编写创建数据库表的代码

> 提示：以下代码中的中文注释仅为方便理解代码，不需复制或输入到代码区！

```
--设置本地模式
set hive.exec.mode.local.auto=true;
--创建数据库，如果不存在
create database if not exists db_lianjia;
--删除数据表如果存在
drop table if exists db_lianjia.tb_lj;
--再次创建数据表
create table db_lianjia.tb_lj(
village_name string,
house_type string,
house_area string,
region string,
floor_str string,
direction string,
total_price string,
square_price string,
build_date string
)
row format delimited fields terminated by ','
lines terminated by '\n'
stored as textfile;复制代码
```

2）编写加载数据的代码：

```
--加载数据
load data local inpath '/home/atstudy/files/data/2nd_house_price.csv' overwrite into table db_lianjia.tb_lj;复制代码
```

3）使用when函数将房屋面积查询结果导出到表db_lianjia.tb_lj_tmp1中：

```
--使用when函数来创建结果表db_lianjia.tb_lj_tmp1
drop table if exists db_lianjia.tb_lj_tmp1;
create table db_lianjia.tb_lj_tmp1 as 
select t.area_group,count(*) as total
from 
(select 
 case
  when 0<house_area and house_area<=50 then '<=50m2' 
  when 50<house_area and house_area<=70 then '50~70m2' 
  when 70<house_area and house_area<=90 then '70~90m2'
  when 90<house_area and house_area<=110 then '90~110m2'
  when 110<house_area and house_area<=130 then '110~130m2'
  when 130<house_area and house_area<=150 then '130~150m2'
  else '>150m2'
 end as area_group
from db_lianjia.tb_lj) as t 
group by t.area_group order by total desc;复制代码
```

4）将房龄信息统计的结果导出到表db_lianjia.tb_lj_tmp2中：

```
--使用substring函数来完成房龄的结果表db_lianjia.tb_lj_tmp2
drop table if exists db_lianjia.tb_lj_tmp2;
create table db_lianjia.tb_lj_tmp2 as 
select t.year_group,count(*) as total 
from 
(select 
 case
  when (2019-substring(build_date,0,4)) between 0 and 5 then '0~5 year'
  when (2019-substring(build_date,0,4)) between 6 and 10 then '6~10 year'
  when (2019-substring(build_date,0,4)) between 11 and 15 then '11~15 year'
  when (2019-substring(build_date,0,4)) between 16 and 20 then '16~20 year'
  else '>20 year'
 end as year_group
from db_lianjia.tb_lj) as t 
group by t.year_group order by total desc;复制代码
```

5）将房龄多少的房子最受欢迎的统计结果导出到表：tb_lj_info 中：

```
drop table if exists db_lianjia.tb_lj_info;
create table db_lianjia.tb_lj_info as 
select region,direction,
case
 when 0<house_area and house_area<=50 then '<=50m2' 
 when 50<house_area and house_area<=70 then '50~70m2' 
 when 70<house_area and house_area<=90 then '70~90m2'
 when 90<house_area and house_area<=110 then '90~110m2'
 when 110<house_area and house_area<=130 then '110~130m2'
 when 130<house_area and house_area<=150 then '130~150m2'
 else '>150m2'
end as area_group,
case
 when (2019-substring(build_date,0,4)) between 0 and 5 then '<=5 year'
 when (2019-substring(build_date,0,4)) between 6 and 10 then '6~10 year'
 when (2019-substring(build_date,0,4)) between 11 and 15 then '11~15 year'
 when (2019-substring(build_date,0,4)) between 16 and 20 then '16~20 year'
 else '>20 year'
end as year_group
from db_lianjia.tb_lj;复制代码
```

输入完上述代码后，在vim环璄中，按 esc 键，然后再输入`:wq` 保存退出！

##### 2.2、执行 `sql` 文件

回到Linux命令行终端，输入如下代码，用于执行刚刚创建的sql脚本：

```
hive -f /root/lianjia.sql复制代码
```

敲回车，运行过程如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317750503472495.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317750620721887.png)

如出现以上界面，则说明 sql 脚本已经成功运行。

##### 2.3、Hive中查询结果

执行hive命令，进入hive命令行。在上述Linux命令终端中再次输入如下命令，进行hive：

```
hive复制代码
```

![image.png](https://cdn.atstudy.com/lab/manual/16317750743619098.png)

在hive命令行中输入如下脚本，进入数据库，查看数据表

```
use db_lianjia;复制代码
```

敲回车，执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317750863005865.png)

继续在hive命令行输入如下脚本，查看该数据库中有哪些表：

```
show tables;复制代码
```

敲回车，执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317750986652265.png)

输入如下脚本，用于查询`tb_lj_tmp1`表中所有数据：

```
select * from tb_lj_tmp1;复制代码
```

敲回车，执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317751104533420.png)

输入如下脚本，查询`tb_lj_tmp2`表中所有数据

```
select * from tb_lj_tmp2;复制代码
```

敲回车，执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317752617293080.png)

输入如下脚本，查询`tb_lj_info`表中所有数据

```
select * from tb_lj_info limit 5;复制代码
```

敲回车，执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317752777906451.png)

通过以上运行结果，可以看出，我们已经将数据分析统计的结果分别保存到相关表中了。

至此为止，任务4 的内容告一段落，可以退出hive了，在上述hive命令行中输入以下命令退出hive :

```
exit;复制代码
```

敲回车，执行结果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317752889184244.png)

### 任务3：实验验证

#### 【任务目标】

 本次任务的主要目标为验证实验2的实验结果是否正确保存到HDFS中。

#### 【任务步骤】

 Hive的结果表其实就是HDFS上的文件，如果能够得到结果表，对应的文件也一定在HDFS上能够看见。打开浏览器，访问一下网址：`http://你的Linux虚拟机IP:50070`结果文件已经保存在HDFS上了，具体步骤如下：

首先回到沙箱桌面：

![image.png](https://cdn.atstudy.com/lab/manual/16317753048302691.png)

在上述界面的桌面空白处击右键，在弹出的右键菜单中选择`在此打开终端`

![image.png](https://cdn.atstudy.com/lab/manual/16317753155184486.png)

在弹出的Linux命令行终端中，输入命令如下：

```
ifconfig复制代码
```

敲回车，运行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317753288368796.png)

回到桌面，打开浏览器：

![image.png](https://cdn.atstudy.com/lab/manual/1631775340117814.png)

在打开的浏览器中输入如下网址：`http://192.168.60.2:50070`,回车，将会出现如下界面：

> 提示：上面的 url 地址中的 IP一定要与你上面ifconfig命令中的IP地址保持一致，否则将会无法正常访问！

![image.png](https://cdn.atstudy.com/lab/manual/16317753562273698.png)

在上述界面中，选择Utilities菜单下的`Browse the file system` 子菜单，进入Browse Directory界面，在此界面中进入到`/user/hive/warehouse/db_lianjia.db` 目录中，可以看到的效果如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317753890775100.png)

在上述的界面中，可以看到刚刚创建生成的四张表，该四张表的数据实质上是以文件的方式存在于HDFS系统中。

## 实验总结

 二手房交易分析实训的目的是通过对链家网站的真实交易数据进行分析，以了解企业真实分析案例是如何在Hive中实现的。本实训基于真实的二手房交易数据，通过分析交易数据。从而得出哪个朝向的房子卖的最好，哪个地段的，哪种面积的房子最受欢迎等各种指标。从而为二手房销售提供实际营销策略。通过本实训项目可以让学员了解企业网站运营常用指标概念、作用及具体计算方式，能够熟练掌握在企业中如何使用Hive进行大数据分析以及数据分析结果文件存储及导入导出ETL处理等。





# 实验5-3：基于 Hive 的电影大数据分析实战（上）

## 实验概述

 本次实训通过在大数据环境中利用Hive对真实影评数据的评价次数，评价分数，观影年龄，性别，影片上映时间，影片类型 等不同维度的Hive SQL分析。训练了学员对于Hadoop+Hive的离线数据获取，导入、清洗、处理与统计分析及数据导出全流程技术点。从而达到了解商品评论类分析的常见业务需求与分析指标。熟练掌握在企业中如何使用Hive利用HQL语句结合业务进行复杂数据分析的目的。此为本项目的第一部分内容。

## 实验环境

- Linux 操作系统
- Hadoop2.7.7
- Hive-1.2.2
- MySQL5.7

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16317755507818110.png)

## 实验目标

完成本实验后，您将能够

- 了解如何在Hive中导入数据并利用Hive进行数据清洗
- 了解商品评论类分析的常见业务需求与分析指标
- 熟练掌握企业中如何使用Hive利用HQL结合业务进行复杂数据分析
- 掌握如何将Hive中的数据导出至HDFS中

## 实验任务

### 任务1：项目背景与数据集简介

#### 【任务目标】

 本实验主要了解项目需求及对项目涉及的影评数据集作一简单介绍，属于实训的理论部分，为后继更加高级的实训内容打下基础。

#### 【任务步骤】

##### 1.1、数据集简介

本实验数据集是来源于网络公开的数据集，数据内容为某知名影院历年来的影评数据。该数据集格式为dat类型（本质上可以将其理解为文本类型），数据集文件主要有三个，分别为：`users.dat` , `movies.dat` , `ratings.dat` . 具体数据结构及内容简介如下：

1、users.dat , 相当于用户表，数据格式为： 2::M::56::16::70072，

共有6040条数据 对应字段为：UserID BigInt, Gender String, Age Int, Occupation String, Zipcode String 对应字段中文解释：用户id，性别，年龄，职业，邮政编码

2、movies.dat 相当于是电影表，数据格式为： 2::Jumanji (1995)::Adventure|Children's|Fantasy，

共有3883条数据 对应字段为：MovieID BigInt, Title String, Genres String 对应字段中文解释：电影ID，电影名字，电影类型

3、ratings.dat 相当于是电影评分表，数据格式为： 1::1193::5::978300760

共有1000209条数据 对应字段为：UserID BigInt, MovieID BigInt, Rating Double, Timestamped String 对应字段中文解释：用户ID，电影ID，评分，评分时间戳

数据集部分截图如下：

- users.dat：用户表

![image.png](https://cdn.atstudy.com/lab/manual/16317755749994806.png)

- movies.dat：电影表

![image.png](https://cdn.atstudy.com/lab/manual/16317755907775331.png)

- ratings.dat：电影评分表

![image.png](https://cdn.atstudy.com/lab/manual/16317756053526021.png)

##### 1.2、查看实验沙箱数据集

首先进入系统云沙箱环璄，界面应如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317756195101261.png)

在上述界面的桌面空白处击右键，在弹出的右键菜单中选择`在此打开终端`

![image.png](https://cdn.atstudy.com/lab/manual/16317756306531469.png)

在弹出的Linux命令行终端中，输入命令如下：

```
cd /home/atstudy/files/data/
ls复制代码
```

敲回车，运行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/1631775643289412.png)

从上图可以看到在此目录下存在着系统事先为大家准备好的三份实验数据。

接下来分别查看一下数据，仍在上述的Linux命令行终端中，输入如下命令用于查询 users.data 文件的前10条记录：

```
head users.dat复制代码
```

敲回车，运行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317756585742059.png)

继续在Linux命令行终端中输入如下命令，用于查看 movies.dat文件的前10条记录：

```
head movies.dat复制代码
```

敲回车，运行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317756700813759.png)

继续在Linux命令行终端中输入如下命令，用于查看 ratings.dat文件的前10条记录：

```
head ratings.dat复制代码
```

敲回车，运行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/1631775683968913.png)

##### 1.3、项目需求

本项目的具体分析需求如下：

（1）正确建表，导入数据（三张表，三份数据），并验证是否正确

（2）求被评分次数最多的10部电影，并给出评分次数（电影名，评分次数）

（3）分别求男性，女性当中评分最高的10部电影（性别，电影名，影评分）

（4）求movieid = 2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影评（年龄段，影评分）

（5）求最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影平均影评分（观影者，电影名，影评分）

（6）求好片（评分>=4.0）最多的那个年份的最好看的10部电影

（7）求1997年上映的电影中，评分最高的10部Comedy类电影

（8）该影评库中各种类型电影中评价最高的5部电影（类型，电影名，平均影评分）

（9）各年评分最高的电影类型（年份，类型，影评分）

### 任务2：运行环境测试

#### 【任务目标】

 本次任务主要了解该项目运行所需环璄，并且测试运行环璄是否已经在沙箱中正常安装配置，且能正常启动。

#### 【任务步骤】

##### 2.1、了解项目运行环璄

此项运行环璄如下：

1. 在 Linux 中安装 Java 环境
2. 在 Linux 平台中 分式式协调服务程序 Zookeeper
3. 在 Linux 中搭建大数据工作平台-高可用 Hadoop-HA
4. 搭建对海量结构化数据进行分析的 Hive

> 注：本实验沙箱具有开箱即用功能。以上Hive大数据分析环璄已经在本实验沙箱中全部配置完成，学员无需再次手工配置。

##### 2.2、测试运行环璄

**1）启动Hadoop**

因为Hive数据分析是基于Hadoop的，所以在启动Hive之前，必须先启动Hadoop分布式文件存储系统及相关服务，继续在任务1打开的Linux命令行终端中输入命令如下，用于启动Hadoop服务：

```
start-all.sh复制代码
```

启动Hadoop相关服务的过程中因要求设置用户的免密登录，在两处弹出的提示处，均输入yes，敲回车继续，最终运行效果应如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317757118222249.png)

上述命令是用于启动Hadoop系统服务的，如何证明Hadoop服务是否启动成功呢？可以通过使用jps命令查看Hadoop相关服务从而验证Hadoop是否成功启动：

```
jps复制代码
```

敲回车，运行效果应如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317757259511767.png)

出现上图所示效果，则说明Hadoop运行所需的服务已经正常启动，接下来就可以运行Hive了。

**2）启动Hive**

继续在上图的 Linux 命令行中输入命令如下，用于启动Hive客户端：

```
hive复制代码
```

敲回车，出现如下界面，即说明 hive 客户端已正常启动：

![image.png](https://cdn.atstudy.com/lab/manual/16317757388598951.png)

如果上述命令均能正常执行且效果如上面所示，则说明沙箱的大数据分析环璄已经具备且可以正常使用。

### 任务3：建表并导入数据

#### 【任务目标】

 本任务目标为在Hive中创建存储数据所需要的数据库及数据表，并将项目所需数据：`users.dat` , `movies.dat` ,`ratings.dat` 导入到Hive中。

#### 【任务步骤】

##### 3.1、创建数据库

**1）分析需求**

需要创建一个数据库movie，在movie数据库中创建3张表，t_user（用户表），t_movie（电影表），t_rating（电影评分表），三张表的结构应如下所示：

```
t_user:userid bigint,sex string,age int,occupation string,zipcode string
t_movie:movieid bigint,moviename string,movietype string
t_rating:userid bigint,movieid bigint,rate double,times string复制代码
```

通过在任务1中查看数据集时看到，三个dat类型的数据集的原始数据是以::进行切分的，由于hive不支持解析多字节的分隔符，也就是说hive只能解析类似于 ’:’ 这种的, 不支持解析 ’::’ ，所以用普通方式建表来使用是行不通的，要求对数据做一次简单清洗。所以需要使用能解析多字节分隔符的Serde即可。

使用RegexSerde

需要两个参数：

> 以下代码仅供理解，不需要手工输入到hive中

```
input.regex = "(.*):: (.*):: (.*)"
output.format.string = "%1$s %2$s %3$s"复制代码
```

**2）创建数据库**

**特别说明：**在Hive客户端中输入代码执行时，如果输入错误，想撤消或修改而又没有办法正常完成时，请按 `CTRL+C` 进行中断，强行退出Hive客户端。然后再次输入 hive 命令进入即可（下同，不再累述）！

在Hive中创建实验所需数据库：movie，在上述Linux命令行终端输入命令如下：

```
drop database if exists movie;
create database if not exists movie;
use movie;复制代码
```

上述代码解释：

- 第一行代码：用于判断hive系统中是否已经存在movie数据库，如果已经存在则删除；
- 第二行代码：用于创建数据库movies;
- 第三行代码：选择movies数据库。

分别执行上述代码，最终运行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317757605971338.png)

可以通过以下命令查看刚刚的脚本是否正确执行：

```
show databases;复制代码
```

敲回车，运行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317757717483073.png)

说明上述数据库已经成功创建。

##### 3.2、创建数据表

**1）创建 t_user表**

接下来在数据库movie 中创建实验所需数据表：t_user

继续在hive 命令行中输入脚本如下：

```
create table t_user(
userid bigint,
sex string,
age int,
occupation string,
zipcode string) 
row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe' 
with serdeproperties('input.regex'='(.*)::(.*)::(.*)::(.*)::(.*)','output.format.string'='%1$s %2$s %3$s %4$s %5$s')
stored as textfile;复制代码
```

> 特别说明：上述代码中，请注意 row format serde 后面的代码，这里使用RegexSerde利用正则表达式来处理该表中由双字节 :: 间隔的数据。

敲回车，执行效果应如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317757836354900.png)

可以通过以下命令查看该表是否已成功创建：

```
desc t_user;复制代码
```

敲回车，执行效果应如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317757941505620.png)

**2）创建 t_movie 表**

接下来在数据库movie 中创建实验所需数据表：t_movie

继续在hive 命令行中输入脚本如下：

```
create table t_movie(
movieid bigint,
moviename string,
movietype string) 
row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe' 
with serdeproperties('input.regex'='(.*)::(.*)::(.*)','output.format.string'='%1$s %2$s %3$s')
stored as textfile;复制代码
```

> 特别说明：上述代码中，请注意 row format serde 后面的代码，这里使用RegexSerde利用正则表达式来处理该表中由双字节 :: 间隔的数据。

敲回车，执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/163177580506172.png)

可以通过以下命令查看该表是否已成功创建：

```
desc t_movie;复制代码
```

敲回车，执行效果应如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317758147591329.png)

**3）创建 t_rating 表**

接下来在数据库movie 中创建实验所需数据表：t_rating

继续在hive 命令行中输入脚本如下：

```
create table t_rating(
userid bigint,
movieid bigint,
rate double,
times string) 
row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe' 
with serdeproperties('input.regex'='(.*)::(.*)::(.*)::(.*)','output.format.string'='%1$s %2$s %3$s %4$s')
stored as textfile;复制代码
```

> 特别说明：上述代码中，请注意 row format serde 后面的代码，这里使用RegexSerde利用正则表达式来处理该表中由双字节 :: 间隔的数据。

敲回车，执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/1631775827168525.png)

可以通过以下命令查看该表是否已成功创建：

```
desc t_rating;复制代码
```

敲回车，执行效果应如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317758375429900.png)

最后，可以在上述hive命令行中输入以下脚本查看刚刚创建的三张表：

```
show tables;复制代码
```

敲回车，执行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317758484677531.png)

##### 3.3、向表中导入数据

完成数据表的创建后，接下来就是要将系统沙箱中的影评数据分别导师入到到Hive的 `t_user` , `t_movie` `t_rating` 表中。

**1）首先向 t_user 表中导入数据**

- 导入数据

继续在步骤二中的hive命令行中输入脚本如下：

```
load data local inpath "/home/atstudy/files/data/users.dat" into table t_user;复制代码
```

敲回车，执行效果如下： ![image.png](https://cdn.atstudy.com/lab/manual/16317758605886489.png)

> 说明：以上命令中的local表示从本地文件系统中加载文件数据到Hive表中，你也可以选择从HDFS分布式文件系统中加载数据到Hive表

- 验证数据

数据有没有成功导入到 movie 数据库的 t_user 表中呢。可以通过以下代码来进行验证。

接下来，要做的事情就是从`t_user`表中查询部分数据，了解基本的数据内容以及验证数据是否导入成功。

承接以上，继续在hive命令行中输入如下脚本：

```
select userid,sex,age,occupation,zipcode from t_user limit 5;复制代码
```

敲回车，执行效果如下 ：

![image.png](https://cdn.atstudy.com/lab/manual/16317758715005252.png)

从上述的执行效果中可以看出，数据已经成功导入了！

**2）接下来向 t_movie 表中导入数据**

- 导入数据

继续在hive命令行中输入代码如下：

```
load data local inpath "/home/atstudy/files/data/movies.dat" into table t_movie;复制代码
```

敲回车，执行代码效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317758833698349.png)

- 验证数据

数据有没有成功导入到 movie 数据库的 t_movie 表中呢。可以通过以下代码来进行验证。

接下来，要做的事情就是从`t_movie`表中查询部分数据，了解基本的数据内容以及验证数据是否导入成功。

承接以上，继续在hive命令行中输入如下脚本：

```
select movieid,moviename,movietype from t_movie limit 5;复制代码
```

敲回车，执行效果如下 ：

![image.png](https://cdn.atstudy.com/lab/manual/16317758936851197.png)

从上述的执行效果中可以看出，数据已经成功导入了！

**3）最后向 t_rating 表中导入数据**

- 导入数据

继续在hive命令行中输入代码如下：

```
load data local inpath "/home/atstudy/files/data/ratings.dat" into table t_rating;复制代码
```

敲回车，执行代码效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/1631775904166994.png)

- 验证数据

数据有没有成功导入到 movie 数据库的 t_rating 表中呢。可以通过以下代码来进行验证。

接下来，要做的事情就是从`t_rating`表中查询部分数据，了解基本的数据内容以及验证数据是否导入成功。

承接以上，继续在hive命令行中输入如下脚本：

```
select userid,movieid,rate,times from t_rating limit 5;复制代码
```

敲回车，执行效果如下 ：

![image.png](https://cdn.atstudy.com/lab/manual/16317759135806518.png)

从上述的执行效果中可以看出，数据已经成功导入了，至此，我们已经全部完成了项目所需的hive数据库表的创建及数据导入。接下来，就可以根据具体的业务需求完成影评的数据分析了。

## 实验总结

 电影大数据分析实训通过在大数据环境中利用Hive对真实影评数据的评价次数，评价分数，观影年龄，性别，影片上映时间，影片类型 等不同维度的Hive SQL分析。训练了学员对于Hadoop+Hive的离线数据获取，导入、清洗、处理与统计分析及数据导出全流程技术点。本次实训主要介结了项目背景与数据集并进行了Hive运行环境准备和测试，同时在Hive中进行建库建表及其导入数据操作，为后面的大数据分析部分打下基础。





# 实验5-4：基于 Hive 的电影大数据分析实战（下）

## 实验概述

 本次实训通过在大数据环境中利用Hive对真实影评数据的评价次数，评价分数，观影年龄，性别，影片上映时间，影片类型 等不同维度的Hive SQL分析。训练了学员对于Hadoop+Hive的离线数据获取，导入、清洗、处理与统计分析及数据导出全流程技术点。从而达到了解商品评论类分析的常见业务需求与分析指标。熟练掌握在企业中如何使用Hive利用HQL语句结合业务进行复杂数据分析的目的。本次为该实训的第二部分内容。

## 实验环境

- Linux 操作系统
- Hadoop2.7.7
- Hive-1.2.2
- MySQL5.7

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16317793749451940.png)

## 实验目标

完成本实验后，您将能够

- 了解如何在Hive中导入数据并利用Hive进行数据清洗
- 了解商品评论类分析的常见业务需求与分析指标
- 熟练掌握企业中如何使用Hive利用HQL结合业务进行复杂数据分析
- 掌握如何将Hive中的数据导出至HDFS中

## 实验任务

### 任务1：电影大数据分析

#### 【任务目标】

 本任务主要目标为根据现有影评数据，分析与电影相关的业务指标。

#### 【任务步骤】

##### 1.1、需求01：分析评分次数最多的10部电影

**需求：**项目的第一个需求是要求统计出被评分次数最多的10部电影，并显示电影名和评分次数，并将统计的结果保存至 answer2 表中。

**1）分析思路**

- 需求字段:
  - 电影名 `t_movie.moviename` 保存至结果表中为 `moviename`
  - 评分次数 `t_rating.rate count()` 保存至结果表中为 `total`
- 核心SQL：按照电影名进行分组统计，求出每部电影的评分次数并按照评分次数降序排序

**2）SQL实现**

在任务4的基础上，继续在hive客户端中输入以下SQL代码：

```
drop table  if exists answer2;
create table answer2 as 
select a.moviename as moviename,count(a.moviename) as total 
from t_movie a join t_rating b on a.movieid=b.movieid 
group by a.moviename 
order by total desc 
limit 10;复制代码
```

> 说明：上述代码是将此次查询统计的结果导入到表answer2中 (如果该表已经存在先删除！)

分别敲回车执行上述代码后，结果应如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317793932562779.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317794043393770.png)

上述的显示只能说明代码没有语法错误，但是否执行成功，需要使用以下代码验证：

```
select * from answer2;复制代码
```

敲回车，执行结果应如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317794169459036.png)

> 说明：上述结果集显示为两列，第一列为统计排名前10的电影名称，第二列是电影的点评次数。

##### 1.2、需求02：分别统计男性，女性当中评分最高的10部电影

**需求：**要求分别求男性，女性当中评分最高的10部电影，要求显示性别，电影名，影评平均得分，被评次数 四列并将结果分别保存至表 answer3_F 和 answer3_M 中。

**1）分析思路**

- 分析思路:

  要求显示的字段为：

  - 性别　　`t_user.sex` ，保存至结果表中为 `sex`
  - 电影名　`t_movie.moviename` , 保存至结果表中为 `name`
  - 影评平均得分　`t_rating.rate` , 保存至结果表中为 `avgrate`
  - 被评次数：统计出来的数据，保存至结果表中为 `total`

- 核心SQL：三表联合查询，按照性别过滤条件，电影名作为分组条件，影评分作为排序条件进行查询

**2）SQL实现**

首先，统计出女性当中评分最高的10部电影（性别，电影名，影评分）评论次数大于等于50次的电影信息。继续

在hive客户端中输入以下SQL代码：

```
drop table  if exists answer3_F;
create table answer3_F as 
select "F" as sex, c.moviename as name, avg(a.rate) as avgrate, count(c.moviename) as total  from t_rating a 
join t_user b on a.userid=b.userid 
join t_movie c on a.movieid=c.movieid 
where b.sex="F" 
group by c.moviename 
having total >= 50
order by avgrate desc 
limit 10;复制代码
```

分别敲回车执行，结果如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317794323155201.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317794419769566.png)

上述的显示只能说明SQL代码没有语法错误，但是否执行成功，需要使用以下代码验证：

```
select * from answer3_F;复制代码
```

敲回车，执行结果应如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317794513699782.png)

> 说明：上述查询涉及三表联合查询，结果集显示为四列，第一列为性别，第二列是电影的名称，第三列为该电影的女性平均评分，最后一列为该电影被评价次数。

然后，统计出男性当中评分最高的10部电影（性别，电影名，影评分）评论次数大于等于50次的电影信息。继续

在hive客户端中输入以下SQL代码：

```
drop table  if exists answer3_M;
create table answer3_M as 
select "M" as sex, c.moviename as name, avg(a.rate) as avgrate, count(c.moviename) as total  from t_rating a 
join t_user b on a.userid=b.userid 
join t_movie c on a.movieid=c.movieid 
where b.sex="M" 
group by c.moviename 
having total >= 50
order by avgrate desc 
limit 10;复制代码
```

分别敲回车执行，结果如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317794614481413.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317794710993219.png)

上述的显示只能说明上述SQL代码没有语法错误，但是否执行成功，需要使用以下代码验证：

```
select * from answer3_M;复制代码
```

敲回车，执行结果应如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317794842908926.png)

> 说明：上述查询涉及三表联合查询，结果集显示为四列，第一列为性别，第二列是电影的名称，第三列为该电影的男性平均评分，最后一列为该电影被评价次数。

##### 1.3、需求03：统计某部电影的各年龄段的平均影评

**需求：**要求统计出 movieid = 2116这部电影各年龄段（因为年龄就只有7个，就按这个7个分就好了）的平均影

评，要求显示年龄段及影评分（平均），并将结果导出至 answer4 中。

**1）分析思路**

- 需求字段
  - 年龄段　　`t_user.age` 导出结果表中的字段为 age
  - 影评分(平均)　`t_rating.rate` 导出结果表中的字段为 avgrate
- 核心SQL：t_user和t_rating表进行联合查询，用movieid=2116作为过滤条件，用年龄段作为分组条件

**2）SQL实现**

在 hive 客户端中编写如下代码：

```
drop table  if exists answer4;
create table answer4 as 
select a.age as age, avg(b.rate) as avgrate 
from t_user a join t_rating b on a.userid=b.userid 
where b.movieid=2116 
group by a.age;
select * from answer4;复制代码
```

分别敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317794995138642.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317795071258832.png)

上述的显示只能说明上述SQL代码没有语法错误，但是否执行成功，需要使用以下代码验证：

```
select * from answer4;复制代码
```

敲回车，执行结果应如下图所示：

![image.png](https://cdn.atstudy.com/lab/manual/16317795161409217.png)

> 说明：上述查询涉及二表联合查询，结果集显示为四列，第一列代表年龄段，第二列代表影评分。

##### 1.4、需求04：分析最喜欢看电影的那位女性对其他的电影评分

**需求：**要求统计分析最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分，要求显示观影者，电影名，影评分，并将统计结果导出至 answer5_C 表中。

**1）分析思路**

- 需求字段
  - 观影者　`t_rating.userid`
  - 电影名　`t_movie.moviename`
  - 影评分　`t_rating.rate`
- 核心SQL：

A、需要先求出最喜欢看电影的那位女性。需要查询的字段：性别：`t_user.sex` 以及观影次数：`count(t_rating.userid)`

B、根据A中求出的女性userid作为where过滤条件，以看过的电影的影评分rate作为排序条件进行排序，求出评分最高的10部电影需要查询的字段：电影的ID： `t_rating.movieid`

C、求出B中10部电影的平均影评分

需要查询的字段：电影的ID： answer5_B.movieid 及 影评分：`t_rating.rate`

**2）SQL实现**

A、需要先求出最喜欢看电影的那位女性。在Hive客户端中继续编写代码：

```
select a.userid, count(a.userid) as total 
from t_rating a join t_user b on a.userid = b.userid 
where b.sex="F" 
group by a.userid 
order by total desc 
limit 1;复制代码
```

敲回车，执行代码结果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317795281447481.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317795386394079.png)

> 说明：从上述的查询结果中可以看出：编号为1150的女性用户是影评次数最多的用户。

B、接下来，根据A中求出的女性userid作为where过滤条件，以看过的电影的影评分rate作为排序条件进行排序，求出评分最高的10部电影。继续编写SQL如下：

```
drop table  if exists answer5_B;
create table answer5_B as 
select a.movieid as movieid, a.rate as rate  
from t_rating a 
where a.userid=1150 
order by rate desc 
limit 10;复制代码
```

> 说明：上述代码是将查询的结果同时保存到answer5_B表中！

分别敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317795487194601.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317795578693616.png)

查看上述查询生成的结果表：

```
select * from answer5_B;复制代码
```

敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317795671627257.png)

> 说明：上述显示的即为该用户评分排名前10的电影评分。

C、求出B中10部电影的平均影评分，因为需要同时显示一些其他字段，所以用到了多表关联查询 ，同时将查询结果保存至 answer5_C 表中。继续编写SQL如下：

```
drop table  if exists answer5_C;
create table answer5_C as 
select b.movieid as movieid, c.moviename as moviename, avg(b.rate) as avgrate 
from answer5_B a 
join t_rating b on a.movieid=b.movieid 
join t_movie c on b.movieid=c.movieid 
group by b.movieid,c.moviename;复制代码
```

分别敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317795789234755.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317795884149853.png)

可以查看导出的结果表 answer5_C 数据：

```
select * from answer5_C;复制代码
```

敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317796003538465.png)

> 说明：上述查询的结果即为最喜欢看电影（影评次数最多）的那位女性评最高分的10部电影的平均影评分。涉及三表联合查询，结果集显示为三列，分别是：影片ID，影片名称和影片平均评分。

##### 1.5、需求05：分析好片最多的所在的那个年份最好看的10部电影

**需求：** 要求统计分析出好片（评分>=4.0）最多的那个年份的最好看的10部电影有哪些？

**1）分析思路**

- 需求字段
  - 电影 `id　t_rating.movieid`
  - 电影名　`t_movie.moviename`（包含年份）
  - 影评分　`t_rating.rate`
  - 上映年份　`xxx.years`
- 核心SQL：

分A、B、C 三步完成：

A、需要将t_rating和t_movie表进行联合查询，将电影名当中的上映年份截取出来，保存到临时表answer6_A中

需要查询的字段：电影 `id　t_rating.movieid`

电影名　`t_movie.moviename`（包含年份）

影评分　`t_rating.rate`

B、从answer6_A按照年份进行分组条件，按照评分>=4.0作为where过滤条件，按照count(years)作为排序条件进行查询

需要查询的字段：电影的ID：`answer6_A.years`

C、从answer6_A按照years=1998作为where过滤条件，按照评分作为排序条件进行查询

需要查询的字段：电影的ID：`answer6_A.moviename`

影评分：`answer6_A.avgrate`

**2）SQL实现**

A、需要将t_rating和t_movie表进行联合查询，将电影名当中的上映年份截取出来，并将结果保存至answer6_A中。继续在Hive客户端编写SQL代码如下:

```
drop table  if exists answer6_A;
create table answer6_A as
select  a.movieid as movieid, a.moviename as moviename, substr(a.moviename,-5,4) as years, avg(b.rate) as avgrate
from t_movie a join t_rating b on a.movieid=b.movieid 
group by a.movieid, a.moviename;复制代码
```

> 说明：substr函数的作用是用于将电影名中的上映年份截取出来，substr(a.moviename,-5,4)的意思就是将该电影从倒数第5个字符开始，取4个字符，作为年份使用，如：原始电影名为：Jumanji (1995) ，substr('Jumanji (1995)',-5,4) = '1995'

分别敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317796140033888.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317796239579795.png)

可以通过以下代码查看执行结果：

```
select * from answer6_A limit 5;复制代码
```

敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317796353098066.png)

B、从answer6_A按照年份进行分组条件，按照评分>=4.0作为where过滤条件，按照count(years)作为排序条件进行查询，继续在Hive客户端编写SQL代码如下:

```
select years, count(years) as total 
from answer6_A a 
where avgrate >= 4.0 
group by years 
order by total desc 
limit 1;复制代码
```

敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317796473382747.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317796611943506.png)

> 说明：从上面结果可以看出，好片最多的那年份1998年，好片（评分>=4.0）总数有27个!

C、从answer6_A按照years=1998作为where过滤条件，按照评分作为排序条件进行查询。并将最终结果导出至answer6_C表中。继续在Hive客户端编写代码如下：

```
drop table  if exists answer6_C;
create table answer6_C as
select a.moviename as name, a.avgrate as rate 
from answer6_A a 
where a.years=1998 
order by rate desc 
limit 10;复制代码
```

分别敲回车，执行代码如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317796769183059.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631779685718219.png)

可以通过如下代码查看执行结果集：

```
select * from answer6_C;复制代码
```

敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317796952351050.png)

> 说明：上述执行结果即为评分>=4.0最多的那个年份的最好看的10部电影。其中第一列显示的是电影名，第二列是该电影的评分！

##### 1.6、需求06：分析指定某年上映的电影中，评分最高的10部Comedy类电影

**需求：** 统计1997年上映的电影中评分最高的10部Comedy（喜剧）类电影有哪些 ？

**1）分析思路**

- 需求字段
  - 电影id　`t_rating.movieid`
  - 电影名　`t_movie.moviename`（包含年份）
  - 影评分　`t_rating.rate`
  - 上映年份　`xxx.years`（最终查询结果可不显示）
  - 电影类型　`xxx.type`（最终查询结果可不显示）
- 核心SQL：

A、需要电影类型，所有可以将在上一个需求中导出的 answer6_A 表和 t_movie 表进行联合查询

需要查询的字段：电影id　`answer6_A.movieid`

电影名　`answer6_A.moviename`

影评分　`answer6_A.rate`

电影类型　`t_movie.movietype`

上映年份　`answer6_A.years`

B、从answer7_A按照电影类型中是否包含Comedy和按上映年份作为where过滤条件，按照评分作为排序条件进行查询，将结果保存到answer7_B中

需要查询的字段：电影的ID：`answer7_A.id`

电影的名称：`answer7_A.name`

电影的评分：`answer7_A.rate`

**2）SQL实现**

A、需要电影类型，所有可以将第六步中求出answer6_A表和t_movie表进行联合查询，并将查询的结果集导出至answer7_A 表中，继续在 Hive 客户端编写SQL代码：

```
drop table  if exists answer7_A;
create table answer7_A as 
select b.movieid as id, b.moviename as name, b.years as years, b.avgrate as rate, a.movietype as type 
from t_movie a join answer6_A b on a.movieid=b.movieid;复制代码
```

分别敲回车，执行代码如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317797192576975.png)

可以通过如下代码查看导出的数据：

```
select * from answer7_A limit 5;复制代码
```

敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317797301774760.png)

B、从answer7_A按照电影类型中是否包含Comedy和按照评分>=4.0作为where过滤条件，按照评分作为排序条件进行查询，将结果保存到answer7_B中。继续在Hive客户端中编写SQL代码：

```
drop table  if exists answer7_B;
create table answer7_B as 
select t.id as id, t.name as name, t.rate as rate 
from answer7_A t 
where t.years=1997 and instr(lcase(t.type),'comedy') >0 
order by rate desc
limit 10;复制代码
```

分别敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317797402783928.png)

![image.png](https://cdn.atstudy.com/lab/manual/1631779750474884.png)

可以通过如下代码查看导出的结果集：

```
select * from answer7_B;复制代码
```

敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317797632198804.png)

> 说明：上述执行结果即为统计1997年上映的电影中评分最高的10部Comedy（喜剧）类电影有哪些 。其中第一列是电影ID，第二列显示的是电影名，第二列是该电影的评分！

##### 1.7、需求07：分析评价最高的5部电影

**需求：** 分析该影评库中 **各种类型** 电影中评价最高的5部电影，要求显示电影类型，电影名及平均影评分。

**1）分析思路**

- 需求字段:
  - 电影id　`movieid`
  - 电影名　`moviename`
  - 影评分　`rate`（排序条件）
  - 电影类型　`type`（分组条件）
- 核心SQL：

A、需要电影类型，所有需要将answer7_A中的type字段进行裂变，将结果保存到answer8_A中

需要查询的字段：

电影 `id　answer7_A.id`

电影名　`answer7_A.name`（包含年份）

上映年份　`answer7_A.years`

影评分　`answer7_A.rate`

电影类型　`answer7_A.movietype`

B、求TopN，按照type分组，需要添加一列来记录每组的顺序，将结果保存到answer8_B中

`row_number()` ：用来生成 num字段的值

`distribute by movietype` ：按照type进行分组

`sort by avgrate desc` ：每组数据按照rate排降序

`num`：新列， 值就是每一条记录在每一组中按照排序规则计算出来的排序值

C、从answer8_B中取出num列序号<=5的

**2）SQL实现**

A、需要电影类型，所有需要将answer7_A中的type字段进行裂变，将结果保存到answer8_A中，在Hive客户端命令行输入SQL代码如下：

```
drop table  if exists answer8_A;
create table answer8_A as 
select a.id as id, a.name as name, a.years as years, a.rate as rate, tv.type as type 
from answer7_A a 
lateral view explode(split(a.type,"\\|")) tv as type;复制代码
```

> 特别说明：
>
> 1）explode与lateral view在关系型数据库中本身是不该出现的，因为他的出现本身就是在操作不满足第一范式的数据（每个属性都不可再分），本身已经违背了数据库的设计原理（不论是业务系统还是数据仓库系统），不过大数据技术普及后，很多类似pv，uv的数据，在业务系统中是存贮在非关系型数据库中，如本项目中电影表（t_movie）的最后一个字段：movietype 表示的是电影类型（因为一个电影可能被打上多种类型的标签）的数据格式为：Adventure|Children's|Fantasy （不同类型间用|分隔）
>
> 2）如果直接将这种数据导入hive为基础的数仓系统中，就需要经过ETL过程解析这类数据，explode与lateral view在这种场景下大显身手。
>
> 3）explode就是将hive一行中复杂的array或者map结构拆分成多行。
>
> 4）lateral view用于和split, explode等UDTF一起使用，它能够将一行数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。lateral view首先为原始表的每行调用UDTF，UDTF会把一行拆分成一或者多行，lateral view再把结果组合，产生一个支持别名表的虚拟表。
>
> 5）了解更多：https://blog.csdn.net/yahahassr/article/details/97911676

分别敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317797798049278.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317797900434665.png)

可以通过如下SQL代码查看导出的数据集：

```
select * from answer8_A limit 5;复制代码
```

敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317797992406719.png)

> 注意上述结果集中的最后一列是电影类型！

B、求TopN，按照type分组，需要添加一列来记录每组的顺序，将结果保存到answer8_B中，继续在Hive中编写SQL代码如下：

```
drop table  if exists answer8_B;
create table answer8_B as 
select id,name,years,rate,type,row_number() over(distribute by type sort by rate desc ) as num
from answer8_A;复制代码
```

> 注意：
>
> 1）总共产生六列，分别为：电影ID，电影名称，电影所在年份，电影评分，电影类型及 该电影在该类电影中的评分排名。
>
> 2）ROW_NUMBER() OVER(PARTITION BY COLUMN1 ORDER BY COLUMN2) 的含义是首先根据COLUMN1进行结果集分组，结果集内部按照COLUMN2分组，输出结果是类似于双重分组的结果。
>
> 3）row_number() over(distribute by type sort by rate desc ) as num 的意思是产生一个新的列名叫num ,该列使用了相当于MySQL8中的窗口函数row_number() 函数自动产生排名序号，同时也用来解决排名相同的问题（如两个排名相同的如何处理）
>
> 4）distribute by type sort by rate desc 的意思是：对type进行分组，然后再同一分组种再按rate进行降序排列
>
> 5）更多hive的row_number()函数用法请参考：https://blog.csdn.net/javajxz008/article/details/53493509

分别敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/1631779847380885.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317798551594372.png) 可以通过如下代码查看导出的数据集：

```
select * from answer8_B limit 5;复制代码
```

敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317798659665830.png)

C、从answer8_B中取出num列序号<=5的记录，即为该影评库中各种类型电影中评价最高的5部电影。

Hive中的SQL代码如下:

```
select a.* from answer8_B a where a.num <= 5;复制代码
```

敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317798760387819.png)

> 说明：从上述的部分截图可以看出，查询出来了每个分类中评分排名前5的电影评价信息。

##### 1.8、需求08：统计分析出各年评分最高的电影类型（年份，类型，影评分)

**需求：** 统计分析出各年评分最高的电影类型，要求显示电影年份，电影类型和影评分。

**1）分析思路**

- 需求字段:
  - 电影 `id　movieid`
  - 电影名　`moviename`
  - 影评分　`rate`（排序条件）
  - 电影类型　`type`（分组条件）
  - 上映年份　`years`（分组条件）
- 核心SQL：

A、需要按照电影类型和上映年份进行分组，按照影评分进行排序，将结果保存到answer9_A中

需要查询的字段：

上映年份　`answer7_A.years`

影评分　`answer7_A.rate`

电影类型　`answer7_A.movietype`

B、求TopN，按照years分组，需要添加一列来记录每组的顺序，将结果保存到answer9_B中

C、按照num=1作为where过滤条件取出结果数据

**2）SQL实现**

A、需要按照电影类型和上映年份进行分组，按照影评分进行排序，将结果保存到answer9_A中。在上一步打开的Hive客户端中输入SQL如下：

```
drop table  if exists answer9_A;
create table answer9_A as 
select a.years as years, a.type as type, avg(a.rate) as rate 
from answer8_A a 
group by a.years,a.type 
order by rate desc;复制代码
```

分别敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317798890744816.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317798974462743.png)

可以通过如下SQL代码查看刚刚导出的数据：

```
select * from answer9_A limit 5;复制代码
```

敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317799070386945.png)

B、求TopN，按照years分组，需要添加一列来记录每组的顺序，将结果保存到answer9_B中。继续在Hive客户端命令行中输入SQL：

```
drop table  if exists answer9_B;
create table answer9_B as 
select years,type,rate,row_number() over (distribute by years sort by rate) as num
from answer9_A;复制代码
```

> 说明：
>
> 1）row_number() over(distribute by years sort by rate ) as num 的意思是产生一个新的列名叫num ,该列使用了相当于MySQL8中的窗口函数row_number() 函数自动产生排名序号，同时也用来解决排名相同的问题（如两个排名相同的如何处理），distribute by years sort by rate 的意思是：先按years进行分组，然后再同一分组种再按rate进行升序排列

分别敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317799177891604.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317799274457906.png)

可以通过如下SQL语句查看刚刚导出的数据：

```
select * from answer9_B limit 10;复制代码
```

敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317799368615780.png)

因数据量比较大，在这里仅列出前10条， 以了解基本的数据结构和内容 。

C、按照num=1作为where过滤条件取出结果数据，查看最终的统计分析结果。继续在Hive中输入代码如下：

```
select * from answer9_B where num=1;复制代码
```

敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317799477829705.png)

> 说明：以上结果即为： 统计分析出各年评分最高的电影类型，显示的字段分别为：电影年份，电影类型和影评分以及他们所在各年的排名

##### 1.9、需求09：分析出每个地区最高评分的电影名并把结果存入HDFS

**需求：**要求统计分析出每个地区最高评分的电影名并把结果存入HDFS，要求保存的字段有：地区，电影名，影评分。

**1）分析思路**

- 需求字段:
  - 电影id　`t_movie.movieid`
  - 电影名　`t_movie.moviename`
  - 影评分　`t_rating.rate`（排序条件）
  - 地区　`t_user.zipcode`（分组条件）
- 核心SQL：

A、需要把三张表进行联合查询，取出电影id、电影名称、影评分、地区，将结果保存到answer10_A表中

需要查询的字段：电影 `id　t_movie.movieid`

电影名　`t_movie.moviename`

影评分　`t_rating.rate`（排序条件）

地区　`t_user.zipcode`（分组条件）

B、求TopN，按照地区分组，按照平均排序，添加一列num用来记录地区排名，将结果保存到answer10_B表中

C、按照num=1作为where过滤条件取出结果数据

**2）SQL实现**

A、需要把三张表进行联合查询，取出电影id、电影名称、影评分、地区，将结果保存到answer10_A表中。在Hive客户端中编写SQL语句如下：

```
drop table  if exists answer10_A;
create table answer10_A as
select c.movieid, c.moviename, avg(b.rate) as avgrate, a.zipcode
from t_user a 
join t_rating b on a.userid=b.userid 
join t_movie c on b.movieid=c.movieid 
group by a.zipcode,c.movieid, c.moviename;复制代码
```

分别敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317799639067395.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317799716418242.png)

可以通过如下SQL查看导出的数据：

```
select * from answer10_A limit 5;复制代码
```

敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317802996336635.png)

> 说明：导出的answer10_A数据集中的最后一列是邮政编码，可以用它代表不同地区！

B、求TopN，按照地区分组，按照平均排序，添加一列num用来记录地区排名，将结果保存到answer10_B表中。继续在Hive中编写SQL代码 ：

```
drop table  if exists answer10_B;
create table answer10_B as
select movieid,moviename,avgrate,zipcode, row_number() over (distribute by zipcode sort by avgrate) as num 
from answer10_A; 复制代码
```

> 说明：
>
> 1）row_number() over (distribute by zipcode sort by avgrate) as num 的意思是产生一个新的列名叫num ,该列使用了相当于MySQL8中的窗口函数row_number() 函数自动产生排名序号，同时也用来解决排名相同的问题（如两个排名相同的如何处理），distribute by zipcode sort by avgrate 的意思是：先按 zipcode 进行分组，然后再同一分组种再按 avgrate 进行升序排列

分别敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317803110266070.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317803194183705.png)

可以通过如下代码查看导出的数据集：

```
select t.* from answer10_B t limit 10;复制代码
```

敲回车查看执行结果：

![image.png](https://cdn.atstudy.com/lab/manual/16317803305185501.png)

> 因为数据集较大，上述只显示了10条记录，从上图中可以看到00231地区的电影评分排名情况（其他地区的这10条记录里面没有体现）。

C、按照num=1作为where过滤条件取出结果数据并保存到HDFS上。继续在Hive客户端命令行中输入如下代码 ：

```
insert overwrite directory "/movie/answer10/" select t.* from answer10_B t where t.num=1;复制代码
```

敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317803445264168.png)

![image.png](https://cdn.atstudy.com/lab/manual/16317803529161475.png)

> 以上代码将每个地区最高评分的电影信息保存到了HDFS的 /movie/answer10/ 路径下，稍后可以在任务4中进行查看。

至此为止，任务4 的内容告一段落，可以退出hive了，在上述hive命令行中输入以下命令退出hive :

```
exit;复制代码
```

敲回车，执行结果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317803620276732.png)

### 任务2：实验结果验证

#### 【任务目标】

 Hive的结果表其实就是HDFS上的文件，如果能够得到结果表，对应的文件也一定在HDFS上能够看见。 本任务目标为通过查看HDFS系统上的目录及文件再一次验证Hive之前SQL操作是否已经成功。

#### 【任务步骤】

打开浏览器，访问一下网址：`http://你的Linux虚拟机IP:50070`结果文件已经保存在HDFS上了，具体步骤如下：

首先回到沙箱桌面：

![image.png](https://cdn.atstudy.com/lab/manual/16317803717485777.png)

在上述界面的桌面空白处击右键，在弹出的右键菜单中选择`在此打开终端`

![image.png](https://cdn.atstudy.com/lab/manual/16317803804243217.png)

在弹出的Linux命令行终端中，输入命令如下：

```
ifconfig复制代码
```

敲回车，运行效果如下：

![image.png](https://cdn.atstudy.com/lab/manual/16317803902378649.png)

回到桌面，打开浏览器：

![image.png](https://cdn.atstudy.com/lab/manual/16317803991457260.png)

在打开的浏览器中输入如下网址：`http://192.168.60.2:50070`,回车，将会出现如下界面：

> 提示：上面的 url 地址中的 IP一定要与你上面ifconfig命令中的IP地址保持一致，否则将会无法正常访问！

![image.png](https://cdn.atstudy.com/lab/manual/16317804104171133.png)

在上述界面中，选择Utilities菜单下的`Browse the file system` 子菜单，进入Browse Directory界面：

![image.png](https://cdn.atstudy.com/lab/manual/16317804214347326.png)

在上述界面中进入到`/movie/answer10` 目录中，如下图所示 ，该目录中的文件中存储的即为需求09中导出的统计分析数据：

![image.png](https://cdn.atstudy.com/lab/manual/16317804305081101.png)

在上面界面中点击 文件 `00000_0` , 在下面弹出的界面选择 `download` 可以将文件下载到本地沙箱中：

![image.png](https://cdn.atstudy.com/lab/manual/16317804406294263.png)

点击Close，关闭此界面，回到系统沙箱的Linux终端命令行界面中，在命令中输入如下命令来查看刚刚下载的文件内容的前10行：

```
head /home/Downloads/000000_0复制代码
```

敲回车执行：

![image.png](https://cdn.atstudy.com/lab/manual/16317804505568250.png)

> 说明 ：从上图中可以看出，该文件确实保存的是需求09统计出来的数据，每行由四列组成：电影ID,电影名称，电影评分，所在地区及排名，只不过HDFS中各列之间是没有换行的，这一点要特别注意！

继续回到如下界面：

![image.png](https://cdn.atstudy.com/lab/manual/16317804674919367.png)

在以上界面的输入框架中输入 `/user/hive/warehouse/movie.db` 路径，点击 `Go!` 按钮，可以查看该目录下的系列文件：

![image.png](https://cdn.atstudy.com/lab/manual/16317804790487540.png)

从上面的界面中，可以看到刚刚实验创建生成的表的数据实质上是以文件的方式存在于HDFS系统中对应的 ` /user/hive/warehouse/数据名称.db` 目录中的。

## 实验总结

 电影大数据分析项目通过在大数据环境中利用Hive对真实影评数据的评价次数，评价分数，观影年龄，性别，影片上映时间，影片类型 等不同维度的Hive SQL分析。训练了学员对于Hadoop+Hive的离线数据获取，导入、清洗、处理与统计分析及数据导出全流程技术点。从而达到了解商品评论类分析的常见业务需求与分析指标。熟练掌握在企业中如何使用Hive利用HQL语句结合业务进行复杂数据分析的目的。通过本次实训演练可以达到让学员了解如何在Hive中导入数据并利用Hive进行数据清洗，了解商品评论类分析的常见业务需求与分析指标、能够熟练掌握企业中如何使用Hive中的HQL结合业务进行复杂数据分析的目的。



