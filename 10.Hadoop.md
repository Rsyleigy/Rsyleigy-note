# hadoop

# Hadoop3.1.1概述与分而治之思想

# Hadoop的发展史

![image-20230814085556303](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20230814085556303.png)

> Google
>
> ​	爬取全球的网站，然后计算页面的PageRank
>
> ​    要解决网站的问题：
>
> ​			a：这些网站怎么存放
>
> ​            b：这些网站应该怎么计算
>
> ​	发布了三篇论文
>
> ​			a：GFS(Google File System)
>
> ​			b：MapReduce(数据计算方法)
>
> ​			c：BigTable-->HBase
>
> ​	Doug cutting 花费了自己的两年业余时间，将论文实现了出来。看到他儿子在牙牙学语时，抱着黄色小象，亲昵的叫 hadoop，他灵光一闪，就把这技术命名为 Hadoop，而且还用了黄色小象作为标示 Logo，不过，事实上的小象瘦瘦长长，不像 Logo 上呈现的那么圆胖。“我儿子现在 17 岁了，所以就把小象给我了，有活动时就带着小象出席，没活动时，小象就丢在家里放袜子的抽屉里。” Doug Cutting 大笑着说。
> ​	**Hadoop(java)**
>
> ​					**HDFS**
>
> ​					**MapReduce**
>
> ![image-20230813211345860](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20230813211345860.png)
>
> > Hadoop三大开源发行版本：Apache、Cloudera（CDH）、Hortonworks（HDP）。Apache版本最原始（最基础）的版本，对于入门学习最好。Cloudera在大型互联网企业中用的较多。Hortonworks文档较好。
>
> > ​	**Apache Hadoop**
> >
> > ​		官网地址：https://hadoop.apache.org/
> >
> > ​		下载地址：https://hadoop.apache.org/release.html
> >
> > ​	**Cloudera Hadoop**
> >
> > ​		官网地址：https://www.cloudera.com/downloads/cdh.html
> >
> > ​		下载地址：https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_cdh_6_download.html
> >
> > ​		1）2008年成立的Cloudera是最早将Hadoop商用的公司，为合作伙伴提供Hadoop的商用解决方案，主要是包括支持、咨询服务、培训。
> >
> > ​		2）2009年Hadoop的创始人Doug Cutting也加盟Cloudera公司。Cloudera产品主要为CDH，Cloudera Manager，Cloudera Support
> >
> > ​		3）CDH是Cloudera的Hadoop发行版，完全开源，比Apache Hadoop在兼容性，安全性，稳定性上有所增强。Cloudera的标价为每年每个节点**10000美元**。
> >
> > ​		4）Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。
> >
> > ​		5）Cloudera的标价为每年每个节点4000美元。Cloudera开发并贡献了可实时处理大数据的Impala项目。
> >
> > ​	**Hortonworks Hadoop**
> >
> > ​		官网地址：https://www.clouderacn.cn/products/hdp.html
> >
> > ​		下载地址：https://www.cloudera.com/downloads/hdp.html
> >
> > ​		1）2011年成立的Hortonworks是雅虎与硅谷风投公司Benchmark Capital合资组建。
> >
> > ​		2）公司成立之初就吸纳了大约25名至30名专门研究Hadoop的雅虎工程师，上述工程师均在2005年开始协助雅虎开发Hadoop，贡献了Hadoop80%的代码。
> >
> > ​		3）雅虎工程副总裁、雅虎Hadoop开发团队负责人Eric Baldeschwieler出任Hortonworks的首席执行官。
> >
> > ​		4）Hortonworks的主打产品是Hortonworks Data Platform（HDP），也同样是100%开源的产品，HDP除常见的项目外还包括了Ambari，一款开源的安装和管理系统。
> >
> > ​		5）HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook开源的Hive中。Hortonworks的Stinger开创性的极大的优化了Hive项目。Hortonworks为入门提供了一个非常好的，易于使用的沙盒。
> >
> > ​		6）Hortonworks开发了很多增强特性并提交至核心主干，这使得Apache Hadoop能够在包括Window Server和Windows Azure在内的microsoft Windows平台上本地运行。定价以集群为基础，每10个节点每年为12500美元。
> >
> > ​	**2018年10月，均为开源平台的Cloudera与Hortonworks公司宣布他们以52亿美元的价格合并。**
> >
> > ​		官方网站：https://www.cloudera.com/
> >
> > ​		Cloudera 官方文档： https://www.cloudera.com/documentation/enterprise/latest.html
> >
> > ​		两家公司称合并后公司将拥有2500客户、7.2亿美元收入和5亿美元现金，且没有债务，宣布了它们所谓了相对平等的合并。
> >
> > ​		两大开源大数据平台Cloudera与Hortonworks宣布合并，合并后的企业定位为企业数据云提供商，推出了ClouderaDataPlatform（CDP），可以跨AWS、Azure、Google等主要公有云架构进行数据管理。2020年6月，Cloudera发布CDP私有云，将本地部署环境无缝连接至公有云。
> >
> > ​		CDP的版本号延续了之前CDH的版本号，从7.0开始，目前最新的版本号为7.0.3.0。
> >
> > ​		那么CDP对比之前的Cloudera Enterprise Data Hub（CDH企业版）与HDPEnterprise Plus（HDP企业版）到底在组件上发生了哪些变化呢？
> >
> > ​		由于HDP在国内市场上的市场占有量很小，大部分公司都是采用CDH，所以对于HDP带来的一些东西，使用CDH的用户和开发人员会比较陌生，下面带大家详细的了解一下CDP中的组件一些变化，也方便大家为在2022年以及之后的学习做好准备。
> >
> > ​		CDP、CDH、HDP中都包含的部分
> >
> > ```
> > Apache Hadoop（HDFS/YARN/MR）
> > 
> > Apache HBase
> > 
> > Apache Hive
> > 
> > Apache Oozie
> > 
> > Apache Spark
> > 
> > Apache Sqoop
> > 
> > Apache Zookeeper
> > 
> > Apache Parquet
> > 
> > Apache Phoenix（*CDH中需要额外安装）
> > ```
> >
> > ​		**最关键的一点：CDP的组件代码在github上找不到，是不再开源了，CDP7以后就没有社区版了。**
>
> # Hadoop 1.x ---> 3.x
>
> ![image-20220516214707299](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220516214707299.png)
>
> > 官网：https://hadoop.apache.org/
>
> 注意：课程中的hadoop版本以CDH版本为准，稳定且主流，目前国内主流的是2.x，如果面试的时候说你用的是3.x可能会被。。。。并且2.x的变化都不大
>
> **Hadoop Common**：基础型功能
>
> **Hadoop Distributed File System (HDFS™)**：一种分布式文件系统，可提供对应用程序数据的高吞吐量访问。负责存放数据
>
> **Hadoop YARN**：作业调度和集群资源管理的框架。负责资源的调配
>
> **Hadoop MapReduce**：基于 YARN 的系统，用于并行处理大型数据集。大数据的计算框架
>
> Hadoop框架透明地为应⽤提供可靠性和数据移动。它实现了名为MapReduce的编程范式：应⽤程序被分割成许多⼩部分，⽽每个部分都能在集群中的任意节点上执⾏或重新执⾏。此外，Hadoop还提供了分布式⽂件系统，⽤以存储所有计算节点的数据，这为整个集群带来了⾮常⾼的带宽。MapReduce和分布式⽂件系统的设计，使得整个框架能够⾃动处理节点故障。它使应⽤程序与成千上万的独⽴计算的电脑和PB级的数据。
>
> **一句话简述：Hadoop是一个适合海量数据的分布式存储和分布式计算的平台。（面试必问！！！！）**
>
> 国内现在大数据的行情
>
> ​	1）大数据的薪资属于同行业最高的
>
> ​	2）相对来讲这个技术比较稳定，Hadoop只是大数据的一员或者说是基石，大数据开发环境已经稳定。
>
> （**说到这的时候，带同学画一下目前主流的框架图，熟悉主题学习框架和功能,画图总结上面的概念**）
>
> # 分布式文件系统架构
>
> ### 一、名词科普
>
> >**Apache基金会(ASF)**
> >		是专门为支持开源软件项目而办的一个非盈利性组织
> >**服务器(节点)**:	企业里任务和程序基本都是运行在服务器上。服务器内存和cpu以及硬盘等资源和性能远高于pc机
> >		可以理解为我们的一台笔记本/台式机
> >		在这里可以认为是我们的一台虚拟机
> >		后面学习中，我们会把一台服务器称为一个节点
> >
> >![image-20220516220501671](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220516220501671.png)
> >
> >**机架:** 一个公司里，会有很多服务器。尤其是hadoop集群大到上千台服务器搭建成集群
> >		负责存放服务器的架子
> >		可以理解为鞋架(^_^)
> >
> >![image-20220516220536829](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220516220536829.png)
>
> ### 二、分布式文件系统（从这开始，下面都是属于Hadoop中的原理）
>
> #### 1、FS File System
>
> ​		文件系统时极域硬盘之上的文件管理的工具
>
> ​		我们用户操作文件系统可以和硬盘进行解耦
>
> #### 2、DFS Distributed File System
>
> ​		分布式文件系统
>
> ​		将我们的数据存放在多台电脑上存储
>
> ​		分布式文件系统有很多,HDFS(Hadoop Distributed FileSyetem)是Hadoop自带的分布式文件系统
>
> ​		HDFS是mapreduce计算的基础
>
> ### 三、文件切分的思想（引出分而治之的思想  第一个核心思想）
>
> ​	a. 文件存放在一个磁盘上效率肯定是最低的
>
> ​			读取效率低
>
> ​			如果文件特别大会超出单机的存储范围
>
> ​	b. 字节数组
>
> ​			文件在磁盘真实存储文件的抽象概念
>
> ​			数组可以进行拆分和组装,源文件不会收到影响
>
> ​	c. 切分数据
>
> ​			对字节数组进行切分
>
> ​	d. 拼接数据
>
> ​			按照数组的偏移量将数据连接到一起，将字节数组连接到一起
>
> ​	e. 偏移量
>
> ​			当前数据在数组中的相对位置,可以理解为下标
>
> ​			数组都有对应的索引,可以快速定位数据
>
> ​	f. **数据存储的原理:**
>
> ​			不管文件的大小,所有的文件都是由字节数组构成
>
> ​			如果我们要切分文件,就是将一个字节数组分成多份
>
> ​			我们将切分后的数据拼接到一起,数据还可以继续使用
>
> ​			我们需要根据数据的偏移量将他们重新拼接到一起
>
> ### 四、Block拆分标准
>
> **数据块Block**
>
> ​		a. 在hadoop中是磁盘进行数据 读/写的最小单位,数据被切分后的一个整体被称之为块
> ​		b. **在Hadoop 1默认大小为64M,在Hadoop 2及其之后默认大小为128M块**，这么大是为了最小化寻址开销
> ​		c. 同一个文件中,每个数据块的实际大小要一致除了最后一个节点外
> ​				不同文件中,块的大小可以不一致
> ​				文件大小不同可以设置不同的块的数量
> ​				**HDFS中小于一个块的大小的文件不会占据整个块的空间**
> ​		d. 真实情况下,会根据文件大小和集群节点的数量综合考虑块的大小
> ​		e. 数据块的个数=Ceil(文件大小/每个块的大小)
>
> **拆分的数据块需要等大(面试题)**
>
> ​		a. 数据计算的时候简化问题的复杂度(否则进行分布式算法设计的时候会因为数据量不一很难设计)
> ​        b. 数据拉取的时候时间相对一致
> ​		c. 通过偏移量就知道这个块的位置
> ​		d. 相同文件分成的数据块大小应该相等
>
> **注意事项**
>
> ​		a. 只要有任意一个块丢失,整个数据文件被损坏
> ​		b. HDFS中一旦文件被存储,数据不建议被修改
> ​				修改会影响偏移量,修改会导致数据倾斜（单节点数据量过多）,修改数据会导致蝴蝶效应
> ​		c. 但是可以被追加(一般不推荐)
> ​				追加设置需要手动打开
> ​		d. 一般HDFS存储的都是历史数据.所以将来Map Reduce都用来进行离线数据的处理
> ​		f. 块的大小一旦文件上传之后就不允许被修改    128M-512M
>
> ### 五、Block数据安全
>
> ​		a. 只要有任意一个块丢失,整个数据文件被损坏
> ​		b. 肯定要对存储数据做备份
> ​		c. HDFS是直接对原始数据进行备份的,这样能保证恢复效率和读取效率
> ​		d. 备份的数据肯定不能存放在一个节点上,使用数据的时候可以就近获取数据
> ​		f. **备份的数量要小于等于节点的数量**
> ​		g. **每个数据块默认会有三个副本**,相同副本是不会存放在同一个节点上
> ​		h. 副本的数量可以变更
> ​			可能近期数据被分析的可能性很大,副本数可以多设置几个
> ​			后期数据很少被分析,可以减少副本数
>
> ### 六、Block的管理效率(明天细说)
>
> 需要专门给节点进行分工
>
> - 存储 DataNode   实际存储数据的节点
> - 记录 NameNode
> - 日志 Secondary NameNode
>
> ![image-20220516225946842](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220516225946842.png)

## java模拟切分文件

> 切分文件

```java
package com.shujia;

import java.io.*;
import java.util.ArrayList;

public class SplitFileBlock {
    public static void main(String[] args) throws Exception {
        //将数据读取进来
        //字符缓冲输入流
        BufferedReader br = new BufferedReader(new FileReader("data/students.txt"));
        int index = 0;
        //字符缓冲输出流
        BufferedWriter bw = new BufferedWriter(new FileWriter("data/blocks2/block---" + index));

        //现在是假设一行数据是1m,没128m数据，就生成一个block块，不到128m也会生成一个block块
        //每次读到128*1.1约等于140行的数据，就写入128行，剩下的12行计入下一次block块中去存储
        //定义一个集合，用于存储，读取的内容
        ArrayList<String> row = new ArrayList<>();

        String line = null;

        //定义一个变量，记录读取的行数
        int offset = 0;
        //定义一个变量，记录读取的是哪一个block块
        int rowNum = 0;


        while ((line = br.readLine()) != null) {
            row.add(line);
            offset++;
            //当我们的偏移量，128*1.1约等于140行的数据，就写入128行，剩下的12行计入下一次block块中去存储
            if (offset == 140) {
                rowNum = 128 * index;
                //循环128次，将集合存储的数据，写入到block块中
                for (int i = rowNum; i <= rowNum + 127; i++) {
                    String s = row.get(i);
                    bw.write(s);
                    bw.newLine();
                    bw.flush();
                }
                index++;
                //将offset设置为12
                offset = 12;
                bw = new BufferedWriter(new FileWriter("data/blocks2/block---" + index));
            }
        }

        //把剩余的数据写到一个block块中
        for(int i = row.size()-offset;i<row.size();i++){
            String s = row.get(i);
            bw.write(s);
            bw.newLine();
            bw.flush();
        }

        //释放资源
        bw.close();
        br.close();




    }
}

```

> map任务

```java
package com.shujia;

import java.io.*;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;

public class MapTask implements Runnable {
    private File file;
    public int offset;


    public MapTask(File file,int offset) {
        this.file = file;
        this.offset = offset;
    }

    @Override
    public void run() {
        //字符缓冲输入流
        try {
            BufferedReader br = new BufferedReader(new FileReader(file));
            //创建一个Map集合，使用HashMap
            HashMap<String, Integer> map = new HashMap<>();
            String line = null;
            while ((line = br.readLine()) != null) {
                //用逗号进行切分
                String clazz = line.split(",")[4];

                //如果在map中没有该班级作为key，那我们就把这个班级作为key存放在集合，value设置为1
                if (!map.containsKey(clazz)) {
                    map.put(clazz, 1);
                } else {
                    //否则value值加1
                    map.put(clazz, map.get(clazz) + 1);
                }
            }
            //结束读取数据流程
            br.close();
            //将局部的map任务结果写入到文件中
            //创建字符缓冲输出流
            BufferedWriter bw = new BufferedWriter(new FileWriter("data/parts2/part---" + offset));
            //遍历HashMap
            Set<Map.Entry<String, Integer>> entries = map.entrySet();
            for (Map.Entry<String, Integer> keyValue : entries){
                String clazz = keyValue.getKey();
                Integer sumNumber = keyValue.getValue();
                //写入文件
                bw.write(clazz+":"+sumNumber);
                //换行
                bw.newLine();
                bw.flush();
            }
            //关闭写通道
            bw.close();
        } catch (Exception e) {
            e.printStackTrace();
        }

    }
}

```

> map任务执行

```java
package com.shujia;

import java.io.File;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

/*
        Map(通过线程池的方式，简单来说，模拟hadoop中一个block块生成一个map任务，一个map任务相当于一个线程)
        (将切分出来的bolck块中，统计每个班级的人数)
        4423毫秒
 */
public class Map {
    public static void main(String[] args) {

        long start = System.currentTimeMillis();
        //创建一个线程池
        ExecutorService executorService = Executors.newFixedThreadPool(10000);
        File file = new File("data/blocks2");
        //定义一个文件编号，从0开始
        int offset = 0;
        File[] files = file.listFiles();
        for (File f : files) {
            MapTask mapTask = new MapTask(f, offset);
            executorService.submit(mapTask);
            offset++;
        }

        //关闭线程池
        executorService.shutdown();
        long end = System.currentTimeMillis();
        System.out.println("总耗时："+(end - start)+"毫秒");


    }
}

```

> reduce任务

```java
package com.shujia;

import java.io.*;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;

//306毫秒 537毫秒 == 843毫秒
//4423毫秒reduce总耗时：117551毫秒
/*
    模拟hadoop的切分map-reduce处理的方式，总耗时14毫秒
        将每个map任务的结果，再做一次总的聚合，统计出最终的班级人数

    当数据量过大的时候，TB以上的级别的时候
        1、一台服务器不够存
        2、可能一台够存，但是纯java程序是由JVM虚拟机调起的，内存有限，可能会导致，OOM内存溢出

    这时候，就必须使用分布式存储，将大文件进行切分，先局部做运算，这时候局部的数据少很多，然后再总的聚合，数据量少且块！
 */
public class Reduce {
    public static void main(String[] args) throws Exception {
        long start = System.currentTimeMillis();
        //将past目录封装成File对象
        File file = new File("data/parts2");
        //获取下面的所有文件对象数组
        File[] files = file.listFiles();
        //创建一个map集合，接收总的结果数据
        HashMap<String, Integer> map = new HashMap<>();
        //遍历每个part文件
        for (File f : files) {
            //读取文件，进行分割
            //创建缓冲字符输入流对象
            BufferedReader br = new BufferedReader(new FileReader(f));
            String line = null;
            while ((line = br.readLine()) != null) {
                //以冒号进行分割得到班级和人数
                String[] strings = line.split(":");
                String clazz = strings[0];
                //包装类
                Integer sum = Integer.valueOf(strings[1]);
                //判断map集合中是否存在对应的key
                if (!map.containsKey(clazz)) {
                    map.put(clazz, sum);
                } else {
                    //如果存在，value值相加
                    map.put(clazz, map.get(clazz) + sum);
                }
            }
            //关闭读取数据的通道
            br.close();
        }

        //将结果写入到最终一个文件
        BufferedWriter bw = new BufferedWriter(new FileWriter("data/result-big/part-r-000000"));
        //遍历集合
        Set<Map.Entry<String, Integer>> entries = map.entrySet();
        for (Map.Entry<String, Integer> keyValue:entries){
            String clazz = keyValue.getKey();
            Integer number = keyValue.getValue();
            bw.write(clazz+":"+number);
            bw.newLine();
            bw.flush();
        }

        //释放资源
        bw.close();
        long end = System.currentTimeMillis();
        System.out.println("reduce总耗时："+(end-start)+"毫秒");





    }
}

```





# hadoop-3.1.1分布式搭建文档

## 1、上传解压配置环境变量

```shell
# 1、解压
tar -xvf hadoop-3.1.1.tar.gz.gz

# 2、配置环境变量
vim /etc/profile

# 3、在最后增加配置
export HADOOP_HOME=/usr/local/soft/hadoop-3.1.1
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin

# 4、使环境变量剩下
source /etc/profile
```

## 2、修改配置文件

```shell
# 1、进入hadoop配置文件所在位置，修改hadoop配置文件
cd /usr/local/soft/hadoop-3.1.1/etc/hadoop

# 2、修改core-site.xml配置文件,在configuration中间增加配置
vim core-site.xml
# 增加配置
<configuration>
<property>
  <name>fs.defaultFS</name>
  <value>hdfs://master:9000</value>
</property>
<property>
  <name>hadoop.tmp.dir</name>
  <value>/usr/local/soft/hadoop-3.1.1/tmp</value>
</property>
<property>
  <name>fs.trash.interval</name>
  <value>1440</value>
</property>
</configuration>

# 3、修改hdfs-site.xml配置文件，在configuration中间增加配置
vim hdfs-site.xml
# 增加配置
<configuration>
<property>
  <name>dfs.replication</name>
  <value>1</value>
</property>
<property>
  <name>dfs.permissions</name>
  <value>false</value>
</property>
</configuration>

# 4、修改yarn-site.xml配置文件，在configuration中间增加配置
vim yarn-site.xml
# 增加配置
<property>
  <name>yarn.resourcemanager.hostname</name>
  <value>master</value>
</property>
<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
</property>
<property>
  <name>yarn.log-aggregation-enable</name>
  <value>true</value>
</property>


> mapreduce.framework.name：用于执行MapReduce作业的运行时框架。

> mapreduce.jobhistory.address：Hadoop自带了一个历史服务器，可以通过历史服务器查看已经运行完的Mapreduce作业记录，比如用了多少个Map、用了多少个Reduce、作业提交时间、作业启动时间、作业完成时间等信息。默认情况下，Hadoop历史服务器是没有启动的，我们可以通过*mr-**jobhistory-daemon.sh start historyserver**命令来启动Hadoop历史服务器。我们可以通过Hadoop jar的命令来实现我们的程序jar包的运行，关于运行的日志，我们一般都需要通过启动一个服务来进行查看，就是我们的JobHistoryServer，我们可以启动一个进程，专门用于查看我们的任务提交的日志。mapreduce.jobhistory.address和mapreduce.jobhistory.webapp.address默认的值分别是0.0.0.0:10020和0.0.0.0:19888

vim mapred-site.xml
# 2、修改
	<property>
    	<name>mapreduce.framework.name</name>
    	<value>yarn</value>
    </property>

    <property>  
    	<name>mapreduce.jobhistory.address</name>  
    	<value>master:10020</value>  
    </property>  

    <property>  
    	<name>mapreduce.jobhistory.webapp.address</name>  
    	<value>master:19888</value>  
	</property> 



# 5、修改hadoop-env.sh配置文件
vim hadoop-env.sh
# 增加配置
export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_202.jdk/Contents/Home


# 6、修改hadoop-env.sh配置文件
vim workers
# 增加配置
node1
node2
```

#### 方法1：在Hadoop安装目录下找到sbin文件夹，修改里面的四个文件(方法1)

> 1、对于start-dfs.sh和stop-dfs.sh文件，添加下列参数：

```
#!/usr/bin/env bash
HDFS_DATANODE_USER=root
HADOOP_SECURE_DN_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root
```

> 2、对于start-yarn.sh和stop-yarn.sh文件，添加下列参数：

```
#!/usr/bin/env bash
YARN_RESOURCEMANAGER_USER=root
HADOOP_SECURE_DN_USER=yarn
YARN_NODEMANAGER_USER=root
```

#### 方法2：修改hadoop-env.sh的文件(推荐)

```
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
```

#### 将hadoop文件夹分发到字节点中

```
scp -r hadoop-3.1.1 node1:`pwd`
scp -r hadoop-3.1.1 node2:`pwd`
```





## 3、初始化hdfs

```shell
# 初始化
hdfs namenode -format
```

## 4、启动hadoop

```shell
# 启动hadoop
start-all.sh

# 停止hadoop
# stop-all.sh

# hdfs web ui
http://master:9870

# yarn web ui
http://master:8088
```

## 排查问题

```shell
#  进入日志所在目录
cd /usr/local/soft/hadoop-3.1.1/logs

# 1、namenode启动失败
cat hadoop-root-namenode-master.log

# 2、resourcemanager启动失败
cat hadoop-root-resourcemanager-master.log

# 3、datanode启动失败
cat hadoop-root-datanode-master.log
```

## 格式化集群（暴力方式）

### 1. 关闭集群

```
stop-all.sh
```

### 2. 删除每个节点的hadoop根目录下的tmp目录

```
rm -rf tmp/
```

### 3. 重新格式化  在hadoop的bin目录下执行

```
hdfs namenode -format
```

### 4. 重新启动

```
start-all.sh
```





## HDFS常用命令

> 创建文件夹

```shell
Usage: hadoop fs -mkdir [-p] <paths>

Takes path uri’s as argument and creates directories.

Options:

The -p option behavior is much like Unix mkdir -p, creating parent directories along the path.
Example:

hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2
hadoop fs -mkdir hdfs://nn1.example.com/user/hadoop/dir hdfs://nn2.example.com/user/hadoop/dir
Exit Code:

Returns 0 on success and -1 on error.
```

> 将Linux中的文件上传到HDFS文件系统中

```shell
Usage: hadoop fs -put [-f] [-p] [-l] [-d] [ - | <localsrc1> .. ]. <dst>

Copy single src, or multiple srcs from local file system to the destination file system. Also reads input from stdin and writes to destination file system if the source is set to “-”

Copying fails if the file already exists, unless the -f flag is given.

Options:

-p : Preserves access and modification times, ownership and the permissions. (assuming the permissions can be propagated across filesystems)
-f : Overwrites the destination if it already exists.
-l : Allow DataNode to lazily persist the file to disk, Forces a replication factor of 1. This flag will result in reduced durability. Use with care.
-d : Skip creation of temporary file with the suffix ._COPYING_.
Examples:

hadoop fs -put localfile /user/hadoop/hadoopfile
hadoop fs -put -f localfile1 localfile2 /user/hadoop/hadoopdir
hadoop fs -put -d localfile hdfs://nn.example.com/hadoop/hadoopfile
hadoop fs -put - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin.
Exit Code:

Returns 0 on success and -1 on error.
```

> 查看文件内容

```shell
Usage: hadoop fs -cat [-ignoreCrc] URI [URI ...]

Copies source paths to stdout.

Options

The -ignoreCrc option disables checkshum verification.
Example:

hadoop fs -cat hdfs://nn1.example.com/file1 hdfs://nn2.example.com/file2
hadoop fs -cat file:///file3 /user/hadoop/file4
Exit Code:

Returns 0 on success and -1 on error.
```

> 复制文件到HDFS其他目录下

```shell
Usage: hadoop fs -cp [-f] [-p | -p[topax]] URI [URI ...] <dest>

Copy files from source to destination. This command allows multiple sources as well in which case the destination must be a directory.

‘raw.*’ namespace extended attributes are preserved if (1) the source and destination filesystems support them (HDFS only), and (2) all source and destination pathnames are in the /.reserved/raw hierarchy. Determination of whether raw.* namespace xattrs are preserved is independent of the -p (preserve) flag.

Options:

The -f option will overwrite the destination if it already exists.
The -p option will preserve file attributes [topx] (timestamps, ownership, permission, ACL, XAttr). If -p is specified with no arg, then preserves timestamps, ownership, permission. If -pa is specified, then preserves permission also because ACL is a super-set of permission. Determination of whether raw namespace extended attributes are preserved is independent of the -p flag.
Example:

hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2
hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir
Exit Code:

Returns 0 on success and -1 on error.
```

> 将HDFS的文件移动到HDFS其他目录下

```shell
Usage: hadoop fs -mv URI [URI ...] <dest>

Moves files from source to destination. This command allows multiple sources as well in which case the destination needs to be a directory. Moving files across file systems is not permitted.

Example:

hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2
hadoop fs -mv hdfs://nn.example.com/file1 hdfs://nn.example.com/file2 hdfs://nn.example.com/file3 hdfs://nn.example.com/dir1
Exit Code:

Returns 0 on success and -1 on error.
```

hadoop fs -rm -r -f /bigdata29/aaa



> 将HDFS文件下载到Linux中

```shell
Usage: hadoop fs -get [-ignorecrc] [-crc] [-p] [-f] <src> <localdst>

Copy files to the local file system. Files that fail the CRC check may be copied with the -ignorecrc option. Files and CRCs may be copied using the -crc option.

Example:

hadoop fs -get /user/hadoop/file localfile
hadoop fs -get hdfs://nn.example.com/user/hadoop/file localfile
Exit Code:

Returns 0 on success and -1 on error.

Options:

-p : Preserves access and modification times, ownership and the permissions. (assuming the permissions can be propagated across filesystems)
-f : Overwrites the destination if it already exists.
-ignorecrc : Skip CRC checks on the file(s) downloaded.
-crc: write CRC checksums for the files downloaded.
```



hadoop fs -put xxx

hadoop fs -cat xxx

hadoop fs -get xxx

hadoop fs -rm -r -f xxx

hadoop fs -cp xx 

hadoop fs -mv

---------------------------------------------------------

hdfs dfs -put xxx

hdfs dfs -cat xxx

hdfs dfs -get xxx

hdfs dfs -rm -r -f xxx

hdfs dfs  -cp xx 

hdfs dfs -mv



# Hadoop分布式计算框架 MapReduce

## Hadoop-HA问题：

```
dfs.ha.fencing.methods
表示：a list of scripts or Java classes which will be used to fence the Active NameNode during a failover

而配置为shell(true)就是直接返回隔离成功，即表示没进行任何操作，为什么不会导致脑裂现象的发生，这是因为Quorun Journal方式内置了fencing功能，不需要实现单独的fencing机制（epoch number解决互斥问题）。
而如果使用共享存储NAS+NFS那种方式的话，就需要配置具体的真正有fencing功能的，比如：sshfence，下面是sshfence的说明：

sshfence - SSH to the Active NameNode and kill the process
The sshfence option SSHes to the target node and uses fuser to kill the process listening on the service’s TCP port. In order for this fencing option to work, it must be able to SSH to the target node without providing a passphrase. Thus, one must also configure the dfs.ha.fencing.ssh.private-key-files option, which is a comma-separated list of SSH private key files. 即配置sshfence需要两个namenode之间配置无密码认证，如下:(hdfs-site.xml)

   <property>
      <name>dfs.ha.fencing.methods</name>
      <value>sshfence</value>
    </property>

但如果只配置sshfence，如果在机器宕机后不可达，则sshfence会返回false，即fence失败，所以得要配置成：

   <property>
      <name>dfs.ha.fencing.methods</name>
        <value>
            sshfence
            shell(/bin/true)
        </value>
    </property>

这样子配置，顺序执行时，如果可达就执行sshfence执行杀死namenode后返回true，不可达就直接shell(true)返回true。
```



## 一、MapReduce设计理念

> map--->映射
>
> reduce--->归纳
>
> mapreduce必须构建在hdfs之上的一种大数据离线计算框架
>
> ​		在线：实时数据处理
>
> ​		离线：数据处理时效性没有在线那么强，但是相对也需要很快得到结果
>
> mapreduce不会马上得到结果，他会有一定的延时（磁盘IO）
>
> ​		如果数据量小，使用mapreduce反而不合适
>
> ​		杀鸡焉用宰牛刀
>
> **原始数据-->map(Key,Value)-->Reduce**
>
> 分布式计算
>
> ​		将大的数据切分成多个小数据，交给更多的节点参与运算
>
> 计算向数据靠拢
>
> ​		将计算传递给有数据的节点上进行工作

## 二、MapReduce架构特点

### MapReduce1.x

![image-20240401152748780](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20240401152748780.png)

> **JobTracker**
>
> 　　　主节点，单点，负责调度所有的作用和监控整个集群的资源负载。
>
> **TaskTracker**
>
> 　　　从节点，自身节点资源管理和JobTracker进行心跳联系，汇报资源和获取task。
>
> **Client**
>
> 　　　以作业为单位，规划作业计算分布，提交作业资源到HDFS，最终提交作业到JobTracker。
>
> **Slot（槽）：**
>
> ​			属于JobTracker分配的资源（计算能力、IO能力等）。
>
> ​			不管任务大小，资源是恒定的，不灵活但是好管理。
>
> **Task（MapTask-->ReduceTask）：**
>
> ​		开始按照MR的流程执行业务。
>
> ​		当任务完成时，JobTracker告诉TaskTracker回收资源。
>
> ##### MapReduce1.x的弊端
>
> 　　1.JobTracker负载过重，存在单点故障。
>
> 　　2.资源管理和计算调度强耦合，其它计算框架难以复用其资源管理。
>
> 　　3.不同框架对资源不能全局管理。

### MapReduce2.x

>![image-20240401152802418](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20240401152802418.png)
>
>**Client：**
>		客户端发送MR任务到集群，其中客户端有很多种类，例如hadoop jar
>
>**ResourceManager：**
>		资源协调框架的管理者，分为主节点和备用节点（防止单点故障，主备的切换基于ZK的管理），它时刻与NodeManager保持心跳，接受NodeManager的汇报（NodeManager当前节点的资源情况）。
>
>​		当有外部框架要使用资源的时候直接访问ResourceManager即可。
>
>​		如果是有MR任务，先去ResourceManager申请资源，ResourceManager根据汇报分配资源，例如资源在NodeManager1，那么NodeManager1要负责开辟资源。
>
>**Yarn（NodeManager）：**
>	Yarn（Yet Another Resource Negotiator，另一种资源协调者），统一管理资源。以后其他的计算框架可以直接访问yarn获取当前集群的空闲节点。
>
>​	每个DataNode上默认有一个NodeManager，NodeManager汇报自己的信息到ResourceManager。
>
>**Container：**
>		它是动态分配的，2.X资源的代名词。
>
>**ApplicationMaster：**
>		我们本次任务的主导者，负责调度本次被分配的资源Container。当所有的节点任务全部完成，applicaion告诉ResourceManager请求杀死当前ApplicationMaster线程，本次任务的所有资源都会被释放。
>
>**Task（MapTask--ReduceTask）：**
>		开始按照MR的流程执行业务，当任务完成时，ApplicationMaster接收当前节点的反馈。
>
>
>
>**YARN【Yet Another Resource Negotiator】：Hadoop 2.0新引入的资源管理系统，直接从MRv1演化而来的。**
>
>**核心思想：将MRv1中JobTracker的资源管理和任务调度两个功能分开，分别由ResourceManager和ApplicationMaster进程实现：**
>
>　　　ResourceManager：负责整个集群的资源管理和调度。
>
>　　　ApplicationMaster：负责应用程序相关的事务，比如任务调度、任务监控和容错等。
>
>YARN的引入，使得多个计算框架可运行在一个集群中 每个应用程序对应一个ApplicationMaster 目前多个计算框架可以运行在YARN上，比如MapReduce、Spark、Storm等。

## 三、Hadoop搭建yarn环境（我们已经搭建好了，回顾配置文件）

## 四、扑克牌的问题

![image-20240401152839207](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20240401152839207.png)

> **你想数出一摞牌中有多少张黑桃，红桃，方块，梅花。直观方式是一张一张检查并且数出分别有多少张。**
> **MapReduce方法则是：**
> 		1.给在座的所有玩家中分配这摞牌
> 		2.让每个玩家数自己手中的牌有几张是黑桃，然后把这个数目汇报给你
> 		3.你把所有玩家告诉你的数字加起来，得到最后的结论

## 五、MR的计算流程

> 计算1T数据中每个单词出现的次数---->wordcount

![image-20240401152851291](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20240401152851291.png)

### 5.1	原始数据File(可以从网上找一篇英文的文章)

```
The books chronicle the adventures of the adolescent wizard Harry Potter and his best friends Ron Weasley and Hermione Granger, all of whom are students at Hogwarts School of Witchcraft and Wizardry. 
```

> 1T数据被切分成块存放在HDFS上，每一个块有128M大小

### 5.2	数据块Block

> block块是hdfs上数据存储的一个单元，同一个文件中块的大小都是相同的
>
> 因为数据存储到HDFS上不可变，所以有可能块的数量和集群的计算能力不匹配
>
> 我们需要一个动态调整本次参与计算节点数量的一个单位
>
> 我们可以动态的改变这个单位–-->参与的节点

### 5.3	切片Split(画图带同学理解)

> 目的：动态地控制计算单元的数量

> 切片是一个逻辑概念
>
> 在不改变现在数据存储的情况下，可以控制参与计算的节点数目
>
> 通过切片大小可以达到控制计算节点数量的目的
>
> ​		**有多少个切片就会执行多少个Map任务**
>
> 一般切片大小为Block的整数倍(2 1/2)
>
> ​		防止多余创建和很多的数据连接
>
> 如果Split大小 > Block大小 ,计算节点少了
>
> 如果Split大小 < Block大小 ,计算节点多了
>
> 默认情况下，**Split切片的大小等于Block的大小 ,默认128M,如果读取到最后一个block块的时候，与前一个blokc块组合起来的大小小于128M*1.1的话，他们结合生一个split切片，生成一个map任务**
>
> 一个切片对应一个MapTask

### 5.4	MapTask

> map默认从所属切片读取数据，每次读取一行（默认读取器）到内存中（map种的逻辑作用在每一行上）
>
> 我们可以根据自己书写的分词逻辑（空格，逗号等分隔），计算每个单词出现的次数（wordcount）
>
> 这时会产生（Map<String,Integer>）临时数据，存放到内存中
>
> ```
> the books chronicle the adventures of the adolescent wizard Harry Potter and his best friends Ron Weasley and Hermione Granger, all of whom are students at Hogwarts School of Witchcraft and Wizardry
> 
> the 1
> books 1
> chronicle 1
> the 1
> adventures 1
> of 1
> ...
> Wizardry 1
> 
> ```
>
> 但是内存的大小是有限的，如果每个任务随机的去占用内存，会导致内存不可控。多个任务同时执行有可能内存溢出（OOM）
>
> 如果把数据都直接放到硬盘，效率太低
>
> 所以想个方案，内存和硬盘结合，我们要做的就是在OOM和效率低之间提供一个有效方案，可以先往内存中写入一部分数据，然后写出到硬盘

### 5.5	环形缓冲区（KV-Buffer）(画图演示)

> 可以循环利用这块内存区域，减少数据溢写时map的停止时间
>
> ​		每一个Map可以独享的一个内存区域
>
> ​		在内存中构建一个环形数据缓冲区(kvBuffer),默认大小为100M
>
> ​		设置缓冲区的阈值为80%(设置阈值的目的是为了同时写入和写出),当缓冲区的数据达到80M开始向外溢写到硬盘
>
> ​		溢写的时候还有20M的空间可以被使用效率并不会被减缓
>
> ​		而且将数据循环写到硬盘，不用担心OOM问题   
>
> **说完这个先说溢写，合并，拉取（分析出问题得到结论），再说中间的分区排序**

### 5.6	分区Partition(环形缓冲区做的)

> 根据Key直接计算出对应的Reduce
>
> **分区的数量和Reduce的数量是相等的**
>
> hash(key) % partation(reduce的数量) = num
>
> 默认分区的算法是Hash然后取余
>
> ​		Object的hashCode()—equals()
>
> ​		如果两个对象equals,那么两个对象的hashcode一定相等
>
> ​		如果两个对象的hashcode相等，但是对象不一定equlas

### 5.7	排序Sort(环形缓冲区做的，快速排序，对前面分区后的编号进行排序，使得相同编号的在一起)

> 对要溢写的数据进行排序（QuickSort）
>
> 按照先Partation后Key的顺序排序–>相同分区在一起,相同Key的在一起
>
> 我们将来溢写出的小文件也都是有序的

### 5.8	溢写Spill

> 将内存中的数据循环写到硬盘，不用担心OOM问题
>
> 每次会产生一个80M的文件
>
> 如果本次Map产生的数据较多，可能会溢写多个文件

### 5.9	合并Merge

> 因为溢写会产生很多有序(分区 key)的小文件，而且小文件的数目不确定
>
> 后面向reduce传递数据带来很大的问题
>
> 所以将小文件合并成一个大文件，将来拉取的数据直接从大文件拉取即可
>
> 合并小文件的时候同样进行排序(归并 排序),最终产生一个有序的大文件

### 5.10	组合器Combiner(是属于优化方案，不是必须项，也不是所有MR都可以加上的)

> a.  集群的带宽限制了mapreduce作业的数量，因此应该尽量避免map和reduce任务之间的数据传输，hadoop允许用户对map的输出数据进行处理，用户可自定义combiner函数（如同map函数和reduce函数一般），其逻辑一般和reduce函数一样，combiner的输入是map的输出，combiner的输出作为reduce的输入，很多情况下可以i直接将reduce函数作为conbiner函数来试用（job.setCombinerClass(FlowCountReducer.class)）。
>
> b.  combiner属于优化方案，所以无法确定combiner函数会调用多少次，可以在环形缓存区溢出文件时调用combiner函数，也可以在溢出的小文件合并成大文件时调用combiner，但是要保证不管调用多少次，combiner函数都不影响最终的结果，所以不是所有处理逻辑都可以i使用combiner组件，有些逻辑如果试用了conbiner函数会改变最后reduce的输出结果（如求几个数的平均值，就不能先用conbiner求一次各个map输出结果的平均值，再求这些平均值的平均值，那样会导致结果的错误）。
>
> c.  combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量：
>
> ​		原先传给reduce的数据时a1 a1 a1 a1 a1
>
> ​		第一次combiner组合后变成a(1,1,1,1,1)
>
> ​		第二次combiner后传给reduce的数据变为a(5,5,6,7,23,...)

### 5.11	拉取Fetch

> 我们需要将Map的临时结果拉取到Reduce节点
>
> ```
> 第一种方式：两两合并
> 第二种方式：相同的进一个reduce
> 第三种对第二种优化，排序
> 第四种对第三种优化：如果一个reduce处理两种key，而key分布一个首一个尾，解决不连续的问题，给个编号，这个编号怎么算呢，`回到分区，排序`
> ```
>
> **原则(用统计姓氏的例子画图理解)**
>
> ​		相同的Key必须拉取到同一个Reduce节点
>
> ​		但是一个Reduce节点可以有多个Key
>
> 未排序前拉取数据的时候必须对Map产生的最终的合并文件做全序遍历
>
> ​		而且每一个reduce都要做一个全序遍历
>
> 如果map产生的大文件是有序的，每一个reduce只需要从文件中读取自己所需的即可

### 5.12	合并Merge

> 因为reduce拉取的时候，会从多个map拉取数据
>
> 那么每个map都会产生一个小文件,这些小文件（文件与文件之间无序，文件内部有序）
>
> 为了方便计算（没必要读取N个小文件）,需要合并文件
>
> 归并算法合并成2个
>
> 相同的key都在一起

### 5.13	归并Reduce

> 将文件中的数据读取到内存中
>
> 一次性将相同的key全部读取到内存中
>
> 直接将相同的key得到结果–>最终结果

### 5.14	写出Output(说完这个后再画两个案例图总结)

> 每个reduce将自己计算的最终结果都会存放到HDFS上

### 5.15	MapReduce过程截图

![image-20240401152905751](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20240401152905751.png)

## 六、MapReduce程序编写（统计单词个数）

## 七、IK分词器(统计三国演义指定词语个数)

## 八、MapReduce案例

### 8.1	好友推荐系统

> 固定类别推荐
>
> ​		莫扎特---->钢琴---->贝多芬----->命运交响曲
>
> 数据量
>
> ​		QQ好友推荐--->
>
> ​				每个QQ200个好友
>
> ​				5亿QQ号
>
> 解决思路：
>
> ​		需要按照行进行计算
>
> ​		将相同推荐设置成相同的key，便于reduce统一处理
>
> 数据：
>
> ```
> tom hello hadoop cat
> world hello hadoop hive
> cat tom hive
> mr hive hello
> hive cat hadoop world hello mr
> hadoop tom hive world hello
> hello tom world hive mr
> ```
>
> ![image-20240401152915174](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20240401152915174.png)
>
> 分析：
>
> **我们需要在map阶段根据用户的直接联系和间接关系列举出来，map输出的为tom：hadoop 1，hello：hadoop 0，0代表间接关系，1代表直接关系。在Reduce阶段把直接关系的人删除掉，再输出。**
>
> 具体实现
>
> ```java
> 
> ```



### 8.2	PM2.5平均值

```java

```

> 提升：两张表合并或者多张表
>
> 思路：map程序每次都是读取一行数据，读取两个表内的数据，可以根据数据来源（文件名称）判断当前读取的数据是来自哪一张表，然后打上标记送入reduce去处理，map输出的key是id值，value是id对应的数据。
>
> reduce接收到的数据是<key,{value1,value2,value3,…}>，key是输入的id的值，value中包含表一的id对应的数据，也包含表二的id对应的数据，我们可以通过map里打的标记进行区分，分别记录到list1与list2中，然后将两个list中的数据进行笛卡尔积就能得到一个id连接后的数据，将所有id都进行这样的操作，就能把整个表都处理完



## Combiner

> job.setCombinerClass

## 九、MapReduce源码分析（高级部分）

快捷键

```
ctrl+alt+方向键：查看上一个或者下一个方法
ctrl+shift+alt+c: 拷贝方法的全名   com.shujia.airPM25.Pm25Avg#main  
ctrl+alt+b：查看当前接口的实现类
```

### 9.1	Split

>带着问题看源码：
>
>1、map的数量和切片的数量一样？
>
>2、split的大小可以自己调节吗？算法是什么？

源代码的分析从提交任务开始

```java
job.waitForCompletion(true);
```

org.apache.hadoop.mapreduce.Job#waitForCompletion

```java
  /**
   * Submit the job to the cluster and wait for it to finish.
   * @param verbose print the progress to the user
   * @return true if the job succeeded
   * @throws IOException thrown if the communication with the 
   *         <code>JobTracker</code> is lost
   */
  public boolean waitForCompletion(boolean verbose) throws IOException, InterruptedException,ClassNotFoundException {
    //判断当前的状态
    if (state == JobState.DEFINE) {
        //=============关键代码================
      submit();
    }
    //监控任务的运行状态
    if (verbose) {
      monitorAndPrintJob();
    } else {
      // get the completion poll interval from the client.
      int completionPollIntervalMillis = 
        Job.getCompletionPollInterval(cluster.getConf());
      while (!isComplete()) {
        try {
          Thread.sleep(completionPollIntervalMillis);
        } catch (InterruptedException ie) {
        }
      }
    }
      //返回任务状态
    return isSuccessful();
  }
```

org.apache.hadoop.mapreduce.Job#submit

```java
public void submit() throws IOException, InterruptedException, ClassNotFoundException {
    //确认当前任务的状态
    ensureState(JobState.DEFINE);
    //mapreduce1.x和2.x,但是2的时候将1的好多方法进行了优化
    setUseNewAPI();
    //获取当前任务所运行的集群
    connect();
    //创建Job的提交器
    final JobSubmitter submitter = getJobSubmitter(cluster.getFileSystem(), cluster.getClient());
    status = ugi.doAs(new PrivilegedExceptionAction<JobStatus>() {
      public JobStatus run() throws IOException, InterruptedException,  ClassNotFoundException {
        //提交任务到系统去执行 
		//Internal method for submitting jobs to the system
        //===========关键代码============
        return submitter.submitJobInternal(Job.this, cluster);
      }
    });
    //任务的状态修改为运行
    state = JobState.RUNNING;
    LOG.info("The url to track the job: " + getTrackingURL());
   }
```

org.apache.hadoop.mapreduce.JobSubmitter#submitJobInternal

```java
	//validate the jobs output specs 
	//检查一下输出路径存不存在呀，有没有权限之类的
    checkSpecs(job);
    //生成并设置新的JobId
	JobID jobId = submitClient.getNewJobID();
	job.setJobID(jobId);
	//获取任务的提交目录
	Path submitJobDir = new Path(jobStagingArea, jobId.toString());
    // Create the splits for the job
    LOG.debug("Creating splits at " + jtFs.makeQualified(submitJobDir));
	//===========关键代码============ 197行
    int maps = writeSplits(job, submitJobDir);
    //设置map的数量，其中map的数量就等于切片的数量
    conf.setInt(MRJobConfig.NUM_MAPS, maps);
    LOG.info("number of splits:" + maps);
```

org.apache.hadoop.mapreduce.JobSubmitter#writeSplits

```java
  private int writeSplits(org.apache.hadoop.mapreduce.JobContext job,Path jobSubmitDir) throws IOException,InterruptedException, ClassNotFoundException {
    //获取作业的配置文件
    JobConf jConf = (JobConf)job.getConfiguration();
    int maps;
    //今后我们看源码的时候，想都不要想，看新的方式
    if (jConf.getUseNewMapper()) {
      //===========关键代码============
      maps = writeNewSplits(job, jobSubmitDir);
    } else {
      maps = writeOldSplits(jConf, jobSubmitDir);
    }
    return maps;
  }
```

org.apache.hadoop.mapreduce.JobSubmitter#writeNewSplits

```java
  private <T extends InputSplit> int writeNewSplits(JobContext job, Path jobSubmitDir) throws IOException,InterruptedException, ClassNotFoundException {
    //获取集群配置
    Configuration conf = job.getConfiguration();
    //通过反射工具获取文件读取器对象
    //===========关键代码============ job 的实现类
    //org.apache.hadoop.mapreduce.lib.input.TextInputFormat --> input
    //job->org.apache.hadoop.mapreduce.task.JobContextImpl#getInputFormatClass
    InputFormat<?, ?> input = ReflectionUtils.newInstance(job.getInputFormatClass(), conf);
    //获取到切片
    //===========关键代码============ getSplits
    List<InputSplit> splits = input.getSplits(job);
    //转成数组
    T[] array = (T[]) splits.toArray(new InputSplit[splits.size()]);

    // sort the splits into order based on size, so that the biggest
    // go first
    Arrays.sort(array, new SplitComparator());
    JobSplitWriter.createSplitFiles(jobSubmitDir, conf, 
        jobSubmitDir.getFileSystem(conf), array);
    
    //返回的是数组的长度，对应着切片的数量，回到197行验证
    return array.length;
  }
```

org.apache.hadoop.mapreduce.task.JobContextImpl#getInputFormatClass

```java
  /**
   * Get the {@link InputFormat} class for the job.
   * 
   * @return the {@link InputFormat} class for the job.
   */
  @SuppressWarnings("unchecked")
  public Class<? extends InputFormat<?,?>> getInputFormatClass() throws ClassNotFoundException {
      return (Class<? extends InputFormat<?,?>>) 
      //getClass的操作是如果有值返回值，没有的话使用默认值
      conf.getClass(INPUT_FORMAT_CLASS_ATTR, TextInputFormat.class);
  }
```

org.apache.hadoop.mapreduce.lib.input.FileInputFormat#getSplits

```java
  /** 
   * Generate the list of files and make them into FileSplits.
   * @param job the job context
   * @throws IOException
   */
  public List<InputSplit> getSplits(JobContext job) throws IOException {
    StopWatch sw = new StopWatch().start();
    //开始计算两个变量(一个切片最少有一个字节,一个最小切片值也是1)
    //1
    long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));
    //Long.MAX_VALUE
    long maxSize = getMaxSplitSize(job);

    // generate splits
    //创建一个List存放切片
    List<InputSplit> splits = new ArrayList<InputSplit>();
    //获取本次计算中所有的要计算的文件
    List<FileStatus> files = listStatus(job);
    //首先取出一个文件
    for (FileStatus file: files) {
      //获取文件路径
      Path path = file.getPath();
      //获取文件大小
      long length = file.getLen();
      if (length != 0) {
        BlockLocation[] blkLocations;
        if (file instanceof LocatedFileStatus) {
          blkLocations = ((LocatedFileStatus) file).getBlockLocations();
        } else {
          FileSystem fs = path.getFileSystem(job.getConfiguration());
          //获取文件块的信息
          blkLocations = fs.getFileBlockLocations(file, 0, length);
        }
        //判断文件是否可以被切分，比如文件被压缩了，需要解压缩才可以
        if (isSplitable(job, path)) {
          //获取单个块的大小
          long blockSize = file.getBlockSize();
          //开始计算切片大小，这里可以验证切片大小与block大小一样
          //思考如何生成256M的切片
          
            
          //如果切片小于blocksize-->将maxsize小于blocksize
          //如果切片大于blocksize-->将minsize大于blocksize
          long splitSize = computeSplitSize(blockSize, minSize, maxSize);

          //将文件大小分配给bytesRemaining
          long bytesRemaining = length;
          //private static final double SPLIT_SLOP = 1.1;   // 10% slop
          while (((double) bytesRemaining)/splitSize > SPLIT_SLOP) {
            int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
            //制作切片
            //封装切片对象并将其存放到list中
            //makeSplit(路径，偏移量，切片大小，块的位置，备份的位置);
            splits.add(makeSplit(path, length-bytesRemaining, splitSize,
                        blkLocations[blkIndex].getHosts(),
                        blkLocations[blkIndex].getCachedHosts()));
            bytesRemaining -= splitSize;
          }

          //如果最后一个文件过小没有大于1.1，就与上一个一起生成切片
          if (bytesRemaining != 0) {
            int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);
            splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,
                       blkLocations[blkIndex].getHosts(),
                       blkLocations[blkIndex].getCachedHosts()));
          }
        } else { // not splitable
          //如果文件不可切，就生成一个切片
          splits.add(makeSplit(path, 0, length, blkLocations[0].getHosts(),
                      blkLocations[0].getCachedHosts()));
        }
      } else { 
        //Create empty hosts array for zero length files
        splits.add(makeSplit(path, 0, length, new String[0]));
      }
    }
    // Save the number of input files for metrics/loadgen
    job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());
    sw.stop();
    if (LOG.isDebugEnabled()) {
      LOG.debug("Total # of splits generated by getSplits: " + splits.size()
          + ", TimeTaken: " + sw.now(TimeUnit.MILLISECONDS));
    }
    //返回切片（后面的代码我们跟不进去了，是yarn上面的了）
    return splits;
  }
```

计算切片大小逻辑

org.apache.hadoop.mapreduce.lib.input.FileInputFormat#computeSplitSize

```java
 protected long computeSplitSize(long blockSize, long minSize,
                                  long maxSize) {
    //blockSize--128M
    //maxSize--56M ----> 56M
    //minSize--256M ----> 256M
    return Math.max(minSize, Math.min(maxSize, blockSize));
  }
```

### 9.2	Map源码-MapTask

> 带着问题：
>
> 1、map读取数据按照行读取数据？验证一下
>
> 2、如果切片把一行数据放在了两个切片中呢？怎么办？
>
> 3、map里面的第一个参数类型是LongWritable？哪里指定的？

org.apache.hadoop.mapred.MapTask

![image-20240401152927453](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20240401152927453.png)

```java
  @Override
  public void run(final JobConf job, final TaskUmbilicalProtocol umbilical)
    throws IOException, ClassNotFoundException, InterruptedException {
    this.umbilical = umbilical;

    //判断是否为Map任务
    if (isMapTask()) {
      // If there are no reducers then there won't be any sort. Hence the map 
      // phase will govern the entire attempt's progress.
      //判断reduce数量是否等于0，有可能等于0的如果我们只是清洗数据，就不需要
      if (conf.getNumReduceTasks() == 0) {
        //map所占的比例100%，没有reducce就不用分区了
        mapPhase = getProgress().addPhase("map", 1.0f);
      } else {
        // If there are reducers then the entire attempt's progress will be 
        // split between the map phase (67%) and the sort phase (33%).
        //如果有reduce的话分区排序
        mapPhase = getProgress().addPhase("map", 0.667f);
        sortPhase  = getProgress().addPhase("sort", 0.333f);
      }
    }
    //任务报告一下，说明我要处理多少数据
    TaskReporter reporter = startReporter(umbilical);
 
    //使用新api
    boolean useNewApi = job.getUseNewMapper();
    //===========关键代码============
    //使用新api进行初始化
    initialize(job, getJobID(), reporter, useNewApi);

    // check if it is a cleanupJobTask
    if (jobCleanup) {
      runJobCleanupTask(umbilical, reporter);
      return;
    }
    if (jobSetup) {
      runJobSetupTask(umbilical, reporter);
      return;
    }
    if (taskCleanup) {
      runTaskCleanupTask(umbilical, reporter);
      return;
    }

    if (useNewApi) {
      //===========关键代码============
      runNewMapper(job, splitMetaInfo, umbilical, reporter);
    } else {
      runOldMapper(job, splitMetaInfo, umbilical, reporter);
    }
    done(umbilical, reporter);
  }
```

org.apache.hadoop.mapred.Task#initialize

```java
 public void initialize(JobConf job, JobID id, 
                         Reporter reporter,
                         boolean useNewApi) throws IOException, 
                                                   ClassNotFoundException,
                                                   InterruptedException {
    //获取作业的上下文
    jobContext = new JobContextImpl(job, id, reporter);
    //获取任务的上下文
    taskContext = new TaskAttemptContextImpl(job, taskId, reporter);
    if (getState() == TaskStatus.State.UNASSIGNED) {
      setState(TaskStatus.State.RUNNING);
    }
    if (useNewApi) {
      if (LOG.isDebugEnabled()) {
        LOG.debug("using new api for output committer");
      }
      //创建了一个outputFormat对象
      outputFormat = ReflectionUtils.newInstance(taskContext.getOutputFormatClass(), job);
      committer = outputFormat.getOutputCommitter(taskContext);
    } else {
      committer = conf.getOutputCommitter();
    }
    Path outputPath = FileOutputFormat.getOutputPath(conf);
    if (outputPath != null) {
      if ((committer instanceof FileOutputCommitter)) {
        FileOutputFormat.setWorkOutputPath(conf, 
          ((FileOutputCommitter)committer).getTaskAttemptPath(taskContext));
      } else {
        FileOutputFormat.setWorkOutputPath(conf, outputPath);
      }
    }
    committer.setupTask(taskContext);
    Class<? extends ResourceCalculatorProcessTree> clazz =
        conf.getClass(MRConfig.RESOURCE_CALCULATOR_PROCESS_TREE,
            null, ResourceCalculatorProcessTree.class);
    pTree = ResourceCalculatorProcessTree
            .getResourceCalculatorProcessTree(System.getenv().get("JVM_PID"), clazz, conf);
    LOG.info(" Using ResourceCalculatorProcessTree : " + pTree);
    if (pTree != null) {
      pTree.updateProcessTree();
      initCpuCumulativeTime = pTree.getCumulativeCpuTime();
    }
  }
```

org.apache.hadoop.mapred.MapTask#runNewMapper

```java
@SuppressWarnings("unchecked")
  private <INKEY,INVALUE,OUTKEY,OUTVALUE>
  void runNewMapper(final JobConf job,
                    final TaskSplitIndex splitIndex,
                    final TaskUmbilicalProtocol umbilical,
                    TaskReporter reporter
                    ) throws IOException, ClassNotFoundException,
                             InterruptedException {
    // make a task context so we can get the classes
    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext =
      new org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl(job, 
                                                                  getTaskID(),
                                                                  reporter);
    // make a mapper--com.shujia.MyMapper
   //对应自己写的map类  TaskAttemptContextImpl
  org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE> mapper =
      (org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>)
        ReflectionUtils.newInstance(taskContext.getMapperClass(), job);
    // make the input format
    //org.apache.hadoop.mapreduce.lib.input.TextInputFormat
    org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE> inputFormat =
      (org.apache.hadoop.mapreduce.InputFormat<INKEY,INVALUE>)
        ReflectionUtils.newInstance(taskContext.getInputFormatClass(), job);
    // rebuild the input split
    //获取切片
    org.apache.hadoop.mapreduce.InputSplit split = null;
    split = getSplitDetails(new Path(splitIndex.getSplitLocation()),
        splitIndex.getStartOffset());
    LOG.info("Processing split: " + split);

    //===========关键代码============ NewTrackingRecordReader
    org.apache.hadoop.mapreduce.RecordReader<INKEY,INVALUE> input =
      new NewTrackingRecordReader<INKEY,INVALUE>
        (split, inputFormat, reporter, taskContext);
    
    job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());
    org.apache.hadoop.mapreduce.RecordWriter output = null;
    
    // get an output object
    if (job.getNumReduceTasks() == 0) {
      //如果reduce数量等于0，直接输出
      output = 
        new NewDirectOutputCollector(taskContext, job, umbilical, reporter);
    } else {
      //如果reduce数量不等于0，待会来看，看下面的初始化
      output = new NewOutputCollector(taskContext, job, umbilical, reporter);
    }

    org.apache.hadoop.mapreduce.MapContext<INKEY, INVALUE, OUTKEY, OUTVALUE> 
    mapContext = 
      new MapContextImpl<INKEY, INVALUE, OUTKEY, OUTVALUE>(job, getTaskID(), 
          input, output, 
          committer, 
          reporter, split);

    org.apache.hadoop.mapreduce.Mapper<INKEY,INVALUE,OUTKEY,OUTVALUE>.Context 
        mapperContext = 
          new WrappedMapper<INKEY, INVALUE, OUTKEY, OUTVALUE>().getMapContext(
              mapContext);

    try {
      //===========关键代码============
      //初始化的时候有意识的将第一行省略了
      input.initialize(split, mapperContext);
      //实际上调用的就是我们自己重写的map方法
      //===========关键代码============
      mapper.run(mapperContext);
      mapPhase.complete();
      setPhase(TaskStatus.Phase.SORT);
      statusUpdate(umbilical);
      input.close();
      input = null;
      output.close(mapperContext);
      output = null;
    } finally {
      closeQuietly(input);
      closeQuietly(output, mapperContext);
    }
  }
```

org.apache.hadoop.mapred.MapTask.NewTrackingRecordReader#NewTrackingRecordReader

```java
NewTrackingRecordReader(org.apache.hadoop.mapreduce.InputSplit split,
    org.apache.hadoop.mapreduce.InputFormat<K, V> inputFormat,
    TaskReporter reporter,
    org.apache.hadoop.mapreduce.TaskAttemptContext taskContext)
    throws InterruptedException, IOException {
  this.reporter = reporter;
  this.inputRecordCounter = reporter
      .getCounter(TaskCounter.MAP_INPUT_RECORDS);
  this.fileInputByteCounter = reporter
      .getCounter(FileInputFormatCounter.BYTES_READ);

  List <Statistics> matchedStats = null;
  if (split instanceof org.apache.hadoop.mapreduce.lib.input.FileSplit) {
    matchedStats = getFsStatistics(((org.apache.hadoop.mapreduce.lib.input.FileSplit) split)
        .getPath(), taskContext.getConfiguration());
  }
  fsStats = matchedStats;

  long bytesInPrev = getInputBytes(fsStats);
  
  //===========关键代码============
  //真正工作的人是谁，创建一个记录读取器
  //返回的是一个行记录读取器
  this.real = inputFormat.createRecordReader(split, taskContext);
  long bytesInCurr = getInputBytes(fsStats);
  fileInputByteCounter.increment(bytesInCurr - bytesInPrev);
}
```

org.apache.hadoop.mapreduce.RecordReader

```java
@Override
public RecordReader<LongWritable, Text> 
  createRecordReader(InputSplit split,
                     TaskAttemptContext context) {
  String delimiter = context.getConfiguration().get(
      "textinputformat.record.delimiter");
  byte[] recordDelimiterBytes = null;
  if (null != delimiter)
    recordDelimiterBytes = delimiter.getBytes(Charsets.UTF_8);
  return new LineRecordReader(recordDelimiterBytes);
}
```

org.apache.hadoop.mapreduce.lib.input.LineRecordReader#initialize

```java
public void initialize(InputSplit genericSplit,
                       TaskAttemptContext context) throws IOException {
  FileSplit split = (FileSplit) genericSplit;
  Configuration job = context.getConfiguration();
  this.maxLineLength = job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);
  //获取开始的位置
  start = split.getStart();
  end = start + split.getLength();
  final Path file = split.getPath();

  // open the file and seek to the start of the split
  //获取分布式文件系统
  final FileSystem fs = file.getFileSystem(job);
  //获取一个输入流
  fileIn = fs.open(file);
  
  CompressionCodec codec = new CompressionCodecFactory(job).getCodec(file);
  if (null!=codec) {
    isCompressedInput = true;  
    decompressor = CodecPool.getDecompressor(codec);
    if (codec instanceof SplittableCompressionCodec) {
      final SplitCompressionInputStream cIn =
        ((SplittableCompressionCodec)codec).createInputStream(
          fileIn, decompressor, start, end,
          SplittableCompressionCodec.READ_MODE.BYBLOCK);
      in = new CompressedSplitLineReader(cIn, job,
          this.recordDelimiterBytes);
      start = cIn.getAdjustedStart();
      end = cIn.getAdjustedEnd();
      filePosition = cIn;
    } else {
      in = new SplitLineReader(codec.createInputStream(fileIn,
          decompressor), job, this.recordDelimiterBytes);
      filePosition = fileIn;
    }
  } else {
    //读取偏移量
    fileIn.seek(start);
    in = new UncompressedSplitLineReader(
        fileIn, job, this.recordDelimiterBytes, split.getLength());
    filePosition = fileIn;
  }
  // If this is not the first split, we always throw away first record
  // because we always (except the last split) read one extra line in
  // next() method.
  //解决第二个问题 从第二行开始读，把切片的第一行将给上一个切片去读
  if (start != 0) {
    //返回的start正好是下一行数据的开头
    start += in.readLine(new Text(), 0, maxBytesToConsume(start));
  }
  this.pos = start;
}
```

org.apache.hadoop.mapreduce.lib.map.WrappedMapper

```java
    @Override
    public boolean nextKeyValue() throws IOException, InterruptedException {
      return mapContext.nextKeyValue();
    }
```

org.apache.hadoop.mapreduce.task.MapContextImpl#MapContextImpl

```java
  @Override
  public boolean nextKeyValue() throws IOException, InterruptedException {
    //最终调用的是LineRecordReader中的nextKeyValue方法
    return reader.nextKeyValue();
  }
```

org.apache.hadoop.mapreduce.lib.input.LineRecordReader

```java
public boolean nextKeyValue() throws IOException {
  if (key == null) {
    key = new LongWritable();
  }
  //设置当前的偏移量
  key.set(pos);
  if (value == null) {
    value = new Text();
  }
  int newSize = 0;
  // We always read one extra line, which lies outside the upper
  // split limit i.e. (end - 1)
  //循环读取数据
  while (getFilePosition() <= end || in.needAdditionalRecordAfterSplit()) {
    if (pos == 0) {
      newSize = skipUtfByteOrderMark();
    } else {
      //pos是当前数据的定位，value是数据
      newSize = in.readLine(value, maxLineLength, maxBytesToConsume(pos));
      pos += newSize;
    }

    if ((newSize == 0) || (newSize < maxLineLength)) {
      break;
    }

    // line too long. try again
    LOG.info("Skipped line of size " + newSize + " at pos " + 
             (pos - newSize));
  }
  if (newSize == 0) {
    //如果本次啥也没有读到，返回false
    key = null;
    value = null;
    return false;
  } else {
    //读到了返回true
    return true;
  }
}

//看完这里回到map方法，key就是数据的偏移量，value就是一行数据，context上下文，写到环形缓冲区，map结束
```

### 9.3 KV-Buffer

> 通过context中的write方法进行写
>
> 带着问题：
>
> 1、分区的数量和reduce数量一样？
>
> 2、环形缓冲区内存大小100M？80%溢写？可以自己设置吗？
>
> 3、排序是快速排序？
>
> 4、怎么分区的？Hash?

org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl

```java
  /**
   * Generate an output key/value pair.
   */
  public void write(KEYOUT key, VALUEOUT value
                    ) throws IOException, InterruptedException {
    output.write(key, value);
  }
```

> 通过参数的个数

org.apache.hadoop.mapred.MapTask.NewOutputCollector#NewOutputCollector

```java
    @SuppressWarnings("unchecked")
    NewOutputCollector(org.apache.hadoop.mapreduce.JobContext jobContext,
                       JobConf job,
                       TaskUmbilicalProtocol umbilical,
                       TaskReporter reporter
                       ) throws IOException, ClassNotFoundException {
      //========关键代码=============
      collector = createSortingCollector(job, reporter);
      partitions = jobContext.getNumReduceTasks();
      if (partitions > 1) {
        //========关键代码============= 分区器
        partitioner = (org.apache.hadoop.mapreduce.Partitioner<K,V>)
          ReflectionUtils.newInstance(jobContext.getPartitionerClass(), job);
      } else {
        partitioner = new org.apache.hadoop.mapreduce.Partitioner<K,V>() {
          @Override
          public int getPartition(K key, V value, int numPartitions) {
            return partitions - 1;
          }
        };
      }
    }
```

org.apache.hadoop.mapred.MapTask#createSortingCollector

```java
private <KEY, VALUE> MapOutputCollector<KEY, VALUE>
          createSortingCollector(JobConf job, TaskReporter reporter)
    throws IOException, ClassNotFoundException {
    MapOutputCollector.Context context =
      new MapOutputCollector.Context(this, job, reporter);

    //========关键代码==========
    Class<?>[] collectorClasses = job.getClasses(
      JobContext.MAP_OUTPUT_COLLECTOR_CLASS_ATTR, MapOutputBuffer.class);
    int remainingCollectors = collectorClasses.length;
    Exception lastException = null;
    for (Class clazz : collectorClasses) {
      try {
        if (!MapOutputCollector.class.isAssignableFrom(clazz)) {
          throw new IOException("Invalid output collector class: " + clazz.getName() +
            " (does not implement MapOutputCollector)");
        }
        Class<? extends MapOutputCollector> subclazz =
          clazz.asSubclass(MapOutputCollector.class);
        LOG.debug("Trying map output collector class: " + subclazz.getName());
        MapOutputCollector<KEY, VALUE> collector =
          ReflectionUtils.newInstance(subclazz, job);
        
        //====================进行初始化===============
        collector.init(context);
        LOG.info("Map output collector class = " + collector.getClass().getName());
          
        //返回MapOutputBuffer
        return collector;
      } catch (Exception e) {
        String msg = "Unable to initialize MapOutputCollector " + clazz.getName();
        if (--remainingCollectors > 0) {
          msg += " (" + remainingCollectors + " more collector(s) to try)";
        }
        lastException = e;
        LOG.warn(msg, e);
      }
    }
    throw new IOException("Initialization of all the collectors failed. " +
      "Error in last collector was :" + lastException.getMessage(), lastException);
  }
```

org.apache.hadoop.mapred.MapTask.MapOutputBuffer#init

```java
 public void init(MapOutputCollector.Context context
                    ) throws IOException, ClassNotFoundException {
      job = context.getJobConf();
      reporter = context.getReporter();
      mapTask = context.getMapTask();
      mapOutputFile = mapTask.getMapOutputFile();
      sortPhase = mapTask.getSortPhase();
      spilledRecordsCounter = reporter.getCounter(TaskCounter.SPILLED_RECORDS);
      partitions = job.getNumReduceTasks();
      rfs = ((LocalFileSystem)FileSystem.getLocal(job)).getRaw();

      //sanity checks
      //溢写
      final float spillper =
        job.getFloat(JobContext.MAP_SORT_SPILL_PERCENT, (float)0.8);
      final int sortmb = job.getInt(JobContext.IO_SORT_MB, 100);
      indexCacheMemoryLimit = job.getInt(JobContext.INDEX_CACHE_MEMORY_LIMIT,
                                         INDEX_CACHE_MEMORY_LIMIT_DEFAULT);
      if (spillper > (float)1.0 || spillper <= (float)0.0) {
        throw new IOException("Invalid \"" + JobContext.MAP_SORT_SPILL_PERCENT +
            "\": " + spillper);
      }
      if ((sortmb & 0x7FF) != sortmb) {
        throw new IOException(
            "Invalid \"" + JobContext.IO_SORT_MB + "\": " + sortmb);
      }
     //默认排序器是快速排序
      sorter = ReflectionUtils.newInstance(job.getClass("map.sort.class",
            QuickSort.class, IndexedSorter.class), job);
      // buffers and accounting
     
      //100左移2位 ×2^20
      int maxMemUsage = sortmb << 20;
      //对16进行取余，让这个数字变成16的整数倍
      maxMemUsage -= maxMemUsage % METASIZE;
      //环形缓冲区100M
      //并且设置环形缓冲区的一些初始值
      kvbuffer = new byte[maxMemUsage];
      bufvoid = kvbuffer.length;
      kvmeta = ByteBuffer.wrap(kvbuffer)
         .order(ByteOrder.nativeOrder())
         .asIntBuffer();
      setEquator(0);
      bufstart = bufend = bufindex = equator;
      kvstart = kvend = kvindex;

      //双重索引，大家课下可以自己了解
      maxRec = kvmeta.capacity() / NMETA;
      softLimit = (int)(kvbuffer.length * spillper);
      bufferRemaining = softLimit;
      LOG.info(JobContext.IO_SORT_MB + ": " + sortmb);
      LOG.info("soft limit at " + softLimit);
      LOG.info("bufstart = " + bufstart + "; bufvoid = " + bufvoid);
      LOG.info("kvstart = " + kvstart + "; length = " + maxRec);

      // k/v serialization
      //如果是自己自定义的类型，需要自定义排序器
      comparator = job.getOutputKeyComparator();
      keyClass = (Class<K>)job.getMapOutputKeyClass();
      valClass = (Class<V>)job.getMapOutputValueClass();
      //
      serializationFactory = new SerializationFactory(job);
      keySerializer = serializationFactory.getSerializer(keyClass);
      keySerializer.open(bb);
      valSerializer = serializationFactory.getSerializer(valClass);
      valSerializer.open(bb);

      // output counters
      mapOutputByteCounter = reporter.getCounter(TaskCounter.MAP_OUTPUT_BYTES);
      mapOutputRecordCounter =
        reporter.getCounter(TaskCounter.MAP_OUTPUT_RECORDS);
      fileOutputByteCounter = reporter
          .getCounter(TaskCounter.MAP_OUTPUT_MATERIALIZED_BYTES);

      // compression
      if (job.getCompressMapOutput()) {
        Class<? extends CompressionCodec> codecClass =
          job.getMapOutputCompressorClass(DefaultCodec.class);
        codec = ReflectionUtils.newInstance(codecClass, job);
      } else {
        codec = null;
      }

      // combiner
      final Counters.Counter combineInputCounter =
        reporter.getCounter(TaskCounter.COMBINE_INPUT_RECORDS);
      combinerRunner = CombinerRunner.create(job, getTaskID(), 
                                             combineInputCounter,
                                             reporter, null);
      if (combinerRunner != null) {
        final Counters.Counter combineOutputCounter =
          reporter.getCounter(TaskCounter.COMBINE_OUTPUT_RECORDS);
        combineCollector= new CombineOutputCollector<K,V>(combineOutputCounter, reporter, job);
      } else {
        combineCollector = null;
      }
      spillInProgress = false;
      minSpillsForCombine = job.getInt(JobContext.MAP_COMBINE_MIN_SPILLS, 3);
      spillThread.setDaemon(true);
      spillThread.setName("SpillThread");
      spillLock.lock();
      try {
        spillThread.start();
        while (!spillThreadRunning) {
          spillDone.await();
        }
      } catch (InterruptedException e) {
        throw new IOException("Spill thread failed to initialize", e);
      } finally {
        spillLock.unlock();
      }
      if (sortSpillException != null) {
        throw new IOException("Spill thread failed to initialize",
            sortSpillException);
      }
    }
```

org.apache.hadoop.mapreduce.task.JobContextImpl#getPartitionerClass 默认是hash分区

```java
  @SuppressWarnings("unchecked")
  public Class<? extends Partitioner<?,?>> getPartitionerClass() 
     throws ClassNotFoundException {
    return (Class<? extends Partitioner<?,?>>) 
      conf.getClass(PARTITIONER_CLASS_ATTR, HashPartitioner.class);
  }
```

org.apache.hadoop.mapreduce.lib.partition.HashPartitioner

```java
public class HashPartitioner<K, V> extends Partitioner<K, V> {

  /** Use {@link Object#hashCode()} to partition. */
  public int getPartition(K key, V value,
                          int numReduceTasks) {
    return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;
  }

}
```

output-->NewOutputCollector--->write

```java
    @Override
    public void write(K key, V value) throws IOException, InterruptedException {
      collector.collect(key, value,
                        partitioner.getPartition(key, value, partitions));
    }
```

MapOutputBuffer

```java
    /**
     * Serialize the key, value to intermediate storage.
     * When this method returns, kvindex must refer to sufficient unused
     * storage to store one METADATA.
     */
    public synchronized void collect(K key, V value, final int partition
                                     ) throws IOException {
      reporter.progress();
      if (key.getClass() != keyClass) {
        throw new IOException("Type mismatch in key from map: expected "
                              + keyClass.getName() + ", received "
                              + key.getClass().getName());
      }
      if (value.getClass() != valClass) {
        throw new IOException("Type mismatch in value from map: expected "
                              + valClass.getName() + ", received "
                              + value.getClass().getName());
      }
      if (partition < 0 || partition >= partitions) {
        throw new IOException("Illegal partition for " + key + " (" +
            partition + ")");
      }
      checkSpillException();
      bufferRemaining -= METASIZE;
      if (bufferRemaining <= 0) {
        // start spill if the thread is not running and the soft limit has been
        // reached
        spillLock.lock();
        try {
          do {
            if (!spillInProgress) {
              final int kvbidx = 4 * kvindex;
              final int kvbend = 4 * kvend;
              // serialized, unspilled bytes always lie between kvindex and
              // bufindex, crossing the equator. Note that any void space
              // created by a reset must be included in "used" bytes
              final int bUsed = distanceTo(kvbidx, bufindex);
              final boolean bufsoftlimit = bUsed >= softLimit;
              if ((kvbend + METASIZE) % kvbuffer.length !=
                  equator - (equator % METASIZE)) {
                // spill finished, reclaim space
                resetSpill();
                bufferRemaining = Math.min(
                    distanceTo(bufindex, kvbidx) - 2 * METASIZE,
                    softLimit - bUsed) - METASIZE;
                continue;
              } else if (bufsoftlimit && kvindex != kvend) {
                // spill records, if any collected; check latter, as it may
                // be possible for metadata alignment to hit spill pcnt
                //====开始溢写====================
                startSpill();
                final int avgRec = (int)
                  (mapOutputByteCounter.getCounter() /
                  mapOutputRecordCounter.getCounter());
                // leave at least half the split buffer for serialization data
                // ensure that kvindex >= bufindex
                final int distkvi = distanceTo(bufindex, kvbidx);
                final int newPos = (bufindex +
                  Math.max(2 * METASIZE - 1,
                          Math.min(distkvi / 2,
                                   distkvi / (METASIZE + avgRec) * METASIZE)))
                  % kvbuffer.length;
                setEquator(newPos);
                bufmark = bufindex = newPos;
                final int serBound = 4 * kvend;
                // bytes remaining before the lock must be held and limits
                // checked is the minimum of three arcs: the metadata space, the
                // serialization space, and the soft limit
                bufferRemaining = Math.min(
                    // metadata max
                    distanceTo(bufend, newPos),
                    Math.min(
                      // serialization max
                      distanceTo(newPos, serBound),
                      // soft limit
                      softLimit)) - 2 * METASIZE;
              }
            }
          } while (false);
        } finally {
          spillLock.unlock();
        }
      }
```

### 9.4	溢写Spill

output--->NewOutputCollector

```java
  @Override
  public void close(TaskAttemptContext context
                    ) throws IOException,InterruptedException {
    try {
      //===============关键代码===============
      collector.flush();
    } catch (ClassNotFoundException cnf) {
      throw new IOException("can't find class ", cnf);
    }
    collector.close();
  }
}
```

collector--->MapOutputBuffer

```java
public void flush() throws IOException, ClassNotFoundException,
       InterruptedException {
  LOG.info("Starting flush of map output");
  if (kvbuffer == null) {
    LOG.info("kvbuffer is null. Skipping flush.");
    return;
  }
  spillLock.lock();
  try {
    while (spillInProgress) {
      reporter.progress();
      spillDone.await();
    }
    checkSpillException();

    final int kvbend = 4 * kvend;
    if ((kvbend + METASIZE) % kvbuffer.length !=
        equator - (equator % METASIZE)) {
      // spill finished
      resetSpill();
    }
    if (kvindex != kvend) {
      kvend = (kvindex + NMETA) % kvmeta.capacity();
      bufend = bufmark;
      LOG.info("Spilling map output");
      LOG.info("bufstart = " + bufstart + "; bufend = " + bufmark +
               "; bufvoid = " + bufvoid);
      LOG.info("kvstart = " + kvstart + "(" + (kvstart * 4) +
               "); kvend = " + kvend + "(" + (kvend * 4) +
               "); length = " + (distanceTo(kvend, kvstart,
                     kvmeta.capacity()) + 1) + "/" + maxRec);
      sortAndSpill();
    }
  } catch (InterruptedException e) {
    throw new IOException("Interrupted while waiting for the writer", e);
  } finally {
    spillLock.unlock();
  }
  assert !spillLock.isHeldByCurrentThread();
  // shut down spill thread and wait for it to exit. Since the preceding
  // ensures that it is finished with its work (and sortAndSpill did not
  // throw), we elect to use an interrupt instead of setting a flag.
  // Spilling simultaneously from this thread while the spill thread
  // finishes its work might be both a useful way to extend this and also
  // sufficient motivation for the latter approach.
  try {
    spillThread.interrupt();
    spillThread.join();
  } catch (InterruptedException e) {
    throw new IOException("Spill failed", e);
  }
  // release sort buffer before the merge
  kvbuffer = null;
  
  //当最后一个数据写出后，开始对溢写的小文件进行合并
  mergeParts();
  Path outputPath = mapOutputFile.getOutputFile();
  fileOutputByteCounter.increment(rfs.getFileStatus(outputPath).getLen());
}
```

> （1）Read阶段：MapTask通过InputFormat获得的RecordReader，从输入InputSplit中解析出一个个key/value。
> （2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。
> （3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。
> （4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。
> 溢写阶段详情：
> 步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号Partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。
> 步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。
> 步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。
> （5）Merge阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。
> 当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。
> 在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并mapreduce.task.io.sort.factor（默认10）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。
>
> 让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。

### 9.5	Reduce

run方法

```java
@Override
@SuppressWarnings("unchecked")
public void run(JobConf job, final TaskUmbilicalProtocol umbilical)
  throws IOException, InterruptedException, ClassNotFoundException {
  job.setBoolean(JobContext.SKIP_RECORDS, isSkipping());

  //reduce的三个阶段
  if (isMapOrReduce()) {
    copyPhase = getProgress().addPhase("copy");
    sortPhase  = getProgress().addPhase("sort");
    reducePhase = getProgress().addPhase("reduce");
  }
  // start thread that will handle communication with parent
  TaskReporter reporter = startReporter(umbilical);
  
  boolean useNewApi = job.getUseNewReducer();
  //初始化信息
  initialize(job, getJobID(), reporter, useNewApi);

  // check if it is a cleanupJobTask
  if (jobCleanup) {
    runJobCleanupTask(umbilical, reporter);
    return;
  }
  if (jobSetup) {
    runJobSetupTask(umbilical, reporter);
    return;
  }
  if (taskCleanup) {
    runTaskCleanupTask(umbilical, reporter);
    return;
  }
  
  // Initialize the codec
  codec = initCodec();
  RawKeyValueIterator rIter = null;
  ShuffleConsumerPlugin shuffleConsumerPlugin = null;
  
  Class combinerClass = conf.getCombinerClass();
  CombineOutputCollector combineCollector = 
    (null != combinerClass) ? 
   new CombineOutputCollector(reduceCombineOutputCounter, reporter, conf) : null;

  Class<? extends ShuffleConsumerPlugin> clazz =
        job.getClass(MRConfig.SHUFFLE_CONSUMER_PLUGIN, Shuffle.class, ShuffleConsumerPlugin.class);
         
  shuffleConsumerPlugin = ReflectionUtils.newInstance(clazz, job);
  LOG.info("Using ShuffleConsumerPlugin: " + shuffleConsumerPlugin);

  ShuffleConsumerPlugin.Context shuffleContext = 
    new ShuffleConsumerPlugin.Context(getTaskID(), job, FileSystem.getLocal(job), umbilical, 
                super.lDirAlloc, reporter, codec, 
                combinerClass, combineCollector, 
                spilledRecordsCounter, reduceCombineInputCounter,
                shuffledMapsCounter,
                reduceShuffleBytes, failedShuffleCounter,
                mergedMapOutputsCounter,
                taskStatus, copyPhase, sortPhase, this,
                mapOutputFile, localMapFiles);
    
  //=================关键代码=========================
  shuffleConsumerPlugin.init(shuffleContext);

  rIter = shuffleConsumerPlugin.run();

  // free up the data structures
  mapOutputFilesOnDisk.clear();
  
  sortPhase.complete();                         // sort is complete
  setPhase(TaskStatus.Phase.REDUCE); 
  statusUpdate(umbilical);
  Class keyClass = job.getMapOutputKeyClass();
  Class valueClass = job.getMapOutputValueClass();
  RawComparator comparator = job.getOutputValueGroupingComparator();

  if (useNewApi) {
    runNewReducer(job, umbilical, reporter, rIter, comparator, 
                  keyClass, valueClass);
  } else {
    runOldReducer(job, umbilical, reporter, rIter, comparator, 
                  keyClass, valueClass);
  }

  shuffleConsumerPlugin.close();
  done(umbilical, reporter);
}
```





# Hadoop优化

## 案例、天气预报

> 随机生成温度代码

```java
package com.shujia.weather;


import java.text.DateFormat;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.Date;

public class RandomWeather {
    public static void main(String[] args) throws ParseException {
        //创建日期格式
        DateFormat sdf = new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");

        long start = sdf.parse("2000-01-01 00:00:00").getTime();
        long end = sdf.parse("2022-12-31 00:00:00").getTime();
        long difference = end - start;

        for (int i = 0; i < 10000; i++) {
            //随机生成时间2000-2023
            Date date = new Date(start + (long) (Math.random() * difference));
            //随机生成一个温度
            int temperature = -20 + (int) (Math.random() * 60);
            //打印生成的结果
            System.out.println(sdf.format(date) + "\t" + temperature);
        }

    }
}

```

需求：查询某一年，某一月的最高的温度

## 优化1：Combiner

> 使用之前

![image-20220530223918820](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220530223918820.png)

> 使用之后

![image-20220530223718635](https://gitee.com/xiaohuya1/image29_demo1/raw/master/img/image-20220530223718635.png)

> 减少的了reduce 从map拉取数据的过程，提高计算效率。
>
> hadoop 的计算特点：**将计算任务向数据靠拢，而不是将数据向计算靠拢。**
>
> 特点：数据本地化，减少网络io。
>
> 首先需要知道，hadoop数据本地化是指的map任务，reduce任务并不具备数据本地化特征。
> 通常输入的数据首先在**逻辑上**（**注意这里不是真正物理上划分**）将会分片split，每个分片上构建一个map任务，由该任务执行执行用户自定义的map函数，从而处理分片中的每条记录。
> 那么切片的大小一般是趋向一个HDFS的block块的大小。为什么最佳的分片大小是趋向block块的大小呢？是因为这样能够确保单节点上最大输入块的大小，如果分片跨越两个数据块，没有一个block能够同时存储这两块数据，因此需要通过网络传输将部分数据传输到map任务节点上。这样明显比使用本地数据的map效率更低。
> 注意，map任务执行后的结果并没有写到HDFS中，而是作为中间结果存储到本地硬盘，那为什么没有存储到HDFS呢？因为，该中间结果会被reduce处理后产生最终结果后，该中间数据会被删除，如果存储到HDFS中，他会进行备份，这样明显没有意义。如果map将中间结果传输到reduce过程中出现了错误，Hadoop会在另一个节点上重新执行map产生中间结果。
> 那么为什么reduce没有数据本地化的特点呢？对于单个reduce任务来说，他的输入通常是所有mapper经过排序输出，这些输出通过网络传输到reduce节点，数据在reduce节点合并然后由reduce函数进行处理。最终结果输出到HDFS上。当多个有reduce任务的时候，map会针对输出进行分区partition，也就是为每个reduce构建一个分区，分区是由用户指定的partition函数，效率很高。
> 同时为了高效传输可以指定combiner函数，他的作用就是，**减少网络传输和本地传输**
>
> combine操作只适合等幂操作
>
> 
>
> 假设文件是500mb
>
> **注意：将reduce端的聚合操作，放到map 进行执行。适合求和，计数，等一些等幂操作。不适合求平均值，次幂等类似操作**

## 优化2：Join（数据倾斜）

> MapReduce中的join
>
> 　　其实就是类似于关系型数据库中的连接查询一样。需要计算的数据可能存储在不同的文件中或不同表中，两个文件又有一些相同的字段可以相互关联，这时候我们就可以通过这些关联字段将两个文件中的数据组合到一起进行计算了。
>
> 　　我知道的mr有三种join方式。Map join、SemiJoin、reduce join。
>
> Reduce Join（我们之前做的代码连接就是这个方式）
>
> 思路：
>
> 　　分为两个阶段
>
> 　　 （1）map函数主要是对不同文件中的数据打标签。
>
> 　　（2）reduce函数获取key相同的value list，进行笛卡尔积。
>
> Map Join思路：
>
> 　　比如有两个表，有一个表非常大，而另一个表非常小，以至于小表可以直接存放到内存中。这样，我们可以将小表复制多份，让每个map task内存中保存一个hash map，将小表数据放入这个hash map中，key是小表与大表的内个连接字段，value是小表一条记录，然后只扫描大表：对于大表中的每一条记录key/value，在hash map中查找是否有相同的key的记录，如果有，则连接输出即可。
>
> **Semi Join 这个SemiJoin其实就是对reduce join的一种优化。**
>
> 　　就是在map端过滤掉不参加join操作的数据，则可以大大减少数据量，提高网络传输速度。
>
> 这三种join方式适用于不同的场景：
>
> 　　Reduce join要考虑数据量过大时的网络传输问题。
>
> 　　Map join和SemiJoin则要考虑数据量过大时的内存问题。 如果只考虑网络传输，忽略内存问题则。
>
> 　　Map join效率最高，其次是SemiJoin，最低的是reduce join。
>
> DistributedCache DistributedCache是Hadoop提供的文件缓存工具，它能够自动将指定的文件分发到各个节点上，缓存到本地，供用户程序读取使用。一般用户数据字典的分发，和map join使用。一般缓存的文件都是只读。

## 优化3：根据实际情况调整切片大小

> **为什么默认切片是128MB和blk大小一致？（优化）**
>
> 1 切片大小默认一致，是为了数据本地化，减少数据拉取消耗网络io
>
> 2 并不是越大越好，也不是越小越好。根据集群的资源情况而定。
>
> 当集群计算资源充足的情况下：将切片的大小调小，增加map数量，提高读取效率。
>
> 当集群计算资源紧张的情况下：将切片的大小调大，减少资源占用，让任务正常运转。
>
> mapred.min.split.size、mapred.max.split.size、blockSize

```
mapreduce.input.fileinputformat.split.maxsize

mapreduce.input.fileinputformat.split.minsize
```

## 优化4：可以设置yarn资源和队列。

> mr运行日志信息：百分比是按照完成的m或r的任务的个数/m或r的总个数。
>
> MRv1/MRv2/YARN MRv1:
>
> 　　对于经典的MRv1它由三部分组成 :
>
> 　　　　编程模型、 数据处理引擎和运行时环境。
>
> 　　　　编程模型由新旧 API 两部分组成，新旧api只是代码封装上略有变化，性能没变化。
>
> 　　　　数据处理引擎由 MapTask 和 ReduceTask 组成。 运行时环境由 JobTracker 和 TaskTracker 两类服务组成。
>
> 　　MRv2:
>
> 　　　　由于MRv1对JobTracker的功能过多造成负载过重在扩展性、 资源利用率和多框架支持等方面存在不足，因此MRv2框架 的基本设计思想是将MRv1中的JobTracker包含的资源管理和应用管理两部分功能进行拆分，分别交给两个进程实现。 资源管理进程与具体应用程序无关，它负责整个集群的资源管理（内存、 CPU、 磁盘）。 应用管理进程负责管理应用程序，并且每个应用管理进程只管理一个作业。 由于资源管理可以共享给其他框架使用，因此MRv2将其做成了一个通用的系统YARN,YARN系统使得MRv2计算框架在可扩展性，资源利用率，多框架支持方面得到了很大改进。
>
> 　　YARN：yarn由4部分组成。
>
>    　　　　1. ResourceManager主要功能是：
>
> 　　　　　　（1）接收用户请求
>
> 　　　　　　（2）管理调度资源
>
> 　　　　　　（3）启动管理am　　　　
>
> 　　　　　　（4）管理所有nm,处理nm的状态汇报，向nm下达命令。
>
> 　2.Container：yarn的应用都是运行在容器上的，容器包含cpu，内存等信息。
>
> 　3.NodeManager：NM是每个节点上的资源和任务管理器，它会定时地向RM汇报本节点上的资源使用情况和各个容器的运行状态；同时负责对容器的启动和停止。
>
>    　　　　4. ApplicationMaster：管理应用程序。向RM获取资源、为应用程序分配任务、 监控所有任务运行状态。
>
> 　　　　5. 作业提交
>
> 　　首先我们将任务提交给JobClient,JobClient会向RM获取一个appId。 然后我们的JobClient会对作业进行处理, 切分InputSplit, 将作业的Jar包, 配置文件和拷贝InputSplit信息拷贝到HDFS。 最后, 通过调用RM的submitApplication()来提交作业。
>
> 2. 作业初始化
>
> 　　当RM收到submitApplciation()的请求时, 就将该请求发给调度器, 调度器分配第一个容器, 然后RM在该容器内启动ApplicationMaster进程。该进程上运行着一个MRAppMaster的Java应用。其通过创造一些bookkeeping对象来监控作业的进度。 然后通过hdfs得到由JobClient已经处理好的作业信息。为每个Inputsplit创建一个map任务, 并创建相应的reduce任务。然后ApplicationMaster会对整个作业量进行判断，**如果作业量很小, ApplicationMaster会选择在其自己的JVM中运行任务**, **这种作业称作是uber task的方式**。在任务运行之前, 作业的**setup**方法被调用来创建输出路径。
>
> 3. 任务分配
>
> 　　如果不是小作业, 那么ApplicationMaster向RM请求更多的容器来运行所有的map和reduce任务，**每个容器只能对应一个任务**。这些请求是通过心跳来传输的, 包括每个map任务的数据位置, 比如Inputsplit的主机名和机架。调度器利用这些信息来调度任务, 尽量将任务分配给有存储数据的节点, 或者分配给和存放Inputsplit的节点相同机架的节点。
>
> 4. 任务运行
>
> 　　当一个任务由RM的调度器分配了一个容器后, ApplicationMaster与NM通信来启动容器。任务由一个为YarnChild的Java应用执行。在运行任务之前首先本地化任务需要的资源, 比如作业配置, JAR文件, 以及hdfs中保存的任务所需的所有文件。最后, map任务或者reduce运行在一个叫YarnChild的进程当中。
>
> 5. 进度和状态更新
>
> 　　每个NM会向applicationmaster汇报自己的工作状态，JobClient会每秒轮询检测applicationmaster，这样就能随时收到更新信息。
>
> 6. 作业完成
>
> 　　除了向applicationmaster请求作业进度外, JobClient每5分钟都会通过调用waitForCompletion()来检查作业是否完成。作业完成之后,applicationmaster和NM会清理工作状态, OutputCommiter的作业清理方法也会被调用. 作业的信息会被作业历史服务器存储以备之后用户核查.
>
> **yarn对异常task的处理（推测执行）？(重要！！！)**
>
> 　　推测执行是在分布式环境下，因为某种原因造成同一个job的多个task运行速度不一致，有的task运行速度明显慢于其他task，则这些task拖慢了整个job的执行进度，为了避免这种情况发生，Hadoop会为该task启动备份任务，让该speculative task与原始task同时处理一份数据，哪个先运行完，则将谁的结果作为最终结果。推测执行优化机制采用了**典型的以空间换时间的优化策略**，它同时启动多个相同task（备份任务）处理相同的数据块，哪个完成的早，则采用哪个task的结果，这样可防止拖后腿Task任务出现，进而提高作业计算速度，但是，这样却会占用更多的资源。
>
> **yarn调度器的策略？(重要！！！)**
>
> 　　yarn默认是计算能力调度 FifoScheduler:根据先进先出排队，最简单的调度器。  FIFO
>
> ​        CapacityScheduler(计算能力调度)、FairScheduler(公平调度)：
>
> 　　相同点：
>
> 　　　　(1)都是队列。
>
> 　　　　(2)都有资源最大最小上线限制。
>
> 　　　　(3)都是资源共享，每个队列剩余的资源可以给其他队列使用。
>
> 　　不同点：
>
> 　　　　(1)队列排序算法不同：计算能力调度资源使用量小的优先。公平调度根据公平排序算法排序。
>
> 　　　　(2)应该用选择算法不同：计算能力调度是先进先出。公平调度先进先出或者公平排序算法。
>
> 　　　　(3)资源抢占：公平调度如果当前队列有新应用提交后，会把共享出去的资源抢夺回来。





# 实验1-1 了解大数据生态及环境准备

## 实验概述

大数据（Big Data）：指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的 **信息资产**。大数据主要解决，海量数据的`采集`、`存储`和`分析`计算问题。

熟悉我们已经准备好的实验环境，这是我们学习大数据的第一步！

## 实验环境

- AtStudy 实训平台
- Hadoop3.1

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/1679992959224497.png)

## 实验目标

学习完成本实验后，您将能够

- 了解大数据的特点及应用场景
- 了解大数据的发展前景
- 了解企业内部大数据的业务流程及组织架构
- 掌握Hadoop结构及各服务进程
- 熟悉大数据的实验环境

## 实验任务

### 任务一、大数据概述

#### **【任务目标】**

首先我们需要知道什么是大数据，大数据有什么特点以及目前大数据的应用场景和发展前期，了解这些有助于提高我们对大数据更专业的认识，基于此了解一些目前企业中大数据部门间业务流程的分析过程和内部的组织结构，这样有利于我们能够提前了解企业的大数据全景图！

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009360134216)

视频-1、大数据概述

##### 1.1 大数据4V特征

前置补充：数据的存储单位

|                   |                     |                    |                   |
| ----------------- | ------------------- | ------------------ | ----------------- |
| `1Byte = 8bit`    | `1K(千) = 1024Byte` | `1MB (兆) = 1024K` | `1G(吉) = 1024M`  |
| `1T(太) = 1024G`  | `1P (拍) = 1024T`   | `1E(艾) = 1024P`   | `1Z (泽) = 1024E` |
| `1Y (尧) = 1024Z` | `1B (布) = 1024Y`   | `1N (诺) = 1024B`  | `1D(刀) = 1024N`  |

![image-20220829154114202](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829154114202.png)

**1、Volume（数据体量大）** 截至目前，人类生产的所有印刷材料的数据量是200PB，而历史上全人类总共说过的话的数据量大约是5EB。当前，典型个人计算机硬盘的容量为TB量级，而一些大企业的数据量已经接近EB量级。

|                                                              |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![image-20220829154818173](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829154818173.png) | ![image-20220829155007021](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829155007021.png) |

**2、Velocity（速度快）** 大数据的高速性是指数据增长快速，处理快速，每一天，各行各业的数据都在呈现指数性爆炸增长。在许多场景下，数据都具有时效性，如搜索引擎要在几秒中内呈现出用户所需数据。企业或系统在面对快速增长的海量数据时，必须要高速处理，快速响应。 天猫双十一：2020年，订单峰值58.3W/s

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829160620660](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829160620660.png) |

**3、Variety（种类、来源多样）** 这种类型的多样性也让数据被分为结构化数据和非结构化数据。相对于以往便于存储的以数据库/文本为主的结构化数据，非结构化数据越来越多，包括网络日志、音频、视频、图片、地理位置信息等，这些多类型的数据对数据的处理能力提出了更高要求。

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829155942159](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829155942159.png) |

**4、Value（低价值密度）** 价值密度的高低与数据总量的大小成反比。 大数据的低价值密度性是指在海量的数据源中，真正有价值的数据少之又少，许多数据可能是错误的，是不完整的，是无法利用的。总体而言，有价值的数据占据数据总量的密度极低，提炼数据好比浪里淘沙。

> 深度复杂的挖掘分析需要机器学习参与

##### 1.2 大数据应用场景

1、电商领域：精准广告位、个性化推荐等等

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829161922194](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829161922194.png) |

2、传媒领域：精准营销、猜你喜欢、交互推荐

|                                                              |
| ------------------------------------------------------------ |
| ![图片1](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//%E5%9B%BE%E7%89%871.png) |

3、物流仓储：京东物流，上午下单下午送达、下午下单次日上午送达

|                                                              |                                                              |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![图片2](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//%E5%9B%BE%E7%89%872.png) | ![图片3](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//%E5%9B%BE%E7%89%873.png) |

4、金融：多维度体现用户特征，帮助金融机构推荐优质客户，防范欺诈风险。

5、房产：大数据全面助力房地产行业，打造精准投策与营销，选出更合适的地，建造更合适的楼，卖给更合适的人。

##### 1.3 大数据的发展“钱景”

1、政府政策报告中提出“推动互联网、大数据、人工智能和实体经济深度融合”。

2、从2020年初，中央推出万亿“新基建”投资计划

|                                                              |      |
| ------------------------------------------------------------ | ---- |
| ![图片4](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//%E5%9B%BE%E7%89%874.png) |      |

3、风口中 2020年就是5G的元年，国家在大力铺设5G设备，2022年就是5G手机应用的开始，也是大数据要爆发的1年。5G带来的是每秒钟10g的数据，会给每家公司都带来海量的数据。那么传统的Java工具根本解决不了海量数据的存储。就更不用说海量数据的计算了。如果你对5G的感触不够深，可以回忆一下3G和4G的区别。3G时只能打电话、发短信，当时还觉得很好，觉得3G不错。但是4G来了后，大家很少打电话和发短信了，都改为语音、视频、直播、网上购物等生活方式，带火了淘宝、京东、美团、字节跳动等企业。没有跟上节奏的百度，有点摇摇欲坠

> 自古不变的真理：`先入行者吃肉，后入行者喝汤，最后到的买单！`

4、Boss直聘网站上的部分大数据工程师薪水如下

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829165121688](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829165121688.png) |

##### 1.4 大数据业务流程分析

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829165215238](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829165215238.png) |

**步骤1：明确分析目的和思路**

目的是整个分析流程的起点：为数据的收集、处理及分析提供清晰的指引方向 思路是使分析框架体系化：先分析什么，后分析什么，使各分析点之间具有逻辑联系，保证分析维度的完整性、分析结果的有效性以及正确性 数据分析方法论：营销管理相关理论，用户行为理论、PEST分析法、5W2H分析法等

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829165804220](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829165804220.png) |

**步骤2：数据收集**

数据从无到有的过程：比如传感器收集气象数据、埋点收集用户行为数据 数据传输搬运的过程：比如采集数据库数据到数据分析平台

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829165325029](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829165325029.png) |

**步骤3：数据处理**

对收集到的数据进行加工整理，形成适合数据分析的样式 主要包括数据清洗、数据转化、数据提取、数据计算，保证数据的一致性和有效性

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829165519404](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829165519404.png) |

**步骤4：数据分析**

用适当的分析方法及工具，对处理过的数据进行分析，提取有价值的信息，形成有效结论的过程

- 需要掌握各种数据分析方法，还要熟悉数据分析软件的操作

数据挖掘本质是一种高级的数据分析方法

- 数据挖掘侧重解决四类数据分析问题：分类、聚类、关联和预测，重点在寻找模式和规律。

**步骤5：数据展现**

分析结果直观展示，人类是视觉动物 数据是通过表格和图形的方式来呈现，用图表说话

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829170059094](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829170059094.png) |

##### 1.5 大数据部门内部组织结构

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829171052806](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829171052806.png) |

### 任务二、Hadoop是什么

#### **【任务目标】**

大数据领域中hadoop是基础，所以我们主要从hadoop是什么及其发展的历史做一些了解，并且掌握hadoop的优势和组成，在此之上我们也可以提前了解一些大数据技术生态的体系！

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009363237819)

视频-2、Hadoop是什么

##### 2.1 Hadoop是什么

1）Hadoop是一个由Apache基金会所开发的分布式系统基础架构。

2）主要解决，海量数据的存储和海量数据的分析计算问题。

3）广义上来说，Hadoop通常是指一个更广泛的概念——Hadoop生态圈。

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829172336350](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829172336350.png) |

##### 2.2 Hadoop发展历史

1）`Hadoop`创始人`Doug Cutting`，为了实现与`Google`类似的全文搜索功能，他在`Lucene`框架基础上进行优化升级，查询引擎和索引引擎。

| Hadoop创始人Doug Cutting                                     |
| ------------------------------------------------------------ |
| ![image-20220829172657980](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829172657980.png) |

2）2001年年底`Lucene`成为`Apache`基金会的一个子项目。

3）对于海量数据的场景，`Lucene`框架面对与`Google`同样的困难，存储海量数据困难，检索海量速度慢。

4）学习和模仿Google解决这些问题的办法 ：微型版`Nutch`。

5）可以说`Google`是`Hadoop`的思想之源（`Google`在大数据方面的三篇论文）

**`GFS --->HDFS`** **`Map-Reduce --->MR`** **`BigTable --->HBase`**

6）2003-2004年，Google公开了部分 `GFS` 和MapReduce思想的细节，以此为基础`Doug Cutting`等人用了2年业余时间实现了 `DFS` 和 `MapReduce` 机制，使 `Nutch` 性能飙升。

7）2005年Hadoop作为`Lucene`的子项目`Nutch`的一部分正式引入`Apache`基金会。

8）2006年3月份，`Map-Reduce`和`Nutch Distributed File System （NDFS）`分别被纳入到 `Hadoop `项目中，`Hadoop`就此正式诞生，标志着大数据时代来临。

9）名字来源于`Doug Cutting`儿子的玩具大象

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829172557947](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829172557947.png) |

##### 2.3 Hadoop发行版本

开源社区版：Apache开源社区发行，也是官方发行版本，优点：更新迭代快，缺点：兼容稳定不周

商业发行版：基于Apache开源协议，某些服务需要收费，优点：稳定兼容好，缺点：收费 版本更新慢

Hadoop三大发行版本：Apache、Cloudera、Hortonworks。

- Apache版本最原始（最基础）的版本，对于入门学习最好。2006
- Cloudera内部集成了很多大数据框架，对应产品CDH。2008
- Hortonworks文档较好，对应产品HDP。2011
- Hortonworks现在已经被Cloudera公司收购，推出新的品牌CDP。

**1）Apache Hadoop** 官网地址：`http://hadoop.apache.org`

 下载地址：`https://hadoop.apache.org/releases.html`

**2）Cloudera Hadoop** 官网地址：`https://www.cloudera.com/downloads/cdh`

 下载地址：`https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_cdh_6_download.html`

> （1）2008年成立的Cloudera是最早将Hadoop商用的公司，为合作伙伴提供Hadoop的商用解决方案，主要是包括支持、咨询服务、培训。 （2）2009年Hadoop的创始人Doug Cutting也加盟Cloudera公司。Cloudera产品主要为CDH，Cloudera Manager，Cloudera Support （3）CDH是Cloudera的Hadoop发行版，完全开源，比Apache Hadoop在兼容性，安全性，稳定性上有所增强。Cloudera的标价为每年每个节点10000美元。 （4）Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。

##### 2.4 Hadoop优势

- 高可靠性：Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或者存储出现故障，也不会导致数据的丢失。
- 高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。
- 高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。
- 高容错性：能够自动将失效的任务重新分配。

### 任务三、Hadoop组成（面试重点）

#### **【任务目标】**

在熟悉hadoop的运行环境前，我们还需要掌握hadoop架构的各种进程，而且从hadoop1.x时代到现在的hadoop3.x时代各种进程发生了一些变化，这在企业面试过程中也会经常出现，所以本任务的主要目的就是要求大家先熟悉hadoop的组成

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009362959010)

视频-3、Hadoop组成（面试重点）

##### 3.1 `hadoop1.x、2.x、3.x`的区别

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829210609431](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829210609431.png) |

在Hadoop1.x时代，Hadoop中的MapReduce同时处理业务逻辑运算和资源的调度，耦合性较大。

在Hadoop2.x时代，增加了Yarn。Yarn只负责资源的调度，MapReduce只负责运算。

Hadoop3.x在组成上没有变化。

##### 3.2 HDFS架构

Hadoop Distributed File System，简称HDFS，是一个分布式文件系统。

| HDFS架构                                                     |
| ------------------------------------------------------------ |
| ![image-20220829211013696](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829211013696.png) |

1）`NameNode（nn）`：存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的DataNode等。

2）`DataNode(dn)`：在本地文件系统存储文件块数据，以及块数据的校验和。

3）`Secondary NameNode(2nn)`：每隔一段时间对NameNode元数据备份。

##### 3.3 YARN架构概述

`Yet Another Resource Negotiator`简称YARN ，另一种资源协调者，是Hadoop的资源管理器。

| YARN架构                                                     |
| ------------------------------------------------------------ |
| ![yarn_architecture](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//yarn_architecture.gif) |

1）`ResourceManager（RM）`：整个集群资源（内存、CPU等）的老大

2）`NodeManager（NM）`：单个节点服务器资源老大

3）`ApplicationMaster（AM）`：单个任务运行的老大

4）`Container`：容器，相当一台独立的服务器，里面封装了任务运行所需要的资源，如内存、CPU、磁盘、网络等。

> 说明1：客户端可以有多个
>
> 说明2：集群上可以运行多个ApplicationMaster
>
> 说明3：每个NodeManager上可以有多个Container

##### 3.4 MapReduce架构概述

| MR任务执行流程                                               |
| ------------------------------------------------------------ |
| ![image-20220829211502245](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829211502245.png) |

MapReduce将计算过程分为两个阶段：`Map`和`Reduce`

> 1）Map阶段并行处理输入数据 2）Reduce阶段对Map结果进行汇总

##### 3.5 HDFS/YARN/MapReduce相关作用

|                                                              |
| ------------------------------------------------------------ |
| ![QQ图片20220829213401](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//QQ%E5%9B%BE%E7%89%8720220829213401.png) |

- HDFS是进行数据存储
- YARN是进行资源管理
- MapReduce是进行任务的具体执行

### 任务四、熟悉Hadoop运行环境

#### **【任务目标】**

我们已经了解了大数据的一些基本概念和具体内容，也知道了hadoop各进程之间关系和作用，那么接下来我们可以切身感觉一下hadoop的运行环境，掌握hadoop各个进程的启动过程并能运行MR程序，重点是掌握一些基本的操作命令！

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009362712436)

视频-4-1、熟悉Hadoop运行环境1

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009361008567)

视频-4-2、熟悉Hadoop运行环境2

##### 4.1 Hadoop目录结构

1）查看Hadoop目录结构

```
[itluma@4a856475ffe5 hadoop-3.1.3]# ll
total 180
drwxr-xr-x 2 1000 1000    183 9月  12  2019 bin
drwxr-xr-x 1 root root     37 2月   6 11:54 data
drwxr-xr-x 1 1000 1000     20 9月  12  2019 etc
drwxr-xr-x 2 1000 1000    106 9月  12  2019 include
drwxr-xr-x 3 1000 1000     20 9月  12  2019 lib
drwxr-xr-x 4 1000 1000    288 9月  12  2019 libexec
-rw-rw-r-- 1 1000 1000 147145 9月   4  2019 LICENSE.txt
drwxr-xr-x 1 root root   4096 3月  22 15:57 logs
-rw-rw-r-- 1 1000 1000  21867 9月   4  2019 NOTICE.txt
-rw-rw-r-- 1 1000 1000   1366 9月   4  2019 README.txt
drwxr-xr-x 3 1000 1000   4096 2月   6 11:49 sbin
drwxr-xr-x 1 1000 1000     20 9月  12  2019 share
drwxr-xr-x 2 root root     22 2月   6 11:33 wcinput
drwxr-xr-x 2 root root     88 2月   6 11:33 wcoutput复制代码
```

2）重要目录 （1）bin目录：存放对Hadoop相关服务（hdfs，yarn，mapred）进行操作的脚本 （2）etc目录：Hadoop的配置文件目录，存放Hadoop的配置文件 （3）lib目录：存放Hadoop的本地库（对数据进行压缩解压缩功能） （4）sbin目录：存放启动或停止Hadoop相关服务的脚本 （5）share目录：存放Hadoop的依赖jar包、文档、和官方案例

1. 配置文件说明

Hadoop配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值。

（1）默认配置文件：

| 要获取的默认文件     | 文件存放在Hadoop的jar包中的位置                             |
| -------------------- | ----------------------------------------------------------- |
| `core-default.xml`   | `hadoop-common-3.1.3.jar/core-default.xml`                  |
| `hdfs-default.xml`   | `hadoop-hdfs-3.1.3.jar/hdfs-default.xml`                    |
| `yarn-default.xml`   | `hadoop-yarn-common-3.1.3.jar/yarn-default.xml`             |
| `mapred-default.xml` | `hadoop-mapreduce-client-core-3.1.3.jar/mapred-default.xml` |

（2）自定义配置文件： **`core-site.xml`\**、\**`hdfs-site.xml`\**、\**`yarn-site.xml`\**、\**`mapred-site.xml**四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径上，用户可以根据项目需求重新进行修改配置。

##### 4.2 各进程启动/停止

1）第一次启动，需要格式化NameNode（注意：`格式化NameNode，会产生新的id，可能导致NameNode和DataNode的id不一致，节点找不到已往数据。如果节点在运行过程中报错，需要重新格式化NameNode的话，一定要先停止namenode和datanode进程，并且要删除所有机器的data和logs目录，然后再进行格式化`）

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hdfs namenode -format复制代码
```

| HDFS格式化                                                   |
| ------------------------------------------------------------ |
| ![image-20220829222613525](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829222613525.png) |

格式化成功后我们能发现，在当前目录下生成了data和logs目录

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829222715578](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829222715578.png) |

2）启动HDFS

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ ./sbin/start-dfs.sh 复制代码
```

| 启动HDFS进程                                                 |
| ------------------------------------------------------------ |
| ![image-20220829221702035](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829221702035.png) |

3）启动YARN

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ sbin/start-yarn.sh复制代码
```

4）Web端查看HDFS的NameNode （a）浏览器中输入：`http://localhost:9870` （b）查看HDFS上存储的数据信息

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829223024141](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829223024141.png) |

5）Web端查看YARN的ResourceManager （a）浏览器中输入：`http://localhost:8088` （b）查看YARN上运行的Job信息

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829223112955](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829223112955.png) |

6）历史服务器的启动

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ mapred --daemon start historyserver复制代码
```

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829223357011](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829223357011.png) |

浏览器中输入：`http://localhost:19888/jobhistory`

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829223508271](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829223508271.png) |

7）各服务进程的停止

```
#停止HDFS服务
[itluma@4a856475ffe5 hadoop-3.1.3]$ ./sbin/stop-dfs.sh 
#停止yarn服务
[itluma@4a856475ffe5 hadoop-3.1.3]$ ./sbin/stop-yarn.sh 
#停止历史服务进程
[itluma@4a856475ffe5 hadoop-3.1.3]$ mapred --daemon stop historyserver复制代码
```

8）通过java进程的jps命令可以查看当前节点上各服务器进程的执行情况

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ jps
19745 SecondaryNameNode
19554 DataNode
20132 NodeManager
20725 Jps
19979 ResourceManager
19404 NameNode
20557 JobHistoryServer复制代码
```

##### 4.3 启动/停止方式总结

1）各个模块分开启动/停止 常用 （1）整体启动/停止HDFS

```
start-dfs.sh/stop-dfs.sh复制代码
```

 （2）整体启动/停止YARN

```
start-yarn.sh/stop-yarn.sh复制代码
```

2）各个服务组件逐一启动/停止 （1）分别启动/停止HDFS组件

```
hdfs --daemon start/stop namenode/datanode/secondarynamenode复制代码
```

 （2）启动/停止YARN

```
yarn --daemon start/stop  resourcemanager/nodemanager复制代码
```

##### 4.4 编写Hadoop常用脚本

1）Hadoop各服务启停脚本（包含HDFS、Yarn、Historyserver）：`myhadoop.sh`

```
[itluma@4a856475ffe5 ~]$ cd /usr/local/bin
[itluma@4a856475ffe5 bin]$ vim myhadoop.sh复制代码
#!/bin/bash

if [ $# -lt 1 ]
then
    echo "No Args Input..."
    exit ;
fi

case $1 in
"start")
        echo " =================== begin hadoop ==================="

        echo " --------------- start hdfs ---------------"
        ssh localhost "/opt/module/hadoop-3.1.3/sbin/start-dfs.sh"
        echo " --------------- start yarn ---------------"
        ssh localhost "/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"
        echo " --------------- start historyserver ---------------"
        ssh localhost "/opt/module/hadoop-3.1.3/bin/mapred --daemon start historyserver"
;;
"stop")
        echo " =================== stop hadoop ==================="

        echo " --------------- stop historyserver ---------------"
        ssh localhost "/opt/module/hadoop-3.1.3/bin/mapred --daemon stop historyserver"
        echo " --------------- stop yarn ---------------"
        ssh localhost "/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"
        echo " --------------- stop hdfs ---------------"
        ssh localhost "/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh"
;;
*)
    echo "Input Args Error..."
;;
esac复制代码
```

保存后退出，然后赋予脚本执行权限

```
[itluma@4a856475ffe5 bin]$ chmod +x myhadoop.sh复制代码
```

通过`myhadoop.sh`脚本启动hadoop各服务器进程

| 启动服务                                                     |
| ------------------------------------------------------------ |
| ![image-20220829224956649](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829224956649.png) |

通过`myhadoop.sh`脚本停止hadoop各服务器进程

| 停止服务                                                     |
| ------------------------------------------------------------ |
| ![image-20220829224904751](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829224904751.png) |

##### 4.5 HDFS文件系统的使用

（1）本地创建测试文件

 1）创建在hadoop-3.1.3文件下面创建一个`wcinput`文件夹

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ mkdir wcinput复制代码
```

 2）在`wcinput`文件下创建一个`word.txt`文件

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ cd wcinput复制代码
```

 3）编辑`word.txt`文件

```
[itluma@4a856475ffe5 wcinput]$ vim word.txt复制代码
```

- 在文件中输入如下内容

```
hadoop yarn
hadoop mapreduce
itluma 
itluma复制代码
```

- 保存退出：`:wq`

 4）回到Hadoop目录`/opt/module/hadoop-3.1.3`

------

（2）上传文件到集群

- 上传`小文件`：先在`hdfs`上创建一个文件夹

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -mkdir /input复制代码
```

- 通过-put选项实现文件上传

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -put $HADOOP_HOME/wcinput/word.txt /input复制代码
```

| 文件上传                                                     |
| ------------------------------------------------------------ |
| ![image-20220829230207708](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829230207708.png) |

- 上传大文件

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -put  /opt/software/jdk-8u212-linux-x64.tar.gz  /复制代码
```

在`hdfs`的web页面中我们也能看到对应的文件所在位置

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829230431353](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829230431353.png) |

------

（3）上传文件后查看文件本地存放在什么位置

- 查看HDFS文件存储路径

```
[itluma@4a856475ffe5 subdir0]$ pwd
/opt/module/hadoop-3.1.3/data/dfs/data/current/BP-1436128598-192.168.10.102-1610603650062/current/finalized/subdir0/subdir0复制代码
```

- 查看HDFS在磁盘存储文件内容

```
[itluma@4a856475ffe5 subdir0]$ cat blk_1073741825
hadoop yarn
hadoop mapreduce 
itluma
itluma复制代码
```

------

（4）拼接

```
-rw-rw-r--. 1 itluma itluma 134217728 5月  23 16:01 blk_1073741836
-rw-rw-r--. 1 itluma itluma 1048583 5月  23 16:01 blk_1073741836_1012.meta
-rw-rw-r--. 1 itluma itluma 63439959 5月  23 16:01 blk_1073741837
-rw-rw-r--. 1 itluma itluma 495635 5月  23 16:01 blk_1073741837_1013.meta
[itluma@4a856475ffe5 subdir0]$ cat blk_1073741836>>tmp.tar.gz
[itluma@4a856475ffe5 subdir0]$ cat blk_1073741837>>tmp.tar.gz
[itluma@4a856475ffe5 subdir0]$ tar -zxvf tmp.tar.gz复制代码
```

------

（5）采用get选项实现文件的下载

```
[itluma@4a856475ffe5 software]$ hadoop fs -get /jdk-8u212-linux-x64.tar.gz ./复制代码
```

##### 4.6 MapReduce执行过程

（1）通过hadoop提供的一个`wordcount`实现，看看MapReduce的执行过程

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output复制代码
```

完整的MR执行过程

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -put $HADOOP_HOME/wcinput/word.txt /input
2022-08-29 15:01:42,062 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
[itluma@4a856475ffe5 hadoop-3.1.3]$  hadoop fs -put  /opt/software/jdk-8u212-linux-x64.tar.gz  /
2022-08-29 15:04:04,291 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2022-08-29 15:04:04,682 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
[itluma@4a856475ffe5 hadoop-3.1.3]$ 
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output
2022-08-29 15:08:44,098 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032
2022-08-29 15:08:44,596 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/itluma/.staging/job_1661784577797_0001
2022-08-29 15:08:44,693 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2022-08-29 15:08:45,243 INFO input.FileInputFormat: Total input files to process : 1
2022-08-29 15:08:45,272 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2022-08-29 15:08:45,701 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2022-08-29 15:08:46,116 INFO mapreduce.JobSubmitter: number of splits:1
2022-08-29 15:08:46,235 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false
2022-08-29 15:08:46,647 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1661784577797_0001
2022-08-29 15:08:46,647 INFO mapreduce.JobSubmitter: Executing with tokens: []
2022-08-29 15:08:46,799 INFO conf.Configuration: resource-types.xml not found
2022-08-29 15:08:46,799 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.
2022-08-29 15:08:47,185 INFO impl.YarnClientImpl: Submitted application application_1661784577797_0001
2022-08-29 15:08:47,223 INFO mapreduce.Job: The url to track the job: http://4a856475ffe5:8088/proxy/application_1661784577797_0001/
2022-08-29 15:08:47,224 INFO mapreduce.Job: Running job: job_1661784577797_0001
2022-08-29 15:08:53,328 INFO mapreduce.Job: Job job_1661784577797_0001 running in uber mode : false
2022-08-29 15:08:53,330 INFO mapreduce.Job:  map 0% reduce 0%
2022-08-29 15:08:57,412 INFO mapreduce.Job:  map 100% reduce 0%
2022-08-29 15:09:02,456 INFO mapreduce.Job:  map 100% reduce 100%
2022-08-29 15:09:03,473 INFO mapreduce.Job: Job job_1661784577797_0001 completed successfully
2022-08-29 15:09:03,570 INFO mapreduce.Job: Counters: 53
    File System Counters
        FILE: Number of bytes read=59
        FILE: Number of bytes written=434701
        FILE: Number of read operations=0
        FILE: Number of large read operations=0
        FILE: Number of write operations=0
        HDFS: Number of bytes read=145
        HDFS: Number of bytes written=37
        HDFS: Number of read operations=8
        HDFS: Number of large read operations=0
        HDFS: Number of write operations=2
    Job Counters 
        Launched map tasks=1
        Launched reduce tasks=1
        Data-local map tasks=1
        Total time spent by all maps in occupied slots (ms)=1823
        Total time spent by all reduces in occupied slots (ms)=2380
        Total time spent by all map tasks (ms)=1823
        Total time spent by all reduce tasks (ms)=2380
        Total vcore-milliseconds taken by all map tasks=1823
        Total vcore-milliseconds taken by all reduce tasks=2380
        Total megabyte-milliseconds taken by all map tasks=1866752
        Total megabyte-milliseconds taken by all reduce tasks=2437120
    Map-Reduce Framework
        Map input records=4
        Map output records=6
        Map output bytes=67
        Map output materialized bytes=59
        Input split bytes=101
        Combine input records=6
        Combine output records=4
        Reduce input groups=4
        Reduce shuffle bytes=59
        Reduce input records=4
        Reduce output records=4
        Spilled Records=8
        Shuffled Maps =1
        Failed Shuffles=0
        Merged Map outputs=1
        GC time elapsed (ms)=89
        CPU time spent (ms)=1170
        Physical memory (bytes) snapshot=534593536
        Virtual memory (bytes) snapshot=5303119872
        Total committed heap usage (bytes)=608174080
        Peak Map Physical memory (bytes)=312905728
        Peak Map Virtual memory (bytes)=2647982080
        Peak Reduce Physical memory (bytes)=221687808
        Peak Reduce Virtual memory (bytes)=2655137792
    Shuffle Errors
        BAD_ID=0
        CONNECTION=0
        IO_ERROR=0
        WRONG_LENGTH=0
        WRONG_MAP=0
        WRONG_REDUCE=0
    File Input Format Counters 
        Bytes Read=44
    File Output Format Counters 
        Bytes Written=37复制代码
```

查看任务的执行结果

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829231149553](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829231149553.png) |

在yarn资源调度平台上我们可以看到MR任务的执行情况

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829231349116](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829231349116.png) |

历史记录详情

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829231500217](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829231500217.png) |

查看任务的运行日志记录

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829231600891](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829231600891.png) |

运行日志详情

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220829231636678](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_1//image-20220829231636678.png) |

##### 4.7 常用端口说明

| 端口名称                  | Hadoop2.x   | Hadoop3.x          |
| ------------------------- | ----------- | ------------------ |
| NameNode内部通信端口      | 8020 / 9000 | `8020` / 9000/9820 |
| NameNode HTTP UI          | `50070`     | `9870`             |
| MapReduce查看执行任务端口 | 8088        | 8088               |
| 历史服务器通信端口        | 19888       | 19888              |



# 实验1-2 Hadoop之HDFS

## 实验概述

随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是`分布式文件管理系统`。HDFS只是分布式文件管理系统中的一种，这里我们重点是掌握HDFS文件系统。所以在本章节的实验中我们需要掌握HDFS的原理、Shell操作以及读写流程，有了这些基础后我们才能对HDFS的使用得心应手，最后还需要掌握NameNode、SecondaryNameNode以及DataNode各进程的核心工作原理，这也为后续的学习打好坚实的基础！

## 实验环境

- AtStudy 实训平台
- Hadoop3.1

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16799930749819556.png)

## 实验目标

学习完成本实验后，您将能够

- 掌握什么是分布式文件系统
- HDFS的存储原理
- HDFS的Shell操作
- HDFS的读写流程
- NameNode、SecondaryNameNode、DataNode的工作机制

## 实验任务

### 任务一、HDFS这些你应该先知道

#### **【任务目标】**

在操作HDFS之前，我们先需要从理论方面了解一些HDFS一些相关内容，像什么是HDFS及其优缺点，HDFS的组成架构我们应该先知道！

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009362453995)

视频-1、HDFS这些你应该先知道

##### 1.1 HDFS定义

HDFS（Hadoop Distributed File System），它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。

HDFS的使用场景：适合一次写入，多次读出的场景。一个文件经过创建、写入和关闭之后就不需要改变。

------

![image-20210823165628813](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_2/image-20210823165628813.png) **分布式与集群的联系和区别**

> - 分布式是指将不同的业务分布到不同的地方。
> - 而集群是指将几台服务器集中在一起，实在同一个业务。
> - 分布式的每一个节点，都可以用来做集群。而集群不一定就是分布式了
>
> 例如：互联网上访问的人多了，就可以做一个集群，前面放一个响应服务器，后面几台服务器完成同一业务，如果有业务访问的时候，响应服务器看哪台服务器的负载不是很重，就将任务交给哪台去完成。
>
> 而分布式，从狭义上理解，也与集群差不多，但是它的组织比较松散，不像集群，有一定组织性，一台服务器宕了，其他的服务器可以顶上来。分布式的每一个节点，都完成不同的业务，一个节点宕了，这个业务就不可访问了。

##### 1.2 HDFS优缺点

###### 1.2.1 优点

(1) 高容错性 数据自动保存多个副本。它通过增加副本的形式，提高容错性。某一个副本丢失以后，它可以自动恢复，这是由 HDFS 内部机制实现的，我们不必关心。

(2) 适合大数据处理 数据规模：能够处理数据规模达到 GB、TB、甚至PB级别的数据。 文件规模：能够处理百万规模以上的文件数量，数量相当之大。 节点规模：能够处理10K节点的规模。

(3) 流式数据访问 一次写入，多次读取，不能修改，只能追加。 它能保证数据的一致性。

(5) 可构建在廉价机器上 它通过多副本机制，提高可靠性。 它提供了容错和恢复机制。比如某一个副本丢失，可以通过其它副本来恢复。

###### 1.2.2 缺点

(1) 不适合低延时数据访问 比如毫秒级的来存储数据，这是不行的，它做不到。 它适合高吞吐率的场景，就是在某一时间内写入大量的数据。但是它在低延时的情况 下是不行的，比如毫秒级以内读取数据，这样它是很难做到的。

(2) 无法高效的对大量小文件进行存储 存储大量小文件的话，它会占用 NameNode大量的内存来存储文件、目录和块信息。这样是不可取的，因为NameNode的内存总是有限的。 小文件存储的寻道时间会超过读取时间，它违反了HDFS的设计目标。

(3) 并发写入、文件随机修改 一个文件只能有一个写，不允许多个线程同时写。 仅支持数据 append（追加），不支持文件的随机修改。

##### 1.3 HDFS组成架构

| 官方架构图                                                   |
| ------------------------------------------------------------ |
| ![image-20220830100211807](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_2/image-20220830100211807.png) |

参考官方文档：`https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html`

1 ) NameNode （NN）：就是Master，它是一个主管、管理者。

 (1）管理HDFS的名称空间; (2）配置副本策略; (3）管理数据块（Block)映射信息; (4) 处理客户端读写请求。

1. DataNode（DN）：就是Slave。NameNode下达命令，DataNode执行实际的操作。

 (1）存储实际的数据块; (2）执行数据块的读/写操作。

3）Client：就是客户端。

（1）文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传； （2）与NameNode交互，获取文件的位置信息； （3）与DataNode交互，读取或者写入数据； （4）Client提供一些命令来管理HDFS，比如NameNode格式化； （5）Client可以通过一些命令来访问HDFS，比如对HDFS增删查改操作；

4）Secondary NameNode（2NN）：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。

（1）辅助NameNode，分担其工作量，比如定期合并Fsimage和Edits，并推送给NameNode ； （2）在紧急情况下，可辅助恢复NameNode。

##### 1.4 HDFS的文件块大小

HDFS 中的文件在物理上是分块存储（Block），块的大小可以通过配置参数( dfs.blocksize）来规定，默认大小在Hadoop2.x/3.x版本中是128M，1.x版本中是64M。

> ## 举个栗子
>
> - 假如磁盘的寻址时间约为 10ms，即查找到目标 Block 的时间为 10ms
> - 根据规定(专家)，寻址时间为传输时间的 1%时，为最佳状态。因此传输时间= 10ms / 0.1 = 1s
> - 若磁盘的传输速率为 100M/s
> - 那么块的大小最佳为 100M。

以上例子都取的大概值，根据测算，机械硬盘中块的大小最佳为 128M

![image-20210823165628813](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_2/image-20210823161753051.png) **为什么块的大小不能设置太小，也不能设置太大？**

> （1）HDFS的块设置太小，会增加寻址时间，程序一直在找块的开始位置；

> （2）如果块设置的太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。导致程序在处理这块数据时，会非常慢。

> 总结：HDFS块的大小设置主要取决于磁盘传输速率。

### 任务二、HDFS的Shell操作(开发重点)

#### **【任务目标】**

掌握HDFS的各种命令，这是我们使用HDFS的基础，所以，多操作才是王道！

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009363116407)

视频-2、HDFS的Shell操作

##### 2.1、基本语法

**hadoop fs 具体命令** OR **hdfs dfs 具体命令**

两个是完全相同的。

##### 2.2、命令大全

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ bin/hadoop fs

[-appendToFile <localsrc> ... <dst>]
        [-cat [-ignoreCrc] <src> ...]
        [-chgrp [-R] GROUP PATH...]
        [-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]
        [-chown [-R] [OWNER][:[GROUP]] PATH...]
        [-copyFromLocal [-f] [-p] <localsrc> ... <dst>]
        [-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
        [-count [-q] <path> ...]
        [-cp [-f] [-p] <src> ... <dst>]
        [-df [-h] [<path> ...]]
        [-du [-s] [-h] <path> ...]
        [-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
        [-getmerge [-nl] <src> <localdst>]
        [-help [cmd ...]]
        [-ls [-d] [-h] [-R] [<path> ...]]
        [-mkdir [-p] <path> ...]
        [-moveFromLocal <localsrc> ... <dst>]
        [-moveToLocal <src> <localdst>]
        [-mv <src> ... <dst>]
        [-put [-f] [-p] <localsrc> ... <dst>]
        [-rm [-f] [-r|-R] [-skipTrash] <src> ...]
        [-rmdir [--ignore-fail-on-non-empty] <dir> ...]
<acl_spec> <path>]]
        [-setrep [-R] [-w] <rep> <path> ...]
        [-stat [format] <path> ...]
        [-tail [-f] <file>]
        [-test -[defsz] <path>]
        [-text [-ignoreCrc] <src> ...]复制代码
```

##### 2.3、常用操作

###### 2.3.1 准备工作

1）启动Hadoop服务（方便后续的测试）

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ sbin/start-dfs.sh
[itluma@4a856475ffe5 hadoop-3.1.3]$ sbin/start-yarn.sh复制代码
```

2）-help：输出这个命令参数

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -help rm复制代码
```

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220831165520358](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_2/image-20220831165520358.png) |

3）创建/sanguo文件夹

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -mkdir /sanguo复制代码
```

###### 2.3.2 文件上传

1）-moveFromLocal：从本地剪切粘贴到HDFS

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ vim shuguo.txt
#输入：
shuguo

[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs  -moveFromLocal  ./shuguo.txt  /sanguo复制代码
```

2）-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去

```
[itluma@hadoop102 hadoop-3.1.3]$ vim weiguo.txt
#输入：
weiguo

[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -copyFromLocal weiguo.txt /sanguo复制代码
```

3）-put：等同于copyFromLocal，生产环境更习惯用put

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ vim wuguo.txt
#输入：
wuguo

[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -put ./wuguo.txt /sanguo复制代码
```

4）-appendToFile：追加一个文件到已经存在的文件末尾

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ vim liubei.txt
#输入：
liubei

[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -appendToFile liubei.txt /sanguo/shuguo.txt复制代码
```

###### 2.3.3 文件下载

1）-copyToLocal：从HDFS拷贝到本地

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -copyToLocal /sanguo/shuguo.txt ./复制代码
```

2）-get：等同于copyToLocal，生产环境更习惯用get

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -get /sanguo/shuguo.txt ./shuguo2.txt复制代码
```

###### 2.3.4 其他常用命令

1）-ls: 显示目录信息

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -ls /sanguo复制代码
```

2）-cat：显示文件内容

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -cat /sanguo/shuguo.txt复制代码
```

3）-chgrp、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs  -chmod 666  /sanguo/shuguo.txt
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs  -chown  itluma:itluma /sanguo/shuguo.txt复制代码
```

4）-mkdir：创建路径

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -mkdir /jinguo复制代码
```

5）-cp：从HDFS的一个路径拷贝到HDFS的另一个路径

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -cp /sanguo/shuguo.txt /jinguo复制代码
```

6）-mv：在HDFS目录中移动文件

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -mv /sanguo/wuguo.txt /jinguo
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -mv /sanguo/weiguo.txt /jinguo复制代码
```

7）-tail：显示一个文件的末尾1kb的数据

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -tail /jinguo/shuguo.txt复制代码
```

8）-rm：删除文件或文件夹

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -rm /sanguo/shuguo.txt复制代码
```

9）-rm -r：递归删除目录及目录里面内容

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -rm -r /sanguo复制代码
```

10）-du统计文件夹的大小信息

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -du -s -h /jinguo
27  81  /jinguo

[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -du  -h /jinguo
14  42  /jinguo/shuguo.txt
7   21   /jinguo/weiguo.txt
6   18   /jinguo/wuguo.tx复制代码
```

>  说明：27表示文件大小；81表示27*3个副本；`/jinguo`表示查看的目录

11）-setrep：设置HDFS中文件的副本数量

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -setrep 10 /jinguo/shuguo.txt复制代码
```

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830152303094](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_2/image-20220830152303094.png) |

这里设置的副本数只是记录在NameNode的元数据中，是否真的会有这么多副本，还得看DataNode的数量。因为目前只有1台设备，最多也就1个副本，只有节点数的增加到10台时，副本数才能达到10。

### 任务三、HDFS读写流程

#### **【任务目标】**

详细分析HDFS的读写过程，掌握它的基本过程，能够到达将其流程绘制出来的目的，这样我们才能真正的对HDFS理解，这也是企业面试过程中经常被问的内容。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009361253972)

视频-3、HDFS读写流程

##### 3.1、HDFS写数据流程

###### 3.1.1 文件写入的详细流程

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830141027845](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_2/image-20220830141027845.png) |

（1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。

（2）NameNode返回是否可以上传。

（3）客户端请求第一个 Block上传到哪几个DataNode服务器上。

（4）NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。

（5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。

（6）dn1、dn2、dn3逐级应答客户端。

（7）客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。

（8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。

###### 3.1.2 流水线（管道）复制

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830142251498](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_2/image-20220830142251498.png) |

当客户端向 HDFS 文件写入数据的时候，一开始是写到本地临时文件中。假设该文件的副本系数设置为 3 ，当本地临时文件累积到一个数据块的大小时，客户端会从 NameNode 获取一个 DataNode 列表用于存放副本。然后客户端开始向第一个 DataNode 传输数据，第一个 DataNode 一小部分一小部分 (4 KB) 地接收数据，将每一部分写入本地仓库，并同时传输该部分到列表中第二个 DataNode 节点。第二个 DataNode 也是这样，一小部分一小部分地接收数据，写入本地仓库，并同时传给第三个 DataNode 。最后，第三个 DataNode 接收数据并存储在本地。因此， DataNode 能流水线式地从前一个节点接收数据，并在同时转发给下一个节点，数据以流水线的方式从前一个DataNode 复制到下一个

![image-20210823165628813](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_2/image-20210823161753051.png)**为什么采用这种设计方式？**

> 为什么datanode之间采用pipeline流水线复制，而不是一次给三个datanode拓扑式传输呢？
>
> 顺序的沿着一个方向传输，这样能够充分利用每个机器的带宽，避免网络瓶颈和高延迟时的连接，最小化推送所有数据的延时。

###### 3.1.3 机架感知（副本存储节点选择）

副本的放置对HDFS的可靠性和性能至关重要。机架感知副本放置策略的目的是提高数据可靠性、可用性和网络带宽利用率。

大型HDFS实例运行在通常分布在多个机架上的计算机集群上。不同机架上的两个节点之间的通信必须通过交换机。在大多数情况下，同一机架中的计算机之间的网络带宽大于不同机架中的计算机之间的网络带宽。

NameNode通过中确定每个DataNode所属的机架id，一个简单但非最佳的策略是将副本放在唯一的机架上。这可以防止整个机架发生故障时丢失数据，并允许在读取数据时使用多个机架的带宽。此策略在群集中均匀分布副本，这使得在组件故障时很容易平衡负载。但是，此策略会增加写入成本，因为写入操作需要将块传输到多个机架。

HDFS默认放置策略是将一个副本放在本地机架的一个节点上，另一个副本放在本地机架的不同节点上，最后一个副本放在不同机架的不同节点上。

- 此策略可减少机架间的写入流量，从而提高写入性能。机架故障的概率远小于节点故障的概率；
- 此策略不影响数据可靠性和可用性保证。但是，它确实减少了读取数据时使用的总网络带宽，因为一个块只放在两个唯一的机架中，而不是放在三个机架中。
- 使用此策略，文件的副本不会均匀分布在机架上。三分之一的副本位于一个节点上，三分之二的副本位于一个机架上，另外三分之一的副本均匀分布在其余机架上。
- 此策略在不影响数据可靠性或读取性能的情况下提高了写入性能。

| 其实机架感知听起来高大上，但可以理解为副本节点的位置的选择就行 |
| ------------------------------------------------------------ |
| ![image-20220830114359715](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_2/image-20220830114359715.png) |

##### 3.2、HDFS读数据流程

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830142609407](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_2/image-20220830142609407.png) |

（1）客户端通过DistributedFileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。

（2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。

（3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。

（4）客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。

### 任务四、NameNode和SecondaryNameNode

#### **【任务目标】**

深入的掌握HDFS运行机制，还是需要从NameNode存储的内容及工作原理分析起，所以本次任务就是掌握NN的工作机制以及2NN的作用。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009363080049)

视频-4、NameNode和SecondaryNameNode

##### 4.1、NN和2NN的工作机制

![image-20210823165628813](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_2/image-20210823161753051.png)**思考：NameNode中的元数据是存储在哪里的？**

> 首先，我们做个假设，如果存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。`因此产生在磁盘中备份元数据的FsImage`。
>
> 这样又会带来新的问题，当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会发生一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件（只进行追加操作，效率很高）。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。
>
> 但是，如果长时间添加数据到Edits中，会导致该文件数据过大，效率降低，而且一旦断电，恢复元数据需要的时间过长。因此，需要定期进行FsImage和Edits的合并，如果这个操作由NameNode节点完成，又会效率过低。因此，引入一个新的节点SecondaryNamenode，专门用于FsImage和Edits的合并。

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830131307398](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_2/image-20220830131307398.png) |

1）第一阶段：NameNode启动 （1）第一次启动NameNode格式化后，创建Fsimage和Edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。

（2）客户端对元数据进行增删改的请求。

（3）NameNode记录操作日志，更新滚动日志。

（4）NameNode在内存中对元数据进行增删改。

2）第二阶段：Secondary NameNode工作 （1）Secondary NameNode询问NameNode是否需要CheckPoint。直接带回NameNode是否检查结果。

（2）Secondary NameNode请求执行CheckPoint。

（3）NameNode滚动正在写的Edits日志。

（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。

（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。

（6）生成新的镜像文件fsimage.chkpoint。

（7）拷贝fsimage.chkpoint到NameNode。

（8）NameNode将fsimage.chkpoint重新命名成fsimage。

##### 4.2、Fsimage和Edits原理

**Fsimage和Edits概念**

NameNode被格式化之后，将在/opt/module/hadoop-3.1.3/data/tmp/dfs/name/current目录中产生如下文件

```
fsimage_0000000000000000000
fsimage_0000000000000000000.md5
seen_txid
VERSION复制代码
```

（1）Fsimage文件：HDFS文件系统元数据的一个永久性的检查点，其中包含HDFS文件系统的所有目录和文件inode的序列化信息。

（2）Edits文件：存放HDFS文件系统的所有更新操作的路径，文件系统客户端执行的所有写操作首先会被记录到Edits文件中。

（3）seen_txid文件保存的是一个数字，就是最后一个edits_的数字

（4）每次NameNode启动的时候都会将Fsimage文件读入内存，加载Edits里面的更新操作，保证内存中的元数据信息是最新的、同步的，可以看成NameNode启动的时候就将Fsimage和Edits文件进行了合并。

###### 4.2.1 oiv查看Fsimage文件

（1）查看oiv和oev命令

```
[itluma@4a856475ffe5 current]$ hdfs
oiv            apply the offline fsimage viewer to an fsimage
oev            apply the offline edits viewer to an edits file复制代码
```

（2）基本语法

```
hdfs oiv -p 文件类型 -i镜像文件 -o 转换后文件输出路径复制代码
```

（3）案例实操

```
[itluma@4a856475ffe5 current]$ pwd /opt/module/hadoop-3.1.3/data/dfs/name/current

[itluma@4a856475ffe5 current]$ hdfs oiv -p XML -i fsimage_0000000000000000025 -o /opt/module/hadoop-3.1.3/fsimage.xml

[itluma@4a856475ffe5 current]$ cat /opt/module/hadoop-3.1.3/fsimage.xml复制代码
```

将显示的xml文件内容拷贝到Idea中创建的xml文件中，并格式化。部分显示结果如下。

```
<inode>
    <id>16386</id>
    <type>DIRECTORY</type>
    <name>user</name>
    <mtime>1512722284477</mtime>
    <permission>itluma:supergroup:rwxr-xr-x</permission>
    <nsquota>-1</nsquota>
    <dsquota>-1</dsquota>
</inode>
<inode>
    <id>16387</id>
    <type>DIRECTORY</type>
    <name>wcinput</name>
    <mtime>1512790549080</mtime>
    <permission>itluma:supergroup:rwxr-xr-x</permission>
    <nsquota>-1</nsquota>
    <dsquota>-1</dsquota>
</inode>
<inode>
    <id>16389</id>
    <type>FILE</type>
    <name>word.txt</name>
    <replication>1</replication>
    <mtime>1512722322219</mtime>
    <atime>1512722321610</atime>
    <perferredBlockSize>134217728</perferredBlockSize>
    <permission>itluma:supergroup:rw-r--r--</permission>
    <blocks>
        <block>
            <id>1073741825</id>
            <genstamp>1001</genstamp>
            <numBytes>59</numBytes>
        </block>
    </blocks>
</inode >复制代码
```

![image-20210823165628813](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_2/image-20210823161753051.png)**思考：可以看出，Fsimage中没有记录块所对应DataNode，为什么？**

> 在集群启动后，要求DataNode上报数据块信息，并间隔一段时间后再次上报。

###### 4.2.2 oev查看Edits文件

（1）基本语法

```
hdfs oev -p 文件类型 -i编辑日志 -o 转换后文件输出路径复制代码
```

（2）案例实操

```
[itluma@4a856475ffe5 current]$ hdfs oev -p XML -i edits_0000000000000000012-0000000000000000013 -o /opt/module/hadoop-3.1.3/edits.xml

[itluma@4a856475ffe5 current]$ cat /opt/module/hadoop-3.1.3/edits.xml复制代码
```

将显示的xml文件内容拷贝到Idea中创建的xml文件中，并格式化。显示结果如下。

```
<?xml version="1.0" encoding="UTF-8"?>
<EDITS>
    <EDITS_VERSION>-63</EDITS_VERSION>
    <RECORD>
        <OPCODE>OP_START_LOG_SEGMENT</OPCODE>
        <DATA>
            <TXID>129</TXID>
        </DATA>
    </RECORD>
    <RECORD>
        <OPCODE>OP_ADD</OPCODE>
        <DATA>
            <TXID>130</TXID>
            <LENGTH>0</LENGTH>
            <INODEID>16407</INODEID>
            <PATH>/hello7.txt</PATH>
            <REPLICATION>2</REPLICATION>
            <MTIME>1512943607866</MTIME>
            <ATIME>1512943607866</ATIME>
            <BLOCKSIZE>134217728</BLOCKSIZE>
            <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1544295051_1</CLIENT_NAME>
            <CLIENT_MACHINE>192.168.10.102</CLIENT_MACHINE>
            <OVERWRITE>true</OVERWRITE>
            <PERMISSION_STATUS>
                <USERNAME>itluma</USERNAME>
                <GROUPNAME>supergroup</GROUPNAME>
                <MODE>420</MODE>
            </PERMISSION_STATUS>
            <RPC_CLIENTID>908eafd4-9aec-4288-96f1-e8011d181561</RPC_CLIENTID>
            <RPC_CALLID>0</RPC_CALLID>
        </DATA>
    </RECORD>
    <RECORD>
        <OPCODE>OP_ALLOCATE_BLOCK_ID</OPCODE>
        <DATA>
            <TXID>131</TXID>
            <BLOCK_ID>1073741839</BLOCK_ID>
        </DATA>
    </RECORD>
    <RECORD>
        <OPCODE>OP_SET_GENSTAMP_V2</OPCODE>
        <DATA>
            <TXID>132</TXID>
            <GENSTAMPV2>1016</GENSTAMPV2>
        </DATA>
    </RECORD>
    <RECORD>
        <OPCODE>OP_ADD_BLOCK</OPCODE>
        <DATA>
            <TXID>133</TXID>
            <PATH>/hello7.txt</PATH>
            <BLOCK>
                <BLOCK_ID>1073741839</BLOCK_ID>
                <NUM_BYTES>0</NUM_BYTES>
                <GENSTAMP>1016</GENSTAMP>
            </BLOCK>
            <RPC_CLIENTID></RPC_CLIENTID>
            <RPC_CALLID>-2</RPC_CALLID>
        </DATA>
    </RECORD>
    <RECORD>
        <OPCODE>OP_CLOSE</OPCODE>
        <DATA>
            <TXID>134</TXID>
            <LENGTH>0</LENGTH>
            <INODEID>0</INODEID>
            <PATH>/hello7.txt</PATH>
            <REPLICATION>2</REPLICATION>
            <MTIME>1512943608761</MTIME>
            <ATIME>1512943607866</ATIME>
            <BLOCKSIZE>134217728</BLOCKSIZE>
            <CLIENT_NAME></CLIENT_NAME>
            <CLIENT_MACHINE></CLIENT_MACHINE>
            <OVERWRITE>false</OVERWRITE>
            <BLOCK>
                <BLOCK_ID>1073741839</BLOCK_ID>
                <NUM_BYTES>25</NUM_BYTES>
                <GENSTAMP>1016</GENSTAMP>
            </BLOCK>
            <PERMISSION_STATUS>
                <USERNAME>itluma</USERNAME>
                <GROUPNAME>supergroup</GROUPNAME>
                <MODE>420</MODE>
            </PERMISSION_STATUS>
        </DATA>
    </RECORD>
</EDITS >复制代码
```

![image-20210823165628813](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_2/image-20210823161753051.png)**思考：NameNode如何确定下次开机启动的时候合并哪些Edits？**

##### 4.3、CheckPoint时间设置

> 1）通常情况下，SecondaryNameNode每隔一小时执行一次。 [hdfs-default.xml]

```
<property>
  <name>dfs.namenode.checkpoint.period</name>
  <value>3600s</value>
</property>复制代码
```

> 2）一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。

```
<property>
  <name>dfs.namenode.checkpoint.txns</name>
  <value>1000000</value>
<description>操作动作次数</description>
</property>

<property>
  <name>dfs.namenode.checkpoint.check.period</name>
  <value>60s</value>
<description>1分钟检查一次操作次数</description>
</property>复制代码
```

### 任务五、DataNode

#### **【任务目标】**

数据具体是如何存储在HDFS上的，接下来我们通过DataNode分析，掌握DataNode的工作机制及其数据的完整性，重点是掌握DataNode是如何向NameNode进行数据汇报和汇报的具体内容是哪些！

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009362359132)

视频-5、DataNode

##### 5.1、DN工作机制

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830134814071](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_2/image-20220830134814071.png) |

（1）一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。

（2）DataNode启动后向NameNode注册，通过后，周期性（6小时）的向NameNode上报所有的块信息。 DN向NN汇报当前解读信息的时间间隔，默认6小时；

```
<property>
    <name>dfs.blockreport.intervalMsec</name>
    <value>21600000</value>
    <description>Determines block reporting interval in milliseconds.</description>
</property>复制代码
```

DN扫描自己节点块信息列表的时间，默认6小时

```
<property>
    <name>dfs.datanode.directoryscan.interval</name>
    <value>21600s</value>
    <description>Interval in seconds for Datanode to scan data directories and reconcile the difference between blocks in memory and on the disk.
    Support multiple time unit suffix(case insensitive), as described
    in dfs.heartbeat.interval.
    </description>
</property>复制代码
```

（3）心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。

（4）集群运行中可以安全加入和退出一些机器。

##### 5.2、数据完整性

思考：如果电脑磁盘里面存储的数据是控制高铁信号灯的红灯信号（1）和绿灯信号（0），但是存储该数据的磁盘坏了，一直显示是绿灯，是否很危险？同理DataNode节点上的数据损坏了，却没有发现，是否也很危险，那么如何解决呢？

如下是DataNode节点保证数据完整性的方法。 （1）当DataNode读取Block的时候，它会计算CheckSum。 （2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。 （3）Client读取其他DataNode上的Block。 （4）常见的校验算法crc（32），md5（128），sha1（160） （5）DataNode在其文件创建后周期验证CheckSum。

##### 5.3、HDFS中各进程关系总结

| 完整的操作流程                                               |
| ------------------------------------------------------------ |
| ![image-20220830143638272](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_2/image-20220830143638272.png) |





# 实验1-3 Hadoop之MapReduce

## 实验概述

MapReduce是一个分布式运算程序的编程框架，是用户开发”基于Hadoop的数据分析应用”的核心框架。分而治之思想、设计构思、官方示例、执行流程，这些都是我们需要掌握的内容，虽然后期我们编写MapReduce的过程比较少，但是实战中对MapReduce的理论储备还是要有的，通过本实验手册的学习重点是掌握MapReduce各个阶段的流程和原理。

## 实验环境

- AtStudy 实训平台
- Hadoop3.1

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16799942756956249.png)

## 实验目标

学习完成本实验后，您将能够

- 掌握MapReduce的编程思想
- 掌握MapReduce的运行原理
- 掌握MapReduce的提交过程
- 掌握Hadoop的数据压缩

## 实验任务

### 任务一、MapReduce概述

#### **【任务目标】**

掌握MapReduce的核心思想，通过MapReduce的思想我们也可以设计出对应的计算程序，这也决定了你将怎么使用MapReduce。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009363703478)

视频-1、MapReduce概述

##### 1.1 分而治之

**1）是MapReduce核心思想**

- MapReduce的思想核心是“先分再合，分而治之”。
- 所谓“分而治之”就是把一个复杂的问题，按照一定的“分解”方法分为等价的规模较小的若干部分，然后逐个解决，分别找出各部分的结果，然后把各部分的结果组成整个问题的最终结果。
- 这种思想来源于日常生活与工作时的经验。即使是发布过论文实现分布式计算的谷歌也只是实现了这种思想，而不是自己原创。

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830155147732](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830155147732.png) |

**2） MapReduce的过程**

- Map表示第一阶段，负责“拆分”：即把复杂的任务分解为若干个“简单的子任务”来并行处理。可以进行拆分的前提是这些小任务可以并行计算，彼此间几乎没有依赖关系。
- Reduce表示第二阶段，负责“合并”：即对map阶段的结果进行全局汇总。

这两个阶段合起来正是MapReduce思想的体现。

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830155420968](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830155420968.png) |

**3）一个比较形象的语言解释MapReduce**

要数停车场中的所有停放车的总数量。

你数第一列，我数第二列…这就是Map阶段，人越多，能够同时数车的人就越多，速度就越快。

数完之后，聚到一起，把所有人的统计数加在一起。这就是Reduce合并汇总阶段。

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830155606552](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830155606552.png) |

##### 1.2 MapRedue设计思路

![image-20210823165628813](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20210823161753051.png)**MapReduce的思想很好理解，关键在于如何基于这个思想设计出一款分布式计算程序？**

**（1）如何对付大数据处理场景**

- 对相互间不具有计算依赖关系的大数据计算任务，实现并行最自然的办法就是采取MapReduce分而治之的策略。
- 首先Map阶段进行拆分，把大数据拆分成若干份小数据，多个程序同时并行计算产生中间结果；然后是Reduce聚合阶段，通过程序对并行的结果进行最终的汇总计算，得出最终的结果。
- 不可拆分的计算任务或相互间有依赖关系的数据无法进行并行计算！

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830165252351](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830165252351.png) |

**（2）构建抽象编程模型**

- MapReduce中定义了如下的Map和Reduce两个抽象的编程接口，由用户去编程实现:

   map: (k1; v1) → (k2; v2)

   reduce: (k2; [v2]) → (k3; v3)

- 通过以上两个编程接口，大家可以看出MapReduce处理的数据类型是 **<key,value>** 键值对。

**（3）统一架构、隐藏底层细节**

- 如何提供统一的计算框架，如果没有统一封装底层细节，那么程序员则需要考虑诸如数据存储、划分、分发、结果收集、错误恢复等诸多细节；为此，MapReduce设计并提供了统一的计算框架，为程序员隐藏了绝大多数系统层面的处理细节。
- MapReduce最大的亮点在于通过抽象模型和计算框架把需要 **做什么(what need to do)** 与具体 **怎么做(how to do)** 分开了，为程序员提供一个抽象和高层的编程接口和框架。
- 程序员仅需要关心其应用层的具体计算问题，仅需编写少量的处理应用本身计算问题的业务程序代码。
- 至于如何具体完成这个并行计算任务所相关的诸多系统层细节被隐藏起来,交给计算框架去处理：从分布代码的执行，到大到数千小到单个节点集群的自动调度使用。

##### 1.3 认识MapReduce

###### 1.3.1 分布式计算

分布式计算是一种计算方法，和集中式计算是相对的。

随着计算技术的发展，有些应用需要非常巨大的计算能力才能完成，如果采用集中式计算，需要耗费相当长的时间来完成。

分布式计算将该应用分解成许多小的部分，分配给多台计算机进行处理。这样可以节约整体计算时间，大大提高计算效率。

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830160715945](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830160715945.png) |

###### 1.3.2 MapReduce介绍

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830160430963](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830160430963.png) |

Hadoop MapReduce是一个分布式计算框架，用于轻松编写分布式应用程序，这些应用程序以可靠，容错的方式并行处理大型硬件集群（数千个节点）上的大量数据（多TB数据集）。 MapReduce是一种面向海量数据处理的一种指导思想，也是一种用于对大规模数据进行分布式计算的编程模型。

###### 1.3.3 MapReduce特点

**MapReduce优点**

- 易于编程

Mapreduce框架提供了用于二次开发的接口；简单地实现一些接口，就可以完成一个分布式程序。任务计算交给计算框架去处理，将分布式程序部署到hadoop集群上运行，集群节点可以扩展到成百上千个等。

- 良好的扩展性

当计算机资源不能得到满足的时候，可以通过增加机器来扩展它的计算能力。基于MapReduce的分布式计算得特点可以随节点数目增长保持近似于线性的增长，这个特点是MapReduce处理海量数据的关键，通过将计算节点增至几百或者几千可以很容易地处理数百TB甚至PB级别的离线数据

- 高容错性

Hadoop集群是分布式搭建和部署得，任何单一机器节点宕机了，它可以把上面的计算任务转移到另一个节点上运行，不影响整个作业任务得完成，过程完全是由Hadoop内部完成的。

- 适合海量数据的离线处理

可以处理GB、TB和PB级别得数据量

**MapReduce局限性**

MapReduce虽然有很多的优势，也有相对得局限性，局限性不代表不能做，而是在有些场景下实现的效果比较差，并不适合用MapReduce来处理，主要表现在以下结果方面：

- 实时计算性能差

MapReduce主要应用于离线作业，无法作到秒级或者是亚秒级得数据响应。

- 不能进行流式计算

流式计算特点是数据是源源不断得计算，并且数据是动态的；而MapReduce作为一个离线计算框架，主要是针对静态数据集得，数据是不能动态变化得

###### 1.3.4 MapReduce实例进程

一个完整的MapReduce程序在分布式运行时有**三类**

- MRAppMaster：负责整个MR程序的过程调度及状态协调
- MapTask：负责map阶段的整个数据处理流程
- ReduceTask：负责reduce阶段的整个数据处理流程

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830161242403](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830161242403.png) |

###### 1.3.5 阶段组成

- 一个MapReduce编程模型中只能包含一个Map阶段和一个Reduce阶段，或者只有Map阶段；
- 不能有诸如多个map阶段、多个reduce阶段的情景出现；
- 如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序串行运行。

|                                                              |      |
| ------------------------------------------------------------ | ---- |
| ![image-20220830162604897](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830162604897.png) |      |

##### 1.4 数据类型

- 注意：整个MapReduce程序中，数据都是以kv键值对的形式流转的；
- 在实际编程解决各种业务问题中，需要考虑每个阶段的输入输出kv分别是什么；
- MapReduce内置了很多默认属性，比如排序、分组等，都和数据的k有关，所以说kv的类型数据确定及其重要的

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830165642511](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830165642511.png) |

### 任务二、学会提交MapReduce应用

#### **【任务目标】**

MapReduce的核心思想需要我们去了解，MapReduce程序如何在Hadoop平台上进行提交更需要我们去掌握，在这里主要是掌握两个默认案例如何运行！

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009362503743)

视频-2、学会提交MapReduce应用

##### 2.1 概述

一个最终完整版本的MR程序需要用户编写的代码和Hadoop自己实现的代码整合在一起才可以；

- 其中用户负责map、reduce两个阶段的业务问题，Hadoop负责底层所有的技术问题；
- 由于MapReduce计算引擎天生的弊端（慢），当下企业中直接使用率已经日薄西山了，所以在企业中工作很少涉及到MapReduce直接编程，但是某些软件的背后还依赖MapReduce引擎。
- 可以通过官方提供的示例来感受MapReduce及其内部执行流程，因为后续的新的计算引擎比如Spark，当中就有MapReduce深深的影子存在。

##### 2.2 示例说明

示例程序路径：/opt/module/hadoop-3.1.3/share/hadoop/mapreduce

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830171125554](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830171125554.png) |

> - 示例程序：hadoop-mapreduce-examples-3.3.0.jar
> - MapReduce程序提交命令：**[hadoop jar|yarn jar] hadoop-mapreduce-examples-3.3.0.jar args…**

![image-20210823165628813](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20210823161753051.png)**提交到哪里去？**

> 提交到YARN进程上分布式执行。

##### 2.3 评估圆周率π（PI）的值

圆周率π大家都不陌生，如何去估算π的值呢？

Hadoop MapReduce示例提供了Monte Carlo方法计算圆周率。

运行MapReduce程序评估一下圆周率的值，执行中可以去YARN页面上观察程序的执行的情况。

- 第一个参数：pi表示MapReduce程序执行圆周率计算任务；
- 第二个参数：用于指定map阶段运行的任务task次数，并发度，这里是10；
- 第三个参数：用于指定每个map任务取样的个数，这里是50。

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830172352027](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830172352027.png) |

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830172442077](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830172442077.png) |

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830172609350](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830172609350.png) |

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830172750415](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830172750415.png) |

##### 2.4 WordCount单词词频统计

WordCount中文叫做单词统计、词频统计；

指的是统计指定文件中，每个**单词出现的总次数**。

###### 2.4.1 WordCount概述

- WordCount算是大数据计算领域经典的入门案例，相当于Hello World。
- 虽然WordCount业务极其简单，但是希望能够通过案例感受背后MapReduce的执行流程和默认的行为机制，这才是关键。

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830172909727](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830172909727.png) |

###### 2.4.2 WordCount编程实现思路

- map阶段的核心：把输入的数据经过切割，全部标记1，因此输出就是<单词，1>。
- shuffle阶段核心：经过MR程序内部自带默认的排序分组等功能，把key相同的单词会作为一组数据构成新的kv对
- reduce阶段核心：处理shuffle完的一组数据，该组数据就是该单词所有的键值对。对所有的1进行累加求和，就是单词的总次数

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830173013990](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830173013990.png) |

###### 2.4.3 WordCount程序提交

将当前目录下的文本文件1.txt到HDFS文件系统的/input目录下，如果没有这个目录，使用shell创建

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -mkdir /input

[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -put 1.txt /input复制代码
```

准备好之后，执行官方MapReduce实例，对上述文件进行单词次数统计

- 第一个参数：wordcount表示执行单词统计任务；
- 第二个参数：指定输入文件的路径；
- 第三个参数：指定输出结果的路径（该路径不能已存在）；

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830211017212](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830211017212.png) |

###### 2.4.4 WordCount执行结果

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830211323136](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830211323136.png) |

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830211406097](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830211406097.png) |

### 任务三、MapReduce框架原理

#### **【任务目标】**

前面我们已经掌握了如何提交一个MapReduce程序，接下来还需要了解一些MapReduce内部执行的流程，难点是Map阶段到Reduce阶段之间的Shuffle过程，这个也是MapReduce的核心和精髓，也是MapReduce诟病最多的地方，为什么呢？这就需要我们掌握其流程了。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009363658172)

视频-3、MapReduce框架原理

##### 3.1 MapReduce整体执行流程

| WordCount执行流程图                                          |
| ------------------------------------------------------------ |
| ![image-20220830173013990](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830173013990.png) |

MapReduce整体执行流程

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830220032926](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830220032926.png) |

##### 3.2 Map阶段执行流程

- 第一阶段：把输入目录下文件按照一定的标准逐个进行**逻辑切片**，形成切片规划。

  默认Split size = Block size（128M），每一个切片由一个MapTask处理。（getSplits）

- 第二阶段：对切片中的数据按照一定的规则读取解析返回<key,value>对。

  默认是**按行读取数据**。key是每一行的起始位置偏移量，value是本行的文本内容。（TextInputFormat）

- 第三阶段：调用Mapper类中的**map方法处理数据**。每读取解析出来的一个<key,value> ，调用一次map方法。

- 第四阶段：按照一定的规则对Map输出的键值对进行**分区partition**。默认不分区，因为只有一个reducetask。分区的数量就是reducetask运行的数量。

- 第五阶段：Map输出数据写入**内存缓冲区**，达到比例溢出到磁盘上。**溢出spill**的时候根据key进行**排序sort**。默认根据key字典序排序。

- 第六阶段：对所有溢出文件进行最终的**merge合并**，成为一个文件。

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830220337133](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830220337133.png) |

##### 3.3 Reduce阶段执行流程

- 第一阶段：ReduceTask会主动从MapTask**复制拉取**属于需要自己处理的数据。
- 第二阶段：把拉取来数据，全部进行**合并merge**，即把分散的数据合并成一个大的数据。再对合并后的数据**排序**
- 第三阶段：对排序后的键值对**调用reduce方法**。键相等的键值对调用一次reduce方法。最后把这些输出的键值对写入到HDFS文件中

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830220956308](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830220956308.png) |

##### 3.4 Shuffle的过程

###### 3.4.1 Shuffle概念

- **Shuffle**的本意是洗牌、混洗的意思，把一组有规则的数据尽量打乱成无规则的数据。
- 而在MapReduce中，Shuffle更像是洗牌的逆过程，指的是将map端的无规则输出按指定的规则“打乱”成具有一定规则的数据，以便reduce端接收处理。
- 一般把从**Map产生输出开始到Reduce取得数据作为输入之前的过程称作shuffle**。

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830221458957](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830221458957.png) |

###### 3.4.2 Map端Shuffle

- Collect阶段：将MapTask的结果收集输出到默认大小为100M的环形缓冲区，保存之前会对key进行分区的计算，默认Hash分区(key.hashCode()&Integer.MAX_VALUE%numReduces)。
- Spill阶段：当内存中的数据量达到一定的阀值的时候，就会将数据写入本地磁盘，在将数据写入磁盘之前需要对数据进行一次排序的操作，如果配置了combiner，还会将有相同分区号和key的数据进行排序。
- Merge阶段：把所有溢出的临时文件进行一次合并操作，以确保一个MapTask最终只产生一个中间数据文件。

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830222652628](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830222652628.png) |

###### 3.4.3 Reduce端Shuffle

- Copy阶段： ReduceTask启动Fetcher线程到已经完成MapTask的节点上复制一份属于自己的数据。
- Merge阶段：在ReduceTask远程复制数据的同时，会在后台开启两个线程对内存到本地的数据文件进行合并操作。
- Sort阶段：在对数据进行合并的同时，会进行排序操作，由于MapTask阶段已经对数据进行了局部的排序，ReduceTask只需保证Copy的数据的最终整体有效性即可

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830223337159](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830223337159.png) |

##### 3.5 Shuffle弊端

- Shuffle是MapReduce程序的核心与精髓，是MapReduce的灵魂所在。
- Shuffle也是MapReduce被诟病最多的地方所在。MapReduce相比较于Spark、Flink计算引擎慢的原因，跟Shuffle机制有很大的关系。
- Shuffle中**频繁涉及到数据在内存、磁盘之间的多次往复**。

### 任务四、数据压缩

#### **【任务目标】**

Map阶段和Reduce之间频繁涉及到内存和磁盘之间数据读写复制过程，并且每一个MapReduce就是一个job过程，对IO操作是必不可少的，那么压缩就是很好的节省资源的方式，所以在MapReduce过程中如何使用压缩节省开销也是我们需要掌握的。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009364577298)

视频-4、数据压缩

##### 4.1 概述

1）压缩的好处和坏处 压缩的优点：以减少磁盘IO、减少磁盘存储空间。 压缩的缺点：增加CPU开销。

2）压缩原则 运算密集型的Job，少用压缩 IO密集型的Job，多用压缩

##### 4.2 MapReduce支持的压缩方式

1）压缩算法对比介绍

| 压缩格式 | Hadoop自带？ | 算法    | 文件扩展名 | 是否可切片 | 换成压缩格式后，原来的程序是否需要修改 |
| -------- | ------------ | ------- | ---------- | ---------- | -------------------------------------- |
| DEFLATE  | 是，直接使用 | DEFLATE | .deflate   | 否         | 和文本处理一样，不需要修改             |
| Gzip     | 是，直接使用 | DEFLATE | .gz        | 否         | 和文本处理一样，不需要修改             |
| bzip2    | 是，直接使用 | bzip2   | .bz2       | 是         | 和文本处理一样，不需要修改             |
| LZO      | 否，需要安装 | LZO     | .lzo       | 是         | 需要建索引，还需要指定输入格式         |
| Snappy   | 是，直接使用 | Snappy  | .snappy    | 否         | 和文本处理一样，不需要修改             |

2）压缩性能的比较

| 压缩算法 | 原始文件大小 | 压缩文件大小 | 压缩速度 | 解压速度 |
| -------- | ------------ | ------------ | -------- | -------- |
| gzip     | 8.3GB        | 1.8GB        | 17.5MB/s | 58MB/s   |
| bzip2    | 8.3GB        | 1.1GB        | 2.4MB/s  | 9.5MB/s  |
| LZO      | 8.3GB        | 2.9GB        | 49.3MB/s | 74.6MB/s |

##### 4.3 压缩方式选择

 压缩方式选择时重点考虑：压缩/解压缩速度、压缩率（压缩后存储大小）、压缩后是否可以支持切片。

###### **4.3.1** **Gzip压缩**

 优点：压缩率比较高；

 缺点：不支持Split；压缩/解压速度一般；

###### 4.3.2 Bzip2压缩

 优点：压缩率高；支持Split；

 缺点：压缩/解压速度慢。

###### 4.3.3 Lzo压缩

 优点：压缩/解压速度比较快；支持Split；

 缺点：压缩率一般；想支持切片需要额外创建索引。

###### 4.3.4 Snappy压缩

 优点：压缩和解压缩速度快；

 缺点：不支持Split；压缩率一般；

###### 4.3.5 压缩位置选择

 压缩可以在MapReduce作用的任意阶段启用。

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220830225229878](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830225229878.png) |

##### 4.4 压缩参数配置

 1）为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器

| 压缩格式 | 对应的编码/解码器                          |
| -------- | ------------------------------------------ |
| DEFLATE  | org.apache.hadoop.io.compress.DefaultCodec |
| gzip     | org.apache.hadoop.io.compress.GzipCodec    |
| bzip2    | org.apache.hadoop.io.compress.BZip2Codec   |
| LZO      | com.hadoop.compression.lzo.LzopCodec       |
| Snappy   | org.apache.hadoop.io.compress.SnappyCodec  |

 2）要在Hadoop中启用压缩，可以配置如下参数

![image-20220830225839095](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_3/image-20220830225839095.png)





# 实验1-4 Hadoop之Yarn

## 实验概述

文件存储有HDFS，任务处理可以选择MapReduce，那么任务处理的过程中需要的资源如何管理以及如何分配？这就是我们需要掌握的Yarn工作内容，**Yarn**是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个**分布式的操作系统平台**，而**MapReduce**等运算程序则相当于运行于**操作系统之上的应用程序**，所以本实验手册主要任务就是让大家掌握Yarn资源管理平台理论基础和企业实战。

## 实验环境

- AtStudy 实训平台
- Hadoop3.1.3

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16799943263905647.png)

## 实验目标

学习完成本实验后，您将能够

- 掌握Yarn的基础架构和组件
- 程序在Yarn上的交互流程
- 调度器Scheduler及算法
- 掌握多队列任务提交

## 实验任务

### 任务一、Yarn资源调度器

#### **【任务目标】**

YARN是什么，它的基础架构是怎么样的？以及如何理解YARN是一个**通用**资源管理系统和调度平台？

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009364553697)

视频-1、Yarn资源调度器

##### 1.1 Yarn简介

- Apache Hadoop YARN （Yet Another Resource Negotiator，另一种资源协调者）是一种新的Hadoop资源管理器。
- YARN是一个**通用**资源管理系统和调度平台，可为上层应用提供统一的资源管理和调度。
- 它的引入为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220831093503543](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_4/image-20220831093503543.png) |

![image-20210823165628813](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_4/image-20210823161753051.png)**如何理解YARN是一个 `通用` `资源管理系统` 和 `调度平台`**

##### 1.2 Yarn的功能说明

- **资源管理系统**：集群的硬件资源，和程序运行相关，比如内存、CPU等。
- **调度平台**：多个程序同时申请计算资源如何分配，调度的规则（算法）。
- **通用**：不仅仅支持MapReduce程序，理论上支持各种计算程序。YARN不关心你干什么，只关心你要资源，在有的情况下给你，用完之后还我。

##### 1.3 Yarn概述

1、可以把Hadoop YARN理解为相当于一个分布式的操作系统平台，而MapReduce等计算程序则相当于运行于操作系统之上的应用程序，YARN为这些程序提供运算所需的资源（内存、CPU等）。

2、Hadoop能有今天这个地位，YARN可以说是功不可没。因为有了YARN ，更多计算框架可以接入到 HDFS中，而不单单是 MapReduce，正是因为YARN的包容，使得其他计算框架能专注于计算性能的提升。

3、HDFS可能不是最优秀的大数据存储系统，但却是应用最广泛的大数据存储系统， YARN功不可没。

##### 1.4 Yarn架构和组件

###### 1.4.1 架构

YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成。

| 官方架构                                                     |
| ------------------------------------------------------------ |
| ![image-20220831095342083](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_4/image-20220831095342083.png) |

从官方构图中我们可以看到出现的概念有如下：

| YARN3大组件                                                  |
| ------------------------------------------------------------ |
| ![image-20220831095545309](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_4/image-20220831095545309.png) |

###### 1.4.2 三大组件+容器

- **ResourceManager（RM）**

  YARN集群中的主角色，决定系统中所有应用程序之间**资源分配的最终权限，即最终仲裁者**。

  接收用户的作业提交，并通过NM分配、管理各个机器上的计算资源。

- **NodeManager（NM）**

  YARN中的从角色，一台机器上一个，负责**管理本机器上的计算资源**。

  根据RM命令，启动Container容器、监视容器的资源使用情况。并且向RM主角色汇报资源使用情况。

- **ApplicationMaster（AM）**

  用户提交的每个应用程序均包含一个AM。

  **应用程序内的“老大”**，负责程序内部各阶段的资源申请，监督程序的执行情况。

- **Container**

  是YARN中的资源抽象，它封装了某个节点上的多维资源，如内存、CPU、磁盘、网络等。

### 任务二、Yarn交互流程

#### **【任务目标】**

三大组件之间是如何协调成功完成一个任务的，这就涉及到一个MapReduce提交YARN的交互流程。在此我们重点是要了解它的执行过程。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009363579200)

视频-2、Yarn交互流程

##### 2.1 整体概述

当用户向 YARN 中提交一个应用程序后， YARN将分两个阶段运行该应用程序 。

- 第一个阶段是**客户端申请资源启动运行本次程序的**ApplicationMaster**； **
- 第二个阶段是由**ApplicationMaster根据本次程序内部具体情况，为它申请资源，并监控它的整个运行过程**，直到运行完成。

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220831102223582](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_4/image-20220831102223582.png) |

##### 2.2 MR提交YARN交互流程

步骤明细：

- **第1步**、用户通过客户端向YARN中ResourceManager提交应用程序（比如hadoop jar提交MR程序）；
- **第2步**、ResourceManager为该应用程序分配第一个Container（容器），并与对应的NodeManager通信，要求它在这个Container中启动这个应用程序的ApplicationMaster。
- **第3步**、ApplicationMaster启动成功之后，首先向ResourceManager注册并保持通信，这样用户可以直接通过ResourceManager查看应用程序的运行状态（处理了百分之几）;
- **第4步**、AM为本次程序内部的各个Task任务向RM申请资源，并监控它的运行状态
- **第5步**、一旦 ApplicationMaster 申请到资源后，便与对应的 NodeManager 通信，要求它启动任务。
- **第6步**、NodeManager 为任务设置好运行环境后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。
- **第7步**、各个任务通过某个 RPC 协议向 ApplicationMaster 汇报自己的状态和进度，以让 ApplicationMaster 随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。在应用程序运行过程中，用户可随时通过RPC 向 ApplicationMaster 查询应用程序的当前运行状态。
- **第8步**、应用程序运行完成后，ApplicationMaster 向 ResourceManager 注销并关闭自己

### 任务三、Yarn调度器和调度算法

#### **【任务目标】**

在理想情况下，应用程序提出的请求将立即得到YARN批准。但是实际中，资源是有限的，并且在繁忙的群集上，应用程序通常将需要等待其某些请求得到满足。YARN调度程序的工作是**根据一些定义的策略为应用程序分配资源**，那么调度器的具体策略有哪些，具体的作用是怎么样的，这些都需要我们掌握！

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009364322909)

视频-3、Yarn调度器和调度算法

##### 3.1、调度器的理解

要想给任务分配container容器的数量的话，肯定是需要一个东西给这个任务分配对应的数量的container，这个东西就是我们需要了解的调度器。

在YARN中，负责给应用分配资源的叫**Scheduler(调度器)**，它是ResourceManager的核心组件之一。

Scheduler完全专用于调度作业，会根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的程序，调度器仅根据各个应用程序的资源需求进行资源分配，资源分配的单位是container，从而限定每个任务使用的资源量，但它无法跟踪应用程序的状态。 一般而言，调度是一个难题，并且没有一个“最佳”策略，为此，YARN提供了多种调度器和可配置的策略供选择

##### 3.2 三种调度策略(调度算法)

目前，Hadoop作业调度器主要有三种：**FIFO Scheduler（先进先出调度器）**、**Capacity Scheduler（容量调度器）**、**Fair Scheduler（公平调度器）**。

Apache Hadoop3.1.3默认的资源调度器是Capacity Scheduler。

CDH框架默认调度器是Fair Scheduler。

具体设置详见：yarn-default.xml文件

```
<property>
    <description>The class to use as the resource scheduler.</description>
    <name>yarn.resourcemanager.scheduler.class</name>
<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
</property>复制代码
```

###### 3.2.1 FIFO调度器

- **FIFO Scheduler**是Hadoop1.x中JobTracker原有的调度器实现，此调度器在YARN中保留了下来。
- FIFO Scheduler是一个先进先出的思想，即先提交的应用先运行。调度工作不考虑优先级和范围，适用于负载较低的小规模集群。当使用大型共享集群时，它的效率较低且会导致一些问题。
- FIFO Scheduler拥有一个控制全局的队列queue，默认queue名称为default，该调度器会获取当前集群上所有的资源信息作用于这个全局的queue。

| 按照到达时间排序，先到先服务                                 |
| ------------------------------------------------------------ |
| ![image-20220831135938352](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_4/image-20220831135938352.png) |

优势：无需配置、先到先得、易于执行

缺点：任务的优先级不会变高，因此高优先级的作业需要等待，不适合共享集群

###### 3.2.2 Capacity调度器

Capacity Scheduler是Yahoo开发的多用户调度器

- Capacity Scheduler容量调度是Apache Hadoop3.x默认调度策略。该策略允许**多个组织共享整个集群资源**，每个组织可以获得集群的一部分计算能力。**通过为每个组织分配专门的队列，然后再为每个队列分配一定的集群资源**，这样整个集群就可以通过设置多个队列的方式给多个组织提供服务了。
- Capacity可以理解成一个个的资源队列，这个资源队列是用户自己去分配的。队列内部又可以垂直划分，这样一个组织内部的多个成员就可以共享这个队列资源了，在一个队列内部，资源的调度是采用的是先进先出(FIFO)策略

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220831140635995](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_4/image-20220831140635995.png) |

容量调度器的特点

- 层次化的队列设计（Hierarchical Queues）
  - 层次化的管理，可以更容易、更合理分配和限制资源的使用。
- 容量保证（Capacity Guarantees）
  - 每个队列上都可以设置一个资源的占比，保证每个队列都不会占用整个集群的资源。
- 安全（Security）
  - 每个队列有严格的访问控制。用户只能向自己的队列里面提交任务，而且不能修改或者访问其他队列的任务。
- 弹性分配（Elasticity）
  - 空闲的资源可以被分配给任何队列。
  - 当多个队列出现争用的时候，则会按照权重比例进行平衡。

###### 3.2.3 Fair调度器

**1、Fair调度器特点：**

Fair Scheduler是由Facebook贡献的，是Hadoop上一个可插拔式的调度器，容许YARN应用程序在一个大的集群上公平地共享资源

公平调度是一种为应用程序分配资源的方法，多用户的状况下，强调用户公平地使用资源。默认状况下Fair Scheduler根据内存资源对应用程序进行公平调度，经过配置能够修改成根据内存和CPU两种资源进行调度。当集群中只有一个应用程序运行时，那么此应用程序占用这个集群资源。当其余的应用程序提交后，那些释放的资源将会被分配给新的应用程序，因此每一个应用程序最终都能获取几乎同样多的资源。

在Fair Scheduler中，不须要预先占用必定的系统资源，Fair Scheduler会动态调整应用程序的资源分配。例如，当第一个大job提交时，只有这一个job在运行，此时它得到了全部集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220831142119777](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_4/image-20220831142119777.png) |

**![image-20210823165628813](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_4/image-20210823161753051.png)如何理解公平共享？**

> - 有两个用户A和B，每个用户都有自己的队列。
> - A启动一个作业，由于没有B的需求，它分配了集群所有可用的资源。
> - 然后B在A的作业仍在运行时启动了一个作业，经过一段时间，A,B各自作业都使用了一半的资源。
> - 现在，如果B用户在其他作业仍在运行时开始第二个作业，它将与B的另一个作业共享其资源，因此B的每个作业将拥有资源的四分之一，而A的继续将拥有一半的资源。结果是资源在用户之间公平地共享。

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220831144322420](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_4/image-20220831144322420.png) |

**2、公平调度器队列资源分配方式**

1）FIFO 策略 公平调度器每个队列资源分配策略如果选择FIFO的话，此时公平调度器相当于上面讲过的容量调度器。

2）Fair策略 Fair 策略（默认）是一种基于最大最小公平算法实现的资源多路复用方式，默认情况下，每个队列内部采用该方式分配资源。这意味着，如果一个队列中有两个应用程序同时运行，则每个应用程序可得到1/2的资源；如果三个应用程序同时运行，则每个应用程序可得到1/3的资源。

**3、容量调度器和公平调度器的异同点：**

**1）与容量调度器相同点**

- 多队列：支持多队列多作业
- 容量保证：管理员可为每个队列设置资源最低保证和资源使用上线
- 灵活性：如果一个队列中的资源有剩余，可以暂时共享给那些需要资源的队列，而一旦该队列有新的应用程序提交，则其他队列借调的资源会归还给该队列。
- 多租户：支持多用户共享集群和多应用程序同时运行；为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。

**2）与容量调度器不同点**

- 核心调度策略不同 容量调度器：优先选择`资源利用率`低的队列 公平调度器：优先选择对资源的`缺额`比例大的
- 每个队列可以单独设置资源分配方式 容量调度器：FIFO、 `DRF` 公平调度器：FIFO、`FAIR`、`DRF`

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_4/image-20210823165628813.png)**说明：**

> DRF（Dominant Resource Fairness），我们之前说的资源，都是单一标准，例如只考虑内存（也是Yarn默认的情况）。但是很多时候我们资源有很多种，例如内存、CPU、网络带宽等，这样我们很难衡量两个应用应该分配的资源比例。而DRF就是多资源的一种综合衡量标准

### 任务四、Yarn生产实操

#### **【任务目标】**

在实际的生产环境中，我们还需要掌握一些Yarn的操作命令以便我们观察Yarn的整体资源调度过程。并且需要掌握如何创建多队列及其如何使用，这样才能最大限度的利用好Yarn提供的调度器策略。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009362823187)

视频-4、Yarn生产实操

##### 4.1 Yarn常用命令

Yarn状态的查询，除了可以在`localhost:8088`页面查看外，还可以通过命令操作。常见的命令操作如下所示：

需求：执行`WordCount`案例，并用Yarn命令查看任务运行情况。

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ myhadoop.sh start
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output复制代码
```

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220831152501655](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_4/image-20220831152501655.png) |

###### 4.1.1 yarn application查看任务

（1）列出所有Application：

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ yarn application -list
2021-02-06 10:21:19,238 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032
Total number of applications (application-types: [], states: [SUBMITTED, ACCEPTED, RUNNING] and tags: []):0
        Application-Id    Application-Name    Application-Type    User     Queue         State      Final-State     Progress             Tracking-URL复制代码
```

（2）根据Application状态过滤：`yarn application -list -appStates` （所有状态：ALL、NEW、NEW_SAVING、SUBMITTED、ACCEPTED、RUNNING、FINISHED、FAILED、KILLED）

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ yarn application -list -appStates FINISHED
2021-02-06 10:22:20,029 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032
Total number of applications (application-types: [], states: [FINISHED] and tags: []):1
        Application-Id    Application-Name    Application-Type    User     Queue         State      Final-State     Progress             Tracking-URL
application_1612577921195_0001      word count       MAPREDUCE   itluma    default      FINISHED       SUCCEEDED        100% http://4a856475ffe5:19888/jobhistory/job/job_1612577921195_0001复制代码
```

（3）Kill掉Application：

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ yarn application -kill application_1612577921195_0001
2021-02-06 10:23:48,530 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032
Application application_1612577921195_0001 has already finished复制代码
```

###### 4.1.2 yarn logs查看日志

（1）查询Application日志：`yarn logs -applicationId <ApplicationId>`

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ yarn logs -applicationId application_1612577921195_0001复制代码
```

（2）查询Container日志：`yarn logs -applicationId <ApplicationId> -containerId <ContainerId> `

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ yarn logs -applicationId application_1612577921195_0001 -containerId container_1612577921195_0001_01_000001复制代码
```

###### 4.1.3 yarn applicationattempt查看尝试运行的任务

（1）列出所有Application尝试的列表：`yarn applicationattempt -list <ApplicationId>`

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ yarn applicationattempt -list application_1612577921195_0001
2021-02-06 10:26:54,195 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032
Total number of application attempts :1
     ApplicationAttempt-Id          State            AM-Container-Id              Tracking-URL
appattempt_1612577921195_0001_000001        FINISHED  container_1612577921195_0001_01_000001 http://4a856475ffe5:8088/proxy/application_1612577921195_0001/复制代码
```

（2）打印ApplicationAttemp状态：`yarn applicationattempt -status <ApplicationAttemptId>`

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ yarn applicationattempt -status appattempt_1612577921195_0001_000001
2021-02-06 10:27:55,896 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032
Application Attempt Report : 
  ApplicationAttempt-Id : appattempt_1612577921195_0001_000001
  State : FINISHED
  AMContainer : container_1612577921195_0001_01_000001
  Tracking-URL : http://4a856475ffe5:8088/proxy/application_1612577921195_0001/
  RPC Port : 34756
  AM Host : 4a856475ffe5
  Diagnostics :复制代码
```

###### 4.1.4 yarn container查看容器

（1）列出所有Container：`yarn container -list <ApplicationAttemptId>`

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ yarn container -list appattempt_1612577921195_0001_000001
2021-02-06 10:28:41,396 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032
Total number of containers :0
         Container-Id      Start Time       Finish Time         State          Host   Node Http Address 复制代码
```

（2）打印Container状态：` yarn container -status <ContainerId>`

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ yarn container -status container_1612577921195_0001_01_000001
2021-02-06 10:29:58,554 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032
Container with id 'container_1612577921195_0001_01_000001' doesn't exist in RM or Timeline Server.复制代码
```

> 注：只有在任务跑的途中才能看到container的状态

###### 4.1.5 yarn node查看节点状态

列出所有节点：yarn node -list -all

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ yarn node -list -all
2021-02-06 10:31:36,962 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032
Total Nodes:3
     Node-Id    Node-State Node-Http-Address Number-of-Running-Containers
 hadoop103:38168      RUNNING   hadoop103:8042               0
 hadoop102:42012      RUNNING   hadoop102:8042               0
 hadoop104:39702      RUNNING   hadoop104:8042               0复制代码
```

###### 4.1.6 yarn rmadmin更新配置

加载队列配置：`yarn rmadmin -refreshQueues`

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ yarn rmadmin -refreshQueues
2021-02-06 10:32:03,331 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032复制代码
```

###### 4.1.7 yarn queue查看队列

打印队列信息：`yarn queue -status <QueueName>`

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ yarn queue -status default
2021-02-06 10:32:33,403 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032
Queue Information : 
Queue Name : default
  State : RUNNING
  Capacity : 100.0%
  Current Capacity : .0%
  Maximum Capacity : 100.0%
  Default Node Label expression : <DEFAULT_PARTITION>
  Accessible Node Labels : *
  Preemption : disabled
  Intra-queue Preemption : disabled复制代码
```

##### 4.2 Yarn生产环境核心参数

###### 4.2.1 核心参数

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220831160519834](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_4/image-20220831160519834.png) |

**1）ResourceManager相关**

```
yarn.resourcemanager.scheduler.class       `配置调度器，默认容量`
yarn.resourcemanager.scheduler.client.thread-count   `ResourceManager处理调度器请求的线程数量，默认50`复制代码
```

**2）NodeManager相关**

```
yarn.nodemanager.resource.detect-hardware-capabilities   `是否让yarn自己检测硬件进行配置，默认false`
yarn.nodemanager.resource.count-logical-processors-as-cores   `是否将虚拟核数当作CPU核数，默认false`
yarn.nodemanager.resource.pcores-vcores-multiplier     `虚拟核数和物理核数乘数，例如：4核8线程，该参数就应设为2，默认1.0`
yarn.nodemanager.resource.memory-mb       `NodeManager使用内存，默认8G`
yarn.nodemanager.resource.system-reserved-memory-mb `NodeManager为系统保留多少内存以上二个参数配置一个即可`
yarn.nodemanager.resource.cpu-vcores     `NodeManager使用CPU核数，默认8个`
yarn.nodemanager.pmem-check-enabled    `是否开启物理内存检查限制container，默认打开`
yarn.nodemanager.vmem-check-enabled   `是否开启虚拟内存检查限制container，默认打开`
yarn.nodemanager.vmem-pmem-ratio      `虚拟内存物理内存比例，默认2.1`复制代码
```

**3）Container相关**

```
yarn.scheduler.minimum-allocation-mb `容器最最小内存，默认1G`
yarn.scheduler.maximum-allocation-mb `容器最最大内存，默认8G`
yarn.scheduler.minimum-allocation-vcores `容器最小CPU核数，默认1个`
yarn.scheduler.maximum-allocation-vcores `容器最大CPU核数，默认4个`复制代码
```

###### 4.2.2 案例场景

1）需求：从1G数据中，统计每个单词出现次数。服务器3台，每台配置4G内存，4核CPU，4线程。

2）需求分析： 1G / 128m = 8 个 MapTask；1 个 ReduceTask；1 个 mrAppMaster 平均每个节点运行 10 个 / 3 台 ≈ 3 个任务（4 3 3）

3）修改yarn-site.xml配置参数如下：

```
<!-- 选择调度器，默认容量 -->
<property>
    <description>The class to use as the resource scheduler.</description>
    <name>yarn.resourcemanager.scheduler.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
</property>

<!-- ResourceManager处理调度器请求的线程数量,默认50；如果提交的任务数大于50，可以增加该值，但是不能超过3台 * 4线程 = 12线程（去除其他应用程序实际不能超过8） -->
<property>
    <description>Number of threads to handle scheduler interface.</description>
    <name>yarn.resourcemanager.scheduler.client.thread-count</name>
    <value>8</value>
</property>

<!-- 是否让yarn自动检测硬件进行配置，默认是false，如果该节点有很多其他应用程序，建议手动配置。如果该节点没有其他应用程序，可以采用自动 -->
<property>
    <description>Enable auto-detection of node capabilities such as memory and CPU.
    </description>
    <name>yarn.nodemanager.resource.detect-hardware-capabilities</name>
    <value>false</value>
</property>

<!-- 是否将虚拟核数当作CPU核数，默认是false，采用物理CPU核数 -->
<property>
    <description>Flag to determine if logical processors(such as hyperthreads) should be counted as cores. Only applicable on Linux when yarn.nodemanager.resource.cpu-vcores is set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true.
    </description>
    <name>yarn.nodemanager.resource.count-logical-processors-as-cores</name>
    <value>false</value>
</property>

<!-- 虚拟核数和物理核数乘数，默认是1.0 -->
<property>
    <description>Multiplier to determine how to convert phyiscal cores to vcores. This value is used if yarn.nodemanager.resource.cpu-vcores is set to -1(which implies auto-calculate vcores) and yarn.nodemanager.resource.detect-hardware-capabilities is set to true. The	number of vcores will be calculated as	number of CPUs * multiplier.
    </description>
    <name>yarn.nodemanager.resource.pcores-vcores-multiplier</name>
    <value>1.0</value>
</property>

<!-- NodeManager使用内存数，默认8G，修改为4G内存 -->
<property>
    <description>Amount of physical memory, in MB, that can be allocated for containers. If set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is
    automatically calculated(in case of Windows and Linux). In other cases, the default is 8192MB.
    </description>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>4096</value>
</property>

<!-- nodemanager的CPU核数，不按照硬件环境自动设定时默认是8个，修改为4个 -->
<property>
    <description>Number of vcores that can be allocated for containers. This is used by the RM scheduler when allocating resources for containers. This is not used to limit the number of CPUs used by YARN containers. If it is set to -1 and yarn.nodemanager.resource.detect-hardware-capabilities is true, it is automatically determined from the hardware in case of Windows and Linux. In other cases, number of vcores is 8 by default.</description>
    <name>yarn.nodemanager.resource.cpu-vcores</name>
    <value>4</value>
</property>

<!-- 容器最小内存，默认1G -->
<property>
    <description>The minimum allocation for every container request at the RM	in MBs. Memory requests lower than this will be set to the value of this	property. Additionally, a node manager that is configured to have less memory	than this value will be shut down by the resource manager.
    </description>
    <name>yarn.scheduler.minimum-allocation-mb</name>
    <value>1024</value>
</property>

<!-- 容器最大内存，默认8G，修改为2G -->
<property>
    <description>The maximum allocation for every container request at the RM	in MBs. Memory requests higher than this will throw an	InvalidResourceRequestException.
    </description>
    <name>yarn.scheduler.maximum-allocation-mb</name>
    <value>2048</value>
</property>

<!-- 容器最小CPU核数，默认1个 -->
<property>
    <description>The minimum allocation for every container request at the RM	in terms of virtual CPU cores. Requests lower than this will be set to the	value of this property. Additionally, a node manager that is configured to	have fewer virtual cores than this value will be shut down by the resource	manager.
    </description>
    <name>yarn.scheduler.minimum-allocation-vcores</name>
    <value>1</value>
</property>

<!-- 容器最大CPU核数，默认4个，修改为2个 -->
<property>
    <description>The maximum allocation for every container request at the RM	in terms of virtual CPU cores. Requests higher than this will throw an InvalidResourceRequestException.</description>
    <name>yarn.scheduler.maximum-allocation-vcores</name>
    <value>2</value>
</property>

<!-- 虚拟内存检查，默认打开，修改为关闭 -->
<property>
    <description>Whether virtual memory limits will be enforced for
    containers.</description>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>

<!-- 虚拟内存和物理内存设置比例,默认2.1 -->
<property>
    <description>Ratio between virtual memory to physical memory when	setting memory limits for containers. Container allocations are	expressed in terms of physical memory, and virtual memory usage	is allowed to exceed this allocation by this ratio.
    </description>
    <name>yarn.nodemanager.vmem-pmem-ratio</name>
    <value>2.1</value>
</property>复制代码
```

##### 4.3 容量调度器多队列使用

| 默认只有default队列                                          |
| ------------------------------------------------------------ |
| ![image-20220831162705861](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_4/image-20220831162705861.png) |

- 在生产环境怎么创建队列？ （1）调度器默认就1个default队列，不能满足生产要求。 （2）按照框架：hive /spark/ flink 每个框架的任务放入指定的队列（企业用的不是特别多） （3）按照业务模块：登录注册、购物车、下单、业务部门1、业务部门2
- 创建多队列的好处？ （1）因为担心员工不小心，写递归死循环代码，把所有资源全部耗尽 （2）实现任务的**降级**使用，特殊时期保证重要的任务队列资源充足

业务部门1（重要）=》业务部门2（比较重要）=》下单（一般）=》购物车（一般）=》登录注册（次要）

###### 4.3.1 需求

需求1：default队列占总内存的40%，最大资源容量占总资源60%，hive队列占总内存的60%，最大资源容量占总资源80%。

需求2：配置队列优先级

###### 4.3.2 配置多队列的容量调度器

1）在capacity-scheduler.xml中配置如下：

 （1）修改如下配置

```
<!-- 指定多队列，增加hive队列 -->
<property>
    <name>yarn.scheduler.capacity.root.queues</name>
    <value>default,hive</value>
    <description>
      The queues at the this level (root is the root queue).
    </description>
</property>

<!-- 降低default队列资源额定容量为40%，默认100% -->
<property>
    <name>yarn.scheduler.capacity.root.default.capacity</name>
    <value>40</value>
</property>

<!-- 降低default队列资源最大容量为60%，默认100% -->
<property>
    <name>yarn.scheduler.capacity.root.default.maximum-capacity</name>
    <value>60</value>
</property>复制代码
```

 （2）为新加队列添加必要属性：

```
<!-- 指定hive队列的资源额定容量 -->
<property>
    <name>yarn.scheduler.capacity.root.hive.capacity</name>
    <value>60</value>
</property>

<!-- 用户最多可以使用队列多少资源，1表示 -->
<property>
    <name>yarn.scheduler.capacity.root.hive.user-limit-factor</name>
    <value>1</value>
</property>

<!-- 指定hive队列的资源最大容量 -->
<property>
    <name>yarn.scheduler.capacity.root.hive.maximum-capacity</name>
    <value>80</value>
</property>

<!-- 启动hive队列 -->
<property>
    <name>yarn.scheduler.capacity.root.hive.state</name>
    <value>RUNNING</value>
</property>

<!-- 哪些用户有权向队列提交作业 -->
<property>
    <name>yarn.scheduler.capacity.root.hive.acl_submit_applications</name>
    <value>*</value>
</property>

<!-- 哪些用户有权操作队列，管理员权限（查看/杀死） -->
<property>
    <name>yarn.scheduler.capacity.root.hive.acl_administer_queue</name>
    <value>*</value>
</property>

<!-- 哪些用户有权配置提交任务优先级 -->
<property>
    <name>yarn.scheduler.capacity.root.hive.acl_application_max_priority</name>
    <value>*</value>
</property>

<!-- 任务的超时时间设置：yarn application -appId appId -updateLifetime Timeout
参考资料：https://blog.cloudera.com/enforcing-application-lifetime-slas-yarn/ -->
<!-- 如果application指定了超时时间，则提交到该队列的application能够指定的最大超时时间不能超过该值。 
-->
<property>
    <name>yarn.scheduler.capacity.root.hive.maximum-application-lifetime</name>
    <value>-1</value>
</property>

<!-- 如果application没指定超时时间，则用default-application-lifetime作为默认值 -->
<property>
    <name>yarn.scheduler.capacity.root.hive.default-application-lifetime</name>
    <value>-1</value>
</property>复制代码
```

 2）重启Yarn或者执行yarn rmadmin -refreshQueues刷新队列，就可以看到两条队列：

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ yarn rmadmin -refreshQueues复制代码
```

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220808155018800](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_4/image-20220808155018800.png) |

###### 4.3.3 向Hive队列提交任务

 hadoop jar的方式

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount -D mapreduce.job.queuename=hive /input /output复制代码
```

> 注: -D表示运行时改变参数值

这样，这个任务在提交时，就会提交到hive队列：

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220831164300054](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_4/image-20220831164300054.png) |





# 实验1-5 Hadoop高薪加油站

## 实验概述

通过前面章节的学习，大家对Hadoop的使用已经跨过了初级的门槛，那么面对企业的实战环境，我们还需要掌握一些调优手段，只有这样才能在大数据领域游刃有余。HDFS调优、MapReduce、Yarn生产经验以及Hadoop综合调优都是我们需要去了解和熟悉的，这些是我们高薪之路的加油站。

## 实验环境

- AtStudy 实训平台
- Hadoop3.1.3

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16799944286199506.png)

## 实验目标

学习完成本实验后，您将能够

- 了解HDFS生产环境下的优化方式
- 了解HDSF生产环境下的故障处理
- MapReduce实战下的经验总结
- Yarn生产经验的总结
- Hadoop综合性调优方式

## 实验任务

### 任务一、HDFS调优

#### **【任务目标】**

HDFS是大数据数据存储的基石，在生产的过程中我们需要面对各种问题进行综合分析，找出相应的解决办法，在这里我们主要从内存、心跳、多目录配置等方面了解调优的方式和手段。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009364229550)

视频-1、HDFS调优

##### 1.1 NameNode优化

###### 1.1.1 内存配置

1）NameNode内存计算

 每个文件块大概占用`150byte`，一台服务器`128G`内存为例，能存储多少文件块呢？

> 128 * 1024 * 1024 * 1024 / 150Byte ≈ 9.1亿 G MB KB Byte

2）Hadoop2.x 系列，配置 NameNode 内存 NameNode内存默认 2000m，如果服务器内存 4G，NameNode内存可以配置 3G。在hadoop-env.sh文件中配置如下。

```
HADOOP_NAMENODE_OPTS=-Xmx3072m复制代码
```

3）Hadoop3.x系列，配置NameNode内存

- hadoop-env.sh中描述Hadoop的内存是动态分配的

```
# The maximum amount of heap to use (Java -Xmx).  If no unit
# is provided, it will be converted to MB.  Daemons will
# prefer any Xmx setting in their respective _OPT variable.
# There is no default; the JVM will autoscale based upon machine
# memory size.
# export HADOOP_HEAPSIZE_MAX=

# The minimum amount of heap to use (Java -Xms).  If no unit
# is provided, it will be converted to MB.  Daemons will
# prefer any Xms setting in their respective _OPT variable.
# There is no default; the JVM will autoscale based upon machine
# memory size.
# export HADOOP_HEAPSIZE_MIN=
HADOOP_NAMENODE_OPTS=-Xmx102400m复制代码
```

- 查看NameNode占用内存

```
[itluma@4a856475ffe5 ~]$ jps
1488 NameNode
32674 ResourceManager
32248 DataNode
760 JobHistoryServer
32425 SecondaryNameNode
558 NodeManager
16046 Jps复制代码
```

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220831221443597](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220831221443597.png) |

```
[itluma@4a856475ffe5 ~]$ jmap -heap 1488
Heap Configuration:
  MaxHeapSize              = 4164943872 (3972.0MB)复制代码
```

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220831224233376](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220831224233376.png) |

- 查看DataNode占用内存

```
[itluma@4a856475ffe5 ~]$ jmap -heap 32248
Heap Configuration:
  MaxHeapSize              = 4164943872 (3972.0MB)复制代码
```

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220831224401338](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220831224401338.png) |

查看发现4a856475ffe5上的NameNode和DataNode占用内存都是自动分配的，且相等。不是很合理。

经验参考：https://docs.cloudera.com/documentation/enterprise/6/release-notes/topics/rg_hardware_requirements.html#concept_fzz_dq4_gbb

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220831224617601](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220831224617601.png) |

具体修改：hadoop-env.sh

```
export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS -Xmx1024m"
export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS -Xmx1024m"复制代码
```

###### 1.1.2 NameNode心跳并发配置

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220831225049518](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220831225049518.png) |

> hdfs-site.xml
>
> The number of Namenode RPC server threads that listen to requests from clients. If dfs.namenode.servicerpc-address is not configured then Namenode RPC server threads listen to requests from all nodes.

```
NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。
```

对于大集群或者有大量客户端的集群来说，通常需要增大该参数。`默认值是10。`

```
<property>
  <name>dfs.namenode.handler.count</name>
  <value>21</value>
</property>复制代码
```

企业经验：dfs.namenode.handler.count= ![image-20220808172311083](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220808172311083.png)，比如集群规模（DataNode台数）为3台时，此参数设置为21。可通过简单的python代码计算该值，代码如下。

```
[itluma@4a856475ffe5 ~]$ sudo yum install -y python
[itluma@4a856475ffe5 ~]$ python
Python 2.7.5 (default, Apr 11 2018, 07:36:10) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import math
>>> print int(20*math.log(3))
21
>>> quit()复制代码
```

###### 1.1.3 开启回收站配置

开启回收站功能，可以将删除的文件在不超时的情况下，恢复原数据，起到防止误删除、备份等作用。

1）回收站工作机制

![image-20220808172449144](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220808172449144.png)

2）开启回收站功能参数说明

 （1）默认值fs.trash.interval = 0，0表示禁用回收站；其他值表示设置文件的存活时间。

 （2）默认值fs.trash.checkpoint.interval = 0，检查回收站的间隔时间。如果该值为0，则该值设置和fs.trash.interval的参数值相等。

 （3）要求fs.trash.checkpoint.interval <= fs.trash.interval。

3）启用回收站

修改core-site.xml，配置垃圾回收时间为1分钟。

```
<property>
   <name>fs.trash.interval</name>
   <value>1</value>
</property>复制代码
```

4）查看回收站

 回收站目录在HDFS集群中的路径：/user/itluma/.Trash/….

5）`注意：通过网页上直接删除的文件也不会走回收站。`

6）通过程序删除的文件不会经过回收站，需要调用moveToTrash()才进入回收站

```
Trash trash = New Trash(conf);
trash.moveToTrash(path);复制代码
```

7）只有在命令行利用hadoop fs -rm命令删除的文件才会走回收站。

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -rm -r /user/itluma/input
2021-07-14 16:13:42,643 INFO fs.TrashPolicyDefault: Moved: 'hdfs://localhost:9820/user/itluma/input' to trash at: hdfs://localhost:9820/user/itluma/.Trash/Current/user/itluma/input复制代码
```

8）恢复回收站数据

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -mv
/user/itluma/.Trash/Current/user/itluma/input  /user/itluma/input复制代码
```

##### 1.2 HDFS-多目录

###### 1.2.1 NameNode多目录配置

1）NameNode的本地目录可以配置成多个，且每个目录存放内容相同，增加了可靠性

![image-20220808221913188](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220808221913188.png)

2）具体配置如下

 （1）在hdfs-site.xml文件中添加如下内容

```
<property>
   <name>dfs.namenode.name.dir</name>
<value>file://${hadoop.tmp.dir}/dfs/name1,file://${hadoop.tmp.dir}/dfs/name2</value>
</property>复制代码
注意：因为每台服务器节点的磁盘情况不同，所以这个配置配完之后，可以选择不分发
```

 （2）停止集群，删除三台节点的data和logs中所有数据。

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ rm -rf data/ logs/
[itluma@4a856475ffe5 hadoop-3.1.3]$ rm -rf data/ logs/
[itluma@4a856475ffe5 hadoop-3.1.3]$ rm -rf data/ logs/复制代码
```

 （3）格式化集群并启动。

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ bin/hdfs namenode -format
[itluma@4a856475ffe5 hadoop-3.1.3]$ sbin/start-dfs.sh复制代码
```

**3）查看结果**

```
[itluma@4a856475ffe5 dfs]$ ll
总用量 12
drwx------. 3 itluma itluma 4096 12月 11 08:03 data
drwxrwxr-x. 3 itluma itluma 4096 12月 11 08:03 name1
drwxrwxr-x. 3 itluma itluma 4096 12月 11 08:03 name2复制代码
```

> 检查name1和name2里面的内容，发现一模一样。

###### 1.2.2 DataNode多目录配置

1）DataNode可以配置成多个目录，每个目录存储的数据不一样（数据不是副本）

![image-20220808221934444](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220808221934444.png)

2）具体配置如下

在hdfs-site.xml文件中添加如下内容

```
<property>
   <name>dfs.datanode.data.dir</name>
  <value>file://${hadoop.tmp.dir}/dfs/data1,file://${hadoop.tmp.dir}/dfs/data2</value>
</property>复制代码
```

3）查看结果

```
[itluma@4a856475ffe5 dfs]$ ll
总用量 12
drwx------. 3 itluma itluma 4096 4月  4 14:22 data1
drwx------. 3 itluma itluma 4096 4月  4 14:22 data2
drwxrwxr-x. 3 itluma itluma 4096 12月 11 08:03 name1
drwxrwxr-x. 3 itluma itluma 4096 12月 11 08:03 name2复制代码
```

4）向集群上传一个文件，再次观察两个文件夹里面的内容发现不一致（一个有数一个没有）

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop fs -put wcinput/word.txt /复制代码
```

###### 1.2.3 集群数据均衡之磁盘间数据均衡

生产环境，由于硬盘空间不足，往往需要增加一块硬盘。刚加载的硬盘没有数据时，可以执行磁盘数据均衡命令。（Hadoop3.x新特性）

![image-20220808221955113](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220808221955113.png)

（1）生成均衡计划（**我们只有一块磁盘，不会生成计划**）

```
hdfs diskbalancer -plan 服务器103复制代码
```

（2）执行均衡计划

```
hdfs diskbalancer -execute hadoop103.plan.json复制代码
```

（3）查看当前均衡任务的执行情况

```
hdfs diskbalancer -query hadoop103复制代码
```

（4）取消均衡任务

```
hdfs diskbalancer -cancel hadoop103.plan.json复制代码
```

###### 1.2.4 服务器间数据均衡

1）企业经验：

在企业开发中，如果经常在hadoop102和hadoop104上提交任务，且副本数为2，由于数据本地性原则，就会导致hadoop102和hadoop104数据过多，hadoop103存储的数据量小。

另一种情况，就是新服役的服务器数据量比较少，需要执行集群均衡命令。

![image-20220808231404043](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220808231404043.png)

2）开启数据均衡命令：

```
[itluma@hadoop105 hadoop-3.1.3]$ sbin/start-balancer.sh -threshold 10复制代码
```

 对于参数10，代表的是集群中各个节点的磁盘空间利用率相差不超过10%，可根据实际情况进行调整。

3）停止数据均衡命令：

```
[itluma@hadoop105 hadoop-3.1.3]$ sbin/stop-balancer.sh复制代码
```

> 注意：`由于HDFS需要启动单独的Rebalance Server来执行Rebalance操作，所以尽量不要在NameNode上执行start-balancer.sh，而是找一台比较空闲的机器。`

### 任务二、HDFS故障排除

#### **【任务目标】**

故障排除也是生产环境中需要掌握的，主要是NameNode故障处理、集群的安全模式及磁盘修复

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009364576045)

视频-2、HDFS故障排除

##### 1.1 NameNode故障处理

![image-20220809085637313](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220809085637313.png)

1）需求：

 NameNode进程挂了并且存储的数据也丢失了，如何恢复NameNode

2）故障模拟

 （1）kill -9 NameNode进程

```
[itluma@4a856475ffe5 efs]$ kill -9 1488复制代码
```

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220831232554535](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220831232554535.png) |

 （2）删除NameNode存储的数据（/opt/module/hadoop-3.1.3/data/tmp/dfs/name）

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ rm -rf /opt/module/hadoop-3.1.3/data/dfs/name/*复制代码
```

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220831232930193](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220831232930193.png) |

> 我们发生NameNode已经无法启动了，主要因为数据已经被破坏了！

3）问题解决

 （1）拷贝SecondaryNameNode中数据到原NameNode存储数据目录

```
[itluma@4a856475ffe5 dfs]$ cp -r /opt/module/hadoop-3.1.3/data/dfs/namesecondary/* /opt/module/hadoop-3.1.3/data/dfs/name/复制代码
```

 （2）重新启动NameNode

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hdfs --daemon start namenode复制代码
```

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220831233137459](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220831233137459.png) |

> NameNode成功启动，hdfs文件系统也能成功访问了

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220831233218662](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220831233218662.png) |

 （3）向集群上传一个文件

##### 1.2 集群安全模式&磁盘修复

1）安全模式：文件系统只接受读数据请求，而不接受删除、修改等变更请求

2）进入安全模式场景

- NameNode在加载镜像文件和编辑日志期间处于安全模式；
- NameNode再接收DataNode注册时，处于安全模式

![image-20220809085700712](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220809085700712.png)

3）退出安全模式条件

```
dfs.namenode.safemode.min.datanodes:最小可用datanode数量，默认0
dfs.namenode.safemode.threshold-pct:副本数达到最小要求的block占系统总block数的百分比，默认0.999f。（只允许丢一个块）
dfs.namenode.safemode.extension:稳定时间，默认值30000毫秒，即30秒复制代码
```

4）基本语法

集群处于安全模式，不能执行重要操作（写操作）。集群启动完成后，自动退出安全模式。

（1）bin/hdfs dfsadmin -safemode get （功能描述：查看安全模式状态）

（2）bin/hdfs dfsadmin -safemode enter （功能描述：进入安全模式状态）

（3）bin/hdfs dfsadmin -safemode leave （功能描述：离开安全模式状态）

（4）bin/hdfs dfsadmin -safemode wait （功能描述：等待安全模式状态）

**5）案例1：启动集群进入安全模式**

 （1）重新启动集群

```
[itluma@4a856475ffe5 subdir0]$ myhadoop.sh stop
[itluma@4a856475ffe5 subdir0]$ myhadoop.sh start复制代码
```

 （2）集群启动后，立即来到集群上删除数据，提示集群处于安全模式

![image-20220809100535583](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220809100535583.png)

**6）案例2：磁盘修复**

 需求：数据块损坏，进入安全模式，如何处理

（1）进入服务器的/opt/module/hadoop-3.1.3/data/dfs/data/current/BP-1015489500-192.168.10.102-1611909480872/current/finalized/subdir0/subdir0目录，统一删除某2个块信息

```
[itluma@4a856475ffe5 subdir0]$ pwd
/opt/module/hadoop-3.1.3/data/dfs/data/current/BP-1015489500-192.168.10.102-1611909480872/current/finalized/subdir0/subdir0

[itluma@4a856475ffe5 subdir0]$ rm -rf blk_1073741825 blk_1073741825_1001.meta
[itluma@4a856475ffe5 subdir0]$ rm -rf blk_1073741879 blk_1073741879_1055.meta复制代码
```

（2）重新启动集群

```
[itluma@4a856475ffe5 subdir0]$ myhadoop.sh stop
[itluma@4a856475ffe5 subdir0]$ myhadoop.sh start复制代码
```

（3）观察http://localhost:9870/dfshealth.html#tab-overview

![image-20220809102636588](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220809102636588.png)

> 说明：安全模式已经打开，块的数量没有达到要求。

（4）离开安全模式

```
[itluma@4a856475ffe5 subdir0]$ hdfs dfsadmin -safemode get
Safe mode is ON

[itluma@4a856475ffe5 subdir0]$ hdfs dfsadmin -safemode leave
Safe mode is OFF复制代码
```

（5）观察http://hadoop102:9870/dfshealth.html#tab-overview

![image-20220809103749236](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220809103749236.png)

（6）将元数据删除

![image-20220809103959277](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220809103959277.png)

![image-20220809104016174](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220809104016174.png)

（7）观察http://localhost:9870/dfshealth.html#tab-overview，集群已经正常

7）**案例3：**

需求：模拟等待安全模式

（1）查看当前模式

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hdfs dfsadmin -safemode get
Safe mode is OFF复制代码
```

（2）先进入安全模式

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ bin/hdfs dfsadmin -safemode enter复制代码
```

（3）创建并执行下面的脚本

在/opt/module/hadoop-3.1.3路径上，编辑一个脚本safemode.sh

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ vim safemode.sh
#!/bin/bash
hdfs dfsadmin -safemode wait
hdfs dfs -put /opt/module/hadoop-3.1.3/README.txt /

[itluma@4a856475ffe5 hadoop-3.1.3]$ chmod 777 safemode.sh
[itluma@4a856475ffe5 hadoop-3.1.3]$ ./safemode.sh 复制代码
```

（4）再打开一个窗口，执行

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ bin/hdfs dfsadmin -safemode leave复制代码
```

（5）再观察上一个窗口

```
Safe mode is OFF复制代码
```

> （6）HDFS群上已经有上传的数据了

![image-20220809110614594](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220809110614594.png)

### 任务三、MapReduce生产经验

#### **【任务目标】**

MapReduce程序的执行效率在生产环境下也需要从多方面进行优化才能有所提高。所以本任务的重点就是介绍MapReduce的调优经验。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009362814157)

视频-3、MapReduce生产经验

##### 3.1 MapReduce跑的慢的原因

MapReduce程序效率的瓶颈在于两点：

- 计算机性能：CPU、内存、磁盘、网络
- I/O操作优化
  - 数据倾斜
  - Map运行时间太长，导致Reduce等待过久
  - 小文件过多

##### 3.2 MapReduce常用调优参数

###### 3.2.1 MapReduce优化（上）

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220809112436022](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220809112436022-1661996727475.png) |

1）自定义分区，减少数据倾斜;

 定义类，继承Partitioner接口，重写getPartition方法

2）减少溢写的次数

 `mapreduce.task.io.sort.mb` Shuffle的环形缓冲区大小，默认100m，`可以提高到200m`

 `mapreduce.map.sort.spill.percent` 环形缓冲区溢出的阈值，默认80% ，`可以提高的90%`

3）增加每次Merge合并次数

 `mapreduce.task.io.sort.factor`默认10，可以`提高到20`

4）在不影响业务结果的前提条件下可以提前采用Combiner

```
job.setCombinerClass(xxxReducer.class);复制代码
```

5）为了减少磁盘IO，可以采用Snappy或者LZO压缩

```
conf.setBoolean("mapreduce.map.output.compress", true);
conf.setClass("mapreduce.map.output.compress.codec", SnappyCodec.class,CompressionCodec.class);复制代码
```

6）`mapreduce.map.memory.mb` `默认MapTask内存上限1024MB`。可以根据128M数据对应1G内存原则提高该内存。

7）`mapreduce.map.java.opts`：`控制MapTask堆内存大小`。（如果内存不够，报:`java.lang.OutOfMemoryError`）

8）`mapreduce.map.cpu.vcores` `默认MapTask的CPU核数1`。计算密集型任务可以增加CPU核数

9）异常重试

```
mapreduce.map.maxattempts`每个Map Task最大重试次数，一旦重试次数超过该值，则认为Map Task运行失败，`默认值：4。根据机器性能适当提高。
```

###### 3.2.2 MapReduce优化（下）

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220809113722379](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220809113722379-1661996721840.png) |

1）mapreduce.reduce.shuffle.parallelcopies每个Reduce去Map中拉取数据的并行数，`默认值是5。可以提高到10。`

2）mapreduce.reduce.shuffle.input.buffer.percent

 Buffer大小占Reduce可用内存的比例，`默认值0.7。可以提高到0.8`

3）mapreduce.reduce.shuffle.merge.percent Buffer中的数据达到多少比例开始写入磁盘，`默认值0.66。可以提高到0.75`

4）mapreduce.reduce.memory.mb `默认ReduceTask内存上限1024MB`，根据128m数据对应1G内存原则，`适当提高内存到4-6G`

5）mapreduce.reduce.java.opts：`控制ReduceTask堆内存大小`。（如果内存不够，报：java.lang.OutOfMemoryError）

6）mapreduce.reduce.cpu.vcores：默认`ReduceTask的CPU核数1个。可以提高到2-4个`

7）mapreduce.reduce.maxattempts：每个Reduce Task最大重试次数，一旦重试次数超过该值，则认为Map Task运行失败，`默认值：4。`

8）mapreduce.job.reduce.slowstart.completedmaps：当MapTask完成的比例达到该值后才会为ReduceTask申请资源。`默认是0.05。`

9）mapreduce.task.timeout如果一个Task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该Task处于Block状态，可能是卡住了，也许永远会卡住，为了防止因为用户程序永远Block住不退出，则强制设置了一个该超时时间（单位毫秒），`默认是600000（10分钟）。如果你的程序对每条输入数据的处理时间过长，建议将该参数调大。`

10）如果可以不用Reduce，尽可能不用

##### 3.3 MapReduce数据倾斜问题

1）数据倾斜现象

 数据频率倾斜——某一个区域的数据量要远远大于其他区域。

 数据大小倾斜——部分记录的大小远远大于平均值。

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220809114054751](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220809114054751-1661996712880.png) |

2）减少数据倾斜的方法

（1）首先检查是否空值过多造成的数据倾斜

 生产环境，可以直接过滤掉空值；如果想保留空值，就自定义分区，将空值加随机数打散。最后再二次聚合。

（2）能在map阶段提前处理，最好先在Map阶段处理。如：Combiner、MapJoin

（3）设置多个reduce个数

### 任务四、Yarn生产经验

#### **【任务目标】**

在Yarn的实战环境下也存在很多调优的方式和手段，这里主要是要了解一些常用的调优参数和调度器的各种算法，并能根据一定的场景选择相对合适的调度算法

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/07_Flink/lab1_2/image-20210824110318276.png?fileid=3270835009362243624)

视频-4、Hadoop综合实战

##### 4.1 常用的调优参数

1）调优参数列表

（1）Resourcemanager相关

```
yarn.resourcemanager.scheduler.client.thread-count  #ResourceManager处理调度器请求的线程数量
yarn.resourcemanager.scheduler.class  #配置调度器复制代码
```

（2）Nodemanager相关

```
yarn.nodemanager.resource.memory-mb     #NodeManager使用内存数				
yarn.nodemanager.resource.system-reserved-memory-mb #NodeManager为系统保留多少内存，和上一个参数二者取一即可
yarn.nodemanager.resource.cpu-vcores  #NodeManager使用CPU核数
yarn.nodemanager.resource.count-logical-processors-as-cores #是否将虚拟核数当作CPU核数
yarn.nodemanager.resource.pcores-vcores-multiplier #虚拟核数和物理核数乘数，例如：4核8线程，该参数就应设为2
yarn.nodemanager.resource.detect-hardware-capabilities #是否让yarn自己检测硬件进行配置
yarn.nodemanager.pmem-check-enabled  #是否开启物理内存检查限制container
yarn.nodemanager.vmem-check-enabled  #是否开启虚拟内存检查限制container
yarn.nodemanager.vmem-pmem-ratio    #虚拟内存物理内存比例	复制代码
```

（3）Container容器相关

```
yarn.scheduler.minimum-allocation-mb    #容器最小内存
yarn.scheduler.maximum-allocation-mb    #容器最大内存
yarn.scheduler.minimum-allocation-vcores  #容器最小核数
yarn.scheduler.maximum-allocation-vcores  #容器最大核数复制代码
```

2）参数具体使用案例

详见《实验1-4 Hadoop之Yarn》，任务三、Yarn调度器和调度算法。

##### 4.2 容量调度器使用

详见《实验1-4 Hadoop之Yarn》，任务三、Yarn调度器和调度算法

##### 4.3 公平调度器使用

详见《实验1-4 Hadoop之Yarn》，任务三、Yarn调度器和调度算法。

### 任务五、Hadoop综合实战

#### **【任务目标】**

结合场景案例，从实战角度做出总结，让大家从整体了解调优的方方面面。

#### **【任务步骤】**

##### 5.1 Hadoop小文件优化方法

###### 5.1.1 Hadoop小文件弊端

 HDFS上每个文件都要在NameNode上创建对应的元数据，这个元数据的大小约为150byte，这样当小文件比较多的时候，就会产生很多的元数据文件，`一方面会大量占用NameNode的内存空间，另一方面就是元数据文件过多，使得寻址索引速度变慢`。

 小文件过多，在进行MR计算时，会生成过多切片，需要启动过多的MapTask。每个MapTask处理的数据量小，`导致MapTask的处理时间比启动时间还小，白白消耗资源。`

###### 5.1.2 Hadoop小文件解决方案

1）在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS（数据源头）

2）Hadoop Archive（存储方向）

 是一个高效的将小文件放入HDFS块中的文件存档工具，能够将多个小文件打包成一个HAR文件，从而达到减少NameNode的内存使用

3）CombineTextInputFormat（计算方向）

 CombineTextInputFormat用于将多个小文件在切片过程中生成一个单独的切片或者少量的切片。

4）开启uber模式，实现JVM重用（计算方向）

 默认情况下，每个Task任务都需要启动一个JVM来运行，如果Task任务计算的数据量很小，我们可以让同一个Job的多个Task运行在一个JVM中，不必为每个Task都开启一个JVM。

 （1）未开启uber模式，在/input路径上上传多个小文件并执行wordcount程序

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output2复制代码
```

 （2）观察控制台

```
2021-02-14 16:13:50,607 INFO mapreduce.Job: Job job_1613281510851_0002 running in uber mode : false复制代码
```

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220901094217760](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220901094217760.png) |

 （3）观察`http://localhost:8088/cluster`

|                                                              |
| ------------------------------------------------------------ |
| ![image-20220809135036249](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220809135036249.png) |

 （4）开启uber模式，在mapred-site.xml中添加如下配置

```
<!-- 开启uber模式，默认关闭 -->
<property>
  <name>mapreduce.job.ubertask.enable</name>
  <value>true</value>
</property>

<!-- uber模式中最大的mapTask数量，可向下修改 --> 
<property>
  <name>mapreduce.job.ubertask.maxmaps</name>
  <value>9</value>
</property>

<!-- uber模式中最大的reduce数量，可向下修改 -->
<property>
  <name>mapreduce.job.ubertask.maxreduces</name>
  <value>1</value>
</property>

<!-- uber模式中最大的输入数据量，默认使用dfs.blocksize 的值，可向下修改 -->
<property>
  <name>mapreduce.job.ubertask.maxbytes</name>
  <value></value>
</property>复制代码
```

 （5）再次执行wordcount程序

```
[itluma@4a856475ffe5 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output2复制代码
```

 （6）观察控制台

```
2021-02-14 16:28:36,198 INFO mapreduce.Job: Job job_1613281510851_0003 running in uber mode : true复制代码
```

 （7）观察`http://localhost:8088/cluster`

![image-20220809125754607](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/02_hadoop/lab1_5/image-20220809125754607.png)

##### 5.2 企业开发场景案例（集群环境）

###### 5.2.1 需求

（1）需求：从1G数据中，统计每个单词出现次数。服务器3台，每台配置4G内存，4核CPU，4线程。

（2）需求分析： 1G / 128m = 8个MapTask；1个ReduceTask；1个mrAppMaster 平均每个节点运行 10 个 / 3 台 ≈ 3 个任务（4 3 3）

（3）各服务器节点分配情况

- NameNode和SecondaryNameNode一般不会安装在同一台服务器
- ResourceManager也很消耗内存，一般也不会将NameNode、SecondaryNameNode配置在同一台机器上。

|      | 服务器一（hadoop101） | 服务器二（hadoop102）       | 服务器三（hadoop103）      |
| ---- | --------------------- | --------------------------- | -------------------------- |
| HDFS | NameNode DataNode     | DataNode                    | SecondaryNameNode DataNode |
| YARN | NodeManager           | ResourceManager NodeManager | NodeManager                |

###### 5.2.2 HDFS参数调优

（1）修改：hadoop-env.sh

```
export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS -Xmx1024m"
export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS -Xmx1024m"复制代码
```

（2）修改hdfs-site.xml

```
<!-- NameNode有一个工作线程池，默认值是10 -->
<property>
  <name>dfs.namenode.handler.count</name>
  <value>21</value>
</property>复制代码
```

（3）修改core-site.xml

```
<!-- 配置垃圾回收时间为60分钟 -->
<property>
  <name>fs.trash.interval</name>
  <value>60</value>
</property>复制代码
```

（4）分发配置，需要将修改的配置文件同步到其他节点中

```
[itluma@hadoop102 hadoop]$ 分发命令 hadoop-env.sh hdfs-site.xml core-site.xml复制代码
```

###### 5.2.3 MapReduce参数调优

（1）修改mapred-site.xml

```
<!-- 环形缓冲区大小，默认100m -->
<property>
 <name>mapreduce.task.io.sort.mb</name>
 <value>100</value>
</property>
 
<!-- 环形缓冲区溢写阈值，默认0.8 -->
<property>
 <name>mapreduce.map.sort.spill.percent</name>
 <value>0.80</value>
</property>

<!-- merge合并次数，默认10个 -->
<property>
 <name>mapreduce.task.io.sort.factor</name>
 <value>10</value>
</property>

<!-- maptask内存，默认1g； maptask堆内存大小默认和该值大小一致mapreduce.map.java.opts -->
<property>
 <name>mapreduce.map.memory.mb</name>
 <value>-1</value>
 <description>The amount of memory to request from the scheduler for each  map task. If this is not specified or is non-positive, it is inferred from mapreduce.map.java.opts and mapreduce.job.heap.memory-mb.ratio. If java-opts are also not specified, we set it to 1024.
 </description>
</property>
 
<!-- matask的CPU核数，默认1个 -->
<property>
 <name>mapreduce.map.cpu.vcores</name>
 <value>1</value>
</property>

<!-- matask异常重试次数，默认4次 -->
<property>
 <name>mapreduce.map.maxattempts</name>
 <value>4</value>
</property>
 
<!-- 每个Reduce去Map中拉取数据的并行数。默认值是5 -->
<property>
 <name>mapreduce.reduce.shuffle.parallelcopies</name>
 <value>5</value>
</property>
 
<!-- Buffer大小占Reduce可用内存的比例，默认值0.7 -->
<property>
 <name>mapreduce.reduce.shuffle.input.buffer.percent</name>
 <value>0.70</value>
</property>

<!-- Buffer中的数据达到多少比例开始写入磁盘，默认值0.66。 -->
<property>
 <name>mapreduce.reduce.shuffle.merge.percent</name>
 <value>0.66</value>
</property>

<!-- reducetask内存，默认1g；reducetask堆内存大小默认和该值大小一致mapreduce.reduce.java.opts -->
<property>
 <name>mapreduce.reduce.memory.mb</name>
 <value>-1</value>
 <description>The amount of memory to request from the scheduler for each  reduce task. If this is not specified or is non-positive, it is inferred
  from mapreduce.reduce.java.opts and mapreduce.job.heap.memory-mb.ratio.
  If java-opts are also not specified, we set it to 1024.
 </description>
</property>

<!-- reducetask的CPU核数，默认1个 -->
<property>
 <name>mapreduce.reduce.cpu.vcores</name>
 <value>2</value>
</property>

<!-- reducetask失败重试次数，默认4次 -->
<property>
 <name>mapreduce.reduce.maxattempts</name>
 <value>4</value>
</property>

<!-- 当MapTask完成的比例达到该值后才会为ReduceTask申请资源。默认是0.05 -->
<property>
 <name>mapreduce.job.reduce.slowstart.completedmaps</name>
 <value>0.05</value>
</property>

<!-- 如果程序在规定的默认10分钟内没有读到数据，将强制超时退出 -->
<property>
 <name>mapreduce.task.timeout</name>
 <value>600000</value>
</property>复制代码
```

（2）分发配置

```
[itluma@hadoop102 hadoop]$ 分发命令 mapred-site.xml复制代码
```

###### 5.2.4 Yarn参数调优

（1）修改yarn-site.xml配置参数如下：

```
<!-- 选择调度器，默认容量 -->
<property>
  <description>The class to use as the resource scheduler.</description>
 <name>yarn.resourcemanager.scheduler.class</name>
 <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
</property>

<!-- ResourceManager处理调度器请求的线程数量,默认50；如果提交的任务数大于50，可以增加该值，但是不能超过3台 * 4线程 = 12线程（去除其他应用程序实际不能超过8） -->
<property>
  <description>Number of threads to handle scheduler interface.</description>
 <name>yarn.resourcemanager.scheduler.client.thread-count</name>
  <value>8</value>
</property>

<!-- 是否让yarn自动检测硬件进行配置，默认是false，如果该节点有很多其他应用程序，建议手动配置。如果该节点没有其他应用程序，可以采用自动 -->
<property>
  <description>Enable auto-detection of node capabilities such as
  memory and CPU.
  </description>
 <name>yarn.nodemanager.resource.detect-hardware-capabilities</name>
  <value>false</value>
</property>

<!-- 是否将虚拟核数当作CPU核数，默认是false，采用物理CPU核数 -->
<property>
  <description>Flag to determine if logical processors(such as
  hyperthreads) should be counted as cores. Only applicable on Linux
  when yarn.nodemanager.resource.cpu-vcores is set to -1 and
  yarn.nodemanager.resource.detect-hardware-capabilities is true.
  </description>
  <name>yarn.nodemanager.resource.count-logical-processors-as-cores</name>
  <value>false</value>
</property>

<!-- 虚拟核数和物理核数乘数，默认是1.0 -->
<property>
  <description>Multiplier to determine how to convert phyiscal cores to
  vcores. This value is used if yarn.nodemanager.resource.cpu-vcores
  is set to -1(which implies auto-calculate vcores) and
  yarn.nodemanager.resource.detect-hardware-capabilities is set to true. The  number of vcores will be calculated as  number of CPUs * multiplier.
  </description>
 <name>yarn.nodemanager.resource.pcores-vcores-multiplier</name>
  <value>1.0</value>
</property>

<!-- NodeManager使用内存数，默认8G，修改为4G内存 -->
<property>
  <description>Amount of physical memory, in MB, that can be allocated 
  for containers. If set to -1 and
  yarn.nodemanager.resource.detect-hardware-capabilities is true, it is
  automatically calculated(in case of Windows and Linux).
  In other cases, the default is 8192MB.
  </description>
 <name>yarn.nodemanager.resource.memory-mb</name>
  <value>4096</value>
</property>

<!-- nodemanager的CPU核数，不按照硬件环境自动设定时默认是8个，修改为4个 -->
<property>
  <description>Number of vcores that can be allocated
  for containers. This is used by the RM scheduler when allocating
  resources for containers. This is not used to limit the number of
  CPUs used by YARN containers. If it is set to -1 and
  yarn.nodemanager.resource.detect-hardware-capabilities is true, it is
  automatically determined from the hardware in case of Windows and Linux.
  In other cases, number of vcores is 8 by default.</description>
  <name>yarn.nodemanager.resource.cpu-vcores</name>
  <value>4</value>
</property>

<!-- 容器最小内存，默认1G -->
<property>
  <description>The minimum allocation for every container request at the RM in MBs. Memory requests lower than this will be set to the value of this property. Additionally, a node manager that is configured to have less memory than this value will be shut down by the resource manager.
  </description>
  <name>yarn.scheduler.minimum-allocation-mb</name>
  <value>1024</value>
</property>

<!-- 容器最大内存，默认8G，修改为2G -->
<property>
  <description>The maximum allocation for every container request at the RM in MBs. Memory requests higher than this will throw an  InvalidResourceRequestException.
  </description>
  <name>yarn.scheduler.maximum-allocation-mb</name>
  <value>2048</value>
</property> 

<!-- 容器最小CPU核数，默认1个 -->
<property>
  <description>The minimum allocation for every container request at the RM in terms of virtual CPU cores. Requests lower than this will be set to the  value of this property. Additionally, a node manager that is configured to have fewer virtual cores than this value will be shut down by the resource  manager.
  </description>
  <name>yarn.scheduler.minimum-allocation-vcores</name>
  <value>1</value>
</property>

<!-- 容器最大CPU核数，默认4个，修改为2个 -->
<property>
  <description>The maximum allocation for every container request at the RM in terms of virtual CPU cores. Requests higher than this will throw an
 InvalidResourceRequestException.</description>
  <name>yarn.scheduler.maximum-allocation-vcores</name>
  <value>2</value>
</property>

<!-- 虚拟内存检查，默认打开，修改为关闭 -->
<property>
  <description>Whether virtual memory limits will be enforced for
  containers.</description>
  <name>yarn.nodemanager.vmem-check-enabled</name>
  <value>false</value>
</property>

<!-- 虚拟内存和物理内存设置比例,默认2.1 -->
<property>
  <description>Ratio between virtual memory to physical memory when setting memory limits for containers. Container allocations are expressed in terms of physical memory, and virtual memory usage  is allowed to exceed this allocation by this ratio.
  </description>
  <name>yarn.nodemanager.vmem-pmem-ratio</name>
  <value>2.1</value>
</property>复制代码
```

（2）分发配置

```
[itluma@hadoop102 hadoop]$ 分发命令 yarn-site.xml复制代码
```

###### 5.2.5 执行程序

（1）重启集群

```
[itluma@hadoop102 hadoop-3.1.3]$ sbin/stop-yarn.sh
[itluma@hadoop103 hadoop-3.1.3]$ sbin/start-yarn.sh复制代码
```

（2）执行WordCount程序

```
[itluma@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /input /output复制代码
```

（3）观察Yarn任务执行页面

```
http://hadoop103:8088/cluster/apps
```



