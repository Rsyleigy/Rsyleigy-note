# Spark笔记

## 一、Spark与MapReduce的区别

### 1、Spark的运行速度比MR快

- spark处理数据是基于内存的，而MapReduce是基于磁盘处理数据的。

- Spark在处理数据时构建了DAG有向无环图，减少了shuffle和数据落地磁盘的次数

- **spark是粗粒度申请资源**指的是在提交资源时，spark会提前向资源管理器（yarn，mess）将资源申请完毕，如果申请不到资源就等待，如果申请到就运行task任务，而不需要task再去申请资源。

  **MapReduce是细粒度申请资源**，提交任务，task自己申请资源自己运行程序，自己释放资源，虽然资源能够充分利用，但是这样任务运行的很慢。

### 2、Spark的代码比MR简洁

### 3、Spark的稳定性没有MR好，当数据达到TB级别以上的时候，容易出现内存溢出错误

## 二、Spark的简介

### 1、什么是Spark

- Apache Spark 是一个用于大规模数据处理的快速通用计算引擎，最初由加州大学伯克利分校的 AMPLab 开发，于 2010 年开源。Spark 提供了高效的分布式数据处理功能，支持多种数据处理任务，包括批处理、实时流处理、机器学习和图形处理等。

### 2、Spark的组件

![image-20240529192531313](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202405291925458.png)

1. **Spark Core：**包含Spark的基本功能；尤其是定义RDD的API、操作以及这两者上的动作。其他Spark的库都是构建在RDD和Spark Core之上的。
2. **Spark SQL**：用于处理结构化数据。它允许用户通过SQL查询进行数据分析，并能将SQL查询和Spark的程序结合起来使用，提供高效的数据处理能力。
3. **Spark Streaming**：用于实时数据流处理。它能够处理实时的数据流，支持将数据分成微批次进行处理，并与其他Spark组件无缝集成。
4. **MLlib (Machine Learning Library)**：用于机器学习。MLlib提供了一组常用的机器学习算法和工具，如分类、回归、聚类、协同过滤等，帮助用户在Spark上进行大规模机器学习任务。
5. **GraphX**：用于图计算。GraphX是Spark的图计算库，提供了一系列API用于构建、操作和计算图数据结构，并支持图算法如PageRank、最短路径等。

### 3、Spark的运行模式

1. **Standalone模式**：

   - **概述**：Spark自带的集群管理模式，适用于小型集群。
   - **特点**：易于配置和设置，不依赖外部的集群管理工具。所有资源调度和任务管理都由Spark自身完成。
   - **使用场景**：适用于测试和开发环境，以及资源管理需求不高的小型集群。

   ~~~java
    /**
        *  将项目打包放到spark集群中使用standalone模式运行
        * standalone client
        * spark-submit --class com.shujia.core.Demo17SparkStandaloneSubmit --master spark://master:7077 --executor-memory 512m --total-executor-cores 1 spark-1.0.jar 100
        *
        * standalone cluster
        * spark-submit --class com.shujia.core.Demo17SparkStandaloneSubmit --master spark://master:7077 --executor-memory 512m --total-executor-cores 1 --deploy-mode cluster spark-1.0.jar 100
        *
        */
   ~~~

2. **YARN模式**：

   - **概述**：使用Hadoop的YARN（Yet Another Resource Negotiator）作为资源管理器。
   - **特点**：与Hadoop生态系统深度集成，能够共享Hadoop集群的资源，支持多租户和高资源利用率。
   - **使用场景**：适用于已经有Hadoop集群的环境，特别是需要与Hadoop组件（如HDFS、Hive）紧密集成的情况。
   - **--master yarn-client**

   > spark	on yarn client模式   日志在本地输出，一班用于上线前测试
   >
   > 注意：如果在本地大量使用client模式提交任务，会导致本地节点网卡流量剧增

   ~~~Java
   spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client spark-examples_2.12-3.1.3.jar 100
   ~~~

   - **--master yarn-cluster**

   > spark on yarn cluster模式   Driver在yarn中随机的一个节点中启动，在本地看不到详细的执行日志，一般上线使用（每日调度使用）

   ~~~Java
   spark-submit --class org.apache.spark.examples.SparkPi --master yarn-cluster spark-examples_2.12-3.1.3.jar 100
   ~~~

   

3. **Mesos模式**：

   - **概述**：使用Apache Mesos作为资源管理和调度框架。
   - **特点**：提供高度可扩展的资源管理，能够支持多种不同的应用框架（如Hadoop、Storm）在同一集群中运行。
   - **使用场景**：适用于需要在一个集群中运行多种不同的大数据处理框架的情况，提供灵活的资源共享和调度。

4. **Kubernetes模式**：

   - **概述**：使用Kubernetes作为集群管理工具，进行容器化部署和调度。
   - **特点**：利用Kubernetes的容器编排和管理能力，提供高可用性和易于扩展的部署方案。
   - **使用场景**：适用于已经采用Kubernetes进行容器化管理的环境，特别是需要微服务架构和动态资源分配的情况。

5. **Local模式**：

   - **概述**：在单机上运行，通常用于开发和调试。
   - **特点**：无需集群配置，运行在单个节点上，可以在本地快速进行开发和测试。
   - **使用场景**：适用于开发、调试和小规模数据处理任务，不适合生产环境的大规模数据处理。

### 4、Spark的连接

- 要编写 Spark 应用程序，您需要添加对 Spark 的 Maven 依赖项。Spark 可通过 Maven Central 获取：

~~~java
groupId = org.apache.spark
artifactId = spark-core_2.12
version = 3.1.3
~~~

- 如果您希望访问 HDFS 集群，则需要 `hadoop-client`为您的 HDFS 版本添加依赖项。

~~~Java
groupId = org.apache.hadoop
artifactId = hadoop-client
version = <your-hdfs-version>
~~~

### 5、Spark的初始化

Spark 程序必须做的第一件事是创建一个[SparkContext](https://spark.apache.org/docs/3.1.3/api/scala/org/apache/spark/SparkContext.html)对象，该对象告诉 Spark 如何访问集群。要创建，`SparkContext`您首先需要构建一个包含有关应用程序的信息的[SparkConf对象。](https://spark.apache.org/docs/3.1.3/api/scala/org/apache/spark/SparkConf.html)

每个 JVM 只能有一个 SparkContext 处于活动状态。`stop()`在创建新的 SparkContext 之前，必须先激活 SparkContext。

~~~java
val conf = new SparkConf().setAppName(appName).setMaster(master)
new SparkContext(conf)
~~~

## 三、RDD（弹性的分布式数据集）

### 1、什么是RDD

- Spark 提供的主要抽象是*弹性分布式数据集*(RDD)，它是跨集群节点分区的元素集合，可以并行操作。
- 弹性分布式数据集
  - 弹性：RDD将来在计算的时候，其中的数据可以是很大，也可以是很小
  - 分布式：数据可以分布在多台服务器中，RDD中的分区来自于block块，而今后的block块会来自不同的datanode
  - 数据集：RDD自身是不存储数据的，只是一个代码计算逻辑，今后触发作业执行的时候，数据会在RDD之间流动

### 2、RDD的五大特性

- RDD是由一组的分区构成的，默认一个block对应一个分区

- 函数（算子）是作用在分区上的

- RDD之间存在依赖关系，因为有了依赖关系，将整个作业划分了一个一个stage阶段，sumNum(stage) = Num(宽依赖) + 1

  - **宽依赖**

    后一个RDD中分区数据来自于前一个RDD中的多个分区数据  1对多的关系 会产生shuffle

  - **窄依赖**

    后一个RDD中分区数据对应前一个RDD中的一个分区数据 1对1的关系

  - **总结：**窄依赖的分区数是不可以改变，取决于第一个RDD分区数，宽依赖可以在产生shuffle的算子上设置分区数

- 分区类的算子只能作用在kv格式的RDD上，例如：reducebyKey，groupByKey, join

- spark为task的执行提供了最佳执行位置，移动计算而不是移动数据，spark会尽量将task发送到数据所在的节点去执

### 3、算子

- 分为两大类**转换算子 （Transformations）** 和 **行动算子（Actions）**

#### 3.1、Transformations

##### 3.1.1、Map

- 通过将源中的每个元素传递给函数func，返回一个新的分布式数据集。

~~~java
object Demo3Map {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
    conf.setMaster("local")
    conf.setAppName("map算子演示")
    val context = new SparkContext(conf)
    //====================================================

    val studentRDD: RDD[String] = context.textFile("spark/data/students.csv")

    /**
     * map算子：将rdd中的数据，一条一条的取出来传入到map函数中，map会返回一个新的rdd，map不会改变总数据条数
     */
    val splitRDD: RDD[List[String]] = studentRDD.map((s: String) => {
      println("============数加防伪码================")
      s.split(",").toList
    })

//    splitRDD.foreach(println)
  }
}
~~~

##### 3.1.2、Filter

- 返回通过选择源数据中函数返回true的元素而形成的新数据集。

~~~Java
object Demo4Filter {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
    conf.setMaster("local")
    conf.setAppName("map算子演示")
    val context = new SparkContext(conf)
    //====================================================
    val studentRDD: RDD[String] = context.textFile("spark/data/students.csv")

    /**
     *  filter: 过滤，将RDD中的数据一条一条取出传递给filter后面的函数，如果函数的结果是true，该条数据就保留，否则丢弃
     *
     *  filter一般情况下会减少数据的条数
     */
    val filterRDD: RDD[String] = studentRDD.filter((s: String) => {
      val strings: Array[String] = s.split(",")
      "男".equals(strings(3))
    })


    filterRDD.foreach(println)



  }
}
~~~

##### 3.1.3、FlatMap

- 类似于map，但每个输入项可以映射到0个或多个输出项(因此func应该返回一个Seq而不是单个项)。

~~~Java
object Demo5flatMap {
  def main(args: Array[String]): Unit = {

    val conf = new SparkConf()
    conf.setMaster("local")
    conf.setAppName("flatMap算子演示")
    val context = new SparkContext(conf)
    //====================================================
    val linesRDD: RDD[String] = context.textFile("spark/data/words.txt")

    /**
     *  flatMap算子：将RDD中的数据一条一条的取出传递给后面的函数，函数的返回值必须是一个集合。最后会将集合展开构成一个新的RDD
     */
    val wordsRDD: RDD[String] = linesRDD.flatMap((line: String) => line.split("\\|"))

    wordsRDD.foreach(println)
  }
}
~~~

##### 3.1.4、Sample

- 使用给定的随机数生成器种子，对数据的一小部分进行采样，无论是否进行替换

~~~Java
object Demo6sample {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
    conf.setMaster("local")
    conf.setAppName("flatMap算子演示")
    val context = new SparkContext(conf)
    //====================================================
    val studentRDD: RDD[String] = context.textFile("spark/data/students.csv")

    /**
     *  sample算子：从前一个RDD的数据中抽样一部分数据
     *
     *  抽取的比例不是正好对应的，在抽取的比例上下浮动 比如1000条抽取10% 抽取的结果在100条左右
     */
    val sampleRDD: RDD[String] = studentRDD.sample(withReplacement = true, 0.1)
    sampleRDD.foreach(println)
  }

}
~~~

##### 3.1.5、GroupBy

- 按照指定的字段进行分组，返回的是一个键是分组字段，值是一个存放原本数据的迭代器的键值对 返回的是kv格式的RDD， key: 是分组字段，value: 是spark中的迭代器，迭代器中的数据，不是完全被加载到内存中计算，迭代器只能迭代一次，groupBy会产生shuffle

~~~Java
object Demo7GroupBy {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
    conf.setMaster("local")
    conf.setAppName("groupBy算子演示")
    val context = new SparkContext(conf)
    //====================================================
    val studentRDD: RDD[String] = context.textFile("spark/data/students.csv")

    val splitRDD: RDD[Array[String]] = studentRDD.map((s: String) => s.split(","))

    //需求：求出每个班级平均年龄
    //使用模式匹配的方式取出班级和年龄
    val clazzWithAgeRDD: RDD[(String, Int)] = splitRDD.map {
      case Array(_, _, age: String, _, clazz: String) => (clazz, age.toInt)
    }

    /**
     *  groupBy：按照指定的字段进行分组，返回的是一个键是分组字段，值是一个存放原本数据的迭代器的键值对 返回的是kv格式的RDD
     *
     *  key: 是分组字段
     *  value: 是spark中的迭代器
     *  迭代器中的数据，不是完全被加载到内存中计算，迭代器只能迭代一次
     *
     *  groupBy会产生shuffle
     */
    //按照班级进行分组
    //val stringToStudents: Map[String, List[Student]] = stuList.groupBy((s: Student) => s.clazz)
    val kvRDD: RDD[(String, Iterable[(String, Int)])] = clazzWithAgeRDD.groupBy(_._1)
    val clazzAvgAgeRDD: RDD[(String, Double)] = kvRDD.map {
      case (clazz: String, itr: Iterable[(String, Int)]) =>
        //CompactBuffer((理科二班,21), (理科二班,23), (理科二班,21), (理科二班,23), (理科二班,21), (理科二班,21), (理科二班,24))
        //CompactBuffer(21,23,21,23,21,21,24)
        val allAge: Iterable[Int] = itr.map((kv: (String, Int)) => kv._2)
        val avgAge: Double = allAge.sum.toDouble / allAge.size
        (clazz, avgAge)
    }

    clazzAvgAgeRDD.foreach(println)

    while (true){

    }


  }

}
~~~

##### 3.1.6、GroupByKey

- 当对(K, V)对的数据集调用时，返回(K, Iterable<V>)对的数据集。注意:如果您分组是为了对每个键执行聚合(例如求和或求平均值)，那么使用reduceByKey或aggregateByKey将产生更好的性能。注意:默认情况下，输出中的并行度级别取决于父RDD的分区数量。您可以传递一个可选的numPartitions参数来设置不同数量的任务。

~~~java
object Demo8GroupByKey {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
    conf.setMaster("local")
    conf.setAppName("groupByKey算子演示")
    val context = new SparkContext(conf)
    //====================================================
    val studentRDD: RDD[String] = context.textFile("spark/data/students.csv")

    val splitRDD: RDD[Array[String]] = studentRDD.map((s: String) => s.split(","))

    //需求：求出每个班级平均年龄
    //使用模式匹配的方式取出班级和年龄
    val clazzWithAgeRDD: RDD[(String, Int)] = splitRDD.map {
      case Array(_, _, age: String, _, clazz: String) => (clazz, age.toInt)
    }


    /**
     *  groupByKey: 按照键进行分组，将value值构成迭代器返回
     *  将来你在spark中看到RDD[(xx, xxx)] 这样的RDD就是kv键值对类型的RDD
     *  只有kv类型键值对RDD才可以调用groupByKey算子
     *
     */
    val kvRDD: RDD[(String, Iterable[Int])] = clazzWithAgeRDD.groupByKey()
    val clazzAvgAgeRDD: RDD[(String, Double)] = kvRDD.map {
      case (clazz: String, ageItr: Iterable[Int]) =>
        (clazz, ageItr.sum.toDouble / ageItr.size)
    }
    clazzAvgAgeRDD.foreach(println)

    while (true){

    }

    /**
     *  groupBy与groupByKey的区别（spark的面试题）
     *  1、代码上的区别：任意一个RDD都可以调用groupBy算子，只有kv类型的RDD才可以调用groupByKey
     *  2、groupByKey之后产生的RDD的结构比较简单，方便后续处理
     *  3、groupByKey的性能更好，执行速度更快，因为groupByKey相比较与groupBy算子来说，shuffle所需要的数据量较少
     */
  }
}
~~~

##### 3.1.7、ReduceByKey

- 当在(K, V)对的数据集上调用时，返回(K, V)对的数据集，其中每个键的值使用给定的reduce函数func聚合，该函数必须是类型(V,V) => V。与groupByKey一样，reduce任务的数量可以通过可选的第二个参数配置。

~~~Java
object Demo9ReduceByKey {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
    conf.setMaster("local")
    conf.setAppName("reduceByKey算子演示")
    val context = new SparkContext(conf)
    //====================================================
    val studentRDD: RDD[String] = context.textFile("spark/data/students.csv")

    val splitRDD: RDD[Array[String]] = studentRDD.map((s: String) => s.split(","))

    //求每个班级的人数
    val clazzKVRDD: RDD[(String, Int)] = splitRDD.map {
      case Array(_, _, _, _, clazz: String) => (clazz, 1)
    }

    /**
     * 利用groupByKey实现
     */
    //    val kvRDD: RDD[(String, Iterable[Int])] = clazzKVRDD.groupByKey()
    //    val clazzAvgAgeRDD: RDD[(String, Double)] = kvRDD.map {
    //      case (clazz: String, n: Iterable[Int]) =>
    //        (clazz, n.sum)
    //    }
    //    clazzAvgAgeRDD.foreach(println)

    /**
     * 利用reduceByKey实现：按照键key对value值直接进行聚合，需要传入聚合的方式
     * reduceByKey算子也是只有kv类型的RDD才能调用
     *
     *
     */
    val countRDD: RDD[(String, Int)] = clazzKVRDD.reduceByKey((x: Int, y: Int) => x + y)
    countRDD.foreach(println)



//    clazzKVRDD.groupByKey()
//      .map(kv=>(kv._1,kv._2.sum))
//      .foreach(println)

    while (true){

    }

    /**
     *  reduceByKey与groupByKey的区别
     *  1、reduceByKey比groupByKey在map端多了一个预聚合的操作，预聚合之后的shuffle数据量肯定是要少很多的，性能上比groupByKey要好
     *  2、从灵活角度来看，reduceByKey并没有groupByKey灵活
     *   比如reduceByKey无法做方差，groupByKey后续可以完成
     *
     */



  }
}
~~~

- ReduceByKey和GroupByKey的区别

![reduceByKey与groupByKey](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202405292053783.png)

##### 3.1.8、Union

- 返回一个新数据集，其中包含源数据集和参数中元素的并集。

~~~Java
object Demo10Union {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
    conf.setMaster("local")
    conf.setAppName("Union算子演示")
    val context = new SparkContext(conf)
    //====================================================
    val w1RDD: RDD[String] = context.textFile("spark/data/ws/w1.txt") // 1
    val w2RDD: RDD[String] = context.textFile("spark/data/ws/w2.txt") // 1

    /**
     *  union:上下合并两个RDD,前提是两个RDD中的数据类型要一致，合并后不会对结果进行去重
     *
     *  注：这里的合并只是逻辑层面上的合并，物理层面其实是没有合并
     */

    val unionRDD: RDD[String] = w1RDD.union(w2RDD)
    println(unionRDD.getNumPartitions) // 2


    unionRDD.foreach(println)
    while (true){

    }


  }
}
~~~

##### 3.1.9、Join

- 当对类型为(K, V)和(K, W)的数据集调用时，返回(K， (V, W))对的数据集，其中每个键的所有元素对。通过支持外部连接`leftOuterJoin`, `rightOuterJoin`, and `fullOuterJoin`.

~~~Java
object Demo11Join {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
    conf.setMaster("local")
    conf.setAppName("Join算子演示")
    val context = new SparkContext(conf)
    //====================================================

    //两个kv类型的RDD之间的关联
    //通过scala中的集合构建RDD
    val rdd1: RDD[(String, String)] = context.parallelize(
      List(
        ("1001", "尚平"),
        ("1002", "丁义杰"),
        ("1003", "徐昊宇"),
        ("1004", "包旭"),
        ("1005", "朱大牛"),
        ("1006","汪权")
      )
    )

    val rdd2: RDD[(String, String)] = context.parallelize(
      List(
        ("1001", "崩坏"),
        ("1002", "原神"),
        ("1003", "王者"),
        ("1004", "修仙"),
        ("1005", "学习"),
        ("1007", "敲代码")
      )
    )

    /**
     *  内连接：join
     *  左连接：leftJoin
     *  右连接：rightJoin
     *  全连接：fullJoin
     */

    //内连接
//    val innerJoinRDD: RDD[(String, (String, String))] = rdd1.join(rdd2)
//    //加工一下RDD
//    val innerJoinRDD2: RDD[(String, String, String)] = innerJoinRDD.map {
//      case (id: String, (name: String, like: String)) => (id, name, like)
//    }
//    innerJoinRDD2.foreach(println)

    //左连接
    val leftJoinRDD: RDD[(String, (String, Option[String]))] = rdd1.leftOuterJoin(rdd2)
    //加工一下RDD
    val leftJoinRDD2: RDD[(String, String, String)] = leftJoinRDD.map {
      case (id: String, (name: String, Some(like))) => (id, name, like)
      case (id: String, (name: String, None)) => (id, name, "无爱好")
    }
    leftJoinRDD2.foreach(println)

    println("=================================")
    //右连接自己试
    //TODO:自己试右连接

    //全连接
    val fullJoinRDD: RDD[(String, (Option[String], Option[String]))] = rdd1.fullOuterJoin(rdd2)
    //加工一下RDD
    val fullJoinRDD2: RDD[(String, String, String)] = fullJoinRDD.map {
      case (id: String, (Some(name), Some(like))) => (id, name, like)
      case (id: String, (Some(name), None)) => (id, name, "无爱好")
      case (id: String, (None, Some(like))) => (id, "无姓名", like)
    }
    fullJoinRDD2.foreach(println)




  }

}
~~~

##### 3.1.10、MapValues

- 作用在kv型RDD上，只对值进行处理，键不动

~~~Java
object Demo11Join {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
    conf.setMaster("local")
    conf.setAppName("Join算子演示")
    val context = new SparkContext(conf)
    //====================================================

    //两个kv类型的RDD之间的关联
    //通过scala中的集合构建RDD
    val rdd1: RDD[(String, String)] = context.parallelize(
      List(
        ("1001", "尚平"),
        ("1002", "丁义杰"),
        ("1003", "徐昊宇"),
        ("1004", "包旭"),
        ("1005", "朱大牛"),
        ("1006","汪权")
      )
    )

    val rdd2: RDD[(String, String)] = context.parallelize(
      List(
        ("1001", "崩坏"),
        ("1002", "原神"),
        ("1003", "王者"),
        ("1004", "修仙"),
        ("1005", "学习"),
        ("1007", "敲代码")
      )
    )

    /**
     *  内连接：join
     *  左连接：leftJoin
     *  右连接：rightJoin
     *  全连接：fullJoin
     */

    //内连接
//    val innerJoinRDD: RDD[(String, (String, String))] = rdd1.join(rdd2)
//    //加工一下RDD
//    val innerJoinRDD2: RDD[(String, String, String)] = innerJoinRDD.map {
//      case (id: String, (name: String, like: String)) => (id, name, like)
//    }
//    innerJoinRDD2.foreach(println)

    //左连接
    val leftJoinRDD: RDD[(String, (String, Option[String]))] = rdd1.leftOuterJoin(rdd2)
    //加工一下RDD
    val leftJoinRDD2: RDD[(String, String, String)] = leftJoinRDD.map {
      case (id: String, (name: String, Some(like))) => (id, name, like)
      case (id: String, (name: String, None)) => (id, name, "无爱好")
    }
    leftJoinRDD2.foreach(println)

    println("=================================")
    //右连接自己试
    //TODO:自己试右连接

    //全连接
    val fullJoinRDD: RDD[(String, (Option[String], Option[String]))] = rdd1.fullOuterJoin(rdd2)
    //加工一下RDD
    val fullJoinRDD2: RDD[(String, String, String)] = fullJoinRDD.map {
      case (id: String, (Some(name), Some(like))) => (id, name, like)
      case (id: String, (Some(name), None)) => (id, name, "无爱好")
      case (id: String, (None, Some(like))) => (id, "无姓名", like)
    }
    fullJoinRDD2.foreach(println)




  }

}
~~~

##### 3.1.11、MapPartitions

- 与map类似，但在RDD的每个分区(块)上分别运行，因此在类型为T的RDD上运行时，func必须为类型Iterator<T> => Iterator<U>。

~~~Java
object Demo14mapPartition {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
    conf.setMaster("local")
    conf.setAppName("mapPartition算子演示")
    val context = new SparkContext(conf)
    //====================================================
    //需求：统计总分年级排名前10的学生的各科分数
    //读取分数文件数据
    val scoreRDD: RDD[String] = context.textFile("spark/data/ws/*") // 读取数据文件

    println(scoreRDD.getNumPartitions)

    /**
     *  mapPartition: 主要作用是一次处理一个分区的数据，将一个分区的数据一个一个传给后面的函数进行处理
     *
     *  迭代器中存放的是一个分区的数据
     */
//    val mapPartitionRDD: RDD[String] = scoreRDD.mapPartitions((itr: Iterator[String]) => {
//
//      println(s"====================当前处理的分区====================")
//      //这里写的逻辑是作用在一个分区上的所有数据
//      val words: Iterator[String] = itr.flatMap(_.split("\\|"))
//      words
//    })

//    mapPartitionRDD.foreach(println)

    scoreRDD.mapPartitionsWithIndex{
      case (index:Int,itr: Iterator[String]) =>
        println(s"当前所处理的分区编号是:${index}")
        itr.flatMap(_.split("\\|"))
    }.foreach(println)

  }

}
~~~

#### 3.2、Actions

##### 3.2.1、**reduce**(*func*)

- 使用函数func(接受两个参数并返回一个参数)聚合数据集的元素。函数应该是可交换的和可关联的，这样才能正确地并行计算。

##### 3.2.2、**collect**()

- 在驱动程序中以数组的形式返回数据集的所有元素。这通常在过滤器或其他返回足够小的数据子集的操作之后非常有用。

~~~Java
object Demo15Actions {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
    conf.setMaster("local")
    conf.setAppName("Action算子演示")
    val context = new SparkContext(conf)
    //====================================================

    val studentRDD: RDD[String] = context.textFile("spark/data/students.csv")

    /**
     * 转换算子：transformation 将一个RDD转换成另一个RDD，转换算子是懒执行的，需要一个action算子触发执行
     *
     * 行动算子（操作算子）：action算子，触发任务执行。一个action算子就会触发一次任务执行
     */
    println("$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$")
    val studentsRDD: RDD[(String, String, String, String, String)] = studentRDD.map(_.split(","))
      .map {
        case Array(id: String, name: String, age: String, gender: String, clazz: String) =>
          println("**************************** 数加防伪码 ^_^ ********************************")
          (id, name, age, gender, clazz)
      }
    println("$$$$$$$$$$$$$$$$$$$$$$***__***$$$$$$$$$$$$$$$$$$$$$$$$$")

    // foreach其实就是一个action算子
//    studentsRDD.foreach(println)
    //    println("="*100)
    //    studentsRDD.foreach(println)

    //    while (true){
    //
    //    }

    /**
     *  collect()行动算子 主要作用是将RDD转成scala中的数据结构
     *
     */
    val tuples: Array[(String, String, String, String, String)] = studentsRDD.collect()



  }

}
~~~

##### 3.2.3、**count**()

- 返回数据集中元素的个数。

##### 3.2.4、**first**()

- 返回数据集的第一个元素(类似于take(1))。

##### 3.2.5、**take**(*n*)

- 返回一个包含数据集的前n个元素的数组。

##### 3.2.6、**takeSample**(*withReplacement*, *num*, [*seed*])

- 返回一个数组，其中包含数据集的num个元素的随机样本，可替换或不替换，可选择预先指定随机数生成器种子。

##### 3.2.7、**takeOrdered**(*n*, *[ordering]*)

- 使用自然顺序或自定义比较器返回RDD的前n个元素。

##### 3.2.8、**saveAsTextFile**(*path*)

- 将数据集的元素作为文本文件(或文本文件集)写入本地文件系统、HDFS或任何其他hadoop支持的文件系统的给定目录中。Spark将对每个元素调用toString，将其转换为文件中的一行文本。

##### 3.2.9、**countByKey**()

- 仅在(K, V)类型的rdd上可用。返回(K, Int)对的哈希映射，其中包含每个键的计数。

##### 3.2.10、**foreach**(*func*)

- 对数据集的每个元素运行函数function。这通常是为了处理诸如更新Accumulator或与外部存储系统交互等副作用。
  注意:在foreach()之外修改累加器以外的变量可能会导致未定义的行为。有关详细信息，请参见理解闭包。

### 4、累加器（Accumulators）

![](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202406052002728.png)

- spark代码分为算子内的代码和算子外的代码，算子内的代码在Executor端执行，算子外的代码在Driver端执行，如果在算子内对算子外的变量进行修改，是不会生效的，所以需要使用spark的累加器进行全局累加。

~~~java 
object Demo20Accumulator {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
    conf.setMaster("local")
    conf.setAppName("map算子演示")
    val context = new SparkContext(conf)
    //====================================================

    val studentRDD: RDD[String] = context.textFile("spark/data/students.csv")

    val scoreRDD: RDD[String] = context.textFile("spark/data/score.txt")

//    var count = 0
//    studentRDD.foreach((line:String)=>{
//      count+=1
//      println("-------------------------")
//      println(count)
//      println("-------------------------")
//    })
//    println(s"count的值为：${count}")

    /**
     * 累加器
     *
     * 由SparkContext来创建
     * 注意：
     *  1、因为累加器的执行实在RDD中执行的，而RDD是在Executor中执行的，而要想在Executor中执行就得有一个action算子触发任务调度
     *  2、sparkRDD中无法使用其他的RDD
     *  3、SparkContext无法在RDD内部使用，因为SparkContext对象无法进行序列化，不能够通过网络发送到Executor中
     */

//    val accumulator: LongAccumulator = context.longAccumulator
//    studentRDD.foreach((line:String)=>{
//      accumulator.add(1)
//    })

//    studentRDD.map((line:String)=>{
//      accumulator.add(1)
//    }).collect()
//    println(s"accumulator的值为：${accumulator.value}")

//    val value: RDD[RDD[(String, String)]] = studentRDD.map((stuLine: String) => {
//      scoreRDD.map((scoreLine: String) => {
//        val strings: Array[String] = scoreLine.split(",")
//        val strings1: Array[String] = stuLine.split(",")
//        val str1: String = strings.mkString("|")
//        val str2: String = strings1.mkString("|")
//        (str1, str2)
//      })
//    })

//    value.foreach(println)

//    val value: RDD[RDD[String]] = studentRDD.map((stuLine: String) => {
//      val scoreRDD: RDD[String] = context.textFile("spark/data/score.txt")
//      scoreRDD
//    })
//    value.foreach(println)




  }
}

~~~

### 5、广播变量

- 可以将Driver端的一个变量广播到Executor，可以减少变量副本数，一般用于mapjoin  spark sql map join的底层就是将小表广播出去

~~~Java
object Demo21Broadcast {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
    conf.setMaster("local")
    conf.setAppName("广播变量演示")
    val context = new SparkContext(conf)
    //====================================================
    //使用Scala的方式读取学生数据文件，将其转换以学号作为键的map集合，属于在Driver端的一个变量
    val studentsMap: Map[String, String] = Source.fromFile("spark/data/students.csv")
      .getLines()
      .toList
      .map((line: String) => {
        val infos: Array[String] = line.split(",")
        val stuInfo: String = infos.mkString(",")
        (infos(0), stuInfo)
      }).toMap

    val scoresRDD: RDD[String] = context.textFile("spark/data/score.txt")

    /**
     * 将studentsMap变成一个广播变量，让每一个将来需要执行关联的Executor中都有一份studentsMap数据
     * 避免了每次Task任务拉取都要附带一个副本，拉取的速度变快了，执行速度也就变快了
     *
     * 广播大变量
     */
    val studentsMapBroadcast: Broadcast[Map[String, String]] = context.broadcast(studentsMap)


    /**
     * 将Spark读取的分数RDD与外部变量学生Map集合进行关联
     * 循环遍历scoresRDD，将学号一样的学生信息关联起来
     */
//    val resMapRDD: RDD[(String, String)] = scoresRDD.map((score: String) => {
//      val id: String = score.split(",")(0)
//      //使用学号到学生map集合中获取学生信息
//      val studentInfo: String = studentsMap.getOrElse(id, "无学生信息")
//      (score, studentInfo)
//    })
//    resMapRDD.foreach(println)

    /**
     * 使用广播变量进行关联
     */
    val resMapRDD: RDD[(String, String)] = scoresRDD.map((score: String) => {
      val id: String = score.split(",")(0)
      val stuMap: Map[String, String] = studentsMapBroadcast.value
      //使用学号到学生map集合中获取学生信息
      val studentInfo: String = stuMap.getOrElse(id, "无学生信息")
      (score, studentInfo)
    })
    resMapRDD.foreach(println)



  }
}
~~~

### 6、RDD持久化/缓存

- RDD的缓存有两种方式
  - cache：默认情况是将数据缓存到内存中，cache的底层是调用persist方法
  - persist：
- 缓存级别
  - MEMORY_ONLY：
  - MEMORY_AND_DISK_SER
- 当重复使用同一个RDD时，可以使用RDD缓存

~~~Java
object Demo16Catch {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
    conf.setMaster("local")
    conf.setAppName("Action算子演示")
    val context = new SparkContext(conf)
    //设置checkpoint路径，将来对应的是HDFS上的路径
    context.setCheckpointDir("spark/data/checkpoint")
    //====================================================

    val linesRDD: RDD[String] = context.textFile("spark/data/students.csv")

    val splitRDD: RDD[Array[String]] = linesRDD.map(_.split(","))

    //处理数据
    val studentsRDD: RDD[(String, String, String, String, String)] = splitRDD.map {
      case Array(id: String, name: String, age: String, gender: String, clazz: String) =>
        (id, name, age, gender, clazz)
    }

    //对studentsRDD进行缓存
    /**
     * 特点带来的问题：既然叫做缓存，所以在程序运行过程中无论是只放内存还是磁盘内存一起使用，一旦程序结束，缓存数据全部丢失。
     *
     * spark针对上面的场景提供了一个解决方案：可以将RDD运行时的数据永久持久化在HDFS上，这个方案叫做checkpoint,需要在spark环境中设置checkpoint的路径
     */
    //    studentsRDD.cache() //默认情况下，是将数据缓存在内存中
//    studentsRDD.persist(StorageLevel.MEMORY_AND_DISK)
      studentsRDD.checkpoint()


    //统计每个班级的人数
    val clazzKVRDD: RDD[(String, Int)] = studentsRDD.map {
      case (_, _, _, _, clazz: String) => (clazz, 1)
    }
    val clazzNumRDD: RDD[(String, Int)] = clazzKVRDD.reduceByKey(_ + _)
    clazzNumRDD.saveAsTextFile("spark/data/clazz_num")

    //统计性别的人数
    val genderKVRDD: RDD[(String, Int)] = studentsRDD.map {
      case (_, _, _, gender: String, _) => (gender, 1)
    }
    val genderNumRDD: RDD[(String, Int)] = genderKVRDD.reduceByKey(_ + _)
    genderNumRDD.saveAsTextFile("spark/data/gender_num")
//
//    while (true){
//
//    }


  }
}
~~~

**总结：**

- RDD 持久化/缓存的目的是为了提高后续操作的速度
- 缓存的级别有很多，默认只存在内存中,开发中使用 memory_and_disk
- 只有执行 action 操作的时候才会真正将 RDD 数据进行持久化/缓存
- 实际开发中如果某一个 RDD 后续会被频繁的使用，可以将该 RDD 进行持久化/缓存

### 7、RDD容错机制Checkpoint

- 持久化/缓存可以把数据放在内存中，虽然是快速的，但是也是最不可靠的；也可以把数据放在磁盘上，也不是完全可靠的！例如磁盘会损坏等。

**问题解决**：

- `Checkpoint` 的产生就是为了更加可靠的数据持久化，在`Checkpoint`的时候一般把数据放在在 `HDFS` 上，这就天然的借助了 `HDFS` 天生的高容错、高可靠来实现数据最大程度上的安全，实现了 `RDD` 的容错和高可用。

~~~Scala
SparkContext.setCheckpointDir("目录") //HDFS的目录
RDD.checkpoint
~~~

~~~Scala
object Demo12Checkpoint {
  def main(args: Array[String]): Unit = {
    val sparkSession: SparkSession = SparkSession.builder()
      .master("local")
      .appName("checkpoint演示")
      .config("spark.sql.shuffle.partitions", 1)
      .getOrCreate()

    val sparkContext: SparkContext = sparkSession.sparkContext

    sparkContext.setCheckpointDir("hdfs://master:9000/bigdata29/data")

    val lineRDD: RDD[String] = sparkContext.textFile("spark/data/students.csv")

    val clazzRDD: RDD[(String, Int)] = lineRDD.map(line => {
      val split: Array[String] = line.split(",")
      val clazz: String = split(4)
      (clazz, 1)
    })
    val countRDD: RDD[(String, Int)] = clazzRDD.reduceByKey(_ + _)

    countRDD.checkpoint()
    countRDD.foreach(println)
  }

}
~~~

**总结：**

- 开发中如何保证数据的安全性性及读取效率：可以对频繁使用且重要的数据，先做缓存/持久化，再做 checkpint 操作。

**持久化和 Checkpoint 的区别：**

- 位置：Persist 和 Cache 只能保存在本地的磁盘和内存中(或者堆外内存–实验中) Checkpoint 可以保存数据到 HDFS 这类可靠的存储上。
- 生命周期：Cache 和 Persist 的 RDD 会在程序结束后会被清除或者手动调用 unpersist 方法 Checkpoint 的 RDD 在程序结束后依然存在，不会被删除。

### 8、DAG的生成以及Stage的划分

> **DAG(`Directed Acyclic Graph` 有向无环图)**：指的是数据转换执行的过程，有方向，无闭环(其实就是 RDD 执行的流程)；

**DAG 的边界**:

- **开始**：通过 SparkContext 创建的 RDD；
- **结束**：触发 Action，一旦触发 Action 就形成了一个完整的 DAG。

![image-20240605194740156](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202406051950520.png)

从上图可以看出：

- 一个 Spark 程序可以有多个 DAG(有几个 Action，就有几个 DAG，上图最后只有一个 Action（图中未表现）,那么就是一个 DAG);
- 一个 DAG 可以有多个 Stage(根据宽依赖/shuffle 进行划分)；
- 同一个 Stage 可以有多个 Task 并行执行(task 数=分区数，如上图，Stage1 中有三个分区 P1、P2、P3，对应的也有三个 Task)；
- 可以看到这个 DAG 中只 reduceByKey 操作是一个宽依赖，Spark 内核会以此为边界将其前后划分成不同的 Stage；
- 在图中 Stage1 中，从 textFile 到 flatMap 到 map 都是窄依赖，这几步操作可以形成一个流水线操作，通过 flatMap 操作生成的 partition 可以不用等待整个 RDD 计算结束，而是继续进行 map 操作，这样大大提高了计算的效率。

### 9、资源调度和任务调度

![](https://gitee.com/Yinlei_com/image29_demo1/raw/master/img/202406052002686.png)

- **资源调度**

> yarn-client为例
> 1、在本地启动Driver
> 2、Driver向ResourceManager申请资源
> 3、RM分配一个节点启动ApplicationMaster
> 4、AM向RM申请启动Executor
> 5、RM随机分配一批节点启动Executor
> 6、Executor反向注册给Driver

- **任务调度**

> 1、当遇到一个Action算子时，开始任务调度
> 2、构建DAG有向无环图
> 3、将DAG传递给DAGScheduler
> 4、DAGScheduler根据宽窄依赖切分Stage
> 5、DAGScheduler将Stage以taskSet的形式发送给TaskScheduler
> 6、TaskScheduler将task发送到Executor中执行，会尽量将task发送到数据所在的节点执行

## 四、Spark SQL

### 1、spark sql的API

> 通过统计单词的数量引入

~~~Scala
def main(args: Array[String]): Unit = {
    /**
     * 在新版本的spark中，如果想要编写spark sql的话，需要使用新的spark入口类：SparkSession
     */
    val sparkSession: SparkSession = SparkSession.builder()
      .master("local")
      .appName("wc spark sql")
      .getOrCreate()

    /**
     * spark sql和spark core的核心数据类型不太一样
     *
     * 1、读取数据构建一个DataFrame,相当于一张表
     */
    val linesDF: DataFrame = sparkSession.read
      .format("csv") //指定读取数据的格式
      .schema("line STRING") //指定列的名和列的类型，多个列之间使用,分割
      .option("sep", "\n") //指定分割符，csv格式读取默认是英文逗号
      .load("spark/data/words.txt") // 指定要读取数据的位置，可以使用相对路径

//    println(linesDF)
//    linesDF.show() //查看DF中的数据内容（表内容）
//    linesDF.printSchema() //查看DF表结构

    /**
     * 2、DF本身是无法直接在上面写sql的，需要将DF注册成一个视图，才可以写sql数据分析
     */
    linesDF.createOrReplaceTempView("lines") // 起一个表名，后面的sql语句可以做查询分析

    /**
     * 3、可以编写sql语句 （统计单词的数量）
     * spark sql是完全兼容hive sql
     */
    val resDF: DataFrame = sparkSession.sql(
      """
        |select
        |t1.word as word,
        |count(1) as counts
        |from
        |(select
        | explode(split(line,'\\|')) as word from lines) t1
        | group by t1.word
        |""".stripMargin)

    /**
     * 4、将计算的结果DF保存到HDFS上
     */
    val resDS: Dataset[Row] = resDF.repartition(1)
    resDS.write
      .format("csv") //指定输出数据文件格式
      .option("sep","\t") // 指定列之间的分隔符
      .mode(SaveMode.Overwrite) // 使用SaveMode枚举类，设置为覆盖写
      .save("spark/data/sqlout1") // 指定输出的文件夹

  }
~~~

### 2、DSLAPI

> ```scala
> /**
>  * DSL: 类SQL语法 api  介于代码和纯sql之间的一种api
>  *
>  * spark在DSL语法api中，将纯sql中的函数都使用了隐式转换变成一个scala中的函数
>  * 如果想要在DSL语法中使用这些函数，需要导入隐式转换
>  *
>  */
> //导入Spark sql中所有的sql隐式转换函数
> import org.apache.spark.sql.functions._
> //导入另一个隐式转换，后面可以直接使用$函数引用字段进行处理
> import sparkSession.implicits._
> ```

~~~Scala
def main(args: Array[String]): Unit = {

    /**
     * 在新版本的spark中，如果想要编写spark sql的话，需要使用新的spark入口类：SparkSession
     */
    val sparkSession: SparkSession = SparkSession.builder()
      .master("local")
      .appName("dsl语法api演示")
      .config("spark.sql.shuffle.partitions",1) //默认分区的数量是200个
      .getOrCreate()

    //导入Spark sql中所有的sql隐式转换函数
    import org.apache.spark.sql.functions._
    //导入另一个隐式转换，后面可以直接使用$函数引用字段进行处理
    import sparkSession.implicits._

    /**
     * DSL api
     */
    //新版本的读取方式，读取一个json数据,不需要手动指定列名
    //    val stuDF1: DataFrame = sparkSession.read
    //      .json("spark/data/students.json")

    //以前版本读取方式，灵活度要高一些
    val stuDF: DataFrame = sparkSession.read
      .format("json")
      .load("spark/data/students.json")

    //    stuDF2.show(100, truncate = false) //传入展示总条数,并完全显示数据内容
    /**
     * select 函数：选择数据【字段】，和纯sql语句中select意思基本是一样，在数据的前提上选择要留下的列
     *
     */
    //根据字段的名字选择要查询的字段
    //    stuDF.select("id","name","age").show(1)
    //    //根据字段的名字选择要查询的字段,selectExpr 可以传入表达式字符串形式
    //    stuDF.selectExpr("id","name","age","age + 1 as new_age").show()
    //使用隐式转换中的$函数将字段变成一个对象
    //    stuDF.select($"id",$"name",$"age").show(10)
    //使用对象做处理
    //    stuDF.select($"id",$"name",$"age" + 1  as "new_age").show(10)
    //可以在select中使用sql的函数
    //下面的操作等同于sql：select id,name,age+1 as new_age,substring(clazz,0,2) as km from lines;
    //    stuDF.select($"id",$"name",$"age" + 1  as "new_age",substring($"clazz",0,2) as "km").show(10)

    /**
     * where 函数：过滤数据
     */
    //直接将sql中where语句以字符串的形式传参
    //    stuDF.where("gender='女' and age=23").show()
    //使用$列对象的形式过滤
    // =!= 不等于
    // === 等于
    //    stuDF.where($"gender" === "女" and $"age" === 23).show()
    //    stuDF.where($"gender" =!= "男" and $"age" === 23).show()
    //过滤文科的学生
    //    stuDF.where(substring($"clazz", 0, 2) === "文科").show()

    /**
     * groupBy 分组函数
     * agg 聚合函数
     * 分组聚合要在一起使用
     * 分组聚合之后的结果DF中只会包含分组字段和聚合字段
     * select中无法出现不是分组的字段
     */
    //根据班级分组，求每个班级的人数和平均年龄
    //    stuDF.groupBy($"clazz")
    //      .agg(count($"clazz") as "number",round(avg($"age"),2) as "avg_age").show()

    /**
     * orderBy: 排序
     */
    //    stuDF.groupBy($"clazz")
    //          .agg(count($"clazz") as "number")
    //          .orderBy($"number").show()

    /**
     * join: 表关联
     */
    val scoreDF: DataFrame = sparkSession.read
      .format("csv")
      .option("sep", ",")
      .schema("id STRING,subject_id STRING,score INT")
      .load("spark/data/score.txt")
    //    scoreDF.show()

    //关联场景1：所关联的字段名字不一样的时候
    //    stuDF.join(scoreDF, $"id" === $"sid", "inner").show()
    //关联场景2：所关联的字段名字一样的时候
    //    stuDF.join(scoreDF,"id").show()

    /**
     * 开窗函数
     * 统计每个班级总分前3的学生
     *
     * 开窗不会改变总条数的，会以新增一列的形式加上开窗的结果
     * withColumn 新增一列
     */
    val joinStuAndScoreWithIDDF: DataFrame = stuDF.join(scoreDF, "id")
    joinStuAndScoreWithIDDF.groupBy($"id", $"clazz") //根据学号和班级一起分组
      .agg(sum($"score") as "sumScore") //计算总分
      .withColumn("rn", row_number() over Window.partitionBy($"clazz").orderBy($"sumScore".desc))
      //.select($"id", $"clazz", $"sumScore", row_number() over Window.partitionBy($"clazz").orderBy($"sumScore".desc) as "rn")
      .where($"rn" <= 3)
      .show()


  }
~~~

### 3、Data SourceAPI

~~~scala
def main(args: Array[String]): Unit = {
    /**
     * 在新版本的spark中，如果想要编写spark sql的话，需要使用新的spark入口类：SparkSession
     */
    val sparkSession: SparkSession = SparkSession.builder()
      .master("local")
      .appName("dsl语法api演示")
      .config("spark.sql.shuffle.partitions",1) //默认分区的数量是200个
      .getOrCreate()

    //导入Spark sql中所有的sql隐式转换函数
    import org.apache.spark.sql.functions._
    //导入另一个隐式转换，后面可以直接使用$函数引用字段进行处理
    import sparkSession.implicits._

    /**
     * 读取csv格式的数据，默认是以英文逗号分割的
     *
     */
//    val stuCsvDF: DataFrame = sparkSession.read
//      .format("csv")
//      .schema("id STRING,name STRING,age INT,gender STRING,clazz STRING")
//      .option("sep", ",")
//      .load("spark/data/students.csv")
//    //求每个班级的人数，保存到文件中
//    stuCsvDF.groupBy($"clazz")
//      .agg(count($"clazz") as "number")
//      .write
//      .format("csv")
//      .option("sep",",")
//      .mode(SaveMode.Overwrite)
//      .save("spark/data/souceout1")


    /**
     * 读取json数据格式，因为json数据有键值对。会自动地将键作为列名，值作为列值，不需要手动设置表结构
     */
    val stuJsonDF: DataFrame = sparkSession.read
      .format("json")
      .load("spark/data/students2.json")
//    //统计每个性别的人数
//    stuJsonDF.groupBy($"gender")
//      .agg(count($"gender") as "number")
//      .write
//      .format("json")
//      .mode(SaveMode.Overwrite)
//      .save("spark/data/jsonout1")

    /**
     *  parquet
     *  压缩的比例是由【信息熵】来决定的
     */
//    stuJsonDF.write
//      .format("parquet")
//      .mode(SaveMode.Overwrite)
//      .save("spark/data/parquetout2")
    //读取parquet格式文件的时候，也是不需要手动指定表结构
//    val stuParquetDF: DataFrame = sparkSession.read
//      .format("parquet")
//      .load("spark/data/parquetout1/part-00000-c5917bb6-172b-49bd-a90c-90b7f09b69d6-c000.snappy.parquet")
//    stuParquetDF.show()

    /**
     * 读取数据库中的数据，mysql
     *
     */
    val jdDF: DataFrame = sparkSession.read
      .format("jdbc")
      .option("url", "jdbc:mysql://192.168.220.100:3306")
      .option("dbtable", "bigdata29.jd_goods")
      .option("user", "root")
      .option("password", "123456")
      .load()

    jdDF.show(10,truncate = false)

  }
~~~

### 4、RDD转DF

```scala
def main(args: Array[String]): Unit = {
  val sparkSession: SparkSession = SparkSession.builder()
    .master("local")
    .appName("RDD To DF")
    .config("spark.sql.shuffle.partitions", 1)
    .getOrCreate()
  import org.apache.spark.sql.functions._
  import sparkSession.implicits._
  val studentRDD: RDD[String] = sparkSession.sparkContext.textFile("spark/data/students.csv")
  val stuRDD: RDD[(String, String, String, String, String)] = studentRDD.map((s: String) => s.split(","))
    .map {
      case Array(id: String, name: String, age: String, gender: String, clazz: String) =>
        (id, name, age, gender, clazz)
    }
  /**
   * RDD转DF
   */
  val studentDF: DataFrame = stuRDD.toDF("id", "name", "age", "gender", "clazz")
  studentDF.groupBy("clazz")
    .agg(count("id"))
    .show
  /**
   * 在Row的数据类型中 所有整数类型统一为Long  小数类型统一为Double
   * 转RDD
   */
  val studentsRDD: RDD[Row] = studentDF.rdd
  val clazzRDD: RDD[(String, String)] = studentsRDD.map {
    case Row(id: String, _, _, _, clazz: String) =>
      (id, clazz)
  }
  val clazzCountRDD: RDD[(String, Int)] = clazzRDD.groupBy(_._2)
    .map((kv: (String, Iterable[(String, String)])) => (kv._1, kv._2.toList.size))
  clazzCountRDD.foreach(println)

}
```

### 5、window

~~~Scala
/**
 * 开窗函数
 * 聚合开窗函数：sum  count avg min max
 * 排序开窗函数：row_number rank desen_rank lag（向上取） lead（向后取）
 */
object Demo6Window {
  def main(args: Array[String]): Unit = {
    //创建SparkSession对象
    /**
     * 在新版本的spark中，如果想要编写spark sql的话，需要使用新的spark入口类：SparkSession
     */
    val sparkSession: SparkSession = SparkSession.builder()
      .master("local")
      .appName("开窗函数DSL API演示")
      .config("spark.sql.shuffle.partitions", 1) //默认分区的数量是200个
      .getOrCreate()

    //导入Spark sql中所有的sql隐式转换函数
    import org.apache.spark.sql.functions._
    //导入另一个隐式转换，后面可以直接使用$函数引用字段进行处理,如果需要做RDD和DF之间的转换
    import sparkSession.implicits._

    //学生表
    val studentsDF: DataFrame = sparkSession.read
      .format("csv")
      .option("sep", ",")
      .schema("id STRING,name STRING,age INT,gender STRING,clazz STRING")
      .load("spark/data/students.csv")

    //成绩表
    val scoresDF: DataFrame = sparkSession.read
      .format("csv")
      .option("sep", ",")
      .schema("sid STRING,subject_id STRING,score INT")
      .load("spark/data/score.txt")

    //科目表
    val subjectDF: DataFrame = sparkSession.read
      .format("csv")
      .option("sep", ",")
      .schema("subject_id STRING,subject_name STRING,subject_sum_score INT")
      .load("spark/data/subject.csv")

    //将学生数据与成绩数据进行关联
    val joinDF: DataFrame = studentsDF.join(scoresDF, $"id" === $"sid")
    //    joinDF.show(10)

    /**
     * 1、统计总分年级排名前十学生各科的分数
     *
     * 未排序之前，是将开窗中所有数据一起聚合得到一个结果
     * 若排序了，依次从上到下聚合得到一个结果
     *
     */
    joinDF
      // sum(score) over(partition by id ) as sumScore
      .withColumn("sumScore", sum($"score") over Window.partitionBy($"id"))
      .orderBy($"sumScore".desc)
      .limit(60)
    //.show(60)

    /**
     * 3、统计每科都及格的学生
     */
    scoresDF
      .join(subjectDF, "subject_id")
      .where($"score" >= $"subject_sum_score" * 0.6)
      //统计学生及格的科目数
      .withColumn("jiGeCounts", count($"sid") over Window.partitionBy($"sid"))
      .where($"jiGeCounts" === 6)
    //      .show(100)

    /**
     * 2、统计总分大于年级平均分的学生
     */
    joinDF
      //计算每个学生的总分，新增一列
      .withColumn("sumScore", sum($"score") over Window.partitionBy($"id"))
      .withColumn("avgScore", avg($"sumScore") over Window.partitionBy(substring($"clazz", 0, 2)))
      .where($"sumScore" > $"avgScore")
    //      .show(200)

    /**
     * 统计每个班级的每个名次之间的分数差
     *
     */
    joinDF
      .groupBy($"id", $"clazz")
      .agg(sum($"score") as "sumScore")
      //开窗，班级开窗，总分降序排序，排个名次
      .withColumn("rn", row_number() over Window.partitionBy($"clazz").orderBy($"sumScore".desc))
      //开窗，取出前一名的总分
      .withColumn("front_score", lag($"sumScore",1,750) over Window.partitionBy($"clazz").orderBy($"sumScore".desc))
      .withColumn("cha",$"front_score" - $"sumScore")
      .show(100)


  }
}
~~~

### 6、submit yarn

~~~shell
spark-submit --master yarn --deploy-mode client --class com.shujia.sql.Demo8SubmitYarn --conf spark.sql.shuffle.partitions=1 spark-1.0.jar 
~~~



~~~Scala
  def main(args: Array[String]): Unit = {
    val sparkSession: SparkSession = SparkSession.builder()
      //提交到yarn上运行不需要设置master参数
//      .master("local")
      .config("spark.sql.shuffle.partitions", 1) //优先级：代码的参数 > 命令行提交的参数 > 配置文件
      .appName("spark sql提交到Yarn演示")
      .getOrCreate()
    //导入Spark sql中所有的sql隐式转换函数
    import org.apache.spark.sql.functions._
    //导入另一个隐式转换，后面可以直接使用$函数引用字段进行处理
    import sparkSession.implicits._
    val studentsDF: DataFrame = sparkSession.read
      .format("csv")
      .schema("id STRING,name STRING,age Int,gender STRING,clazz STRING")
      .option("sep", ",")
      .load("/bigdata29/data/students.txt")
    val genderDF: DataFrame = studentsDF.groupBy($"clazz")
      .agg(count($"id") as "num")

    genderDF.write
      .format("csv")
      .option("sep",",")
      .mode(SaveMode.Overwrite)
      .save("/bigdata29/sparkOut/out3")
    //spark-submit --master yarn --deploy-mode client --class com.shujia.test.Demo1SubmitYarn --conf spark.sql.shuffle.partitions=1 spark-1.0.jar
  }

~~~

### 7、行列转换

~~~Scala
/**
 * 1、行列转换
 *
 * 表1
 * 姓名,科目,分数
 * name,item,score
 * 张三,数学,33
 * 张三,英语,77
 * 李四,数学,66
 * 李四,英语,78
 *
 *
 * 表2
 * 姓名,数学,英语
 * name,math,english
 * 张三,数学,33,英语,77
 * 李四,数学,66,英语78
 *
 * 1、将表1转化成表2
 * 2、将表2转化成表1
 */
object Demo10RowAndColumn {
  def main(args: Array[String]): Unit = {
    val sparkSession: SparkSession = SparkSession.builder()
      .master("local")
      .appName("行列转换演示")
      .config("spark.sql.shuffle.partitions", 1)
      .getOrCreate()


    import org.apache.spark.sql.functions._
    import sparkSession.implicits._
    val tb1DF: DataFrame = sparkSession.read
      .format("csv")
      .schema("name STRING,item STRING ,score STRING ")
      .option("sep", ",")
      .load("spark/data/tb1.txt")
    tb1DF.groupBy($"name")
      .agg(sum(when($"item"==="数学",$"score").otherwise(0)) as "math",sum(when($"item"==="英语",$"score").otherwise(0)) as "english")
      .write
      .format("csv")
      .option("sep",",")
      .mode(SaveMode.Overwrite)
      .save("spark/data/tb1")

    val tb2DF: DataFrame = sparkSession.read
      .format("csv")
      .schema("name STRING,math STRING,english STRING")
      .option("sep", ",")
      .load("spark/data/tb2.txt")
    val m: Column = map(
      expr("'数学'"), $"math",
      expr("'英语'"), $"english"

    )
    tb2DF.select($"name",explode(m) as Array("item","score"))
      .show()
  }

}
~~~

### 8、UDF(自定义函数)

- DSL和SQL中的自定义函数

~~~scala
object Demo11UDF {
  def main(args: Array[String]): Unit = {
    val sparkSession: SparkSession = SparkSession.builder()
      .master("local")
      .appName("udf函数演示")
      .config("spark.sql.shuffle.partitions", 1)
      .getOrCreate()
    import sparkSession.implicits._
    import org.apache.spark.sql.functions._
    /**
     * 1、在使用DSL的时候使用自定义函数
     */
    val studentsDF: DataFrame = sparkSession.read
      .format("csv")
      .option("sep", ",")
      .schema("id STRING,name STRING,age INT,gender STRING,clazz STRING")
      .load("spark/data/students.csv")

    //    studentsDF.show()

    //编写自定义函数
    //udf中编写的是scala代码
    val shujia_fun1: UserDefinedFunction = udf((str: String) => "数加：" + str)

//    studentsDF.select(shujia_fun1($"clazz")).show()

    /**
     * 1、使用SQL语句中使用自定函数
     */
    studentsDF.createOrReplaceTempView("students")
    //将自定义的函数变量注册成一个函数
    sparkSession.udf.register("shujia_str",shujia_fun1)
    sparkSession.sql(
      """
        |select clazz,shujia_str(clazz) as new_clazz from students
        |""".stripMargin).show()


  }
}
~~~

- spark-sql 客户端的自定义函数

> /**
>
> 1、将类打包，放在linux中spark的jars目录下
>
> 2、进入spark-sql的客户端
>
> 3、使用上传的jar中的udf类来创建一个函数
>
> create function shujia_str as 'com.shujia.sql.Demo12ShuJiaStr';
> */

~~~scala
class Demo12ShuJiaStr extends UDF {
  def evaluate(str: String): String = {
    "shujia: " + str
  }
}

/**
 * 1、将类打包，放在linux中spark的jars目录下
 * 2、进入spark-sql的客户端
 * 3、使用上传的jar中的udf类来创建一个函数
 * create function shujia_str as 'com.shujia.sql.Demo12ShuJiaStr';
 */
~~~

### 9、mapjoin(广播变量)

> 基于广播变量（Broadcast Variable）实现。广播变量允许将小表的数据广播到集群中的每个节点，使得连接操作可以在每个节点上本地完成，而不需要在节点之间传输大量的数据。
>
> 在map端进行表关联，不会产生shuffle

- ####  自动广播（默认）

  > Spark SQL会自动广播小表，前提是小表的大小在配置的阈值以内（默认为10MB）。这个阈值可以通过配置参数`spark.sql.autoBroadcastJoinThreshold`来调整：

  ~~~scala
  spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 10 * 1024 * 1024) // 设置阈值为10MB
  ~~~

- **显示广播**

>  你可以显式地将某个表标记为广播变量，确保使用Broadcast Join。使用`broadcast`函数来实现:

  ~~~scala
import org.apache.spark.sql.functions.broadcast

val smallTable = spark.table("small_table")
val largeTable = spark.table("large_table")

val joinedTable = largeTable.join(broadcast(smallTable), "key")
  ~~~

**配置参数**

以下是一些与Broadcast Join相关的重要配置参数：

> `spark.sql.autoBroadcastJoinThreshold`：自动广播表大小的阈值（默认10MB）。
>
> `spark.sql.broadcastTimeout`：广播变量的超时时间（默认300秒）。
>
> `spark.sql.broadcast.hash.join.enabled`：是否启用Broadcast Hash Join（默认启用）。

- spark-sql端

~~~SQL
select /*+broadcast(a)  */ * from 
student as a
join 
score as b
on
a.id=b.student_id
~~~

## 五、Spark Streaming





# Spark Sql

### spark-sql  写代码方式

#### 1、idea里面将代码编写好打包上传到集群中运行，上线使用

```
--conf spark.sql.shuffle.partitions=1 -- 设置spark sqlshuffle之后分区数据马，和代码里面设置是一样的，代码中优先级高
spark-submit提交
spark-submit --master yarn-client --class com.shujia.sql.Demo8SubmitYarn --conf spark.sql.shuffle.partitions=1 spark-1.0.jar 

//新版本spark提交yarn的命令
spark-submit --master yarn --deploy-mode client --class com.shujia.sql.Demo8SubmitYarn --conf spark.sql.shuffle.partitions=1 spark-1.0.jar 

```

### 2、spark shell  (repl) 里面使用sqlContext     测试使用，简单任务使用

```
spark-shell --master yarn-client
不能使用yarn-cluster Driver必须再本地启动
```

### 3、spark-sql    spark-sql --master yarn --deploy-mode client   不能使用yarn-cluster    和hive的命令行一样，直接写sql

````
在spark-sql时完全兼容hive sql的
spark-sql底层使用的时spark进行计算的
hive 底层使用的是MR进行计算的
````

### spark sql整合hive

> 在spark sql中使用hive的元数据
>
> spark sql是使用spark进行计算的，hive使用MR进行计算的

#### 1、在hive的hive-site.xml修改一行配置，增加了这一行配置之后，以后在使用hive之前都需要先启动元数据服务

cd /usr/local/soft/hive-1.2.1/conf/

```xml
<property>
<name>hive.metastore.uris</name>
<value>thrift://master:9083</value>
</property>


       <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-hive_2.12</artifactId>
        </dependency>

        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-client</artifactId>
        </dependency>

        <dependency>
            <groupId>org.apache.hive</groupId>
            <artifactId>hive-exec</artifactId>
        </dependency>


<dependency>
    <groupId>com.fasterxml.jackson.core</groupId>
    <artifactId>jackson-databind</artifactId>
    <version>2.10.0</version>
</dependency>
 
<dependency>
    <groupId>com.fasterxml.jackson.core</groupId>
    <artifactId>jackson-core</artifactId>
    <version>2.10.0</version>
```

#### 2、启动hive元数据服务, 将hvie的元数据暴露给第三方使用

````shell
nohup  hive --service metastore >> metastore.log 2>&1 &
````

#### 3、将hive-site.xml  复制到spark conf目录下

```
cp hive-site.xml /usr/local/soft/spark-2.4.5/conf/
```

#### 4、 将mysql 驱动包复制到spark jars目录下

````shell
cd /usr/local/soft/hive-1.2.1/lib
cp mysql-connector-java-5.1.49.jar /usr/local/soft/spark-2.4.5/jars/

````

##### 5、整合好之后在spark-sql 里面就可以使用hive的表了

```shell
# 模式是local模式
spark-sql -conf  spark.sql.shuffle.partitions=2
# 使用yarn-client模式
spark-sql --master yarn-client  --conf  spark.sql.shuffle.partitions=1

#在spark-sql中设置运行参数
set spark.sql.shuffle.partitions=2;
```

#### spark-sql -e

````sql
-- 执行一条sql语句，执行完，自动退出
spark-sql -e "select * from student"
````

#### spark-sql -f

```sql
vim a.sql
select * from student
-- 执行一个sql文件
spark-sql -f a.sql
```



### 当spark-sql 和hive整合好之后再代码中也可以直接使用hive的表

```scala
val spark: SparkSession = SparkSession
.builder()
.appName("onhive")
.enableHiveSupport() //开启hive的元数据支持，在代码中读取hive的元数据
.getOrCreate()

//读取hie的表
val studentDF = spark.talbe("studnet")

//写好的代码不能再本地运行， 需要打包上传到集群运行
```



### spark sql和hvie的建表语句一样

````sql
create external table students
(
id  string,
name string,
age int,
gender string,
clazz string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
location '/bigdata29/spark_in/data/student';

create table score
(
student_id  string,
cource_id string,
sco int
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
STORED AS textfile
location '/data/score/';
````





### 禁用集群spark日志

```
cd /usr/local/soft/spark-2.4.5/conf
mv log4j.properties.template log4j.properties
vim log4j.properties
修改配置
log4j.rootCategory=ERROR, console
```



### spark sql和hive区别

#### 1、spark sql缓存

```sql
-- 进入spark sql命令行
spark-sql
-- 可以通过一个网址访问spark任务
http://master:4040
-- 设置并行度
set spark.sql.shuffle.partitions=1;

-- 再spark-sql中对同一个表进行多次查询的时候可以将表缓存起来
cache table student;
-- 删除缓存
uncache table student;

-- 再代码中也可以缓存DF
 studentDF.persist(StorageLevel.MEMORY_ONLY)
```

#### 2、spark sql mapjoin    --- 广播变量

##### Reduce Join

````sql
select * from 
student as a
join 
score as b
on
a.id=b.student_id
````

##### MapJoin

> 当一个大表关联小表的时候可以将小表加载到内存中进行关联---- 广播变量
>
> 在map端进行表关联，不会产生shuffle

 ```sql
select /*+broadcast(a)  */ * from 
student as a
join 
score as b
on
a.id=b.student_id
 ```

> /*+broadcast(a)  */   HINT:给sql加提示的语法



  

# 实验1-1：Spark从陌生到初识

## 实验概述

在学习Spark之前，先了解Spark的特点，了解Spark和之前我们学习的Hadoop之前的异同点，基于此创建我们的一个Spark WordCoCount程序，对深入了解Spark做好前期准备。

## 实验环境

- AtStudy 实训平台
- Spark3

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16776607300309228.png)

## 实验目标

学习完成本实验后，您将能够

- 理解 Spark 的特点和作用
- Spark快速上手
- 掌握Spark的运行环境
- 掌握Spark程序的编写

## 实验任务

### 任务一、Spark概述

#### **【任务目标】**

了解 Spark 的历史和产生原因, 从而浅显的理解 Spark 的作用

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20210824110318276.png?fileid=3270835009071807560)

视频-1、Spark概述

##### 1.1 Spark是什么

Apache Spark 是一个快速的，多用途的集群计算系统，相对于 Hadoop MapReduce 将中间结果保存在磁盘中， Spark 使用了内存保存中间结果，能在数据尚未写入硬盘时在内存中进行运算。

Spark 只是一个计算框架，不像 Hadoop 一样包含了分布式文件系统和完备的调度系统，如果要使用 Spark，需要搭载其它的文件系统和更成熟的调度系统。

![image-20230208133742067](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230208133742067.png)

> 官网：https://spark.apache.org/

**Spark的简史**

> 1、2009年，Spark诞生于伯克利大学AMPLab，属于伯克利大学的研究性项目；
>
> 2、2010 年，通过BSD 许可协议正式对外开源发布；
>
> 3、2012年，Spark第一篇论文发布，第一个正式版（Spark 0.6.0）发布；
>
> 4、2013年，成为了Aparch基金项目；发布Spark Streaming、Spark Mllib（机器学习）、Shark（Spark on Hadoop）；
>
> 5、2014 年，Spark 成为 Apache 的顶级项目；11 月, Spark的母公司Databricks团队使用Spark刷新数据排序世界记录；
>
> 6、2015年，推出DataFrame（大数据分析）；2015年至今，Spark在国内IT行业变得愈发火爆，大量的公司开始重点部署或者使用Spark来替代MapReduce、Hive、Storm等传统的大数据计算框架；

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20210823161648091.png)**为什么会有Spark?**

> Spark 产生之前, 已经有非常成熟的计算系统存在了, 例如 MapReduce, 这些计算系统提供了高层次的API, 把计算运行在集群中并提供容错能力, 从而实现分布式计算，那么为什么还需要Spark呢？

> 虽然这些框架提供了大量的对访问利用计算资源的抽象，但是它们缺少了对利用分布式内存的抽象，这些框架多个计算之间的数据复用就是将中间数据写到一个稳定的文件系统中(例如HDFS)，所以会产生数据的复制备份, 磁盘的I/O以及数据的序列化，所以这些框架在遇到需要在多个计算之间复用中间结果的操作时会非常的不高效。
>
> 而这类操作是非常常见的，例如迭代式计算、交互式数据挖掘、图计算等。
>
> 认识到这个问题后，学术界的 AMPLab 提出了一个新的模型，叫做 `RDDs`。
>
> `RDDs` 是一个可以容错且并行的数据结构，它可以让用户显式的将中间结果数据集保存在内中，并且通过控制数据集的分区来达到数据存放处理最优化。
>
> 同时 `RDDs` 也提供了丰富的 API 来操作数据集。
>
> 后来 RDDs 被 AMPLab 在一个叫做 Spark 的框架中提供并开源。

**总结：**

1. Spark 是Apache的开源框架
2. Spark 的母公司叫做 Databricks
3. Spark 是为了解决 MapReduce 等过去的计算系统无法在内存中保存中间结果的问题
4. Spark 的核心是 RDDs，RDDs 不仅是一种计算框架, 也是一种数据结构

##### 1.2 Spark的特点

（1）**速度快**

- Spark 的在内存时的运行速度是 Hadoop MapReduce 的100倍
- 基于硬盘的运算速度大概是 Hadoop MapReduce 的10倍
- Spark 实现了一种叫做 RDDs 的 DAG 执行引擎, 其数据缓存在内存中可以进行迭代处理

（2）**易用**

```
df = spark.read.json("logs.json")
df.where("age > 21") \
  .select("name.first") \
  .show()复制代码
```

- Spark 支持 Java、Scala、Python、R、SQL 等多种语言的API
- Spark 支持超过80个高级运算符使得用户非常轻易的构建并行计算程序
- Spark 可以使用基于 Scala、Python、R、SQL的 Shell 交互式查询

（3）**通用**

- Spark 提供一个完整的技术栈，包括 SQL执行、Dataset命令式API、机器学习库MLlib、图计算框架GraphX、流计算SparkStreaming
- 用户可以在同一个应用中同时使用这些工具，这一点是划时代的

（4）**兼容**

- Spark 可以运行在 Hadoop Yarn、Apache Mesos、Kubernets、Spark Standalone等集群中
- Spark 可以访问 HBase、HDFS、Hive、Cassandra 在内的多种数据库

##### 1.3 Spark核心模块

![image-20230208143030854](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230208143030854.png)

- Spark Core

Spark Core 中提供了 Spark 最基础与最核心的功能，Spark 其他的功能如：Spark SQL， Spark Streaming，GraphX, MLlib 都是在 Spark Core 的基础上进行扩展的

- Spark SQL

Spark SQL 是Spark 用来操作结构化数据的组件。通过 Spark SQL，用户可以使用 SQL

或者Apache Hive 版本的 SQL 方言（HQL）来查询数据。

- Spark Streaming

Spark Streaming 是 Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的API。

- Spark MLlib

MLlib 是 Spark 提供的一个机器学习算法库。MLlib 不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。

- Spark GraphX

GraphX 是 Spark 面向图计算提供的框架与算法库。

##### 1.4 Spark和Hadoop

###### 1.4.1 异同

|            | Hadoop                          | Spark                        |
| ---------- | ------------------------------- | ---------------------------- |
| **类型**   | 基础平台，包含计算，存储， 调度 | 分布式计算工具               |
| **场景**   | 大规模数据集上的批处理          | 迭代计算，交互式计算，流计算 |
| **延迟**   | 大                              | 小                           |
| **易用性** | API 较为底层，算法适应性差      | API 较为顶层，方便使用       |
| **价格**   | 对机器要求低，便宜              | 对内存有要求，相对较贵       |

###### 1.4.2 抉择

Hadoop 的 MR 框架和Spark 框架都是数据处理框架，那么我们在使用时如何选择呢？

- Hadoop MapReduce 由于其设计初衷并不是为了满足循环迭代式数据流处理，因此在多并行运行的数据可复用场景（如：机器学习、图挖掘算法、交互式数据挖掘算法）中存在诸多计算效率等问题。所以 Spark 应运而生，Spark 就是在传统的MapReduce 计算框架的基础上，利用其计算过程的优化，从而大大加快了数据分析、挖掘的运行和读写速度，并将计算单元缩小到更适合并行计算和重复使用的RDD 计算模型。
- 机器学习中 ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。MR 这种模式不太合适，即使多 MR 串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR 显然不擅长。而Spark 所基于的 scala 语言恰恰擅长函数的处理。
- Spark 是一个分布式数据快速分析项目。它的核心技术是弹性分布式数据集（Resilient Distributed Datasets），提供了比MapReduce 丰富的模型，可以快速在内存中对数据集进行多次迭代，来支持复杂的数据挖掘算法和图形计算算法。
- Spark 和Hadoop 的根本差异是多个作业之间的数据通信问题 : Spark 多个作业之间数据通信是基于内存，而 Hadoop 是基于磁盘。
- Spark Task 的启动时间快。Spark 采用 fork 线程的方式，而 Hadoop 采用创建新的进程的方式。
- Spark 只有在 shuffle 的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交互都要依赖于磁盘交互
- Spark 的缓存机制比 HDFS 的缓存机制高效。

经过上面的比较，我们可以看出在绝大多数的数据计算场景中，Spark 确实会比 MapReduce 更有优势。但是Spark 是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够导致 Job 执行失败，此时，MapReduce 其实是一个更好的选择，所以 Spark 并不能完全替代 MR。

### 任务二、Spark运行环境

#### **【任务目标】**

Spark 作为一个数据处理框架和计算引擎，被设计在各种环境中运行，这里主要任务是掌握不同环境下Spark 的运行。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20210824110318276.png?fileid=3270835009073997727)

视频-2、Spark运行环境

##### 2.1 Spark运行环境

![image-20230208150543903](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230208150543903.png)

Spark程序运行的环境分为：

- Local本地模式
- Standalone独立模式
- YARN运行模式
- 容器（K8S）模式等等

**各模式对比：**

| 模式       | 需启动的进程  | 所属   | 应用场景 |
| ---------- | ------------- | ------ | -------- |
| Local      | 无            | Spark  | 测试     |
| Standalone | Master Worker | Spark  | 独立部署 |
| YARN       | YARN HDFS     | Hadoop | 混合部署 |

##### 2.2 Local本地模式

所谓的 Local 模式，就是不需要其他任何节点资源就可以在本地执行 Spark 代码的环境。

> 提示：一般用于教学、调试、演示等

###### 2.2.1 文件准备

将 `/opt/software/spark-3.0.0-bin-hadoop3.2.tgz` 文件，进行解压缩，并改名。

![image-20230209112220217](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209112220217.png)

```
[root@cd62d9893d1f software]# tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module/

[root@cd62d9893d1f software]# mv spark-3.0.0-bin-hadoop3.2 spark-local复制代码
```

###### 2.2.2 启动Local环境

（1）进入解压缩后的路径，执行如下指令

```
./bin/spark-shell复制代码
```

![image-20230209131906996](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209131906996.png)

（2）启动成功后，可以输入网址进行 Web UI 监控页面访问

```
http://localhost:4040复制代码
```

![image-20230209132033527](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209132033527.png)

###### 2.2.3 命令行工具

在命令行中可以执行Spark相关代码，我们可以先了解一下：

例如：在当前`spark-local`目录下有个文件夹叫 data，里面有个`word.txt`文本文件，我们可以通过Spark代码统计单词的个数

![image-20230209132557679](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209132557679.png)

```
sc.textFile("data/word.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect复制代码
```

![image-20230209132825322](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209132825322.png)

###### 2.2.4 退出本地模式

按键Ctrl+C 或输入 Scala 指令

```
:quit复制代码
```

![image-20230209133028780](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209133028780.png)

###### 2.2.5 启动第一个 Spark 应用

（1）进入spark-local目录中

（2）运行 Spark 应用

```
bin/spark-submit \
    --class org.apache.spark.examples.SparkPi \
    --master local[*] \
    ./examples/jars/spark-examples_2.12-3.0.0.jar \ 
    10复制代码
```

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20210823165604377.png)**说明**

> - `--class`表示要执行程序的主类，此处可以更换为咱们自己写的应用程序
> - `--master local[*]` 部署模式，默认为本地模式，数字表示分配的虚拟CPU 核数量
> - `spark-examples_2.12-3.0.0.jar` 运行的应用类所在的 jar 包，实际使用时，可以设定为咱们自己打的 jar 包
> - 数字 `10` 表示程序的入口参数，用于设定当前应用的任务数量

（3）运行结果

![image-20230209133506410](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209133506410.png)

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20210823161648091.png)**蒙特卡洛算法概述**

> 刚才所运行的程序是 Spark 的一个示例程序，使用 Spark 编写了一个以蒙特卡洛算法来计算圆周率的任务
>
> ![image-20230209133708219](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209133708219.png)
>
> 1. 在一个正方形中, 内切出一个圆形
>
>    ![image-20230209133825257](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209133825257.png)
>
> 2. 随机向正方形内均匀投 n 个点, 其落入内切圆内的内外点的概率满足如下：
>
>    ![image-20230209133835991](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209133835991.png)
>
> 以上就是蒙特卡洛的大致理论，通过这个蒙特卡洛，便可以通过迭代循环投点的方式实现蒙特卡洛算法求圆周率

##### 2.3 Standalone模式

`local 本地模式`毕竟只是用来进行练习演示的，真实工作中还是要将应用提交到对应的集群中去执行，在这个集群中执行也就是我们所谓的独立部署（Standalone）模式。Spark 的 Standalone 模式体现了经典的master-slave 模式，一般需要多台机器形成集群模式，比如：

|       | 服务器1       | 服务器2 | 服务器3 |
| ----- | ------------- | ------- | ------- |
| Spark | Worker Master | Worker  | Worker  |

> 提示：
>
> - Standalone 集群中，将节点分为两个角色：Master 和 Slave, 而 Slave 就是 Worker，**`Master`** 负责总控、 调度、管理和协调 Worker、保留资源状况等；**`Slave`** 用于执行任务，定期向 Master汇报。

但是由于我们的实验环境限制，这里采用伪分布的方式模拟集群，就是在一台服务器上启动master-slave模式：

|       | 服务器1       |
| ----- | ------------- |
| Spark | Worker Master |

###### 2.3.1 解压缩文件

将 `/opt/software/spark-3.0.0-bin-hadoop3.2.tgz` 文件，进行解压缩，并改名。

```
[root@cd62d9893d1f software]# tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module/

[root@cd62d9893d1f software]# mv spark-3.0.0-bin-hadoop3.2 spark-standalone复制代码
```

![image-20230209135416793](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209135416793.png)

###### 2.3.2 修改配置文件

（1）进入解压缩后路径的 conf 目录，修改 slaves.template 文件名为 slaves

![image-20230209135607492](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209135607492.png)

（2）修改 slaves 文件，添加 work 节点，由于实验环境中只有一台服务器，所以是`localhost`

```
localhost复制代码
```

（3）修改 spark-env.sh.template 文件名为 spark-env.sh

```
mv spark-env.sh.template spark-env.sh复制代码
```

（4）修改 spark-env.sh 文件，添加 JAVA_HOME 环境变量和集群对应的 master 节点

```
export SPARK_MASTER_IP=127.0.0.1
export SPARK_MASTER_HOST=localhost
export SPARK_LOCAL_IP=127.0.0.1
export SPARK_MASTER_PORT=7077复制代码
```

> 注意：7077 端口，相当于 hadoop 内部通信的 8020 端口

![image-20230209135820174](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209135820174.png)

###### 2.3.3 启动集群

```
sbin/start-all.sh复制代码
```

![image-20230209140233120](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209140233120.png)

启动成功后，我们在后台进程中可以看到Master和Worker进程！

查看 Master 资源监控Web UI 界面: `http://localhost:8080`

![image-20230209140419815](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209140419815.png)

###### 2.3.4 提交应用

```
bin/spark-submit \
    --class org.apache.spark.examples.SparkPi \
    --master spark://127.0.0.1:7077 \
    ./examples/jars/spark-examples_2.12-3.0.0.jar \ 
    10复制代码
```

> - `--class` 表示要执行程序的主类
> - `--master spark://127.0.0.1:7077`独立部署模式，连接到Spark 集群
> - `spark-examples_2.12-3.0.0.jar` 运行类所在的 jar 包
> - 数字 `10`表示程序的入口参数，用于设定当前应用的任务数量

> 注意：由于实验环境的限制，127.0.0.1，需要替换成当前的服务器ip地址，所以，需要先ifconfig命令查询ip地址，然后将其替换成具体的ip。

![image-20230209140707944](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209140707944.png)

执行任务时，会产生 Java 进程

![image-20230209142636195](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209142636195.png)

执行任务时，默认采用服务器节点的总核数，节点内存 1024M。

![image-20230209142649637](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209142649637.png)

##### 2.4 YARN模式

独立部署（Standalone）模式由 Spark 自身提供计算资源，无需其他框架提供资源。这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但是 Spark 主要是计算框架，而不是资源调度框架，所以本身提供的资源调度并不是它的强项，所以还是和其他专业的资源调度框架集成会更靠谱一些。所以接下来我们来学习在强大的Yarn 环境下 Spark 是如何工作的（其实是因为在国内工作中，Yarn 使用的非常多）。

###### 2.4.1 解压文件

将 `/opt/software/spark-3.0.0-bin-hadoop3.2.tgz` 文件，进行解压缩，并改名。

```
[root@cd62d9893d1f software]# tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module/

[root@cd62d9893d1f software]# mv spark-3.0.0-bin-hadoop3.2 spark-yarn复制代码
```

![image-20230209144117319](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209144117319.png)

###### 2.4.2 文件配置

（1）修改 hadoop 配置文件`/opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml`

```
<!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true-->
<property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>false</value>
</property>

<!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true-->
<property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
</property>复制代码
```

（2）修改 `conf/spark-env.sh`，添加` JAVA_HOME` 和`YARN_CONF_DIR` 配置

```
export JAVA_HOME=/opt/module/jdk1.8.0_212
export YARN_CONF_DIR=/opt/module/hadoop-3.1.3/etc/hadoop复制代码
```

> 如果启动的时候出现环境变量不生效问题，则需要将YARN_CONF_DIR的内容配置在/etc/profile中，然后source生效环境变量。

###### 2.4.3 启动HDFS及YARN集群

![image-20230209145605191](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209145605191.png)

###### 2.4.4 提交应用

```
bin/spark-submit \
    --class org.apache.spark.examples.SparkPi \
    --master yarn \
    ./examples/jars/spark-examples_2.12-3.0.0.jar \ 
    10复制代码
```

![image-20230209150108020](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209150108020.png)

查看 `http://localhost:8088` 页面，点击History，查看历史页面

![image-20230209150215421](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209150215421.png)

> 注意：在访问历史的时候，需要将域名，替换成`localhost`，否则历史服务无法访问。

![image-20230209150324380](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209150324380.png)

### 任务三、Spark WordCount

#### **【任务目标】**

创建我们的第一个Spark WordCount应用程序。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20210824110318276.png?fileid=3270835009073368675)

视频-3、Spark Hello World

##### 3.1 Spark程序执行方式

前面我们已经将Spark的运行环境进行了详细介绍，并且启动了Spark的应用程序，但是使用的都是Spark默认自带的应用，那么如何创建我们自己需要的应用呢？

Spark 官方提供了两种方式编写代码, 都比较重要, 分别如下

- `spark-shell` Spark shell 是 Spark 提供的一个基于 Scala 语言的交互式解释器，类似于 Scala 提供的交互式解释器，Spark shell 也可以直接在 Shell 中编写代码执行。 这种方式也比较重要，因为一般的数据分析任务可能需要探索着进行，不是一蹴而就的， 使用 Spark shell 先进行探索，当代码稳定以后，使用独立应用的方式来提交任务，这样是一个比较常见的流程。
- `spark-submit` Spark submit 是一个命令，用于提交 Scala 编写的基于 Spark 框架，这种提交方式常用作于在集群中运行任务。

##### 3.2 Spark shell 的方式编写 WordCount

**Spark shell 简介**

- 启动 Spark shell 进入 Spark 安装目录后执行 `spark-shell --master master地址` 就可以提交Spark 任务
- Spark shell 的原理是把每一行 Scala 代码编译成类，最终交由 Spark 执行

**Master地址的设置**

Master 的地址可以有如下几种设置方式：

| 地址                | 解释                                                         |
| ------------------- | ------------------------------------------------------------ |
| `local[N]`          | 使用 N 条 Worker 线程在本地运行                              |
| `spark://host:port` | 在 Spark standalone 中运行, 指定 Spark 集群的 Master 地址, 端口默认为 7077 |
| `yarn`              | 在 Yarn 中运行, Yarn 的地址由环境变量 `HADOOP_CONF_DIR` 来指定 |

> 提示：为了方便观察，我们采用Standalone模式执行Spark应用程序。

（1）准备文件

在`/opt/module/spark-standalone/data`目录下创建一个`word.txt`文件，添加如下内容

```
hadoop spark hive
spark hadoop
flume hadoop复制代码
```

（2）启动Spark shell

```
[root@cd62d9893d1f spark-standalone]# bin/spark-shell --master local[*]复制代码
```

![image-20230209152403082](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209152403082.png)

（3）执行如下代码

```
scala> val sourceRdd = sc.textFile("data/word.txt")
sourceRdd: org.apache.spark.rdd.RDD[String] = data/word.txt MapPartitionsRDD[1] at textFile at <console>:24

scala> val flattenCountRdd = sourceRdd.flatMap(_.split(" ")).map((_, 1))
flattenCountRdd: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at <console>:25

scala> val aggCountRdd = flattenCountRdd.reduceByKey(_+_)
aggCountRdd: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at <console>:25

scala> val result = aggCountRdd.collect
result: Array[(String, Int)] = Array((hive,1), (spark,2), (hadoop,3), (flume,1))复制代码
```

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20210823161648091.png)**sc 解释**

> - `sc` 变量指的是 SparkContext，是 Spark 程序的上下文和入口
> - 正常情况下我们需要自己创建，但是如果使用 Spark shell 的话，Spark shell 会帮助我们创建，并且以变量 `sc` 的形式提供给我们调用。

（4）运行流程

![image-20230209152838079](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209152838079.png)

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20210823165604377.png)**解释**

> 1. `flatMap(_.split(" "))` 将数据转为数组的形式，并展平为多个数据
> 2. `map_, 1` 将数据转换为元组的形式
> 3. `reduceByKey(_ + _)` 计算每个 Key 出现的次数

##### 3.3 编写独立应用提交 Spark 任务

###### 3.3.1 创建Maven项目

- 双击图标`IDEA`

![image-20230209153230664](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209153230664.png)

- 创建新项目

![image-20230209153451852](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209153451852.png)

- 选择`Maven`

![image-20230209153515600](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209153515600.png)

- 确定项目名称：
  - GroupId：`com.atstudy`
  - ArtifactId：`Spark01`

![image-20230209153636623](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209153636623.png)

- 确定存储位置

![image-20230209153744211](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209153744211.png)

###### 3.3.2 添加Scala依赖

- 选中`Spark01`项目，右键，`Add Framework Support...`

![image-20230209153851399](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209153851399.png)

- 选中`Scala`，并确定版本

![image-20230209153949110](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209153949110.png)

###### 3.3.3 添加依赖关系

- 工程根目录下文件 `pom.xml`中添加如下内容：

```
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.atstudy</groupId>
    <artifactId>Spark1.0</artifactId>
    <version>1.0-SNAPSHOT</version>

    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.12</artifactId>
            <version>3.0.0</version>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>3.2.2</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-assembly-plugin</artifactId>
                <version>3.1.0</version>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>复制代码
```

###### 3.3.4 编写WorldCount

- 准备数据

  在项目中创建一个目录，名称为`data`,并将之前的`word.txt`，拷贝进来

![image-20230209161048970](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209161048970.png)

- 创建包

  在项目的`src/main/java`目录中创建包

![image-20230209154358813](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209154358813.png)

 包名为：`com.atstudy.spark.demo`

- 创建Object

![image-20230209154509844](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209154509844.png)

 类名为：`WordCount`,类型选择：`Object`

![image-20230209154547285](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209154547285.png)

- 编写代码

```
object WordCount {

    def main(args: Array[String]): Unit = {
        // 创建 Spark 运行配置对象
        val sparkConf = new SparkConf().setMaster("local[*]").setAppName("WordCount")

        // 创建 Spark 上下文环境对象（连接对象）
        val sc : SparkContext = new SparkContext(sparkConf)

        // 读取文件数据
        val fileRDD: RDD[String] = sc.textFile("data/word.txt")

        // 将文件中的数据进行分词
        val wordRDD: RDD[String] = fileRDD.flatMap( _.split(" ") )

        // 转换数据结构 word => (word, 1)
        val word2OneRDD: RDD[(String, Int)] = wordRDD.map((_,1))

        // 将转换结构后的数据按照相同的单词进行分组聚合
        val word2CountRDD: RDD[(String, Int)] = word2OneRDD.reduceByKey(_+_)

        // 将数据聚合结果采集到内存中
        val word2Count: Array[(String, Int)] = word2CountRDD.collect()

        // 打印结果
        word2Count.foreach(println)

        //关闭 Spark 连接
        sc.stop()
    }
}复制代码
```

- 直接运行方式：点击`main`方法，边上的蓝色三角箭头，`Run WordCount`

  > 提示：也可以将这个应用，打包成jar包，提交到Spark集群中，然后通过spark-submit命令执行，我们这里就直接在IDEA上运行我们的Spark应用了。

![image-20230209161746568](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_1/image-20230209161746568.png)

执行过程中，会产生大量的执行日志，如果为了能够更好的查看程序的执行结果，可以在项目的 resources 目录中创建log4j.properties 文件，并添加日志配置信息：

```
log4j.rootCategory=ERROR, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# Set the default spark-shell log level to ERROR. When running the spark-shell, the
# log level for this class is used to overwrite the root logger's log level, so that
# the user can have different defaults for the shell and regular Spark apps. 
log4j.logger.org.apache.spark.repl.Main=ERROR

# Settings to quiet third party logs that are too verbose 
log4j.logger.org.spark_project.jetty=ERROR 
log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR 
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=ERROR 
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=ERROR 
log4j.logger.org.apache.parquet=ERROR
log4j.logger.parquet=ERROR

# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support 
log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL 
log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR复制代码
```

- 执行结果如下：

```
(hive, 1)
(spark, 2)
(hadoop, 3)
(flume, 1)复制代码
```





# 实验1-2：Spark RDD弹性分布式数据集

## 实验概述

在熟悉了Spark和认识了Spark程序的基础上，我们需要掌握Spark的运行架构，理解分布式调度的基本概念，并熟悉Spark中最基本的数据处理模型，RDD弹性分布式数据集。

## 实验环境

- AtStudy 实训平台
- Spark3

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16776607717129196.png)

## 实验目标

学习完成本实验后，您将能够

- 了解Spark运行架构
- 掌握Spark的核心概念
- 了解RDD
- 了解RDD算子

## 实验任务

### 任务一、Spark运行架构

#### **【任务目标】**

了解Spark运行架构

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_2/image-20210824110318276.png?fileid=3270835009073735595)

视频-1、Spark运行架构

##### 1.1 运行架构

Spark 框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构。如下图所示，它展示了一个 Spark 执行时的基本结构。图形中的Driver 表示 master，负责管理整个集群中的作业任务调度。图形中的Executor 则是 slave，负责实际执行任务。

![图9-5-Spark运行架构](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_2/%E5%9B%BE9-5-Spark%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84.jpg)

##### 1.2 核心组件

由上图可以看出，对于 Spark 框架有核心组件：

- `Driver`

  每个应用的任务控制节点，主要是调用 Spark 程序的 main 方法，并且启动 SparkContext

- `Executor`

  每个工作节点上负责具体任务的执行进程，是集群中工作节点（Worker）中的一个 JVM 进程，负责在 Spark 作业中运行具体任务（Task）

- `Cluster Manager`

  集群资源管理器，该进程负责和外部集群工具打交道，申请或释放集群资源

- `Worker Node`

  运行作业任务的工作节点

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_2/image-20210823161648091.png)**Spark架构的优点**

> 与Hadoop MapReduce计算框架相比，Spark所采用的Executor有两个优点
>
> - 利用多线程来执行具体的任务（Hadoop MapReduce采用的是进程模型），减少任务的启动开销
> - Executor中有一个BlockManager存储模块，会将内存和磁盘共同作为存储设备，当需要多轮迭代计算时，可以将中间结果存储到这个存储模块里，下次需要时，就可以直接读该存储模块里的数据，而不需要读写到HDFS等文件系统里，因而有效减少了IO开销

**组件间的协作过程：**

在Spark中，一个应用（Application）由一个任务控制节点（Driver）和若干个作业（Job）构成，一个作业由多个阶段（Stage）构成，一个阶段由多个任务（Task）组成。当执行一个应用时，任务控制节点会向集群管理器（Cluster Manager）申请资源，启动Executor，并向Executor发送应用程序代码和文件，然后在Executor上执行任务，运行结束后，执行结果会返回给任务控制节点，或者写到HDFS或者其他数据库中。

![图9-6-Spark中各种概念之间的相互关系](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_2/%E5%9B%BE9-6-Spark%E4%B8%AD%E5%90%84%E7%A7%8D%E6%A6%82%E5%BF%B5%E4%B9%8B%E9%97%B4%E7%9A%84%E7%9B%B8%E4%BA%92%E5%85%B3%E7%B3%BB.jpg)

##### 1.3 Spark运行流程

![图9-7-Spark运行基本流程图](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_2/%E5%9B%BE9-7-Spark%E8%BF%90%E8%A1%8C%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg)

1. 当一个Spark应用被提交时，首先需要为这个应用构建起基本的运行环境，即由任务控制节点（Driver）创建一个SparkContext，由SparkContext负责和资源管理器（Cluster Manager）的通信以及进行资源的申请、任务的分配和监控等，SparkContext会向资源管理器注册并申请运行Executor的资源；
2. 资源管理器为Executor分配资源，并启动Executor进程，Executor运行情况将随着“心跳”发送到资源管理器上；
3. SparkContext根据RDD的依赖关系构建DAG图，DAG图提交给DAG调度器（DAGScheduler）进行解析，将DAG图分解成多个“阶段”（每个阶段都是一个任务集），并且计算出各个阶段之间的依赖关系，然后把一个个“任务集”提交给底层的任务调度器（TaskScheduler）进行处理；Executor向SparkContext申请任务，任务调度器将任务分发给Executor运行，同时，SparkContext将应用程序代码发放给Executor；
4. 任务在Executor上运行，把执行结果反馈给任务调度器，然后反馈给DAG调度器，运行完毕后写入数据并释放所有资源。

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_2/image-20210823161648091.png)**DAG(有向无环图)**

> - DAG（Directed Acyclic Graph）有向无环图，并不是真正意义的图形，而是由 Spark 程序直接映射成的数据流的高级抽象模型。简单理解就是将整个程序计算的执行过程用图形表示出来，这样更直观， 更便于理解，可以用于表示程序的拓扑结构
> - 有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环

### 任务二、RDD入门

#### **【任务目标】**

理解 Spark 中的编程模型 RDD，并了解RDD的一些基本算子

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_2/image-20210824110318276.png?fileid=3270835009074396993)

视频-2、RDD入门

##### 2.1 回顾 WordCount 代码

```
object WordCount {

  def main(args: Array[String]): Unit = {
    // 1. 创建 Spark Context
    val conf = new SparkConf().setMaster("local[*]").setAppName("WordCount")
    val sc: SparkContext = new SparkContext(conf)

    // 2. 读取文件并计算词频
    val source: RDD[String] = sc.textFile("data/word.txt")
    val words: RDD[String] = source.flatMap(_.split(" "))
    val wordsTuple: RDD[(String, Int)] = words.map((_, 1)
    val wordsCount: RDD[(String, Int)] = wordsTuple.reduceByKey(_ + _)

    // 3. 查看执行结果
    wordsCount.collect().foreach(println)
                                                   
    sc.stop()
  }
}复制代码
```

在这份 WordCount 代码中, 大致的思路如下:

1. 使用 `sc.textFile()` 方法读取文件, 并生成一个 `RDD`
2. 使用 `flatMap` 算子将读取到的每一行字符串打散成单词，并把每个单词变成新的行
3. 使用 `map` 算子将每个单词转换成 `(word, 1)` 这种元组形式
4. 使用 `reduceByKey` 统计单词对应的频率

其中所使用到的**算子**有如下几个:

- `flatMap` 是一对多
- `map` 是 一对一
- `reduceByKey` 是按照 Key 聚合, 类似 MapReduce 中的 Shuffled

如果用图形表示的话, 如下:

![image-20230209173803162](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_2/image-20230209173803162.png)

上面大概说了两件事:

1. 代码流程
2. 算子

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_2/image-20210823170539971.png)**在代码中有一些东西并未交代**

> 1. source, words, wordsTuple 这些变量的类型是 `RDD[Type]`, 什么是 `RDD`?
> 2. 还有更多算子吗?

##### 2.2 RDD

###### 2.2.1 RDD是什么

RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是 Spark 中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。

- **分布式**

  RDD 支持分区，可以运行在集群中

- **弹性**

  RDD 支持高效的容错

  RDD 中的数据即可以缓存在内存中，也可以缓存在磁盘中，也可以缓存在外部存储中

- **数据集**

  RDD 可以不保存具体数据，只保留创建自己的必备信息，例如依赖和计算函数

  RDD 也可以缓存起来，相当于存储具体数据

###### 2.2.2 执行原理

从计算的角度来讲，数据处理过程中需要计算资源（内存 & CPU）和计算模型（逻辑）。执行时，需要将计算资源和计算模型进行协调和整合。

Spark 框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。然后将任务发到已经分配资源的计算节点上, 按照指定的计算模型进行数据计算。最后得到计算结果。

RDD 是 Spark 框架中用于数据处理的核心模型，接下来我们看看，在 Yarn 环境中，RDD 的工作原理:

（1） 启动 Yarn 集群环境

![image-20230209213817072](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_2/image-20230209213817072.png)

（2） Spark 通过申请资源创建调度节点和计算节点

![image-20230209214059360](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_2/image-20230209214059360.png)

（3）Spark 框架根据需求将计算逻辑根据分区（partition）划分成不同的任务

![image-20230209215146585](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_2/image-20230209215146585.png)

（4）调度节点将任务根据计算节点状态发送到对应的计算节点进行计算

![image-20230209215517505](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_2/image-20230209215517505.png)

从以上流程可以看出 RDD 在整个流程中主要用于将逻辑进行封装，并生成 Task 发送给Executor 节点执行计算，接下来我们就一起看看 Spark 框架中RDD 是具体是如何进行数据处理的。

##### 2.3 创建RDD

###### 2.3.1 程序入口SparkContext

```
val conf = new SparkConf().setMaster("local[*]").setAppName("WordCount")
val sc: SparkContext = new SparkContext(conf)复制代码
```

`SparkContext` 是 spark-core 的入口组件，是一个 Spark 程序的入口，在 Spark 0.x 版本就已经存在 `SparkContext` 了, 是一个元老级的 API。

如果把一个 Spark 程序分为前后端，那么服务端就是可以运行 Spark 程序的集群，而 `Driver` 就是 Spark 的前端，在 `Driver` 中 `SparkContext` 是最主要的组件，也是 `Driver` 在运行时首先会创建的组件，是 `Driver` 的核心。

`SparkContext` 从提供的 API 来看, 主要作用是连接集群、创建 RDD、累加器、广播变量等。

###### 2.3.2 RDD的构建

在 Spark 中 RDD 的创建方式主要为：

（1）从集合（内存）中创建RDD

```
val sparkConf = new SparkConf().setMaster("local[*]").setAppName("WordCount") 
val sparkContext = new SparkContext(sparkConf)
val rdd1 = sparkContext.parallelize(List(1,2,3,4),2)
val rdd2 = sparkContext.makeRDD(List(1,2,3,4),2)
rdd1.collect().foreach(println) 复制代码
```

从底层代码实现来讲，makeRDD 方法其实就是parallelize 方法

> 因为不是从外部直接读取数据集的，所以没有外部的分区可以借鉴，于是在这两个方法都有两个参数，第一个参数是本地集合,，第二个参数是分区数。

（2）从外部存储（文件）创建

由外部存储系统的数据集创建RDD 包括：本地的文件系统，所有Hadoop 支持的数据集， 比如HDFS、HBase 等

```
val sparkConf = new SparkConf().setMaster("local[*]").setAppName("WordCount") 
val sparkContext = new SparkContext(sparkConf)
val fileRDD: RDD[String] = sparkContext.textFile("data/word.txt") fileRDD.collect().foreach(println)
sparkContext.stop()复制代码
```

从HDFS获取

```
val fileRDD: RDD[String] = sparkContext.textFile("hdfs://节点:8020/dataset/word.txt")复制代码
```

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_2/image-20210823161648091.png)**HDFS分区**

> - 默认情况下读取 HDFS 中文件的时候，每个 HDFS 的 `block` 对应一个 RDD 的 `partition`, `block` 的默认是128M
> - 通过第二个参数, 可以指定分区数量，例如 `sparkContext.textFile("hdfs://节点:8020/dataset/word.txt", 20)`
> - 如果通过第二个参数指定了分区，这个分区数量一定不能小于`block`数

##### 2.4 RDD 算子

其实就是通过RDD提供的各种数据处理方法，我们称为算子。例如：

```
sc.parallelize(List(1, 2, 3))
  .map( num => num * 10 )
  .collect()复制代码
```

![c59d44296918b864a975ebbeb60d4c04](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab1_2/c59d44296918b864a975ebbeb60d4c04.png)

Map方法就是一个算子，作用是把 RDD 中的数据 一对一 的转为另一种形式，Map 算子是 `原RDD → 新RDD` 的过程，这个函数的参数是原 RDD 数据，返回值是经过函数转换的新 RDD 的数据。

在Spark中，可以根据不同的数据处理需求，选择各种算子，具体的还有哪些，请听下回分解！





# 实验2-1：转换和行动算子

## 实验概述

在Spark应用开发中，RDD算子的使用是关键，所以，需要再次深入理解RDD的内在逻辑及常见的各种RDD算子。

## 实验环境

- AtStudy 实训平台
- Spark3

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/1677660821292695.png)

## 实验目标

学习完成本实验后，您将能够

- 深入理解 RDD 的内在逻辑
- 能够使用 RDD 的算子
- 掌握转换算子的使用
- 掌握行动算子的使用

## 实验任务

### 任务一、深入RDD

#### **【任务目标】**

了解RDD的特点，为深入学习RDD的各种算子做好前期准备。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/image-20210824110318276.png?fileid=3270835009073787332)

视频-1、深入RDD

##### 1.1 RDD为什么会出现

在 RDD 出现之前，当时 MapReduce 是比较主流的，而 MapReduce 如何执行迭代计算的任务呢?

![306061ee343d8515ecafbce43bc54bc6](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/306061ee343d8515ecafbce43bc54bc6.png)

多个 MapReduce 任务之间没有基于内存的数据共享方式，只能通过磁盘来进行共享。

这种方式明显比较低效。

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/image-20210823161648091.png)**RDD 如何解决迭代计算非常低效的问题呢?**

![4fc644616fb13ef896eb3a8cea5d3bd7](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/4fc644616fb13ef896eb3a8cea5d3bd7.png)

在 Spark 中，其实最终 Job3 从逻辑上的计算过程是: `Job3 = (Job1.map).filter`, 整个过程是共享内存的，而不需要将中间结果存放在可靠的分布式文件系统中。

这种方式可以在保证容错的前提下，提供更多的灵活，更快的执行速度，RDD 在执行迭代型任务时候的表现可以通过下面代码体现。

```
// 线性回归
val points = sc.textFile(...)
    .map(...)
    .persist(...)
val w = randomValue
for (i <- 1 to 10000) {
    val gradient = points.map(p => p.x * (1 / (1 + exp(-p.y * (w dot p.x))) - 1) * p.y)
    .reduce(_ + _)
    w -= gradient
}复制代码
```

在这个例子中, 进行了大致 10000 次迭代，如果在 MapReduce 中实现，可能需要运行很多 Job，每个 Job 之间都要通过 HDFS 共享结果，熟快熟慢一窥便知。

##### 1.2 RDD 的特点

###### 1.2.1 RDD 不仅是数据集, 也是编程模型

RDD 即是一种数据结构，同时也提供了上层 API，同时 RDD 的 API 和 Scala 中对集合运算的 API 非常类似，同样也都是各种算子

![02adfc1bcd91e70c1619fc6a67b13f92](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/02adfc1bcd91e70c1619fc6a67b13f92.png)

RDD 的算子大致分为两类：

- Transformation 转换操作，例如 `map` `flatMap` `filter` 等
- Action 动作操作，例如 `reduce` `collect` `show` 等

执行 RDD 的时候，在执行到转换操作的时候，并不会立刻执行，直到遇见了 Action 操作，才会触发真正的执行，这个特点叫做 **惰性求值**。

###### 1.2.2 RDD 可以分区

![2ba2cc9ad8e745c26df482b4e968c802](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/2ba2cc9ad8e745c26df482b4e968c802.png)

RDD 是一个分布式计算框架，所以，一定是要能够进行分区计算的，只有分区了，才能利用集群的并行计算能力

同时，RDD 不需要始终被具体化，也就是说: RDD 中可以没有数据, 只要有足够的信息知道自己是从谁计算得来的就可以，这是一种非常高效的容错方式。

###### 1.2.3 RDD 是只读的

![ed6a534cfe0a56de3c34ac6e1e8d504e](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/ed6a534cfe0a56de3c34ac6e1e8d504e.png)

RDD 是只读的，不允许任何形式的修改。虽说不能因为 RDD 和 HDFS 是只读的，就认为分布式存储系统必须设计为只读的。但是设计为只读的，会显著降低问题的复杂度，因为 RDD 需要可以容错，可以惰性求值，可以移动计算，所以很难支持修改。

- RDD2 中可能没有数据， 只是保留了依赖关系和计算函数，那修改啥?
- 如果因为支持修改，而必须保存数据的话，怎么容错?
- 如果允许修改，如何定位要修改的那一行？
  - RDD 的转换是粗粒度的, 也就是说, RDD 并不感知具体每一行在哪。

###### 1.2.4 RDD 是可以容错的

![5c7bef41f177a96e99c7ad8a500b7310](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/5c7bef41f177a96e99c7ad8a500b7310.png)

RDD 的容错有两种方式

- 保存 RDD 之间的依赖关系，以及计算函数，出现错误重新计算。
- 直接将 RDD 的数据存放在外部存储系统，出现错误直接读取，Checkpoint。

##### 1.3 RDD算子分类

RDD 中的算子从功能上分为两大类

1. Transformation(转换) 它会在一个已经存在的 RDD 上创建一个新的 RDD，将旧的 RDD 的数据转换为另外一种形式后放入新的 RDD。
2. Action(动作) 执行各个分区的计算任务，将的到的结果返回到 Driver 中。

### 任务二、转换算子

#### **【任务目标】**

掌握转换算子的使用。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/image-20210824110318276.png?fileid=3270835009072875311)

视频-2、转换算子

##### 2.1 转换算子

Spark 中所有的 转换（Transformations） 是 Lazy(惰性) 的，它们不会立即执行获得结果。相反，它们只会记录在数据集上要应用的操作。只有当需要返回结果给 Driver 时，才会执行这些操作，通过 `DAGScheduler `和 `TaskScheduler` 分发到集群中运行，这个特性叫做 **惰性求值**

转换算子根据数据处理方式的不同将算子整体上分为`Value 类型`、`双 Value 类型`和`Key-Value类型`。

- Value类型：Map、flatMap、filter、sortBy等
- 双Value类型：union、zip等
- Key-Value类型：reduceByKey、groupByKey、sortByKey、join、cogroup、mapValues等

##### 2.2 Value类型

###### 2.2.1 Map

- 函数签名：`def map[U: ClassTag](f: T => U): RDD[U]`

![c59d44296918b864a975ebbeb60d4c04 (1)](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/c59d44296918b864a975ebbeb60d4c04 (1).png)

- 函数说明：将处理的数据逐条进行映射转换，这里的转换可以是类型的转换，也可以是值的转换。
- 案例：

```
val dataRDD: RDD[Int] = sparkContext.makeRDD(List(1,2,3,4)) 
val dataRDD1: RDD[Int] = dataRDD.map(
    num => {
        num * 2
    }
)
val dataRDD2: RDD[String] = 
    dataRDD1.map( num => {
        "" + num
    }
)   复制代码
```

**需求：**从服务器日志数据 `/opt/software/data/apache.log` 中获取用户请求URL 资源路径

![image-20230210165357372](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/image-20230210165357372.png)

```
val dataRdd:RDD[String] = sc.textFile("/opt/software/data/apache.log")
val mapRdd:RDD[String] = dataRdd.map(
    line=>{
        var datas = line.split(“ ”)
        datas(6)
    }
)复制代码
```

###### 2.2.2 flatMap

- 函数签名：`def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U]`

![image-20230211100735490](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/image-20230211100735490.png)

- 函数说明：将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射。
  - 参数 f → 是原 RDD 数据, 返回值是经过函数转换的新 RDD 的数据, 需要注意的是返回值是一个集合, 集合中的数据会被展平后再放入新的 RDD
- 案例：

```
val dataRDD:RDD[List[Int]] = 
    sparkContext.makeRDD(List( List(1,2),List(3,4)), 1)
val dataRDD1:RDD[Int] = 
    dataRDD.flatMap( list => list)复制代码
```

**需求：**将 List(List(1,2),3,List(4,5))进行扁平化操作

```
val dataRDD = sc.makeRDD(List(List(1, 2), 3, List(4,5)))
val flatRDD= dataRDD.flatMap(data=>{
    data match{
        case list:List[_] =>list
        case dat=>List(dat)
    }
})复制代码
```

###### 2.2.3 filter

- 函数签名：`def filter(f: T => Boolean): RDD[T]`

![image-20230211112631853](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/image-20230211112631853.png)

- 函数说明：将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃。当数据进行筛选过滤后，分区不变，但是分区内的数据可能不均衡，生产环境下，可能会出现数据倾斜。
- 案例：

```
val dataRDD = sparkContext.makeRDD(List( 1,2,3,4),1)
val dataRDD1 = dataRDD.filter(_%2 == 0)复制代码
```

**需求**：从服务器日志数据 `/opt/software/data/apache.log` 中获取 `2015 年 5 月 17 日`的请求路径

```
val rdd = sc.textFile(“datas/apache.log”)
rdd.filter(
    line=>{
        val datas = line.split(" ")
        val time = datas(3)
        time.startWith("17/05/2015")
    }).collect().foreach(println)复制代码
```

###### 2.3.4 sortBy

- 函数签名：`def sortBy[K]( f: (T) => K, ascending: Boolean = true, numPartitions: Int = this.partitions.length)(implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]`
- 函数说明：该操作用于排序数据。在排序之前，可以将数据通过 f 函数进行处理，之后按照 f 函数处理的结果进行排序，默认为升序排列。排序后新产生的 RDD 的分区数与原RDD 的分区数一致。中间存在 shuffle 的过程。
- 案例：

```
val dataRDD = sparkContext.makeRDD(List( 1,2,3,4,1,2), false, 2)
val dataRDD1 = dataRDD.sortBy(num=>num, false, 4)//false:表示降序排列， 4:表示分区个数
复制代码
```

##### 2.3 双Value类型

###### 2.3.1 union

- 函数签名：`def union(other: RDD[T]): RDD[T]`

![image-20230211131552918](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/image-20230211131552918.png)

- 函数说明：对源RDD 和参数RDD 求并集后返回一个新的RDD
- 案例：

```
val dataRDD1 = sparkContext.makeRDD(List(1,2,3,4)) 
val dataRDD2 = sparkContext.makeRDD(List(3,4,5,6)) 
val dataRDD = dataRDD1.union(dataRDD2)
//结果为：1,2,3,4,3,4,5,6复制代码
```

###### 2.3.2 zip

- 函数签名：`def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)]`
- 函数说明：将两个 RDD 中的元素，以键值对的形式进行合并。其中，键值对中的Key 为第 1 个 RDD中的元素，Value 为第 2 个 RDD 中的相同位置的元素
- 案例

```
val dataRDD1 = sparkContext.makeRDD(List(1,2,3,4)) 
val dataRDD2 = sparkContext.makeRDD(List(3,4,5,6)) 
val dataRDD = dataRDD1.zip(dataRDD2)
//结果为：(1,3)(2,4)(3,5)(4,6)复制代码
```

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/image-20210823161648091.png)**注意**

> 思考问题1：如果两个 RDD 数据类型不一致怎么办？
>
> 思考问题2：如果两个 RDD 数据分区不一致怎么办？
>
> 思考问题3：如果两个 RDD 分区数据数量不一致怎么办？

##### 2.4 Key-Value类型

###### 2.4.1 reduceByKey

- 函数签名：
  - `def reduceByKey(func: (V, V) => V): RDD[(K, V)]`
  - `def reduceByKey(func: (V, V) => V, numPartitions: Int): RDD[(K, V)]`

![07678e1b4d6ba1dfaf2f5df89489def4](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/07678e1b4d6ba1dfaf2f5df89489def4.png)

- 函数说明：可以将数据按照相同的Key 对Value 进行聚合
- 案例

```
val dataRDD1 = sparkContext.makeRDD(List(("a",1),("a",2),("b",4))) 
//val dataRDD2 = dataRDD1.reduceByKey((x:Int,y:Int)=>{x+y})
val dataRDD2 = dataRDD1.reduceByKey(_+_)
val dataRDD3 = dataRDD1.reduceByKey(_+_, 2)//2表示分两个区进行存储

//结果为：("a", 3) ("b", 4)复制代码
```

###### 2.4.2 groupByKey

- 函数签名：
  - `def groupByKey(): RDD[(K, Iterable[V])]`
  - `def groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]`
  - `def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]`

![27de81df110abb6709bf1c5ffad184ab](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/27de81df110abb6709bf1c5ffad184ab.png)

- 函数说明：将数据源的数据根据 key 对 value 进行分组
- 案例：

```
val dataRDD1 = sparkContext.makeRDD(List(("a",1),("b",2),("c",3)))
val dataRDD2 = dataRDD1.groupByKey() 
val dataRDD3 = dataRDD1.groupByKey(2)
val dataRDD4 = dataRDD1.groupByKey(new HashPartitioner(2))复制代码
```

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/image-20210823161648091.png)**思考？**

> `reduceByKey` 和 `groupByKey` 的区别？
>
> **从** **shuffle** **的角度**：`reduceByKey` 和 `groupByKey` 都存在 shuffle 的操作，但是`reduceByKey` 可以在 shuffle 前对分区内相同 key 的数据进行预聚合（combine）功能，这样会减少落盘的数据量，而`groupByKey` 只是进行分组，不存在数据量减少的问题，`reduceByKey` 性能比较高。
>
> **从功能的角度**：`reduceByKey` 其实包含分组和聚合的功能。`groupByKey` 只能分组，不能聚合，所以在分组聚合的场合下，推荐使用 `reduceByKey`，如果仅仅是分组而不需要聚合。那么还是只能使用`groupByKey`

###### 2.4.3 join

- 函数签名：`def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]`

![bb3eda1410d3b0f6e1bff6d5e6a45879](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/bb3eda1410d3b0f6e1bff6d5e6a45879.png)

- 函数说明：在类型为(K, V)和(K, W)的 RDD 上调用，返回一个相同 key 对应的所有元素连接在一起的(K,(V,W))的 RDD
- 案例：

```
val rdd: RDD[(Int, String)] = sc.makeRDD(Array((1, "a"), (2, "b"), (3, "c")))
val rdd1: RDD[(Int, Int)] = sc.makeRDD(Array((1, 4), (2, 5), (3, 6))) rdd.join(rdd1).collect().foreach(println)
//结果为：(1, (a,4)) (2, (b, 5))  (3, (c, 6))复制代码
```

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/warning.png)**注意：**

> - Join 有点类似于 SQL 中的内连接，只会在结果中包含能够连接到的 Key
> - Join 的结果是一个笛卡尔积形式，例如`("a", 1), ("a", 2)`和`("a", 10), ("a", 11)`的 Join 结果集是 `("a", (1, 10)), ("a", (1, 11)), ("a", (2, 10)), ("a", (2, 11))`

###### 2.4.4 cogroup

- 函数签名：`def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))]`

![42262ffe7f3ff35013fbe534d78e3518](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/42262ffe7f3ff35013fbe534d78e3518.png)

- 函数说明：在类型为 (K, V)和(K, W)的 RDD 上调用，返回一个`(K, (Iterable<V>,Iterable<W>))`类型的 RDD
- 案例：

```
val dataRDD1 = sparkContext.makeRDD(List(("a",1),("a",2),("c",3)))
val dataRDD2 = sparkContext.makeRDD(List(("a",1),("c",2),("c",3))) 
val value: RDD[(String, (Iterable[Int], Iterable[Int]))] = dataRDD1.cogroup(dataRDD2)复制代码
```

###### 2.4.5 mapValues

- 函数说明：和 Map 类似，也是使用函数按照转换数据, 不同点是 MapValues 只转换 Key-Value 中的 Value。

![5551847febe453b134f3a4009df01bec](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/5551847febe453b134f3a4009df01bec.png)

- 案例：

```
sc.makeRDD(List(("a", 1), ("b", 2), ("c", 3)))
  .mapValues( value => value * 10 )
  .collect()复制代码
```

##### 2.5 综合案例

（1）数据准备：在`/opt/software/data`目录下有个一个日志文件`agent.log`，文件内容如下：

 时间戳，省份，城市，用户，广告，中间字段使用空格分隔。

（2）需求描述

 统计出每一个省份**每个广告被点击数量**排行的 Top3

（3）过程分析

```
1.获取原始数据:时间戳，省份，城市，用户，广告

2.将原始数据进行结构的转换，方便统计
    时问戳，省份，城市，用户，广告
    =>
    ( (省份，广告 )， 1)
    
3．将转换结构后的数据，进行分组聚合
       ( (省份，广告)，1 ) =>( (省份，广告 )， sum )  
    
4.将聚合的结果进行结构的转换
    ((省份，广告 ), sum ) =>(省份，（广告， sum ) )
    
5．将转换结构后的数据根据省份进行分组
    (省份，【(广告A， sumA )，(广告B, sumB )】)

6.将分组后的数据组内排序（降序)，取前3名
    
7．采集数据打印在控制台复制代码
```

（4）功能实现

```
def main(args: Array[String]): Unit = {
    val spakConf = new SparkConf().setMaster("local[*]").setAppName("Operator")
    val sc = new SparkContext(spakConf)
    //1.获取原始数据:时间戳，省份，城市，用户，广告
    val dataRDD = sc.textFile("/opt/software/data/agent.log")

    //2.将原始数据进行结构的转换。方便统计
    //  时问戳，省份，城市，用户，广告
    //=>
    //( (省份，广告 )，1 )
    val mapRDD = dataRDD.map(
        line=>{
            val datas = line.split(" ")
            ((datas(1), datas(4)), 1)
        }
    )

    //3．将转换结构后的数据，进行分组聚合
    //	( (省份，广告)，1 ) =>( (省份，广告 )， sum )
    val reduceRDD = mapRDD.reduceByKey(_+_)

    //4.将聚合的结果进行结构的转换
    //((省份，广告 ), sum ) =>(省份，（广告， sum ) )
    val newMapRDD = reduceRDD.map{
        case ((pre,add), sum)=>{
            (pre, (add, sum))
        }
    }

    //5．将转换结构后的数据根据省份进行分组
    //(省份，【(广告A， sumA )，(广告B, sumB )】)
    val groupRDD:RDD[(String, Iterable[(String,Int)])] = newMapRDD.groupByKey()

    //6.将分组后的数据组内排序（降序)，取前3名
    val resultRDD = groupRDD.mapValues(
        iter=>{
            iter.toList.sortBy(_._2)(Ordering.Int.reverse).take(3)
        }
    )

    //7．采集数据打印在控制台
    resultRDD.collect().foreach(println)
}复制代码
```

### 任务三、行动算子

#### **【任务目标】**

掌握行动算子的使用。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_1/image-20210824110318276.png?fileid=3270835009073948658)

视频-3、行动算子

##### 3.1 行动算子

默认情况下，每一个 行动（Action）算子运行的时候，其所关联的所有 Transformation RDD 都会重新计算，但是也可以使用 `presist` 方法将 RDD 持久化到磁盘或者内存中。这个时候为了下次可以更快的访问，会把数据保存到集群上。

行动算子主要有：reduce、collect、take、save、foreach、countbyKey

###### 3.1.1 reduce

- 函数签名：`def reduce(f: (T, T) => T): T`
- 函数说明：聚集RDD 中的所有元素，先聚合分区内数据，再聚合分区间数据
- 案例：

```
val rdd = sc.makeRDD(List(("手机", 10.0), ("手机", 15.0), ("电脑", 20.0)))
val result = rdd.reduce((curr, agg) => ("总价", curr._2 + agg._2))
println(result)复制代码
```

###### 3.1.2 collect

- 函数签名：`def collect(): Array[T]`
- 函数说明：在驱动程序中，以数组Array 的形式返回数据集的所有元素
- 案例

```
val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))

// 收集数据到 Driver 
rdd.collect().foreach(println)复制代码
```

###### 3.1.3 take

- 函数签名：`def take(num: Int): Array[T]`
- 函数说明：返回一个由RDD 的前 n 个元素组成的数组
- 案例

```
vval rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))

// 返回 RDD 中元素的个数
val takeResult: Array[Int] = rdd.take(2) 
println(takeResult.mkString(","))复制代码
```

###### 3.1.4 save

- 函数签名：
  - `def saveAsTextFile(path: String): Unit `
  - `def saveAsObjectFile(path: String): Unit `
  - `def saveAsSequenceFile(path: String,codec: Option[Class[_ <: CompressionCodec]] = None): Unit`
- 函数说明：将数据保存到不同格式的文件中
- 案例

```
// 保存成 Text 文件
rdd.saveAsTextFile("output")

// 序列化成对象保存到文件
rdd.saveAsObjectFile("output1")

// 保存成  文件
rdd.map((_,1)).saveAsSequenceFile("output2")复制代码
```

###### 3.1.5 foreach

- 函数签名：`def foreach(f: T => Unit): Unit = withScope { val cleanF = sc.clean(f) ;sc.runJob(this, (iter: Iterator[T]) => iter.foreach(cleanF))}`
- 函数说明：分布式遍历RDD 中的每一个元素，调用指定函数
- 案例

```
val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))

// 收集后打印
rdd.map(num=>num).collect().foreach(println) 
println("****************")
// 分布式打印
rdd.foreach(println)复制代码
```

###### 3.1.6 countByKey

- 函数签名：`def countByKey(): Map[K, Long]`
- 函数说明：统计每种 key 的个数
- 案例

```
val rdd: RDD[(Int, String)] = sc.makeRDD(List((1, "a"), (1, "a"), (1, "a"), (2, "b"), (3, "c"), (3, "c")))

// 统计每种 key 的个数
val result: collection.Map[Int, Long] = rdd.countByKey()复制代码
```

##### 3.2 WordCount案例

需求：分别用`groupBy`、`groupByKey`、`reduceByKey`、`countByKey`、`reduce`算子统计单词的个数：

```
// groupBy
def wordcount1(sc : SparkContext): Unit = {

    val rdd = sc.makeRDD(List("Hello Scala", "Hello Spark"))
    val words = rdd.flatMap(_.split(" "))
    val group: RDD[(String, Iterable[String])] = words.groupBy(word=>word)
    val wordCount: RDD[(String, Int)] = group.mapValues(iter=>iter.size)
}

// groupByKey
def wordcount2(sc : SparkContext): Unit = {
    val rdd = sc.makeRDD(List("Hello Scala", "Hello Spark"))
    val words = rdd.flatMap(_.split(" "))
    val wordOne = words.map((_,1))
    val group: RDD[(String, Iterable[Int])] = wordOne.groupByKey()
    val wordCount: RDD[(String, Int)] = group.mapValues(iter=>iter.size)
}

// reduceByKey
def wordcount3(sc : SparkContext): Unit = {
    val rdd = sc.makeRDD(List("Hello Scala", "Hello Spark"))
    val words = rdd.flatMap(_.split(" "))
    val wordOne = words.map((_,1))
    val wordCount: RDD[(String, Int)] = wordOne.reduceByKey(_+_)
}

// countByKey
def wordcount4(sc : SparkContext): Unit = {
    val rdd = sc.makeRDD(List("Hello Scala", "Hello Spark"))
    val words = rdd.flatMap(_.split(" "))
    val wordOne = words.map((_,1))
    val wordCount: collection.Map[String, Long] = wordOne.countByKey()
}

// countByValue
def wordcount5(sc : SparkContext): Unit = {
    val rdd = sc.makeRDD(List("Hello Scala", "Hello Spark"))
    val words = rdd.flatMap(_.split(" "))
    val wordCount: collection.Map[String, Long] = words.countByValue()
}

// reduce
def wordcount6(sc : SparkContext): Unit = {
    val rdd = sc.makeRDD(List("Hello Scala", "Hello Spark"))
    val words = rdd.flatMap(_.split(" "))

    // 【（word, count）,(word, count)】
    // word => Map[(word,1)]
    val mapWord = words.map(
        word => {
            mutable.Map[String, Long]((word,1))
        }
    )

    val wordCount = mapWord.reduce(
        (map1, map2) => {
            map2.foreach{
                case (word, count) => {
                    val newCount = map1.getOrElse(word, 0L) + count
                    map1.update(word, newCount)
                }
            }
            map1
        }
    )
    println(wordCount)
}
```





# 实验2-2：RDD高阶特性

## 实验概述

在掌握了RDD常用算子基础上，我们才可以根据具体的业务场景实现不用的应用开发，但是在实际中我们还需要了解一些更高阶的特性，这样我们才能真正的掌握Spark Core的内涵。

## 实验环境

- AtStudy 实训平台
- Spark3

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16776608668043026.png)

## 实验目标

学习完成本实验后，您将能够

- 掌握RDD的分区和Shuffle
- 掌握缓存和检查点的作用
- 能够深入了解Spark底层执行逻辑

## 实验任务

### 任务一、RDD的Shuffle和分区

#### **【任务目标】**

掌握RDD的分区和什么是Shuffle。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_2/image-20210824110318276.png?fileid=3270835009074539145)

视频-1、RDD的Shuffle和分区

##### 1.1 RDD 的分区操作

**分区的作用：**

分区的主要作用是用来实现并行计算，并且要做到尽量少的在不同的 Executor 之间使用网络交换数据，所以当使用 RDD 读取数据的时候，会尽量的在物理上靠近数据源，比如说在读取 HDFS 中数据的时候， 会尽量的保持 RDD 的分区和数据源的分区数，分区模式等一一对应。

**查看分区**

采用本地模式启动Spark

```
./bin/spark-shell --master local[*]复制代码
```

![image-20230211161820024](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_2/image-20230211161820024.png)

```
scala> sc.makeRDD(1 to 100).count
res0: Long = 100复制代码
```

![image-20230211162002168](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_2/image-20230211162002168.png)

同时也可以通过 spark-shell 的 WebUI 来查看分区情况

![image-20230211162155476](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_2/image-20230211162155476.png)

> 之所以会有 4 个 Tasks，是因为在启动的时候指定的命令是 `spark-shell --master local[*]`，这样会生成 1 个 Executors, 这个 Executors 有 4 个 Cores（默认服务器是4核的CPU）， 所以默认会有 4 个 Tasks, 每个 Cores 对应一个分区，每个分区对应一个 Tasks，也可以通过 `rdd.partitions.size` 来查看分区数量

![image-20230211162347435](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_2/image-20230211162347435.png)

通过 spark-shell 的 WebUI 来查看Executors情况

![image-20230211162556316](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_2/image-20230211162556316.png)

> 默认的分区数量是和 Cores 的数量有关的。

**在创建RDD时也可以指定分区**

```
scala> val rdd1 = sc.parallelize(1 to 100, 6)
rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24

scala> rdd1.partitions.size
res1: Int = 6

scala> val rdd2 = sc.textFile("/opt/software/data/agent.log", 6)
rdd2: org.apache.spark.rdd.RDD[String] = /opt/software/data/agent.log MapPartitionsRDD[4] at textFile at <console>:24

scala> rdd2.partitions.size
res2: Int = 6复制代码
```

> - rdd1 是通过本地集合创建的，创建的时候通过第二个参数指定了分区数量
> - rdd2 是通过读取文件创建的

##### 1.2 RDD 的 Shuffle 是什么

例如：

```
val sourceRdd = sc.textFile("datas/word.txt")
val flattenCountRdd = sourceRdd.flatMap(_.split(" ")).map((_, 1))
val aggCountRdd = flattenCountRdd.reduceByKey(_ + _)
val result = aggCountRdd.collect复制代码
```

![23377ac4a368fc94b6f8f3117af67154](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_2/23377ac4a368fc94b6f8f3117af67154.png)

![10b536c17409ec37fa1f1b308b2b521e](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_2/10b536c17409ec37fa1f1b308b2b521e.png)

`reduceByKey` 这个算子本质上就是先按照 Key 分组，后对每一组数据进行 `reduce`，所面临的挑战就是 Key 相同的所有数据可能分布在不同的 Partition 分区中，甚至可能在不同的节点中，但是它们必须被共同计算

为了让来自相同 Key 的所有数据都在 `reduceByKey` 的同一个 `reduce` 中处理，需要执行一个 `all-to-all` 的操作，需要在不同的节点(不同的分区)之间拷贝数据，必须跨分区聚集相同 Key 的所有数据, 这个过程叫做 `Shuffle`，如果存在频繁的Shuffle过程，会影响程序的性能，那么持久化就是一个很好的方式。

### 任务二、RDD持久化

#### **【任务目标】**

掌握缓存和检查点的使用。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_2/image-20210824110318276.png?fileid=3270835009075058651)

视频-2、RDD持久化

##### 2.1 RDD 缓存

###### 2.1.1 缓存的意义

需求1：在日志文件中找到访问次数最少的 IP 和访问次数最多的 IP

```
val conf = new SparkConf().setMaster("local[*]").setAppName("debug_string")
val sc = new SparkContext(conf)

val interimRDD = sc.textFile("/opt/software/data/apache.log")
  .map(item => (item.split(" ")(0), 1))
  .filter(item => StringUtils.isNotBlank(item._1))
  .reduceByKey((curr, agg) => curr + agg) //这是一个 Shuffle 操作, Shuffle 操作会在集群内进行数据拷贝


val resultLess = interimRDD.sortBy(item => item._2, ascending = true).first()
val resultMore = interimRDD.sortBy(item => item._2, ascending = false).first()

println(s"出现次数最少的 IP : $resultLess, 出现次数最多的 IP : $resultMore")

sc.stop()复制代码
```

> 在上述代码中, 多次使用到了 `interimRDD`, 导致文件读取两次，计算两次，有没有什么办法增进上述代码的性能？

需求2：当在计算 RDD3 的时候如果出错了，如何进行容错?

![20190511163654](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_2/20190511163654.png)

会再次计算 RDD1 和 RDD2 的整个链条，假设 RDD1 和 RDD2 是通过比较昂贵的操作得来的, 有没有什么办法减少这种开销?

**缓存**的就是很好的解决方案！

###### 2.1.1 Cache缓存

RDD 通过Cache 或者 Persist 方法将前面的计算结果缓存，默认情况下会把数据以缓存在 JVM 的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的 action 算子时，该RDD 将会被缓存在计算节点的内存中，并供后面重用。

```
val conf = new SparkConf().setMaster("local[*]").setAppName("debug_string")
val sc = new SparkContext(conf)

val interimRDD = sc.textFile("/opt/software/data/apache.log")
  .map(item => (item.split(" ")(0), 1))
  .filter(item => StringUtils.isNotBlank(item._1))
  .reduceByKey((curr, agg) => curr + agg)
  .cache()	//缓存

val resultLess = interimRDD.sortBy(item => item._2, ascending = true).first()
val resultMore = interimRDD.sortBy(item => item._2, ascending = false).first()

println(s"出现次数最少的 IP : $resultLess, 出现次数最多的 IP : $resultMore")

sc.stop()复制代码
```

##### 2.2 CheckPoint检查点

RDD缓存一般将数据存放在内存中，是不可靠的，如果出现了一些错误(例如 Executor 宕机)，这个 RDD 的容错就只能通过回溯依赖链，重放计算出来。

而所谓的检查点其实就是通过将 RDD 中间结果写入磁盘。

由于RDD依赖过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果检查点之后有节点出现问题，可以从检查点开始重做，减少了开销。

对 RDD 进行 checkpoint 操作并不会马上被执行，必须执行 Action 操作才能触发。

代码案例：

```
// 设置检查点路径
sc.setCheckpointDir("./checkpoint1")

// 创建一个 RDD，读取指定位置文件:hello spark spark
val lineRdd: RDD[String] = sc.textFile("input/1.txt")

// 业务逻辑
val wordRdd: RDD[String] = lineRdd.flatMap(line => line.split(" "))

val wordToOneRdd: RDD[(String, Long)] = wordRdd.map { 
    word => {
        (word, System.currentTimeMillis())
    }
}

// 增加缓存,避免再重新跑一个 job 做 checkpoint wordToOneRdd.cache()
// 数据检查点：针对 wordToOneRdd 做检查点计算
wordToOneRdd.checkpoint()

// 触发执行逻辑
wordToOneRdd.collect().foreach(println)复制代码
```

##### 2.3 缓存和检查点关系

- Cache 缓存只是将数据保存起来，不切断RDD依赖关系。Checkpoint 检查点切断RDD依赖关系。
- Cache 缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint 的数据通常存储在HDFS 等容错、高可用的文件系统，可靠性高。
- 建议对checkpoint()的 RDD 使用Cache 缓存，这样 checkpoint 的 job 只需从 Cache 缓存中读取数据即可，否则需要再从头计算一次RDD。

### 任务三、Spark执行流程

#### **【任务目标】**

细分Spark执行流程，搞清楚各种组件对象之间的逻辑关系。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_2/image-20210824110318276.png?fileid=3270835009075153403)

视频-3、Spark执行流程

##### 3.1 执行流程

因为要理解执行计划，所以本案例以一个简单案例作为入门，就是我们的`WordCount`案例。

```
val sc = ...

val textRDD = sc.makeRDD(List("Hadoop Spark", "Hadoop Flume", "Spark Sqoop"))
val splitRDD = textRDD.flatMap(_.split(" "))
val tupleRDD = splitRDD.map((_, 1))
val reduceRDD = tupleRDD.reduceByKey(_ + _)
val strRDD = reduceRDD.map(item => s"${item._1}, ${item._2}")

println(strRDD.toDebugString)
strRDD.collect.foreach(item => println(item))复制代码
```

整个运行过程大致如下:

1. 通过代码的运行, 生成对应的 `RDD` 逻辑执行图
2. 通过 `Action` 操作, 根据逻辑执行图生成对应的物理执行图, 也就是 `Stage` 和 `Task`
3. 将物理执行图运行在集群中

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_2/image-20210823165604377.png)**提示**

> 其实 RDD 并没有什么严格的逻辑执行图和物理执行图的概念，这里也只是借用这个概念，从而让整个 RDD 的原理可以解释, 好理解。

##### 3.2 逻辑执行图

对于上面代码中的 `reduceRDD` 如果使用 `toDebugString` 打印调试信息的话, 会显式如下内容

```
(6) MapPartitionsRDD[4] at map at WordCount.scala:20 []
 |  ShuffledRDD[3] at reduceByKey at WordCount.scala:19 []
 +-(6) MapPartitionsRDD[2] at map at WordCount.scala:18 []
    |  MapPartitionsRDD[1] at flatMap at WordCount.scala:17 []
    |  ParallelCollectionRDD[0] at parallelize at WordCount.scala:16 []复制代码
```

根据这段内容，大致能得到这样的一张逻辑执行图

![20190515002803](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_2/20190515002803.png)

对于 RDD 的逻辑执行图，起始于第一个入口 RDD 的创建，结束于 Action 算子执行之前，主要的过程就是生成一组互相有依赖关系的 RDD，其并不会真的执行，只是表示 RDD 之间的关系，数据的流转过程。

在 `Action` 调用之前, 会生成一系列的 `RDD`, 这些 `RDD` 之间的关系, 其实就是整个逻辑计划。

例如上述代码, 如果生成逻辑计划的, 会生成如下一些 `RDD`, 这些 `RDD` 是相互关联的, 这些 `RDD` 之间，逻辑图其实本质上描述的就是**数据的计算过程**（数据从哪来, 经过什么样的计算, 得到什么样的结果, 再执行什么计算, 得到什么结果）

![20190519000019](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_2/20190519000019.png)

可是数据的计算是描述好了, 这种计算该如何执行呢?

##### 3.3 物理执行图

当触发 Action 执行的时候，这一组互相依赖的 RDD 要被处理，所以要转化为可运行的物理执行图，调度到集群中执行。

所以：物理图解决的其实就是 `RDD` 流程生成以后，如何计算和运行的问题，也就是如何把 RDD 放在集群中执行的问题。

因为大部分 RDD 是不真正存放数据的，只是数据从中流转，所以，不能直接在集群中运行 RDD，要有一种 Pipeline 的思想， 需要将这组 RDD 转为 Stage 和 Task, 从而运行 Task, 优化整体执行速度。

以上的逻辑执行图会生成如下的物理执行图, 这一切发生在 Action 操作被执行时.

![20190515235205](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_2/20190515235205.png)

从上图可以总结如下几个点

- ![image-20230211174348456](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_2/image-20230211174348456.png) 在第一个 `Stage` 中, 每一个这样的执行流程是一个 `Task`, 也就是在同一个 Stage 中的所有 RDD 的对应分区, 在同一个 Task 中执行。
- Stage 的划分是由 Shuffle 操作来确定的，有 Shuffle 的地方，Stage 断开。

##### 3.4 Job执行

当一个 `RDD` 调用了 `Action` 算子的时候, 在 `Action` 算子内部会创建 `Job`，通过`Job`来执行RDD。

`Job` 在 `RDD` 调用 `Action` 算子的时候生成, 而且调用一次 `Action` 算子, 就会生成一个 `Job`, 如果一个 `SparkApplication` 中调用了多次 `Action` 算子, 会生成多个 `Job` 串行执行, 每个 `Job` 独立运作, 被独立调度, 所以 `RDD` 的计算也会被执行多次。

`Job` 拆分为 `Stage` 和 `Task` 去调度分发和运行，所以，一个 `Job` 就是一个 `Spark` 程序从 `读取 → 计算 → 运行` 的过程。

整体流程如下：

![20190522015026](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_2/20190522015026.png)





# 实验2-3：Spark实战操作

## 实验概述

在之前的学习中，我们已经学习了 Spark 的基础编程方式，接下来，我们看看在实际的工作中如何使用这些 API 实现具体的需求。

## 实验环境

- AtStudy 实训平台
- Spark3

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16776609182462367.png)

## 实验目标

学习完成本实验后，您将能够

- 掌握SparkCore的实战分析过程
- 掌握热门商品的统计过程
- 掌握活跃用户的分析过程

## 实验任务

### 任务一、数据准备

#### **【任务目标】**

需求是电商网站的真实需求，所以在实现功能前，咱们必须先将数据准备好并且熟悉数据的结构。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_3/image-20210824110318276.png?fileid=3270835009074919265)

视频-1、数据准备

##### 1.1 数据结构

![image-20230213094505920](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_3/image-20230213094505920.png)

上面的数据图是从数据文件中截取的一部分内容，表示为电商网站的用户行为数据，主要包含用户的 4 种行为：搜索，点击，下单，支付。

##### 1.2 数据规则

- 数据文件中每行数据采用下划线分隔数据
- 每一行数据表示用户的一次行为，这个行为只能是 4 种行为的一种
- 如果搜索关键字为 null，表示数据不是搜索数据
- 如果点击的品类 ID 和产品 ID 为-1，表示数据不是点击数据
- 针对于下单行为，一次可以下单多个商品，所以品类 ID 和产品 ID 可以是多个，id 之间采用逗号分隔，如果本次不是下单行为，则数据采用 null 表示
- 支付行为和下单行为类似

##### 1.3 详细字段说明

| 编号 | 字段名称           | 字段类型 | 字段含义                     |
| ---- | ------------------ | -------- | ---------------------------- |
| 1    | date               | String   | 用户点击行为的日期           |
| 2    | user_id            | Long     | 用户的 ID                    |
| 3    | session_id         | String   | Session 的 ID                |
| 4    | page_id            | Long     | 某个页面的 ID                |
| 5    | action_time        | String   | 动作的时间点                 |
| 6    | search_keyword     | String   | 用户搜索的关键词             |
| 7    | click_category_id  | Long     | 某一个商品品类的 ID          |
| 8    | click_product_id   | Long     | 某一个商品的 ID              |
| 9    | order_category_ids | String   | 一次订单中所有品类的 ID 集合 |
| 10   | order_product_ids  | String   | 一次订单中所有商品的 ID 集合 |
| 11   | pay_category_ids   | String   | 一次支付中所有品类的 ID 集合 |
| 12   | pay_product_ids    | String   | 一次支付中所有商品的 ID 集合 |
| 13   | city_id            | Long     | 城市 id                      |

样例类：

```
//用户访问动作表
case class UserVisitAction(
    date: String,				//用户点击行为的日期
    user_id: Long,				//用户的 ID 
    session_id: String,			//Session的 ID 
    page_id: Long,				//某个页面的 ID 
    action_time: String,		//动作的时间点
    search_keyword: String,		//用户搜索的关键词
    click_category_id: Long,	//某一个商品品类的 ID 
    click_product_id: Long,		//某一个商品的 ID 
    order_category_ids: String,	//一次订单中所有品类的 ID 集合
    order_product_ids: String,	//一次订单中所有商品的 ID 集合
    pay_category_ids: String,	//一次支付中所有品类的 ID 集合
    pay_product_ids: String,	//一次支付中所有商品的 ID 集合
    city_id: Long
)//城市 id复制代码
```

### 任务二、热门商品

#### **【任务目标】**

获取热门品类的`Top10`。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_3/image-20210824110318276.png?fileid=3270835009074821497)

视频-2、热门商品

##### 2.1 需求说明

![image-20230213095311508](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_3/image-20230213095311508.png)

品类是指产品的分类，大型电商网站品类分多级，咱们的项目中品类只有一级，不同的公司可能对热门的定义不一样。我们按照每个品类的点击、下单、支付的量来统计热门品类。

鞋 点击数 下单数 支付数

衣服 点击数 下单数 支付数

电脑 点击数 下单数 支付数

例如，`综合排名 = 点击数 * 20% + 下单数 * 30% + 支付数 * 50%`

本项目需求优化为：先按照点击数排名，靠前的就排名高；如果点击数相同，再比较下单数；下单数再相同，就比较支付数。

##### 2.2 需求分析

实现过程主要分两步：

- 第一：首先、统计每个品类点击的次数、下单的次数和支付的次数：

  （品类，点击总数）（品类，下单总数）（品类，支付总数）

- 第二：然后、将各自的次数按照品类进行连接

- 第三：最后、排序并取前10个品类

##### 2.3 实现过程

```
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object HotCategoryTop10Analysis {

    def main(args: Array[String]): Unit = {
        // Top10热门品类
        val sparConf = new SparkConf().setMaster("local[*]").setAppName("HotCategoryTop10Analysis")
        val sc = new SparkContext(sparConf)

        // 1. 读取原始日志数据
        val actionRDD = sc.textFile("/opt/software/data/user_visit_action.txt")

        // 2. 统计品类的点击数量：（品类ID，点击数量）
        val clickActionRDD = actionRDD.filter(
            action => {
                val datas = action.split("_")
                datas(6) != "-1"
            }
        )

        val clickCountRDD: RDD[(String, Int)] = clickActionRDD.map(
            action => {
                val datas = action.split("_")
                (datas(6), 1)
            }
        ).reduceByKey(_ + _)

        // 3. 统计品类的下单数量：（品类ID，下单数量）
        val orderActionRDD = actionRDD.filter(
            action => {
                val datas = action.split("_")
                datas(8) != "null"
            }
        )

        // orderid => 1,2,3
        // 【(1,1)，(2,1)，(3,1)】
        val orderCountRDD = orderActionRDD.flatMap(
            action => {
                val datas = action.split("_")
                val cid = datas(8)
                val cids = cid.split(",")
                cids.map(id=>(id, 1))
            }
        ).reduceByKey(_+_)

        // 4. 统计品类的支付数量：（品类ID，支付数量）
        val payActionRDD = actionRDD.filter(
            action => {
                val datas = action.split("_")
                datas(10) != "null"
            }
        )

        // orderid => 1,2,3
        // 【(1,1)，(2,1)，(3,1)】
        val payCountRDD = payActionRDD.flatMap(
            action => {
                val datas = action.split("_")
                val cid = datas(10)
                val cids = cid.split(",")
                cids.map(id=>(id, 1))
            }
        ).reduceByKey(_+_)

        // 5. 将品类进行排序，并且取前10名
        //    点击数量排序，下单数量排序，支付数量排序
        //    元组排序：先比较第一个，再比较第二个，再比较第三个，依此类推
        //    ( 品类ID, ( 点击数量, 下单数量, 支付数量 ) )
        //
        //  cogroup = connect + group
        val cogroupRDD: RDD[(String, (Iterable[Int], Iterable[Int], Iterable[Int]))] =
            clickCountRDD.cogroup(orderCountRDD, payCountRDD)
        val analysisRDD = cogroupRDD.mapValues{
            case ( clickIter, orderIter, payIter ) => {

                var clickCnt = 0
                val iter1 = clickIter.iterator
                if ( iter1.hasNext ) {
                    clickCnt = iter1.next()
                }
                var orderCnt = 0
                val iter2 = orderIter.iterator
                if ( iter2.hasNext ) {
                    orderCnt = iter2.next()
                }
                var payCnt = 0
                val iter3 = payIter.iterator
                if ( iter3.hasNext ) {
                    payCnt = iter3.next()
                }

                ( clickCnt, orderCnt, payCnt )
            }
        }

        val resultRDD = analysisRDD.sortBy(_._2, false).take(10)

        // 6. 将结果采集到控制台打印出来
        resultRDD.foreach(println)

        sc.stop()
    }
}复制代码
```

### 任务三、活跃用户

#### **【任务目标】**

获取热门品类中每个品类的`Top10`活跃Session统计。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab2_3/image-20210824110318276.png?fileid=3270835009073991149)

视频-3、活跃用户

##### 3.1 需求说明

在需求一的基础上，增加每个品类用户 session 的点击统计，就是在热门品类中统计出点击量前10的session用户。

实现步骤：

- 首先、获取Top10热门品类
- 然后、根据热门品类ID，筛选出点击量
- 再次，根据品类ID和sessionid进行点击量的统计
- 最后、根据品类分组，排序取Top10的sessionid

##### 2.2 实现过程

```
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

object Req2_HotCategoryTop10SessionAnalysis {
    def main(args: Array[String]): Unit = {
        // TODO : Top10热门品类
        val sparConf = new SparkConf().setMaster("local[*]").setAppName("HotCategoryTop10Analysis")
        val sc = new SparkContext(sparConf)

        val actionRDD = sc.textFile("datas/user_visit_action.txt")
        actionRDD.cache()
        val top10Ids: Array[String] = top10Category(actionRDD)

        // 1. 过滤原始数据,保留点击和前10品类ID
        val filterActionRDD = actionRDD.filter(
            action => {
                val datas = action.split("_")
                if ( datas(6) != "-1" ) {
                    top10Ids.contains(datas(6))
                } else {
                    false
                }
            }
        )

        // 2. 根据品类ID和sessionid进行点击量的统计
        val reduceRDD: RDD[((String, String), Int)] = filterActionRDD.map(
            action => {
                val datas = action.split("_")
                ((datas(6), datas(2)), 1)
            }
        ).reduceByKey(_ + _)

        // 3. 将统计的结果进行结构的转换
        //  (（ 品类ID，sessionId ）,sum) => ( 品类ID，（sessionId, sum） )
        val mapRDD = reduceRDD.map{
            case ( (cid, sid), sum ) => {
                ( cid, (sid, sum) )
            }
        }

        // 4. 相同的品类进行分组
        val groupRDD: RDD[(String, Iterable[(String, Int)])] = mapRDD.groupByKey()

        // 5. 将分组后的数据进行点击量的排序，取前10名
        val resultRDD = groupRDD.mapValues(
            iter => {
                iter.toList.sortBy(_._2)(Ordering.Int.reverse).take(10)
            }
        )

        resultRDD.collect().foreach(println)


        sc.stop()
    }
    def top10Category(actionRDD:RDD[String]) = {
        val clickActionRDD = actionRDD.filter(
            action => {
                val datas = action.split("_")
                datas(6) != "-1"
            }
        )

        val clickCountRDD: RDD[(String, Int)] = clickActionRDD.map(
            action => {
                val datas = action.split("_")
                (datas(6), 1)
            }
        ).reduceByKey(_ + _)

        val orderActionRDD = actionRDD.filter(
            action => {
                val datas = action.split("_")
                datas(8) != "null"
            }
        )
        val orderCountRDD = orderActionRDD.flatMap(
            action => {
                val datas = action.split("_")
                val cid = datas(8)
                val cids = cid.split(",")
                cids.map(id=>(id, 1))
            }
        ).reduceByKey(_+_)

        val payActionRDD = actionRDD.filter(
            action => {
                val datas = action.split("_")
                datas(10) != "null"
            }
        )

        val payCountRDD = payActionRDD.flatMap(
            action => {
                val datas = action.split("_")
                val cid = datas(10)
                val cids = cid.split(",")
                cids.map(id=>(id, 1))
            }
        ).reduceByKey(_+_)

        val cogroupRDD: RDD[(String, (Iterable[Int], Iterable[Int], Iterable[Int]))] = clickCountRDD.cogroup(orderCountRDD, payCountRDD)
        val analysisRDD = cogroupRDD.mapValues{
            case ( clickIter, orderIter, payIter ) => {

                var clickCnt = 0
                val iter1 = clickIter.iterator
                if ( iter1.hasNext ) {
                    clickCnt = iter1.next()
                }
                var orderCnt = 0
                val iter2 = orderIter.iterator
                if ( iter2.hasNext ) {
                    orderCnt = iter2.next()
                }
                var payCnt = 0
                val iter3 = payIter.iterator
                if ( iter3.hasNext ) {
                    payCnt = iter3.next()
                }

                ( clickCnt, orderCnt, payCnt )
            }
        }

        analysisRDD.sortBy(_._2, false).take(10)
    }
}
```





# 实验3-1：SparkSQL核心编程

## 实验概述

SparkSQL是什么，SparkSQL如何使用以及SparkSQL的核心内容。

## 实验环境

- AtStudy 实训平台
- Spark3

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16776609637943527.png)

## 实验目标

学习完成本实验后，您将能够

- 了解SparkSQL
- 掌握SparkSQL的基本使用
- 掌握SparkSQL中DataSet和DataFrame数据结构

## 实验任务

### 任务一、SparkSQL介绍

#### **【任务目标】**

了解SparkSQL出现的原因，SparkSQL是什么。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab3_1/image-20210824110318276.png?fileid=3270835009224362270)

视频-1、SparkSQL介绍

##### 1.1 数据分析的方式

数据分析的方式大致上可以划分为 `SQL` 和 命令式两种

- 命令式

  在前面的 `RDD` 部分，非常明显可以感觉的到是命令式的，主要特征是通过一个算子，可以得到一个结果，通过结果再进行后续计算。

  ```
  sc.textFile("...")
    .flatMap(_.split(" "))
    .map((_, 1))
    .reduceByKey(_ + _)
    .collect()复制代码
  ```

  命令式的优点

  - 操作粒度更细，能够控制数据的每一个处理环节
  - 操作更明确，步骤更清晰，容易维护
  - 支持非结构化数据的操作

  命令式的缺点

  - 需要一定的代码功底
  - 写起来比较麻烦

- SQL

  对于一些数据科学家，要求他们为了做一个非常简单的查询，写一大堆代码，明显是一件非常残忍的事情，所以 `SQL on Hadoop` 是一个非常重要的方向。

  ```
  SELECT
      name,
      age,
      school
  FROM students
  WHERE age > 10复制代码
  ```

  SQL 的优点

  - 表达非常清晰，比如说这段 `SQL` 明显就是为了查询三个字段，又比如说这段 `SQL` 明显能看到是想查询年龄大于 10 岁的条目

  SQL 的缺点

  - 想想一下 3 层嵌套的 `SQL`，维护起来应该挺力不从心的吧

`SQL` 擅长数据分析和通过简单的语法表示查询，命令式操作适合过程式处理和算法性的处理。在 `Spark` 出现之前，对于结构化数据的查询和处理，一个工具一向只能支持 `SQL` 或者命令式，使用者被迫要使用多个工具来适应两种场景，并且多个工具配合起来比较费劲。

而 `Spark` 出现了以后，统一了两种数据处理范式，是一种革新性的进步。

##### 1.2 SparkSQL 是什么

因为 `SQL` 是数据分析领域一个非常重要的范式，所以 `Spark` 一直想要支持这种范式，而伴随着一些决策失误，这个过程其实还是非常曲折的。

![7a1cdf107b8636713c2502a99d058061](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab3_1/7a1cdf107b8636713c2502a99d058061.png)

**1、Hive**

解决的问题

- `Hive` 实现了 `SQL on Hadoop`，使用 `MapReduce` 执行任务
- 简化了 `MapReduce` 任务

新的问题

- `Hive` 的查询延迟比较高，原因是**使用 `MapReduce` 做调度**

**2、Shark**

- 解决的问题

  `Shark` 改写 `Hive` 的物理执行计划，使**用 `Spark` 作业代替 `MapReduce` 执行物理计划**使用列式内存存储以上两点使得 `Shark` 的查询效率很高。

- 新的问题

  `Shark` 重用了 `Hive` 的 `SQL` 解析，逻辑计划生成以及优化，所以其实可以认为 `Shark` 只是把 `Hive` 的物理执行替换为了 `Spark` 作业执行计划的生成严重依赖 `Hive`，想要增加新的优化非常困难`Hive` 使用 `MapReduce` 执行作业，**所以 `Hive` 是进程级别的并行**, 而 `Spark` 是线程级别的并行， 所以 `Hive` 中很多线程不安全的代码不适用于 `Spark`。

由于以上问题, `Shark` 维护了 `Hive` 的一个分支, 并且无法合并进主线, 难以为继

**3、SparkSQL**

- 解决的问题
  - `Spark SQL` 使用 `Hive` 解析 `SQL` 生成 `AST` 语法树，将其后的逻辑计划生成、优化、物理计划都自己完成，而不依赖 `Hive`
  - 执行计划和优化交给优化器 `Catalyst`
  - 内建了一套简单的 `SQL` 解析器，可以不使用 `HQL`，此外，还引入和 `DataFrame` 这样的 `DSL API`，完全可以不依赖任何 `Hive` 的组件
  - `Shark` 只能查询文件，`Spark SQL` 可以直接降查询作用于 `RDD`， 这一点是一个大进步
- 新的问题
  - 对于初期版本的 `SparkSQL`，依然有挺多问题， 例如只能支持 `SQL` 的使用，不能很好的兼容命令式，入口不够统一等

**4、Dataset**

`SparkSQL` 在 `2.0` 时代，增加了一个新的 `API`，叫做 `Dataset`，`Dataset` 统一和结合了 `SQL` 的访问和命令式 `API` 的使用，这是一个划时代的进步

在 `Dataset` 中可以轻易的做到使用 `SQL` 查询并且筛选数据，然后使用命令式 `API` 进行探索式分析。

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab3_1/image-20210823165604377.png)**总结：SparkSQL是什么？**

> SparkSQL 是一个为了支持 SQL 而设计的工具，但同时也支持命令式的 API

### 任务二、 SparkSQL 初体验

#### **【任务目标】**

了解 `SparkSQL` 的 `API` 由哪些部分组成。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab3_1/image-20210824110318276.png?fileid=3270835009224355991)

视频-2、 SparkSQL 初体验

##### 2.1 RDD 版本的 WordCount

```
scala> val config = new SparkConf().setAppName("WordCount").setMaster("local[*]")
scala> val sc = new SparkContext(config)

scala> sc.textFile("/opt/software/data/word.txt")
    .flatMap(_.split(" "))
    .map((_, 1))
    .reduceByKey(_ + _)
    .collect复制代码
```

`RDD` 版本的代码有一个非常明显的特点，就是它所处理的数据是基本类型的，在算子中对整个数据进行处理。

##### 2.2 命令式 API 的入门案例

![image-20230213133953987](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab3_1/image-20230213133953987.png)

```
scala> case class Student(name: String, age: Int)

scala> val studentRDD = spark.sparkContext.makeRDD(List(Student("tom", 9), Student("jerry", 15)))

//	SparkSQL 中有一个新的类型叫做 Dataset
scala> val studentDS = studentRDD.toDS()    

//	SparkSQL 有能力直接通过字段名访问数据集, 说明 SparkSQL 的 API 中是携带 Schema 信息的
scala> val res = studentDS.where('age > 10)     
    .where('age < 20)
    .select('name)
    .as[String]

scala> res.show()复制代码
```

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab3_1/image-20210823165604377.png)**Spark对象：SparkSession**

![image-20230213134407337](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab3_1/image-20230213134407337.png)

> SparkSQL 中的入口点, 叫做 SparkSession，特点如下：
>
> - 能够整合 `SQLContext`, `HiveContext`, `SparkContext`, `StreamingContext` 等不同的入口点
> - 支持多个数据源，有完善的读取和写入体系
> - 并且能够向下兼容
> - 构建方式：

```
//创建上下文环境配置对象
val conf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("WordCount")

//创建 SparkSession 对象
val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()复制代码
```

##### 2.3 SQL 版本 WordCount

```
scala> val studentRDD = spark.sparkContext.makeRDD(List(Student("zhangsan", 9), Student("lisi", 15)))

scala> val studentDS = studentRDD.toDS()
scala> studentDS.createOrReplaceTempView("student")

scala> val res = spark.sql("select name from student where age > 10 and age < 20")

/*
+----+
|name|
+----+
|lisi|
+----+
 */
scala> res.show()复制代码
```

![image-20230213140144866](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab3_1/image-20230213140144866.png)

##### 2.4 总结：

`SparkSQL` 提供了 `SQL` 和 命令式 `API` 两种不同的访问结构化数据的形式, 并且它们之间可以无缝的衔接

命令式 `API` 由一个叫做 `Dataset` 的组件提供，其还有一个变形, 叫做 `DataFrame`

### 任务三、 SparkSQL 核心

#### **【任务目标】**

重点是掌握Dataset和Dataframe。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab3_1/image-20210824110318276.png?fileid=3270835009223364696)

视频-3、SparkSQL 核心

##### 2.1 Dataset 是什么？

```
scala> val dataset: Dataset[Student] = spark.createDataset(List(Student("tom", 9), Student("jerry", 15)))

// 方式1: 通过对象来处理
scala> dataset.filter(item => item.age > 10).show()

// 方式2: 通过字段来处理
scala> dataset.filter('age > 10).show()

// 方式3: 通过类似SQL的表达式来处理
scala> dataset.filter("age > 10").show()复制代码
```

- **问题1**: `Student` 是什么?

  `Student` 是一个强类型的类

- **问题2**: 这个 `Dataset` 中是结构化的数据吗?

  非常明显是的，因为 `Student` 对象中有结构信息，例如字段名和字段类型

- **问题3**: 这个 `Dataset` 能够使用类似 `SQL` 这样声明式结构化查询语句的形式来查询吗?

  当然可以，已经演示过了

- **问题4**: `Dataset` 是什么?

  Dataset 是一个强类型，并且类型安全的数据容器，并且提供了结构化查询 `API` 和类似 `RDD` 一样的命令式 `API`

###### 2.1.1 RDD转换为DataSet

SparkSQL 也能够自动将包含有 case 类的 RDD 转换成DataSet，case 类定义了 table 的结构，case 类属性通过反射变成了表的列名。case 类可以包含诸如 Seq 或者 Array 等复杂的结构。

```
scala> case class User(name:String, age:Int) 
defined class User

scala> val rdd = sc.makeRDD(List(("zhangsan",30), ("lisi",49)))
rdd: org.apache.spark.rdd.RDD[(String, Int)]

scala> val ds = rdd.map(t=>User(t._1, t._2)).toDS
res12: org.apache.spark.sql.Dataset[User] = [name: string, age: int]复制代码
```

###### 2.1.2 DataSet转换为RDD

DataSet 其实也是对 RDD 的封装，所以可以直接获取内部的RDD

```
scala> case class User(name:String, age:Int) defined class User

scala> val rdd1 = sc.makeRDD(List(("zhangsan",30), ("lisi",49)))
rdd: org.apache.spark.rdd.RDD[(String, Int)]

scala> val ds = rdd1.map(t=>User(t._1, t._2)).toDS
res12: org.apache.spark.sql.Dataset[User] = [name: string, age: int]

scala> val rdd2 = res12.rdd
rdd: org.apache.spark.rdd.RDD[User] = MapPartitionsRDD[51] at rdd at
<console>:25

scala> rdd2.collect
res12: Array[User] = Array(User(zhangsan,30), User(lisi,49))复制代码
```

##### 2.2 DataFrame是什么？

DataFrame 是 `SparkSQL` 中一个表示关系型数据库中 `表` 的函数式抽象，其作用是让 `Spark` 处理大规模结构化数据的时候更加容易。

一般 `DataFrame` 可以处理类似于表结构的数据，因为这种数据可以获取到 `Schema` 信息。也就是说 `DataFrame` 中有 `Schema` 信息，可以像操作表一样操作 DataFrame。

![eca0d2e1e2b5ce678161438d87707b61](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab3_1/eca0d2e1e2b5ce678161438d87707b61.png)

`DataFrame` 由两部分构成，一是 `row` 的集合，每个 `row` 对象表示一个行，二是描述 `DataFrame` 结构的 `Schema`。

![238c241593cd5b0fd06d4d74294680e2](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab3_1/238c241593cd5b0fd06d4d74294680e2.png)

`DataFrame` 支持 `SQL` 中常见的操作，例如： `select`, `filter`, `join`, `group`, `sort`, `join` 等

```
scala> val peopleDF: DataFrame = List(People("zhangsan", 15), People("lisi", 15)).toDF()

/*
root
|-- name: string(nullable = true)
|-- age: integer(nullable = false)
*/
scala> peopleDF.printSchema

/*
+---+-----+
|age|count|
+---+-----+
| 15|    2|
+---+-----+
 */
scala> peopleDF.groupBy('age)
  .count()
  .show()复制代码
```

###### 2.2.1 创建DataFrame

1、通过集合创建 `DataFrame`

集合中不仅可以包含样例类，也可以只有普通数据类型，后通过指定列名来创建

```
scala> val df1: DataFrame = List("nihao", "hello").toDF("text")

/*
+-----+
| text|
+-----+
|nihao|
|hello|
+-----+
 */
scala> df1.show()

scala> val df2: DataFrame = List(("a", 1), ("b", 1)).toDF("word", "count")

/*
+----+-----+
|word|count|
+----+-----+
|   a|    1|
|   b|    1|
+----+-----+
 */
scala> df2.show()复制代码
```

2、读取外部文件方式创建 `DataFrame`

创建json文件存储数据如下：

```
{"name":"jerry", "age":20, "regyear":"2022", "regmonth":"5"}
{"name":"tom", "age":19, "regyear":"2021", "regmonth":"10"}
{"name":"kitty", "age":22, "regyear":"2022", "regmonth":"5"}复制代码
```

读取json文件

```
scala> val df = spark.read.json("data/user.json")
df: org.apache.spark.sql.DataFrame = [age: bigint， name: string ...2 more fields ]

scala> df.show()

scala> df.printSchema()
/*
root
|-- age: long(nullable = true)
|-- name: string(nullable = true)
|-- regmonth: long(nullable = true)
|-- regyear: long(nullable = true)
*/复制代码
```

###### 2.2.2 需求: 查看每个月注册的用户数量

**Step 1**: 首先可以打印`DataFrame` 的 `Schema`, 查看其中所包含的列, 以及列的类型

```
scala> val df = spark.read.json("data/user.json")
df: org.apache.spark.sql.DataFrame = [age: bigint， name: string ...2 more fields ]

df.printSchema()复制代码
```

**Step 2**: 对于大部分计算来说，可能不会使用所有的列，所以可以选择其中某些重要的列

```
scala> df.select('regyear, 'regmonth, 'name)复制代码
```

**Step 3**: 可以针对某些列进行分组，后对每组数据通过函数做聚合

```
scala> df.select('regyear, 'regmonth, 'name).where('name=!='Na').groupBy('regyear, 'regmonth).count().show()
/*
+---------+----------+------+
| regyear | regmonth | count|
+---------+----------+------+
|     2022|         5|     2|
|     2021|        10|     1|
+---------+----------+------+
*/复制代码
```

**使用** `SQL` **操作** `DataFrame`

使用 `SQL` 来操作某个 `DataFrame` 的话, `SQL` 中必须要有一个 `from` 子句, 所以需要先将 `DataFrame` 注册为一张临时表

```
scala> val df = spark.read.json("data/user.json")

// 对 DataFrame 创建一个临时表
scala> df.createOrReplaceTempView("temp_table")

scala> spark.sql("select regyear, regmonth, count(*) from temp_table where name != 'NA' group by regyear, regmonth")
  .show()复制代码
```

##### 2.3 RDD、DataFrame、DataSet关系

在 SparkSQL 中 Spark 为我们提供了两个新的抽象，分别是 DataFrame 和 DataSet。他们和 RDD 有什么区别呢？首先从版本的产生上来看：

- Spark1.0 => RDD
- Spark1.3 => DataFrame
- Spark1.6 => Dataset

如果同样的数据都给到这三个数据结构，他们分别计算之后，都会给出相同的结果。不同是的他们的执行效率和执行方式。在后期的 Spark 版本中，DataSet 有可能会逐步取代RDD和 DataFrame 成为唯一的API 接口。

具体的关系如下：

- **第一**：RDD 不支持 SparkSQL

- **第二**：`DataFrame` 就是 `Dataset`

  确实，这两个组件是同一个东西，`DataFrame` 是 `Dataset` 的一种特殊情况, 也就是说 `DataFrame` 是 `Dataset[Row]` 的别名， 例如：`type DataFrame = Dataset[Row]`

- **第三**：`DataFrame` 和`Dataset` 所表达的语义不

  `DataFrame` 表达的含义是一个支持函数式操作的 `表`,存放的是Row对象， 而 `Dataset` 表达是是一个类似 `RDD` 的东西，`Dataset` 可以处理任何对象

- 第四：`DataFrame` 和`Dataset` 之间可以非常简单的相互转换

  ```
  val df: DataFrame = List(User("zhangsan", 15), User("lisi", 15)).toDF()
  val ds_fdf: Dataset[User] = df.as[User]
  
  val ds: Dataset[User] = List(User("zhangsan", 15), User("lisi", 15)).toDS()
  val df_fds: DataFrame = ds.toDF()复制代码
  ```





# 实验3-2：SparkSQL实战案例

## 实验概述

IDEA在实际中是应用比较广泛的开发工具，SparkSQL也不例外，基于此我们还需要掌握数据的加载和保存，并能根据具体的业务需要实现SparkSQL的应用开发

## 实验环境

- AtStudy 实训平台
- Spark3

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16776610125788370.png)

## 实验目标

学习完成本实验后，您将能够

- 掌握IDEA中SparkSQL的开发过程
- 掌握各种数据的加载和保存
- 并能在实际场景中应用SparkSQL

## 实验任务

### 任务一、IDEA开发Spark SQL

#### **【任务目标】**

实际开发中，都是使用 IDEA 进行开发的，所以本任务的重点是掌握如何用IDEA开发SparkSQL应用。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab3_2//image-20210824110318276.png?fileid=3270835009225810941)

视频-1、IDEA开发Spark SQL

##### 1.1 创建Maven工程

该过程和之前一样，此处省略一千字。。。

##### 1.2 添加依赖

需要添加`spark_sql_2.12`的依赖。

```
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.atstudy</groupId>
    <artifactId>Spark2.0</artifactId>
    <version>1.0-SNAPSHOT</version>

    <dependencies>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.12</artifactId>
            <version>3.0.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.12</artifactId>
            <version>3.0.0</version>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>3.2.2</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>testCompile</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-assembly-plugin</artifactId>
                <version>3.1.0</version>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>复制代码
```

##### 1.3 代码实现

```
object SparkSQL_Demo {
    def main(args: Array[String]): Unit = {
        //创建上下文环境配置对象
        val conf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkSQL_Demo")

        //创建 SparkSession 对象
        val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate()
        //RDD=>DataFrame=>DataSet 转换需要引入隐式转换规则，否则无法转换
        //spark 不是包名，是上下文环境对象名
        import spark.implicits._

        //读取 json 文件 创建 DataFrame {"username": "lisi","age": 18} 
        val df: DataFrame = spark.read.json("input/test.json")
        //df.show()

        //SQL 风格语法
        df.createOrReplaceTempView("user")
        //spark.sql("select avg(age) from user").show

        //DSL 风格语法
        //df.select("username","age").show()

        //RDD=>DataFrame=>DataSet
        //RDD
        val rdd1: RDD[(Int, String, Int)] = spark.sparkContext.makeRDD(List((1,"zhangsan",30),(2,"lisi",28),(3,"wangwu", 20)))

        //DataFrame
        val df1: DataFrame = rdd1.toDF("id","name","age")
        //df1.show()

        //DateSet
        val ds1: Dataset[User] = df1.as[User]
        //ds1.show()

        //DataSet=>DataFrame=>RDD
        //DataFrame
        val df2: DataFrame = ds1.toDF()

        //RDD 返回的 RDD 类型为 Row，里面提供的 getXXX 方法可以获取字段值，类似 jdbc 处理结果集， 但是索引从 0 开始
        val rdd2: RDD[Row] = df2.rdd
        //释放资源
        spark.stop()
    }
}
case class User(id:Int,name:String,age:Int)复制代码
```

### 任务二、数据加载和保存

#### **【任务目标】**

实际开发中，都是使用 IDEA 进行开发的，所以本任务的重点是掌握如何用IDEA开发SparkSQL应用。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab3_2//image-20210824110318276.png?fileid=3270835009224605902)

视频-2、数据加载和保存

##### 2.1 加载和保存方式

SparkSQL 提供了通用的保存数据和数据加载的方式。

这里的通用指的是使用相同的API，根据不同的参数读取和保存不同格式的数据，SparkSQL 默认读取和保存的文件格式为 parquet。

###### 2.1.1 加载数据

`spark.read.load`是加载数据的通用方法

```
scala> spark.read.format("…")[.option("…")].load("…")	复制代码
```

- `format("…")`：指定加载的数据类型，包括`csv`、`jdbc`、`json`、`parquet`和`textFile`。

- ```
  load("…")
  ```

  ：在

  ```
  csv
  ```

  、

  ```
  jdbc
  ```

  、

  ```
  json
  ```

  、

  ```
  parquet
  ```

  和

  ```
  textFile
  ```

  格式下需要传入加载数据的路径。

  - `option("…")`：在`jdbc`格式下需要传入 JDBC 相应参数，`url`、`user`、`password` 和 `dbtable`

###### 2.1.2 保存数据

`df.write.save` 是保存数据的通用方法

```
scala> df.write.
csv jdbc json orc	parquet textFile… …
scala> df.write.format("…")[.option("…")].save("…")复制代码
```

如果保存不同格式的数据，可以对不同的数据格式进行设定

- format("…")：指定保存的数据类型，包括`csv`、`jdbc`、`json`、`parquet`和`textFile`。
- save ("…")：在`csv`、`parquet`和`textFile`格式下需要传入保存数据的路径。
- option("…")：在`jdbc`格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab3_2//image-20210823165604377.png)**Parquet格式**

> Spark SQL 的默认数据源为 Parquet 格式。
>
> Parquet 是一种能够有效存储嵌套数据的列式存储格式。数据源为 Parquet 文件时，Spark SQL 可以方便的执行所有的操作，不需要使用 format。修改配置项`spark.sql.sources.default`，可修改默认数据源格式。

##### 2.2 JSON

Spark SQL 能够自动推测 `JSON `数据集的结构，并将它加载为一个`Dataset[Row]`。可以通过 `SparkSession.read.json()`去加载 JSON 文件

```
{"name": "Michael"}
{"name": "Andy", "age":30}
[{"name": "Justin"， "age": 19},{"name": "Justin"， "age": 19}]复制代码
```

- 1、导入隐式转换

  ```
  import spark.implicits._复制代码
  ```

- 2、加载 JSON 文件

  ```
  val path = "/opt/module/spark-local/people.json"
  val peopleDF = spark.read.json(path)复制代码
  ```

- 3、创建临时表

  ```
  peopleDF.createOrReplaceTempView("people")复制代码
  ```

- 4、 数据查询

  ```
  val teenagerNamesDF = spark.sql("SELECT	name FROM people WHERE age BETWEEN 13 AND 19")
  teenagerNamesDF.show()
  +------+
  | name|
  +------+
  |Justin|
  +------+复制代码
  ```

##### 2.3 MySQL

Spark SQL 也可以通过 JDBC 从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame 一系列的计算后，还可以将数据再写回关系型数据库中。

如果使用 spark-shell 操作，可在启动shell 时指定相关的数据库驱动路径或者将相关的数据库驱动放到 spark 的类路径下。

```
bin/spark-shell --jars mysql-connector-java-5.1.27-bin.jar复制代码
```

我们这里只演示在Idea 中通过 JDBC 对 MySQL 进行操作

1） 导入依赖

```
<dependency>
    <groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
    <version>5.1.27</version>
</dependency>复制代码
```

2） 读取数据

```
val conf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkSQL")

//创建 SparkSession 对象
val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate() 

import spark.implicits._

//方式 1：通用的 load 方法读取
spark.read.format("jdbc")
    .option("url", "jdbc:mysql://localhost:3306/spark-sql")
    .option("driver", "com.mysql.jdbc.Driver")
    .option("user", "root")
    .option("password", "123123")
    .option("dbtable", "user")
    .load().show


//方式 2:通用的 load 方法读取 参数另一种形式
spark.read.format("jdbc")
    .options(Map("url"->"jdbc:mysql://localhost:3306/spark-sql?user=root&password= 123123","dbtable"->"user","driver"->"com.mysql.jdbc.Driver")).load().show

//方式 3:使用 jdbc 方法读取
val props: Properties = new Properties()
props.setProperty("user", "root") 
props.setProperty("password", "123123")
val df: DataFrame = spark.read.jdbc("jdbc:mysql://localhost:3306/spark-sql", "user", props)
df.show

//释放资源
spark.stop()复制代码
```

3）写入数据

```
case class User2(name: String, age: Long)
。。。
val conf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkSQL")

//创建 SparkSession 对象
val spark: SparkSession = SparkSession.builder().config(conf).getOrCreate() 

import spark.implicits._

val rdd: RDD[User2] = spark.sparkContext.makeRDD(List(User2("lisi", 20), User2("zs", 30)))
val ds: Dataset[User2] = rdd.toDS
//方式 1：通用的方式 format 指定写出类型
ds.write
    .format("jdbc")
    .option("url", "jdbc:mysql://localhost:3306/spark-sql")
    .option("user", "root")
    .option("password", "123123")
    .option("dbtable", "user")
    .mode(SaveMode.Append)
    .save()

//方式 2：通过 jdbc 方法
val props: Properties = new Properties() 
props.setProperty("user", "root") 
props.setProperty("password", "123123")
ds.write.mode(SaveMode.Append).jdbc("jdbc:mysql://localhost:3306/spark-sql", "user", props)

//释放资源
spark.stop()复制代码
```

### 任务三、货品交易数据分析

#### **【任务目标】**

根据货品交易数据集实现相应的数据分析。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab3_2//image-20210824110318276.png?fileid=3270835009223532696)

视频-3、货品交易数据分析

##### 3.1 数据说明

数据集是货品交易数据集

- tbDate数据

| 字段名    | 类型    | 备注   |
| --------- | ------- | ------ |
| dateid    | date    | 日期   |
| years     | varchar | 年月   |
| theyear   | varchar | 年     |
| month     | varchar | 月     |
| day       | varchar | 日     |
| weekday   | varchar | 周几   |
| week      | varchar | 第几周 |
| quarter   | varchar | 季度   |
| period    | varchar | 旬     |
| halfmonth | varchar | 半月   |

- tbStockDetail数据

| 字段名      | 类型    | 备注   |
| ----------- | ------- | ------ |
| ordernumber | varchar | 订单号 |
| rownum      | varchar | 行号   |
| itemid      | varchar | 货品   |
| number      | varchar | 数量   |
| price       | varchar | 单价   |
| amount      | int     | 销售额 |

- tbStock数据

| 字段名      | 类型    | 备注     |
| ----------- | ------- | -------- |
| ordernumber | varchar | 订单号   |
| locationid  | varchar | 交易位置 |
| dateid      | date    | 交易日期 |

每个订单可能包含多个货品，每个订单可以产生多次交易，不同的货品有不同的单价

- 加载数据，并将数据转换成样例类对象。

```
scala> case class tbStock(ordernumber:String,locationid:String,dateid:String)

scala> val tbStockRdd = spark.sparkContext.textFile("/opt/software/data/tbStock.txt")

scala> val tbStockDS = tbStockRdd.map(_.split(",")).map(attr=>tbStock(attr(0),attr(1),attr(2))).toDS

scala> tbStockDS.show()
+------------+----------+---------+
| ordernumber|locationid| dataid|
+------------+----------+---------+
|BYSL00000893| ZHAO|2007-8-23|
|BYSL00000897| ZHAO|2007-8-24|
|BYSL00000898| ZHAO|2007-8-25|
|BYSL00000899| ZHAO|2007-8-26|
|BYSL00000900| ZHAO|2007-8-26|
|BYSL00000901| ZHAO|2007-8-27|
|BYSL00000902| ZHAO|2007-8-27|
|BYSL00000904| ZHAO|2007-8-28|
|BYSL00000905| ZHAO|2007-8-28|
|BYSL00000906| ZHAO|2007-8-28|
|BYSL00000907| ZHAO|2007-8-29|
|BYSL00000908| ZHAO|2007-8-30|
|BYSL00000909| ZHAO| 2007-9-1|
|BYSL00000910| ZHAO| 2007-9-1|
|BYSL00000911| ZHAO|2007-8-31|
|BYSL00000912| ZHAO| 2007-9-2|
|BYSL00000913| ZHAO| 2007-9-3|
|BYSL00000914| ZHAO| 2007-9-3|
|BYSL00000915| ZHAO| 2007-9-4|
|BYSL00000916| ZHAO| 2007-9-4|
+------------+----------+---------+复制代码
scala> case class tbStockDetail(ordernumber:String, rownum:Int, itemid:String, number:Int, price:Double, amount:Double)

scala> val tbStockDetailRdd = spark.sparkContext.textFile("/opt/software/data/tbStockDetail.txt")

scala> val tbStockDetailDS = tbStockDetailRdd.map(_.split(",")).map(attr=> tbStockDetail(attr(0),attr(1).trim().toInt,attr(2),attr(3).trim().toInt,attr(4).trim().toDouble, attr(5).trim().toDouble)).toDS

scala> tbStockDetailDS.show()
+------------+------+--------------+------+-----+------+
| ordernumber|rownum|        itemid|number|price|amount|
+------------+------+--------------+------+-----+------+
|BYSL00000893|     0|FS527258160501|    -1|268.0|-268.0|
|BYSL00000893|     1|FS527258169701|     1|268.0| 268.0|
|BYSL00000893|     2|FS527230163001|     1|198.0| 198.0|
|BYSL00000893|     3|24627209125406|     1|298.0| 298.0|
|BYSL00000893|     4|K9527220210202|     1|120.0| 120.0|
|BYSL00000893|     5|01527291670102|     1|268.0| 268.0|
|BYSL00000893|     6|QY527271800242|     1|158.0| 158.0|
|BYSL00000893|     7|ST040000010000|     8|  0.0|   0.0|
|BYSL00000897|     0|04527200711305|     1|198.0| 198.0|
|BYSL00000897|     1|MY627234650201|     1|120.0| 120.0|
|BYSL00000897|     2|01227111791001|     1|249.0| 249.0|
|BYSL00000897|     3|MY627234610402|     1|120.0| 120.0|
|BYSL00000897|     4|01527282681202|     1|268.0| 268.0|
|BYSL00000897|     5|84126182820102|     1|158.0| 158.0|
|BYSL00000897|     6|K9127105010402|     1|239.0| 239.0|
|BYSL00000897|     7|QY127175210405|     1|199.0| 199.0|
|BYSL00000897|     8|24127151630206|     1|299.0| 299.0|
|BYSL00000897|     9|G1126101350002|     1|158.0| 158.0|
|BYSL00000897|    10|FS527258160501|     1|198.0| 198.0|
|BYSL00000897|    11|ST040000010000|    13|  0.0|   0.0|
+------------+------+--------------+------+-----+------+复制代码
scala> case class tbDate(dateid:String, years:Int, theyear:Int, month:Int, day:Int, weekday:Int, week:Int, quarter:Int, period:Int, halfmonth:Int)

scala> val tbDateRdd = spark.sparkContext.textFile("tbDate.txt")

scala> val tbDateDS = tbDateRdd.map(_.split(",")).map(attr=> tbDate(attr(0),attr(1).trim().toInt, attr(2).trim().toInt,attr(3).trim().toInt, attr(4).trim().toInt, attr(5).trim().toInt, attr(6).trim().toInt, attr(7).trim().toInt, attr(8).trim().toInt, attr(9).trim().toInt)).toDS

scala> tbDateDS.show()
+---------+------+-------+-----+---+-------+----+-------+------+---------+
|   dateid| years|theyear|month|day|weekday|week|quarter|period|halfmonth|
+---------+------+-------+-----+---+-------+----+-------+------+---------+
| 2003-1-1|200301|   2003|    1|  1|      3|   1|      1|     1|        1|
| 2003-1-2|200301|   2003|    1|  2|      4|   1|      1|     1|        1|
| 2003-1-3|200301|   2003|    1|  3|      5|   1|      1|     1|        1|
| 2003-1-4|200301|   2003|    1|  4|      6|   1|      1|     1|        1|
| 2003-1-5|200301|   2003|    1|  5|      7|   1|      1|     1|        1|
| 2003-1-6|200301|   2003|    1|  6|      1|   2|      1|     1|        1|
| 2003-1-7|200301|   2003|    1|  7|      2|   2|      1|     1|        1|
| 2003-1-8|200301|   2003|    1|  8|      3|   2|      1|     1|        1|
| 2003-1-9|200301|   2003|    1|  9|      4|   2|      1|     1|        1|
|2003-1-10|200301|   2003|    1| 10|      5|   2|      1|     1|        1|
|2003-1-11|200301|   2003|    1| 11|      6|   2|      1|     2|        1|
|2003-1-12|200301|   2003|    1| 12|      7|   2|      1|     2|        1|
|2003-1-13|200301|   2003|    1| 13|      1|   3|      1|     2|        1|
|2003-1-14|200301|   2003|    1| 14|      2|   3|      1|     2|        1|
|2003-1-15|200301|   2003|    1| 15|      3|   3|      1|     2|        1|
|2003-1-16|200301|   2003|    1| 16|      4|   3|      1|     2|        2|
|2003-1-17|200301|   2003|    1| 17|      5|   3|      1|     2|        2|
|2003-1-18|200301|   2003|    1| 18|      6|   3|      1|     2|        2|
|2003-1-19|200301|   2003|    1| 19|      7|   3|      1|     2|        2|
|2003-1-20|200301|   2003|    1| 20|      1|   4|      1|     2|        2|
+---------+------+-------+-----+---+-------+----+-------+------+---------+复制代码
```

##### 3.2 计算所有订单中每年的销售单数、销售总额

任务要求：统计所有订单中每年的销售单数、销售总额。

实现分析：三个表连接后以count(distinct a.ordernumber)计销售单数，sum(b.amount)计销售总额

```
SELECT c.theyear, COUNT(DISTINCT a.ordernumber), SUM(b.amount)
FROM tbStock a
    JOIN tbStockDetail b ON a.ordernumber = b.ordernumber
    JOIN tbDate c ON a.dateid = c.dateid
GROUP BY c.theyear
ORDER BY c.theyear复制代码
```

实现过程：

```
spark.sql("SELECT c.theyear, COUNT(DISTINCT a.ordernumber), SUM(b.amount) FROM tbStock a JOIN tbStockDetail b ON a.ordernumber = b.ordernumber JOIN tbDate c ON a.dateid = c.dateid GROUP BY c.theyear ORDER BY c.theyear").show复制代码
```

结果如下：

```
+-------+---------------------------+--------------------+                      
|theyear|count(DISTINCT ordernumber)|         sum(amount)|
+-------+---------------------------+--------------------+
|   2004|                       1094|   3268115.499199999|
|   2005|                       3828|1.3257564149999991E7|
|   2006|                      	3772|1.3680982900000006E7|
|   2007|                   	4885|1.6719354559999993E7|
|   2008|                    	4861| 1.467429530000001E7|
|   2009|                       2619|   6323697.189999999|
|   2010|                         94|  210949.65999999997|
+-------+---------------------------+--------------------+复制代码
```

##### 3.3 计算所有订单每年最大金额订单的销售额

任务要求：统计每年最大金额订单的销售额

任务分析：

 首先，统计每年，每个订单一共有多少销售额

 然后，以上一步查询结果为基础表，和表tbDate使用dateid join，求出每年最大金额订单的销售额

实现过程：

- 首先：统计销售每个订单的销售额

  统计销售额的SQL语句如下：

  ```
  SELECT a.dateid, a.ordernumber, SUM(b.amount) AS SumOfAmount
  FROM tbStock a
      JOIN tbStockDetail b ON a.ordernumber = b.ordernumber
  GROUP BY a.dateid, a.ordernumber复制代码
  ```

  ```
  spark.sql("SELECT a.dateid, a.ordernumber, SUM(b.amount) AS SumOfAmount FROM tbStock a JOIN tbStockDetail b ON a.ordernumber = b.ordernumber GROUP BY a.dateid, a.ordernumber").show复制代码
  ```

  结果如下：

  ```
  +----------+------------+------------------+
  |    dateid| ordernumber|       SumOfAmount|
  +----------+------------+------------------+
  |  2008-4-9|BYSL00001175|             350.0|
  | 2008-5-12|BYSL00001214|             592.0|
  | 2008-7-29|BYSL00011545|            2064.0|
  |  2008-9-5|DGSL00012056|            1782.0|
  | 2008-12-1|DGSL00013189|             318.0|
  |2008-12-18|DGSL00013374|             963.0|
  |  2009-8-9|DGSL00015223|            4655.0|
  | 2009-10-5|DGSL00015585|            3445.0|
  | 2010-1-14|DGSL00016374|            2934.0|
  | 2006-9-24|GCSL00000673|3556.1000000000004|
  | 2007-1-26|GCSL00000826| 9375.199999999999|
  | 2007-5-24|GCSL00001020| 6171.300000000002|
  |  2008-1-8|GCSL00001217|            7601.6|
  | 2008-9-16|GCSL00012204|            2018.0|
  | 2006-7-27|GHSL00000603|            2835.6|
  |2006-11-15|GHSL00000741|           3951.94|
  |  2007-6-6|GHSL00001149|               0.0|
  | 2008-4-18|GHSL00001631|              12.0|
  | 2008-7-15|GHSL00011367|             578.0|
  |  2009-5-8|GHSL00014637|            1797.6|
  +----------+------------+------------------+复制代码
  ```

- 最后：求出每年最大金额订单的销售额

  ```
  SELECT theyear, MAX(c.SumOfAmount) AS SumOfAmount
  FROM (SELECT a.dateid, a.ordernumber, SUM(b.amount) AS SumOfAmount
      FROM tbStock a
          JOIN tbStockDetail b ON a.ordernumber = b.ordernumber
      GROUP BY a.dateid, a.ordernumber
      ) c
      JOIN tbDate d ON c.dateid = d.dateid
  GROUP BY theyear
  ORDER BY theyear DESC复制代码
  ```

  spark实现过程如下：

  ```
  spark.sql("SELECT theyear, MAX(c.SumOfAmount) AS SumOfAmount FROM (SELECT a.dateid, a.ordernumber, SUM(b.amount) AS SumOfAmount FROM tbStock a JOIN tbStockDetail b ON a.ordernumber = b.ordernumber GROUP BY a.dateid, a.ordernumber ) c JOIN tbDate d ON c.dateid = d.dateid GROUP BY theyear ORDER BY theyear DESC").show复制代码
  ```

  结果如下：

  ```
  +-------+------------------+                                                    
  |theyear|       SumOfAmount|
  +-------+------------------+
  |   2010|13065.280000000002|
  |   2009|25813.200000000008|
  |   2008|           55828.0|
  |   2007|          159126.0|
  |   2006|           36124.0|
  |   2005|38186.399999999994|
  |   2004| 23656.79999999997|
  +-------+------------------+复制代码
  ```





# 实验4-1：Spark Streaming必知必会

## 实验概述

在传统的数据处理过程中，我们往往先将数据存入数据库中，当需要的时候再去数据库中进行检索查询，将处理的结果返回给请求的用户；另外，MapReduce 这类大数据处理框架，更多应用在离线计算场景中。而对于一些实时性要求较高的场景，我们期望延迟在秒甚至毫秒级别，就需要引出一种新的数据计算结构——流式计算，对无边界的数据进行连续不断的处理、聚合和分析，这就是我们需要掌握的Spark Streaming用武之地。

## 实验环境

- AtStudy 实训平台
- Spark3

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16776610631767386.png)

## 实验目标

学习完成本实验后，您将能够

- 了解Spark Streaming
- 掌握Spark Streaming中WordCount编写过程
- 掌握DStream中的常用方法

## 实验任务

### 任务一、Spark Streaming概述

#### **【任务目标】**

搞清楚流式处理的应用场景以及Spark Streaming的特点。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20210824110318276.png?fileid=3270835009226314475)

视频-1、SparkStreaming概述

##### 1.1 Streaming 处理场景

如下的场景需求，仅仅通过传统的批处理、离线处理、离线计算、处理历史数据是无法完成的：

- 电商实时大屏：每年双十一时，淘宝和京东实时订单销售额和产品数量大屏展示，要求：
  - 数据量大，可能每秒钟上万甚至几十万订单量
  - 快速的处理，统计出不同维度销售订单额，以供前端大屏展示

![image-20230214230520568](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20230214230520568.png)

- 商品推荐：京东和淘宝的商城在购物车、商品详情等地方都有商品推荐的模块，商品推荐的要求：
  - 快速的处理, 加入购物车以后就需要迅速的进行推荐
  - 数据量大
  - 需要使用一些推荐算法

![image-20230214230603936](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20230214230603936.png)

- 工业大数据：现在的工场中，设备是可以联网的，汇报自己的运行状态，在应用层可以针对这些数据来分析运行状况和稳健程度，展示工件完成情况，运行情况等，工业大数据的需求：
  - 快速响应，及时预测问题
  - 数据是以事件的形式动态的产品和汇报
  - 因为是运行状态信息, 且一般都是几十上百台机器，所以汇报的数据量很大

![image-20230214230658653](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20230214230658653.png)

上述展示场景需要实时对数据进行分析处理，属于大数据中领域：实时流式数据处理。

##### 1.2 Streaming 计算模式

流式处理任务是大数据处理中很重要的一个分支，流式计算的框架也非常多，其中Spark Streaming 是构建在Spark 的基础之上，随着Spark的发展，Spark Streaming和Structured Streaming这两种流式数据处理框架越来越多的受到了关注。

对于不同的流式处理框架有不同的特点，也适应不同的场景，主要有如下两种模式。

- 模式一：原生流处理（Native）
  - 所有输入记录会一条接一条地被处理，像 Flink就是采用这种方式。

![image-20230214231251116](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20230214231251116.png)

- 模式二：微批处理（Batch）
  - 将输入的数据以某一时间间隔 T，切分成多个微批量数据，然后对每个批量数据进行处理，Spark Streaming 和 Structured Streaming采用的是这种方式。

![image-20230214231337125](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20230214231337125.png)

##### 1.3 Spark Streaming概述

Spark Streaming是Spark生态系统当中一个重要的框架，它建立在Spark Core之上，下图也可以看出Sparking Streaming在Spark生态系统中地位。

![image-20230214232012497](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20230214232012497.png)

Spark Streaming是一个基于Spark Core之上的实时计算框架，可以从很多数据源消费数据并对数据进行实时的处理，具有高吞吐量和容错能力强等特点。

![image-20230214232258640](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20230214232258640.png)

对于Spark Streaming来说，将流式数据按照时间间隔Batch Interval划分为很多部分，每一部分Batch（批次），针对每批次数据Batch当做`RDD`进行快速分析和处理。

它的核心是`DStream`，`DStream`类似于`RDD`，它实质上一系列的`RDD`的集合，`DStream`可以按照秒、分等时间间隔将数据流进行批量的划分。首先从接收到流数据之后，将其划分为多个batch，然后提交给Spark集群进行计算，最后将结果批量输出到HDFS或者数据库以及前端页面展示等等。

```
概念：DStream = Seq[RDD]
含义：DStream中封装很多RDD，每个RDD数据就是每个时间间隔内产生的数据。复制代码
```

如下图所示：

![image-20230214232459558](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20230214232459558.png)

将流式数据按照【X seconds】划分很多批次Batch，每个Batch数据封装到`RDD`中进行处理分析，最后每批次数据进行输出。

![img](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20210823165604377.png)**注意**

> 对于目前版本的 Spark Streaming 而言，其最小的 Batch Size 的选取在0.5~5秒钟之间，所以Spark Streaming能够满足流式准实时计算场景，对实时性要求非常高的如高频实时交易场景则不太适合。

### 任务二、入门案例

#### **【任务目标】**

掌握Spark Streaming中`WordCount`实现过程，并基于此了解工作原理。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20210824110318276.png?fileid=3270835009225352434)

视频-2、入门案例

##### 2.1 官方案例

###### 2.1.1 案例介绍

Spark Streaming官方提供Example案例

功能描述：从TCP Socket数据源实时消费数据，对每批次Batch数据进行词频统计WordCount，流程图如下：

![image-20230215092609291](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20230215092609291.png)

> **1**、数据源：**TCP Socket**
>
> 从哪里读取实时数据，然后进行实时分析

> **2**、数据终端：输出控制台
>
> 结果数据输出到哪里

> **3**、功能：对每批次数据实时统计，时间间隔BatchInterval：1s

###### 2.1.2 案例运行![image-20230215092813966](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20230215092813966.png)

具体步骤如下：

- 第一步、准备数据源启动端口，准备数据

  ```
  nc -lk 9999
  spark spark hive hadoop spark hive复制代码
  ```

- 第二步、运行官方案例

  - 使用官方提供命令行运行案例

  ```
  # 官方入门案例运行：词频统计
  /opt/module/spark-local/bin/run-example --master local[*] streaming.NetworkWordCount localhost 9999复制代码
  ```

- 第三步、运行结果

  ![image-20230215094210390](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20230215094210390.png)

> Spark Streaming 模块对流式数据处理，介于`Batch批处`理和`RealTime实时处理`之间处理数据方式。

##### 2.2 案例实操

- IDEA创建MAVEN工程，添加 Spark Streaming 依赖

  ```
  <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-streaming_2.12</artifactId>
      <version>3.0.0</version>
  </dependency>复制代码
  ```

- 代码实现

  ```
  /**
  * 基于IDEA集成开发环境，编程实现从TCP Socket实时读取流式数据，对每批次中数据进行词频统计。
  */
  object StreamWordCount {
  
      def main(args: Array[String]): Unit = {
  
          //1.初始化 Spark 配置信息
          // TODO: 1. 构建StreamingContext流式上下文实例对象
          val ssc: StreamingContext = {
              // a. 创建SparkConf对象，设置应用配置信息
              val sparkConf = new SparkConf()
              .setAppName(this.getClass.getSimpleName.stripSuffix("$"))
              .setMaster("local[*]")
              // b.创建流式上下文对象, 传递SparkConf对象，TODO: 时间间隔 -> 用于划分流式数据为很多批次Batch
              val context = new StreamingContext(sparkConf, Seconds(5))
              // c. 返回
              context
          }
  
          //2.从数据源端读取数据，此处是TCP Socket读取数据
          val inputDStream:ReceiverInputDStream[String] = ssc.socketTextStream("localhost", 9999)
  
          //3. 对每批次的数据进行词频统计
          val resultDStream: DStream[(String, Int)] = inputDStream
              // 过滤不合格的数据
              .filter(line => null != line && line.trim.length > 0)
              // 按照分隔符划分单词
              .flatMap(line => line.trim.split("\\s+"))
              // 转换数据为二元组，表示每个单词出现一次
              .map(word => (word, 1))
              // 按照单词分组，聚合统计
              .reduceByKey((tmp, item) => tmp + item)
  
          //4. 将结果数据输出 -> 将每批次的数据处理以后输出
          wordAndCountStreams.print(10)
  
          //5. 对于流式应用来说，需要启动应用
          ssc.start()
          // 流式应用启动以后，正常情况一直运行（接收数据、处理数据和输出数据），除非人为终止程序或者程序异常停止
          ssc.awaitTermination()
          // 关闭流式应用(参数一：是否关闭SparkContext，参数二：是否优雅的关闭）
          ssc.stop(stopSparkContext = true, stopGracefully = true)
      }
  }复制代码
  ```

运行上述词频统计案例，登录到 `WEB UI` 监控页面：`http://localhost:4040`，查看相关监控信息。

![image-20230215100437080](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20230215100437080.png)

##### 2.3 Streaming工作原理

Spark Streaming 处理流式数据时，按照时间间隔划分数据为微批次（Micro-Batch），每批次数据当做`RDD`，再进行处理分析。

![image-20230215103015926](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20230215103015926.png)

Spark Streaming 最主要的抽象是`DStream`（Discretized Stream，离散化数据流），表示连续不断的数据流。在内部实现上，Spark Streaming 的输入数据按照时间片（如1秒）分成一段一段的`DStream`，每一段数据转换为Spark中的 `RDD`，并且对`DStream`的操作都最终转变为对相应的`RDD`的操作。

例如，下图展示了进行单词统计时，每个时间片的数据（存储句子的`RDD`）经`flatMap`操作，生成了存储单词的`RDD`。整个流式计算可根据业务的需求对这些中间的结果进一步处理，或者存储到外部设备中。

![image-20230215104511367](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20230215104511367.png)

对数据的操作也是按照 RDD 为单位来进行的

![image-20230215104526942](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20230215104526942.png)

### 任务三、DStream

#### **【任务目标】**

Spark Streaming模块将流式数据封装的数据结构：`DStream`（Discretized Stream，离散化数据流，连续不断的数据流），代表持续性的数据流和经过各种Spark算子操作后的结果数据流。那么本任务主要是掌握`DStream`的基本使用。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20210824110318276.png?fileid=3270835009225865877)

视频-3、DStream

##### 3.1 `DStream`介绍

```
DStream` 是一种连续的数据流，要么从某种数据源提取数据，要么从其他数据流映射转换而来，`内部是由一系列连续的 RDD 组成的，每个 RDD 都包含了特定时间间隔内的一批数据
```

![image-20230215110602152](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20230215110602152.png)

```
DStream` 本质上是一个：`一系列时间上连续的RDD（Seq[RDD]），DStream = Seq[RDD]
DStream = Seq[RDD]
DStream相当于一个序列（集合），里面存储的数据类型为RDD（Streaming按照时间间隔划分流式数据）复制代码
```

对`DStream`的数据进行操作也是按照`RDD`为单位进行的，对`DStream`调用函数操作，底层就是对`RDD`进行操作，而且很多时候`DStream`中函数与`RDD`中函数一样的。

##### 3.2 `DStream`函数

`DStream`中包含很多函数，大多数与`RDD`中函数类似，主要分为两种类型：

- 转换函数【Transformation函数】

  

- 输出函数【Output函数】

  

在`DStream`中还有两个重要的函数，转换函数`transform`和输出函数`foreachRDD`，接下来我们需要重点介绍。

##### 3.3 `DStream`转换

`DStream`中的转换，根据状态特点又分为有状态和无状态转换。

###### 3.3.1 无状态化转换

状态转化操作就是把简单的`RDD` 转化操作应用到每个批次上，也就是转化`DStream` 中的每一个`RDD`。像`map（）、flatMap()、filter()、groupByKey()...`都是无状态化转换。

需要记住的是，尽管这些函数看起来像作用在整个流上一样，但事实上每个`DStream `在内部是由许多`RDD`（批次）组成，且无状态转化操作是分别应用到每个 `RDD` 上的。

**transform函数**

其中，有个叫`transform`的函数，是在转换中比较重要的一个函数，因为对每批次数据`RDD`进行操作时，更加接近底层，性能更好。主要作用就是：`将一个DStream转换为另外一个DStream`

接下来使用transform函数，修改词频统计程序，具体代码如下：

```
/**
* 基于IDEA集成开发环境，编程实现从TCP Socket实时读取流式数据，对每批次中数据进行词频统计。
*/
object StreamingTransformRDD {
    def main(args: Array[String]): Unit = {
        // 1. 构建StreamingContext流式上下文实例对象
        val ssc: StreamingContext = {
            // a. 创建SparkConf对象，设置应用配置信息
            val sparkConf = new SparkConf()
            .setAppName(this.getClass.getSimpleName.stripSuffix("$"))
            .setMaster("local[*]")
            // b.创建流式上下文对象, 传递SparkConf对象，TODO: 时间间隔 -> 用于划分流式数据为很多批次Batch
            val context = new StreamingContext(sparkConf, Seconds(5))
            // c. 返回
            context
        }
        // 2. 从数据源端读取数据，此处是TCP Socket读取数据
        /*
            def socketTextStream(
                hostname: String,
                port: Int,
                storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK_SER_2
            ): ReceiverInputDStream[String]
        */
        val inputDStream: ReceiverInputDStream[String] = ssc.socketTextStream(
            "localhost", //
            9999, //
            // TODO: 设置Block存储级别为先内存，不足磁盘，副本为1
            storageLevel = StorageLevel.MEMORY_AND_DISK
        )
        // TODO: 3. 对每批次的数据进行词频统计
        /*
            transform表示对DStream中每批次数据RDD进行操作
            def transform[U: ClassTag](transformFunc: RDD[T] => RDD[U]): DStream[U]
        */
        // TODO: 在DStream中，能对RDD操作的不要对DStream操作。
        val resultDStream: DStream[(String, Int)] = inputDStream.transform(rdd => {
            val resultRDD: RDD[(String, Int)] = rdd
            // 过滤不合格的数据
            .filter(line => null != line && line.trim.length > 0)
            // 按照分隔符划分单词
            .flatMap(line => line.trim.split("\\s+"))
            // 转换数据为二元组，表示每个单词出现一次
            .map(word => (word, 1))
            // 按照单词分组，聚合统计
            .reduceByKey((tmp, item) => tmp + item)
            resultRDD
        })
        // 4. 将结果数据输出 -> 将每批次的数据处理以后输出
        resultDStream.print(10)
        // 5. 对于流式应用来说，需要启动应用
        ssc.start()
        // 流式应用启动以后，正常情况一直运行（接收数据、处理数据和输出数据），除非人为终止程序或者程序异常停止
        ssc.awaitTermination()
        // 关闭流式应用(参数一：是否关闭SparkContext，参数二：是否优雅的关闭）
        ssc.stop(stopSparkContext = true, stopGracefully = true)
    }
}复制代码
```

###### 3.3.2 有状态化转换

有时候，我们需要在 `DStream` 中跨批次维护状态（例如流计算中累加WordCount）。这里我们主要是介绍`updateStateByKey`和窗口相关的函数来实现一些有状态化的转换操作。

- `UpdateStateByKey`

  提供了对一个状态变量的访问，用于键值对形式的 `DStream`。给定一个由(键，事件)对构成的 `DStream`，并传递一个指定如何根据新的事件更新每个键对应状态的函数，它可以构建出一个新的 `DStream`，其内部数据为(键，状态) 对。

函数声明如下：

```
def updateStateByKey[S: ClassTag](
    //状态更新函数
    updateFunc: (Seq[V], Option[S]) => Option[S]
): DStream[(K, S)]
/*
    第一个参数：Seq[V],表示是相同Key的所有Value值
    第二个参数：Option[S]，表示的是Key的以前状态，可能有值Some，可能没有值None，使用Option封装S泛型，具体类型有业务具体
*/复制代码
```

`updateStateByKey` 操作使得我们可以在用新信息进行更新时保持任意的状态。为使用这个功能，需要做下面两步：

1. 定义状态，状态可以是一个任意的数据类型。

2. 定义状态更新函数，用此函数阐明如何使用之前的状态和来自输入流的新值对状态进行更新。

   > 使用 `updateStateByKey` 需要对检查点目录进行配置，会使用检查点来保存状态。

更新版的`wordcount`

```
/**
* 基于IDEA集成开发环境，编程实现从TCP Socket实时读取流式数据，对每批次中数据进行词频统计。
*/
object StreamingTransformRDD {
    def main(args: Array[String]): Unit = {
        // 1. 构建StreamingContext流式上下文实例对象
        val ssc: StreamingContext = {
            // a. 创建SparkConf对象，设置应用配置信息
            val sparkConf = new SparkConf()
            .setAppName(this.getClass.getSimpleName.stripSuffix("$"))
            .setMaster("local[*]")
            // b.创建流式上下文对象, 传递SparkConf对象，TODO: 时间间隔 -> 用于划分流式数据为很多批次Batch
            val context = new StreamingContext(sparkConf, Seconds(5))
            // c. 返回
            context
        }
        
        ssc.checkpoint("./ck")
        
        // 2. 从数据源端读取数据，此处是TCP Socket读取数据
        val inputDStream: ReceiverInputDStream[String] = ssc.socketTextStream(
            "localhost", 9999, storageLevel = StorageLevel.MEMORY_AND_DISK)
        
        // 3. 对每批次的数据进行词频统计
        val resultDStream: DStream[(String, Int)] = inputDStream.transform(rdd => {
            val resultRDD: RDD[(String, Int)] = rdd
            // 过滤不合格的数据
            .filter(line => null != line && line.trim.length > 0)
            // 按照分隔符划分单词
            .flatMap(line => line.trim.split("\\s+"))
            // 转换数据为二元组，表示每个单词出现一次
            .map(word => (word, 1))
            // 按照单词分组，聚合统计
            .reduceByKey((tmp, item) => tmp + item)
            resultRDD
        })
        
        /*
            def updateStateByKey[S: ClassTag](
                // 状态更新函数
                updateFunc: (Seq[V], Option[S]) => Option[S]
            ): DStream[(K, S)]
            
            第一个参数：Seq[V]
                表示的是相同Key的所有Value值
            第二个参数：Option[S]
                表示的是Key的以前状态，可能有值Some，可能没值None，使用Option封装
                S泛型，具体类型有业务具体，此处是词频：Int类型
        */
        val stateDStream: DStream[(String, Int)] = resultDStream.updateStateByKey(
            (values: Seq[Int], state: Option[Int]) => {
                // a. 获取以前状态信息
                val previousState = state.getOrElse(0)
                // b. 获取当前批次中Key对应状态
                val currentState = values.sum
                // c. 合并状态
                val latestState = previousState + currentState
                // d. 返回最新状态
                Some(latestState)
            }
        )
        
        // 4. 将结果数据输出 -> 将每批次的数据处理以后输出
        stateDStream.print(10)
        
        // 5. 对于流式应用来说，需要启动应用
        ssc.start()
        ssc.awaitTermination()
        ssc.stop(stopSparkContext = true, stopGracefully = true)
    }
}复制代码
```

运行应用程序，通过WEB UI界面可以发现，将以前状态保存到Checkpoint检查点目录中，更新时在读取。

![image-20230215144041431](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20230215144041431.png)

- `WindowOperations`

  在实际项目中，很多时候需求：每隔一段时间统计最近数据状态，并不是对所有数据进行统计，称为趋势统计或者窗口统计，Spark Streaming中提供相关函数实现功能，业务逻辑如下：

  ![image-20230215144335442](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_1/image-20230215144335442.png)

> 例如：实现近期内热门点击商品的Top10，统计最近一段时间范围（比如，最近半个小时或最近2个小时）内热门点击的商品，获取Top10的信息。

窗口函数【window】声明如下，包含两个参数：

- 窗口大小（Window Interval，每次统计数据范围）
- 滑动大小（每隔多久统计一次），都必须是批处理时间间隔`BatchInterval`整数倍

关于【window】的操作方法如下：

（1） `window(windowLength, slideInterval)`: 基于对源`DStream` 窗化的批次进行计算返回一个新的`Dstream`；

（2） `countByWindow(windowLength, slideInterval)`: 返回一个滑动窗口计数流中的元素个数；

（3） `reduceByWindow(func, windowLength, slideInterval)`: 通过使用自定义函数整合滑动区间流元素来创建一个新的单元素流；

（4） `reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])`: 当在一个(K,V) 对的`DStream `上调用此函数，会返回一个新(K,V)对的 `DStream`，此处通过对滑动窗口中批次数据使用 reduce 函数来整合每个 key 的 value 值。

采用窗口统计WordCount

```
/**
* 基于IDEA集成开发环境，编程实现从TCP Socket实时读取流式数据，对每批次中数据进行词频统计。
*/
object StreamingTransformRDD {
    def main(args: Array[String]): Unit = {
        // Streaming应用BatchInterval
        val BATCH_INTERVAL: Int = 5
        // Streaming应用窗口大小
        val WINDOW_INTERVAL: Int = BATCH_INTERVAL * 2
        val SLIDER_INTERVAL: Int = BATCH_INTERVAL * 1
        
        
        // 1. 构建StreamingContext流式上下文实例对象
        val ssc: StreamingContext = {
            val sparkConf = new SparkConf()
            .setAppName(this.getClass.getSimpleName.stripSuffix("$"))
            .setMaster("local[*]")
            val context = new StreamingContext(sparkConf, Seconds(BATCH_INTERVAL))
            context
        }
        
        // 2. 从数据源端读取数据，此处是TCP Socket读取数据
        val inputDStream: ReceiverInputDStream[String] = ssc.socketTextStream(
            "localhost",9999, storageLevel = StorageLevel.MEMORY_AND_DISK )
        
        // 3. 对每批次的数据进行词频统计
        val resultDStream: DStream[(String, Int)] = inputDStream.transform(rdd => {
            val resultRDD: RDD[(String, Int)] = rdd
            // 过滤不合格的数据
            .filter(line => null != line && line.trim.length > 0)
            // 按照分隔符划分单词
            .flatMap(line => line.trim.split("\\s+"))
            // 转换数据为二元组，表示每个单词出现一次
            .map(word => (word, 1))
            // 按照单词分组，聚合统计
            .reduceByKey((tmp, item) => tmp + item)
            resultRDD
        })
        
        // 4. 对获取流式数据进行ETL后，使用窗口聚合函数统计计算
        /*
            def reduceByKeyAndWindow(
                reduceFunc: (V, V) => V, // 聚合函数
                windowDuration: Duration, // 窗口大小
                slideDuration: Duration // 滑动大小
               ): DStream[(K, V)]
        */
        val windowDStream: DStream[(String, Int)] = resultDStream.reduceByKeyAndWindow(
           (tmp:Int, value:Int) => {temp+value},
            Seconds(WINDOW_INTERVAL),
            Seconds(SLIDER_INTERVAL)
        )
        
        // 5. 将结果数据输出 -> 将每批次的数据处理以后输出
        windowDStream.print()
        
        // 6. 启动流式应用，一直运行，直到程序手动关闭或异常终止
        ssc.start()
        ssc.awaitTermination()
        ssc.stop(stopSparkContext = true, stopGracefully = true)
    }
}复制代码
```

##### 3.4 `DStream`输出

输出函数也比较多，这里主要是介绍`foreachRDD`函数的使用。

`foreachRDD`函数属于将`DStream`中结果数据`RDD`输出的操作，类似`transform`函数，针对每批次`RDD`数据操作，在上面的案例中，我们也可以通过`foreachRDD`自定义输出。

```
object StreamingOutputRDD {
    def main(args: Array[String]): Unit = {
        // 1. 构建StreamingContext流式上下文实例对象
        val ssc: StreamingContext = {
            // a. 创建SparkConf对象，设置应用配置信息
            val sparkConf = new SparkConf()
            .setAppName(this.getClass.getSimpleName.stripSuffix("$"))
            .setMaster("local[*]")
            // b.创建流式上下文对象, 传递SparkConf对象，TODO: 时间间隔 -> 用于划分流式数据为很多批次Batch
            val context = new StreamingContext(sparkConf, Seconds(5))
            // c. 返回
            context
        }
        // 2. 从数据源端读取数据，此处是TCP Socket读取数据
        val inputDStream: ReceiverInputDStream[String] = ssc.socketTextStream(
            "localhost", //
            9999, //
            // TODO: 设置Block存储级别为先内存，不足磁盘，副本为1
            storageLevel = StorageLevel.MEMORY_AND_DISK
        )
        // 3. 对每批次的数据进行词频统计
        /*
            transform表示对DStream中每批次数据RDD进行操作
            def transform[U: ClassTag](transformFunc: RDD[T] => RDD[U]): DStream[U]
        */
        // TODO: 在DStream中，能对RDD操作的不要对DStream操作。
        val resultDStream: DStream[(String, Int)] = inputDStream.transform(rdd => {
            val resultRDD: RDD[(String, Int)] = rdd
            // 过滤不合格的数据
            .filter(line => null != line && line.trim.length > 0)
            // 按照分隔符划分单词
            .flatMap(line => line.trim.split("\\s+"))
            // 转换数据为二元组，表示每个单词出现一次
            .map(word => (word, 1))
            // 按照单词分组，聚合统计
            .reduceByKey((tmp, item) => tmp + item)
            resultRDD
        })
        // TODO: 4. 将结果数据输出 -> 将每批次的数据处理以后输出
        /*
            对DStream中每批次结果RDD数据进行输出操作
            def foreachRDD(foreachFunc: (RDD[T], Time) => Unit): Unit
            其中Time就是每批次BatchTime，Long类型数据, 转换格式：2020/05/10 16:53:25
        */
        resultDStream.foreachRDD{ (rdd, time) =>
            // 使用lang3包下FastDateFormat日期格式类，属于线程安全的
            val batchTime: String = FastDateFormat.getInstance("yyyyMMddHHmmss")
            .format(new Date(time.milliseconds))
            println("-------------------------------------------")
            println(s"Time: $batchTime")
            println("-------------------------------------------")
            // TODO: 先判断RDD是否有数据，有数据在输出
            if(!rdd.isEmpty()){
                // 对于结果RDD输出，需要考虑降低分区数目
                val resultRDD = rdd.coalesce(1)
                // 对分区数据操作
                resultRDD.foreachPartition{iter =>iter.foreach(item => println(item))}
                // 保存数据至HDFS文件
                resultRDD.saveAsTextFile(s"datas/streaming/wc-output-${batchTime}")
            }
        }
        // 5. 对于流式应用来说，需要启动应用
        ssc.start()
        // 流式应用启动以后，正常情况一直运行（接收数据、处理数据和输出数据），除非人为终止程序或者程序异常停止
        ssc.awaitTermination()
        // 关闭流式应用(参数一：是否关闭SparkContext，参数二：是否优雅的关闭）
        ssc.stop(stopSparkContext = true, stopGracefully = true)
    }
}复制代码
```





# 实验4-2：Spark Streaming项目实战

## 实验概述

本实验主要任务就是通过Kafka获取生产数据，然后借助Spark Streaming对生产数据进行实时分析。

## 实验环境

- AtStudy 实训平台
- Kafka
- Spark3

## 实验流程

![image.png](https://cdn.atstudy.com/lab/manual/16776611010019297.png)

## 实验目标

学习完成本实验后，您将能够

- 掌握Kafka的基本使用
- 掌握Spark Streaming和Kafka的集成
- 掌握Spark Streaming实时数据分析的过程

## 实验任务

### 任务一、Kafka集成

#### **【任务目标】**

回顾Kafka的基本使用，并整合Spark Streaming。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_2/image-20210824110318276.png?fileid=1397757889594355479)

视频-1、Kafka集成

##### 1.1 Kafka相关内容回顾

###### 1.1.1 启动服务

- 启动Zookeeper服务

  ```
  [root@cd62d9893d1f kafka_2.12-3.0.0]# ./bin/zookeeper-server-start.sh -daemon config/zookeeper.properties 复制代码
  ```

  确保Zookeeper服务已经成功启动！

  ![image-20230215165928114](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_2/image-20230215165928114.png)

- 启动Kafka服务

  ```
  [root@cd62d9893d1f kafka_2.12-3.0.0]# ./bin/kafka-server-start.sh -daemon config/server.properties 复制代码
  ```

> 提示：在启动Kafka之前，建议将logs目录下的所有内容进行删除
>
> ```
>  [root@cd62d9893d1f kafka_2.12-3.0.0]# rm -rf ./logs/*复制代码
> ```

确保Kafka服务已经成功启动！

![image-20230215170047065](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_2/image-20230215170047065.png)

###### 1.1.2 Kafka Topic

| 参数                                              | 描述                                 |
| ------------------------------------------------- | ------------------------------------ |
| --bootstrap-server <String: server toconnect to>  | 连接的 Kafka Broker 主机名称和端口号 |
| --topic <String: topic>                           | 操作的 topic 名称                    |
| --create                                          | 创建主题                             |
| --delete                                          | 删除主题                             |
| --list                                            | 查看所有主题                         |
| --describe                                        | 查看主题详细描述                     |
| --partitions <Integer: # of partitions>           | 设置分区数                           |
| --replication-factor<Integer: replication factor> | 设置分区副本                         |
| --config <String: name=value>                     | 更新系统默认的配置                   |

- 查看Topic

  ```
  [root@cd62d9893d1f kafka_2.12-3.0.0]# ./bin/kafka-topics.sh --bootstrap-server localhost:9092 --list复制代码
  ```

- 创建first topic

  ```
  [root@cd62d9893d1f kafka_2.12-3.0.0]# ./bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --partitions 1 --replication-factor 1 --topic first
  Created topic first.复制代码
  ```

  - `--topic` 定义 topic 名
  - `--replication-factor` 定义副本数
  - `--partitions` 定义分区数

- 查看 first 主题的详情

  ```
  [root@cd62d9893d1f kafka_2.12-3.0.0]# ./bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic first复制代码
  ```

- 删除 topic

  ```
  [root@cd62d9893d1f kafka_2.12-3.0.0]# ./bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic first复制代码
  ```

###### 1.1.3 生产者操作

```
 [root@cd62d9893d1f kafka_2.12-3.0.0]# bin/kafka-console-producer.sh复制代码
```

| 参数                                             | 描述                                   |
| ------------------------------------------------ | -------------------------------------- |
| --bootstrap-server <String: server toconnect to> | 连接的 Kafka Broker 主机名称和端口号。 |
| --topic <String: topic>                          | 操作的 topic 名称。                    |

```
[root@cd62d9893d1f kafka_2.12-3.0.0]# ./bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic first复制代码
```

###### 1.1.4 消费者操作

```
[root@cd62d9893d1f kafka_2.12-3.0.0]# bin/kafka-console-consumer.sh复制代码
```

| 参数                                             | 描述                                   |
| ------------------------------------------------ | -------------------------------------- |
| --bootstrap-server <String: server toconnect to> | 连接的 Kafka Broker 主机名称和端口号。 |
| --topic <String: topic>                          | 操作的 topic 名称。                    |
| --from-beginning                                 | 从头开始消费。                         |
| --group <String: consumer group id>              | 指定消费者组名称。                     |

- 消费 first 主题中的数据

  ```
  [root@cd62d9893d1f kafka_2.12-3.0.0]# bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first复制代码
  ```

- 把主题中所有的数据都读取出来（包括历史数据）

  ```
  [root@cd62d9893d1f kafka_2.12-3.0.0]# bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic first复制代码
  ```

##### 1.2 集成方式

在实际项目中，无论使用 Storm 还是 Spark Streaming 与 Flink，主要从 Kafka 实时消费数据进行处理分析，流式数据实时处理技术架构大致如下：

![image-20230215222523504](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_2/image-20230215222523504.png)

Spark Streaming集成Kafka主要有两种方式：

1. Receiver 接收方式（了解，实际生产中采用的比较少）
   - 在executor上会有receiver从kafka接收数据并存储在Spark executor中，在到了batch时间后触发job去处理接收到的数据，1个receiver占用1个core，占用资源；
   - 为了不丢数据需要开启WAL机制，保障数据安全，效率低；
   - 通过zookeeper来连接kafka队列，调用Kafka高阶API，offset存储在zookeeper，由Receiver维护，Spark在消费的时候为了保证数据不丢也会在Checkpoint中存一份offset，可能会出现数据不一致。
2. Direct拉取方式（掌握，实际生产中采用的方式）
   - 没有receiver，无需额外的core用于不停地接收数据，而是定期查询kafka中的每个partition的最新的offset，每个批次拉取上次处理的offset和当前查询的offset的范围的数据进行处理；
   - 为了不丢数据，无需将数据备份落地，而只需要手动保存offset即可；
   - 内部使用kafka simple Level API去消费数据，需要手动维护offset，kafka zk上不会自动更新offset。

上述两种方式区别，如下图所示：

![image-20230216100507018](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_2/image-20230216100507018.png)

所以：Direct方式没有receiver层，其会周期性的获取Kafka中每个topic的每个partition中的最新offsets，再根据设定的maxRatePerPartition来处理每个batch。较于Receiver方式的优势：

- 其一、简化的并行：Kafka中的partition与RDD中的partition是一一对应的并行读取Kafka数据；
- 其二、高效：no need for Write Ahead Logs；
- 其三、精确一次：直接使用simple Kafka API，Offsets则利用Spark Streaming的checkpoints进行记录

##### 1.3 Direct方式集成

使用Kafka 0.10.+提供新版本Consumer API集成Streaming，实时消费Topic数据，进行处理。

1、创建Topic及Console控制台发送数据，命令如下：

```
# 1. 启动Zookeeper 服务
[root@cd62d9893d1f kafka_2.12-3.0.0]# ./bin/zookeeper-server-start.sh -daemon config/zookeeper.properties 

# 2. 启动Kafka 服务
[root@cd62d9893d1f kafka_2.12-3.0.0]# ./bin/kafka-server-start.sh -daemon config/server.properties 

# 3. Create Topic
[root@cd62d9893d1f kafka_2.12-3.0.0]# ./bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --partitions 1 --replication-factor 1 --topic wc-topic

# List Topics
[root@cd62d9893d1f kafka_2.12-3.0.0]# ./bin/kafka-topics.sh --bootstrap-server localhost:9092 --list

# Producer
[root@cd62d9893d1f kafka_2.12-3.0.0]# ./bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic wc-topic

# Consumer
[root@cd62d9893d1f kafka_2.12-3.0.0]# ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092  --from-beginning --topic wc-topic复制代码
```

2、添加项目依赖

```
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming-kafka-0-10_2.12</artifactId>
    <version>3.0.0</version>
</dependency>
<dependency>
    <groupId>com.fasterxml.jackson.core</groupId>
    <artifactId>jackson-core</artifactId>
    <version>2.10.1</version>
</dependency>复制代码
```

3、Spark Streaming获取Kafka数据如下：

工具类KafkaUtils中createDirectStream函数API使用说明（函数声明）：

```
def createDirectStream[K, V](
    ssc: StreamingContext,
    locationStrategy: LocationStrategy, //位置策略设置
    consumerStrategy: ConsumerStrategy[K, V] //消费策略设置
):InputDStream[ConsumerRecord[K, V]]复制代码
import org.apache.kafka.clients.consumer.{ConsumerConfig, ConsumerRecord} import org.apache.spark.SparkConf
import org.apache.spark.streaming.dstream.{DStream, InputDStream}
import org.apache.spark.streaming.kafka010.{ConsumerStrategies, KafkaUtils, LocationStrategies}
import org.apache.spark.streaming.{Seconds, StreamingContext}

object DirectAPI {
    def main(args: Array[String]): Unit = {
        // 1. 构建流式上下文实例对象StreamingContext，用于读取流式的数据和调度Batch Job执行
        val ssc: StreamingContext = {
            val sparkConf = new SparkConf().setAppName(this.getClass.getSimpleName.stripSuffix("$")).setMaster("local[*]")
            val context: StreamingContext = new StreamingContext(sparkConf, Seconds(5))
            context
        }
        
        // TODO: 2. 读取Kafka Topic中数据
        // i.位置策略
        val locationStrategy: LocationStrategy = LocationStrategies.PreferConsistent
        
        // ii.读取哪些Topic数据
        val topics:Set[String] = Set("wc-topic")
        
        // iii.消费Kafka 数据配置参数
        val kafkaParams: Map[String, Object] = Map[String, Object](
            ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> "localhost:9092", 
            ConsumerConfig.GROUP_ID_CONFIG -> "group_id_streaming_0001", 
            "key.deserializer" -> "org.apache.kafka.common.serialization.StringDeserializer",
             "value.deserializer" ->                                   "org.apache.kafka.common.serialization.StringDeserializer",
            "auto.offset.reset" -> "latest"
        )
        // iv.消费数据策略
        val consumerStrategy: ConsumerStrategy[String, String] = 		
                ConsumerStrategies.Subscribe(topics, kafkaParams)

        // v.采用新消费者API获取数据，类似于Direct方式
        val kafkaDStream: InputDStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream[String, String](
            ssc, locationStrategy, consumerStrategy)

        //3.对每批次的数据进行词频统计
        val resultDStream: DStream[String] = kafkaDStream.transform(
            rdd=>{
                // 获取从Kafka读取数据的Message
                rdd.map(t=>t.value())
                    // 过滤“脏数据”
                    .filter(line=>null!=null && line.trim.length>0)
                    // 分割为单词
                    .flatMap(line=>line.trim.split("\\s+"))
                    // 转换为二元组
                    .map(word=>(word, 1))
                       // 聚合统计
                    .reduceByKey((a, b) => a+b)
            }
        )
        
        resultDStream.print()
        
        //4.对于流式应用来说，需要启动应用
        ssc.start() 
        //流式应用启动以后，正常情况一直运行（接收数据、处理数据和输出数据），除非人为终止程序或者程序异常停止
        ssc.awaitTermination()
        // 关闭流式应用(参数一：是否关闭SparkContext，参数二：是否优雅的关闭）
        ssc.stop(stopSparkContext = true, stopGracefully = true)
    }
}复制代码
```

### 任务二、应用案例：关键词热搜

#### **【任务目标】**

百度热搜是以数亿用户的真实数据为基础，通过专业的数据挖掘方法，计算关键词的热搜指数，旨在建立全面、热门、时效的各类关键词排行榜，我们通过百度热搜的关键词掌握Spark Streaming在实战中的具体应用。

#### **【任务步骤】**

**视频讲解：**

![image-20210824110318276](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_2/image-20210824110318276.png?fileid=1397757889594718949)

视频-2、应用案例：百度热搜

##### 2.1 业务场景

百度热搜（`https://top.baidu.com/board`），依据搜索关键词实时统计各种维度热点。

![image-20230216151543040](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_2/image-20230216151543040.png)

我们基于百度搜索时产生的日志数据进行分析，主要业务需求如下：

- 业务一：百度热搜排行榜Top10，累加统计所有用户搜索词次数，获取Top10搜索词及次数；
- 业务二：近期时间内热搜Top10，统计最近一段时间范围（比如，最近半个小时或最近2个小时）内用户搜索词次数，获取Top10搜索词及次数；

##### 2.2 初始化环境

编程实现业务之前，首先编写程序模拟产生用户使用百度搜索产生日志数据和创建工具类StreamingContextUtils提供StreamingContext对象与从Kafka接收数据方法。

###### 2.2.1 创建Topic

启动Kafka Broker服务，创建Topic【search-log-topic】，命令如下所示：

```
# 1. 启动Zookeeper 服务
[root@cd62d9893d1f kafka_2.12-3.0.0]# ./bin/zookeeper-server-start.sh -daemon config/zookeeper.properties 

# 2. 启动Kafka 服务
[root@cd62d9893d1f kafka_2.12-3.0.0]# ./bin/kafka-server-start.sh -daemon config/server.properties 

# 3. Create Topic
[root@cd62d9893d1f kafka_2.12-3.0.0]# ./bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --partitions 1 --replication-factor 1 --topic search-log-topic

# List Topics
[root@cd62d9893d1f kafka_2.12-3.0.0]# ./bin/kafka-topics.sh --bootstrap-server localhost:9092 --list

# Producer
[root@cd62d9893d1f kafka_2.12-3.0.0]# ./bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic search-log-topic

# Consumer
[root@cd62d9893d1f kafka_2.12-3.0.0]# ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092  --from-beginning --topic search-log-topic复制代码
```

###### 2.2.2 模拟日志数据

模拟用户搜索日志数据，字段信息封装到CaseClass样例类【SearchLog】类，代码如下：

```
package com.atstudy.spark.app.mock
/**
* 用户百度搜索时日志数据封装样例类CaseClass
* <p>
* @param sessionId 会话ID
* @param ip IP地址
* @param datetime 搜索日期时间
* @param keyword 搜索关键词
*/
case class SearchLog(
    sessionId: String, 
    ip: String, 
    datetime: String, 
    keyword: String 
    ) {
    override def toString: String = s"$sessionId,$ip,$datetime,$keyword"
}复制代码
```

模拟产生搜索日志数据类【MockSearchLogs】具体代码如下：

```
package com.atstudy.spark.app.mock

import java.util.{Properties, Random, UUID}

import org.apache.commons.lang3.time.FastDateFormat
import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord}
import org.apache.kafka.common.serialization.StringSerializer

/**
* 模拟产生用户使用百度搜索引擎时，搜索查询日志数据，包含字段为：
* uid, ip, search_datetime, search_keyword
*/
object MockSearchLogs {
  def main(args: Array[String]): Unit = {
    // 搜索关键词，直接到百度热搜榜获取即可
    val keywords: Array[String] = Array("狂飙", "马斯克", "热气球", "刘文正", "三体")
   
      // 发送Kafka Topic
    val props = new Properties()
    props.put("bootstrap.servers", "localhost:9092")
    props.put("acks", "1")
    props.put("retries", "3")
    props.put("key.serializer", classOf[StringSerializer].getName)
    props.put("value.serializer", classOf[StringSerializer].getName)
    val producer = new KafkaProducer[String, String](props)
      
    val random: Random = new Random()
    while (true) {
      // 随机产生一条搜索查询日志
      val searchLog: SearchLog = SearchLog(
        getUserId(), //
        getRandomIp(), //
        getCurrentDateTime(), //
        keywords(random.nextInt(keywords.length)) //
      )
        
      println(searchLog.toString)
      Thread.sleep(10 + random.nextInt(100))
      val record = new ProducerRecord[String, String]("search-log-topic", searchLog.toString)
      producer.send(record)
    }
    // 关闭连接
    producer.close()
  }

  /**
    * 随机生成用户SessionId
    */
  def getUserId(): String = {
    val uuid: String = UUID.randomUUID().toString
    uuid.replaceAll("-", "").substring(16)
  }

  /**
    * 获取当前日期时间，格式为yyyyMMddHHmmssSSS
    */
  def getCurrentDateTime(): String = {
    val format = FastDateFormat.getInstance("yyyyMMddHHmmssSSS")
    val nowDateTime: Long = System.currentTimeMillis()
    format.format(nowDateTime)
  }

  /**
    * 获取随机IP地址
    */
  def getRandomIp(): String = {
    // ip范围
    val range: Array[(Int, Int)] = Array(
      (607649792, 608174079), //36.56.0.0-36.63.255.255
      (1038614528, 1039007743), //61.232.0.0-61.237.255.255
      (1783627776, 1784676351), //106.80.0.0-106.95.255.255
      (2035023872, 2035154943), //121.76.0.0-121.77.255.255
      (2078801920, 2079064063), //123.232.0.0-123.235.255.255
      (-1950089216, -1948778497), //139.196.0.0-139.215.255.255
      (-1425539072, -1425014785), //171.8.0.0-171.15.255.255
      (-1236271104, -1235419137), //182.80.0.0-182.92.255.255
      (-770113536, -768606209), //210.25.0.0-210.47.255.255
      (-569376768, -564133889) //222.16.0.0-222.95.255.255
    )
    // 随机数：IP地址范围下标
    val random = new Random()
    val index = random.nextInt(10)
    val ipNumber: Int = range(index)._1 + random.nextInt(range(index)._2 - range(index)._1)
    //println(s"ipNumber = ${ipNumber}")
    // 转换Int类型IP地址为IPv4格式
    number2IpString(ipNumber)
  }

  /**
    * 将Int类型IPv4地址转换为字符串类型
    */
  def number2IpString(ip: Int): String = {
    val buffer: Array[Int] = new Array[Int](4)
    buffer(0) = (ip >> 24) & 0xff
    buffer(1) = (ip >> 16) & 0xff
    buffer(2) = (ip >> 8) & 0xff
    buffer(3) = ip & 0xff
    // 返回IPv4地址
    buffer.mkString(".")
  }
}复制代码
```

运行应用程序，源源不断产生日志数据，发送至Kafka（同时在控制台打印），截图如下：

![image-20230216153057687](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_2/image-20230216153057687.png)

###### 2.2.3 StreamingContextUtils 工具类

所有Spark Streaming应用都需要构建StreamingContext实例对象，并且从采用New Kafka Consumer API消费Kafka数据，编写工具类【StreamingContextUtils】，提供两个方法：

- 方法一：getStreamingContext，获取StreamingContext实例对象

  ```
  /**
  * 获取StreamingContext实例，传递批处理时间间隔
  * @param batchInterval 批处理时间间隔，单位为秒
  */
  def getStreamingContext(clazz: Class[_], batchInterval:Int):StreamingContext复制代码
  ```

- 方法二：consumerKafka，消费Kafka Topic中数据

  ```
  /**
  * 从指定的Kafka Topic中消费数据，默认从最新偏移量（largest）开始消费
  * @param ssc StreamingContext实例对象
  * @param topicName 消费Kafka中Topic名称
  */
  def consumerKafka(ssc: StreamingContext, topicName: String): DStream[ConsumerRecord[String, String]]复制代码
  ```

具体代码如下：

```
package com.atstudy.spark.app

import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.kafka010._
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.streaming.{Seconds, StreamingContext}

/**
* 工具类提供：构建流式应用上下文StreamingContext实例对象和从Kafka Topic消费数据
*/
object StreamingContextUtils {
    /**
    * 获取StreamingContext实例，传递批处理时间间隔
    * @param batchInterval 批处理时间间隔，单位为秒
    */
    def getStreamingContext(clazz:Class[_], batchInterval:Int):StreamingContext={
        val sparkConf = new SparkConf()
        .setAppName(clazz.getSimpleName.stripSuffix("$"))
        .setMaster("local[*]")
        val context = new StreamingContext(sparkConf, Seconds(batchInterval))
        context
    }

    /**
    * 从指定的Kafka Topic中消费数据，默认从最新偏移量（largest）开始消费
    * @param ssc StreamingContext实例对象
    * @param topicName 消费Kafka中Topic名称
    */
    def consumerKafka(ssc:StreamingContext, topicName:String):DStream[ConsumerRecord[String, String]]={
        // i.位置策略
        val locationStrategy:LocationStrategy = LocationStrategies.PreferConsistent
       
        // ii.读取哪些Topic数据
        val topics = Array(topicName)

        // iii.消费Kafka 数据配置参数
        val kafkaParams  = Map[String, Object](
            "bootstrap.servers"->"localhost:9092",
            "key.deserializer"->classOf[StringDeserializer],
            "value.deserializer"->classOf[StringDeserializer],
            "group.id"->"group_id_streaming_0001",
            "auto.offset.reset"->"latest"
        )
        // iv.消费数据策略
        val consumerStrategy : ConsumerStrategy[String, String] = ConsumerStrategies.Subscribe(
            topics, kafkaParams
        )
        // v.采用新消费者API获取数据，类似于Direct方式
        val kafkaDStream: DStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream(ssc, locationStrategy, consumerStrategy)
        
        // vi.返回DStream
        kafkaDStream
    }
}复制代码
```

##### 2.3 业务1：实时状态更新统计

实时累加统计用户各个搜索词出现的次数 ， 在 SparkStreaming 中提供函 数【updateStateByKey】实现累加统计。

```
package com.atstudy.spark.app.state

import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.dstream.DStream

/**
* 实时消费Kafka Topic数据，累加统计各个搜索词的搜索次数，实现百度搜索风云榜
*/
object StreamingUpdateState {
    def main(args: Array[String]): Unit = {
        // 1. 获取StreamingContext实例对象
        val ssc: StreamingContext = StreamingContextUtils.getStreamingContext(this.getClass, 5)
        // TODO: 设置检查点目录
        ssc.checkpoint(s"datas/streaming/state-${System.nanoTime()}")
        
        // 2. 从Kafka消费数据，使用Kafka New Consumer API
        val kafkaDStream: DStream[ConsumerRecord[String, String]] = StreamingContextUtils
        .consumerKafka(ssc, "search-log-topic")
        // 3. 对每批次的数据进行搜索词次数统计
        val reduceDStream: DStream[(String, Int)] = kafkaDStream.transform { rdd =>
            val reduceRDD = rdd
            // 过滤不合格的数据
            .filter { record =>
                val message: String = record.value()
                null != message && message.trim.split(",").length == 4
            }
            // 提取搜索词，转换数据为二元组，表示每个搜索词出现一次
            .map { record =>
                val keyword: String = record.value().trim.split(",").last
                keyword -> 1
            }
            // 按照单词分组，聚合统计
            .reduceByKey((tmp, item) => tmp + item) // TODO: 先聚合，再更新，优化
            reduceRDD // 返回
        }
        /*
            def updateStateByKey[S: ClassTag](
                // 状态更新函数
                updateFunc: (Seq[V], Option[S]) => Option[S]
            ): DStream[(K, S)]

            第一个参数：Seq[V]
                表示的是相同Key的所有Value值
            第二个参数：Option[S]
                表示的是Key的以前状态，可能有值Some，可能没值None，使用Option封装
                S泛型，具体类型有业务具体，此处是词频：Int类型
        */
        val stateDStream: DStream[(String, Int)] = reduceDStream.updateStateByKey(
            (values: Seq[Int], state: Option[Int]) => {
                // a. 获取以前状态信息
                val previousState = state.getOrElse(0)
                // b. 获取当前批次中Key对应状态
                val currentState = values.sum
                // c. 合并状态
                val latestState = previousState + currentState
                // d. 返回最新状态
                Some(latestState)
            }
        )
        // 5. 将结果数据输出 -> 将每批次的数据处理以后输出
        stateDStream.print()


        // 6.启动流式应用，一直运行，直到程序手动关闭或异常终止
        ssc.start()
        ssc.awaitTermination()
        ssc.stop(stopSparkContext = true, stopGracefully = true)
    }
}复制代码
```

实现效果如下：

![image-20230216154030341](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_2/image-20230216154030341.png)

但是，我们更多的需要统计最近一段时间热门的关键词！

##### 2.4 业务2：实时窗口统计

针对用户百度搜索日志数据，实现【近期时间内热搜Top10】，统计最近一段时间范围（比如，最近半个小时或最近2个小时）内用户搜索词次数，获取Top10搜索词及次数。

假设BatchInterval为2秒，WindowInterval为4秒，SlideInterval为2秒。

具体实现过程如下：

```
package com.atstudy.spark.app.window

import com.atstudy.spark.app.StreamingContextUtils
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.{Seconds, StreamingContext}

/**
* 实时消费Kafka Topic数据，每隔一段时间统计最近搜索日志中搜索词次数
* 批处理时间间隔：BatchInterval = 2s
* 窗口大小间隔：WindowInterval = 4s
* 滑动大小间隔：SliderInterval = 2s
*/
object StreamingReduceWindow {
    def main(args: Array[String]): Unit = {
        // Streaming应用BatchInterval
        val BATCH_INTERVAL: Int = 2
        // Streaming应用窗口大小
        val WINDOW_INTERVAL: Int = BATCH_INTERVAL * 2
        val SLIDER_INTERVAL: Int = BATCH_INTERVAL * 1
        // 1. 获取StreamingContext实例对象
        val ssc: StreamingContext = StreamingContextUtils.getStreamingContext(this.getClass, BATCH_INTERVAL)
        // 2. 从Kafka消费数据，使用Kafka New Consumer API
        val kafkaDStream: DStream[String] = StreamingContextUtils.consumerKafka(ssc, "search-log-topic").map(recored => recored.value())
        // 3. 对每批次的数据进行搜索词进行次数统计
        val etlDStream: DStream[(String, Int)] = kafkaDStream.transform{ rdd =>
            val etlRDD = rdd
            // 过滤不合格的数据
            .filter( message => null != message && message.trim.split(",").length == 4)
            // 提取搜索词，转换数据为二元组，表示每个搜索词出现一次
            .map{message =>
                val keyword: String = message.trim.split(",").last
                keyword -> 1
            }
            etlRDD // 返回
        }
        // 4. 对获取流式数据进行ETL后，使用窗口聚合函数统计计算
        /*
            def reduceByKeyAndWindow(
                reduceFunc: (V, V) => V, // 聚合函数
                windowDuration: Duration, // 窗口大小
                slideDuration: Duration // 滑动大小
            ): DStream[(K, V)]
        */
        val resultDStream: DStream[(String, Int)] = etlDStream.reduceByKeyAndWindow(
            (tmp: Int, value: Int) => tmp + value, //
            Seconds(WINDOW_INTERVAL), //
            Seconds(SLIDER_INTERVAL) //
        )
        // 5. 将结果数据输出 -> 将每批次的数据处理以后输出
        resultDStream.print()
        
        // 6.启动流式应用，一直运行，直到程序手动关闭或异常终止
        ssc.start()
        ssc.awaitTermination()
        ssc.stop(stopSparkContext = true, stopGracefully = true)
    }
}复制代码
```

实现效果如下：

![image-20230216154806379](https://atstudy-1253850831.cos.ap-shanghai.myqcloud.com/lab-d3/etl-bigdata/06_spark/lab4_2/image-20230216154806379.png)





